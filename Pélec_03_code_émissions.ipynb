{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import pandas as pd\n","\n","pd.options.plotting.backend = 'plotly'\n","import numpy as np\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from sklearn import metrics\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n","from sklearn.pipeline import make_pipeline\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, \\\n","                             GradientBoostingRegressor\n","\n","from Pélec_04_fonctions import reg_modelGrid, visuRMSEGrid\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 17] File exists: './Figures/'\n","[Errno 17] File exists: './Tableaux/'\n"]}],"source":["write_data = True\n","\n","if write_data is True:\n","    try:\n","        os.mkdir(\"./Figures/\")\n","    except OSError as error:\n","        print(error)\n","    try:\n","        os.mkdir(\"./Tableaux/\")\n","    except OSError as error:\n","        print(error)\n","else:\n","    print(\"\"\"Visualisation uniquement dans le notebook\n","    pas de création de figures ni de tableaux\"\"\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["BEB = pd.read_csv('BEB.csv')\n","\n","BEBM = BEB.drop(columns=['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'])\n","SiteEnergyUse = np.array(BEB['SiteEnergyUse(kBtu)']).reshape(-1, 1)\n","TotalGHGEmissions = np.array(BEB.TotalGHGEmissions).reshape(-1, 1)\n","\n","BEBM_train, BEBM_test, TotalGHGEmissions_train, TotalGHGEmissions_test = train_test_split(\n","    BEBM, TotalGHGEmissions, test_size=.2)\n","\n","score = 'neg_root_mean_squared_error'\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Scaler moins sensible aux outlier d'après la doc\n","scaler = RobustScaler(quantile_range=(10, 90))\n"]},{"cell_type":"markdown","metadata":{},"source":[" # 1. Modèle de prédiction sur les émissions (TotalGHGEmissions)\n"," ## 1.1 Émissions brutes"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.1.1 Modèle LinearRegression"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["r2 : -0.3095599261157267\n","rmse : 583.6520545460625\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predLR=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[39.4375,89.34375,216.2265625,36.765625,20.59375,73.6171875,427.3828125,39.4609375,99.96875,-41.453125,328.515625,79.890625,-6.5859375,105.6484375,-74.3828125,1162.0859375,21.2109375,25.078125,-96.1484375,29.46875,13.0625,12.7109375,94.5546875,-78.984375,1.8125,-1.046875,-9.125,-110.84375,107.78125,7.8984375,5.1484375,96.8203125,26.265625,7.21875,41.8125,0.859375,37.421875,11.8046875,-72.921875,-1.3515625,1.4296875,9.671875,4,57.703125,95.421875,170.3671875,23.40625,68.7578125,115.3671875,14.1484375,218.1171875,42.890625,41.71875,-1.4609375,146.1328125,26.3984375,234.15625,19.9140625,17.921875,133.90625,133.203125,82.0546875,161.1328125,41.546875,104.65625,43.0703125,58.5078125,141.3515625,-12.7421875,33.890625,6.0234375,160.65625,-6.4765625,49.328125,-34.8203125,31.8046875,-26.7578125,35.15625,27.828125,753.9296875,232.1640625,55.9140625,29.9453125,11.7734375,166.9765625,12.34375,-3.453125,31.015625,19.6875,33.3515625,467.3515625,-55.9453125,21.7734375,80.671875,39.484375,1463.8125,-14.734375,-30.84375,-43.2421875,-11.3046875,8.234375,4.90625,-17.2421875,14.9765625,92.71875,208.5234375,288.25,58.1484375,129.3828125,51.046875,455.6953125,74.4296875,59.703125,155.28125,94.25,1.25,7.0546875,904.8828125,8.3125,12.515625,59.09375,61.6328125,61.8359375,37.5,146.640625,35.109375,-2.6640625,47.9921875,51.203125,1.671875,36.125,24.65625,14.1796875,65.7578125,20.984375,19.5859375,0.4296875,29.0546875,246.6171875,97.1953125,189.515625,40.1328125,25.203125,789.6953125,-5.9921875,34.671875,153.8125,242.3125,12.9375,77.2578125,110.125,85.6875,-15.2421875,39.0625,-3.1015625,121.71875,-20.1171875,9.5859375,178.6171875,3.1328125,81.4765625,147.4453125,146.6796875,-2.609375,21.7890625,-18.34375,86.171875,42.21875,72.75,375.5546875,25.6640625,99.109375,22.5859375,12.03125,1194.359375,86.203125,72.1328125,-2.390625,181.28125,1235.34375,1326.1171875,223.453125,124.859375,48.4375,17.7265625,56.7265625,201.0703125,67.6875,19.7890625,35.5703125,-7.53125,-3.1015625,50.46875,-3.5546875,-69.390625,96.71875,103.2890625,27.5,385.953125,14.4921875,15.3125,-3.7421875,-1.6015625,15.4765625,98.40625,50.3359375,651.03125,1844.0234375,192.875,-2.1015625,26.859375,282.7890625,127.8828125,-31.34375,137.640625,-24.40625,144.4375,117.8984375,61.3984375,0.140625,-21.4375,-26.0234375,52.5,-18.9609375,-18.125,60.9140625,-55.4453125,138.328125,17.0703125,-3.0546875,15.8984375,14.1484375,99.328125,31.1640625,-21.3203125,10.0234375,12.9296875,65.1953125,107.4296875,50.609375,33.0625,44.4140625,43.5234375,-8.921875,51.3828125,181.7265625,24.9765625,-48.078125,10.9921875,446.9140625,37.2421875,439.671875,72.3203125,-2.7421875,-69.2578125,47.2265625,29.8125,4338.609375,32.0078125,18.2109375,-90.4140625,145.890625,21,154.59375,94.96875,-23.25,75.296875,12.828125,9.0546875,82.765625,4.671875,26.078125,123.1953125,208.1484375,823.1640625,53.5390625,127.484375,93.1171875,126.7734375,63.28125,80.7421875,10.5859375,39.859375,22.59375,76.2421875,9.34375,114.7109375,215.46875,-31.3359375,17.4375,-15.7890625,16.5703125,2.359375,12.609375,-11.1328125,44.3828125,18.328125,-13.671875,4.7734375,98,95.203125,154.34375,10.0625,676.953125,21.2890625,-20.6328125,8,-7.1328125,7.6328125,95.625,-54.7734375,10.171875,22.21875,153.21875,12.6015625,-219.9609375,263.296875,14.9765625,26.8203125,11.25,133.1640625,6.953125,274.2265625,380.9140625,-14.109375,-1.9140625,35.6015625,39.0546875,596.6484375,124.390625,67.1875,230.9921875,29.8828125,-34.2421875,61.4609375,54.671875,249.4375,64.578125,15.765625,24.71875,95.640625,54.328125,-51.2890625,66.6015625,59.515625,248.4609375,6.6484375,49.3984375,78.8046875,167.75,15.078125,50.8125,66.265625,121.0390625,-26.0703125,186.0078125,61.90625,95.6015625,146.9453125,142.6328125,51.5859375,-5.125,-1.7734375,242.296875,51.0859375,10.3671875,27.2578125,106.7265625,-10.8671875,-75.5859375,259.3203125,-5.15625,-17.7421875,-5.9609375,501.9296875,-0.21875,72.6484375,737.1875,8.6796875,58.640625,46.0390625,1005.6328125,180.0234375,-5.1640625,40.265625,-24.84375,100.2890625,98.5,153.375,14.15625,61.8359375,499.3046875,14.6015625,18.84375,18.6796875,-12.390625,105.859375,65.828125,340.84375,1261.7734375,62.1328125,200.5390625,89.6953125,135.40625,78.296875,11.0546875,7.71875,26.046875,189.2265625,62.1328125,-70.90625,75.390625,14.796875,-19.7421875,206.4375,32.2109375,10.0078125,15.4921875,62.6171875,69.203125,2208.8125,84.5390625,-72.125,16.3671875,329.125,-94.6328125,245.2421875,35.359375,99.453125,65.2734375,48.140625,56.0703125,38.1953125,12.046875,-50.2421875,246.0625,17.25,49.8671875,-24.703125,-52.859375,-66.1015625,86.1796875,0.359375,-0.2890625,-134.1875,-6.6875,78.1640625,-13.8203125,9,217.1640625,584.7421875,95.421875,378.4921875,28.2421875,25.3203125,15.9921875,37,-54.1875,39.703125,-27.46875,20.328125,28.359375,294.3359375,43.5625,-60.296875,42.046875,13.3671875,11.75,23.9375,394.8515625,19.75,6.6171875,-7.6171875,214.703125,14.0625,98.734375,478.484375,12.328125,49.7421875,42.0546875,100.109375,33.28125,-44.046875,22.0078125,80.828125,-31.7734375,290.15625,211.21875,13.859375,-28.984375,30.7109375,-1.15625,17.5390625,-23.1484375,87.8046875,57.140625,2.359375,75.2421875,35.84375,-14.1875,-22.15625,195.296875,50.7734375,177.921875,61.03125,1092.5625,5.921875,27.078125,33.40625,17.5703125,17.390625,25.828125,30.8359375,161.1953125,124.4609375,-27.8984375,9.8046875,37.265625,208.140625,74.7734375,59.3984375,35.5234375,128.3828125,-36.546875,6.890625,6.8515625,5.125,220.5625,51.0625,280.71875,23.59375,0.6875,263.2265625,22.734375,267.78125,108.375,1009.5625,-170.375,84.328125,52.5703125,189.2890625,264.25,1385.71875,1046.2421875,-29.421875,552.1875,69.7265625,91.65625,1.4609375,128.984375,152.1484375,105.03125,8.515625,4.421875,209.0078125,26.3828125,-26.8203125,354.75,-32.515625,-24.1796875,149.140625,19.0234375,50.7578125,52.6015625,59.9375,-12.4921875,0.265625,35.8671875,-30.265625,6.9375,118.625,3.6875,152.71875,-6.3203125,2.96875,312.0703125,7.65625,44.2890625,93.1875,58.1484375,14.3203125,56.796875,102.390625,-29.8515625,57.7109375,60.59375,99.8515625,91.75,183.4453125,-24.2109375,13.8046875,76.1015625,255.390625,-36.109375,33.7734375,-130.8671875,34.4375,40.65625,1.75,48.6171875,147.15625,-4.359375,129.1953125,17.9296875,65.21875,57.375,29.4453125,-25.5703125,-65.890625,1566.890625,97.953125,8.21875,178.9296875,59.265625,490.1640625,121.6015625,12.8359375,144.515625,19.5078125,65.65625,-11.6953125,571.5234375,27.1171875,134.375,14.0859375,83.125,21.6171875,21.28125,68.390625,-3.375,70.015625,-0.640625,83.1796875,89.2734375,74.4765625,4.25,108.6640625,159.8046875,39.125,107.9296875,32.25,5.5703125,297.7109375,-33.9140625,31.1171875,44.7890625,107.6875,106.3359375,128.9453125,826.8671875,364.2734375,-20.734375,-39.953125,12.71875,211.0390625,101.96875,-0.3828125,57.765625,26.25,11.484375,34.5078125,17.796875,15.4453125,56.7421875,-35.546875,88.5,202.90625,22.8828125,49.3515625,6.78125,348.65625,28.0390625,490.1640625,25.6015625,32.828125,1261.7734375,653.953125,56.546875,68.3203125,4.9453125,-14.9296875,7.859375,64.6953125,296.609375,61.921875,40.671875,11.296875,14.5390625,38.859375,11.1796875,362.9140625,51.703125,15.0390625,-2.7734375,264.625,80.6875,-1.8046875,42.2265625,122.21875,10.6484375,10.7578125,50.03125,809.609375,29052.6328125,69.9765625,288.0390625,39.296875,93.6796875,37.1953125,3.6328125,301.3359375,293.3125,195.8046875,91.375,154.9296875,0.953125,99.046875,22.859375,38.9609375,4,38.1328125,11.828125,137.6640625,32.7578125,40.3125,32.2265625,138.9375,37.8984375,14.75,26.1171875,-14.7734375,272.015625,468.3984375,189.7734375,143.390625,76.9921875,96.5546875,363.9765625,27.0078125,-21.5859375,265.1640625,169.296875,-3.0625,35.90625,419.015625,36.2109375,-9.8828125,51.5390625,56.015625,459.984375,318.7109375,22.5234375,-53.625,148.2578125,81.921875,-15.421875,-47.9375,40.3984375,37.0234375,-0.1328125,27.59375,363.1328125,70.0234375,633.8515625,32.2734375,231.7890625,75.828125,-2.3046875,102.828125,100.1640625,-48.671875,69.5078125,818.8203125,8.1875,125.2109375,159.34375,48.3828125,15.6015625,455.1953125,289.0546875,47.046875,83.953125,9.4453125,-120.078125,20.2109375,499.3984375,113.671875,51.5078125,-50.421875,203.3125,17.828125,186.9296875,501.8359375,2.109375,4.609375,287.3671875,338.5859375,32.625,82.6875,244.3203125,20.5390625,-23.7734375,13.1875,230.578125,47.9609375,80.8515625,-117.7578125,18.21875,44.8671875,-15.109375,-14.3984375,25.078125,2.2578125,83.5625,42.3046875,-4.78125,202.75,496.875,23.2421875,230.3984375,118.375,-40.109375,7.78125,-66.2265625,47.203125,51.078125,-35.5078125,-16.4765625,41.7890625,65.3984375,244.9375,10.9296875,13.65625,24.8203125,-19.46875,19.9375,51.8984375,263.625,-2.046875,99.265625,431.125,22.015625,53.4375,-25.6171875,-34.6484375,106.328125,344.5625,220.484375,-13.0234375,80.2421875,42.15625,158.421875,52.9609375,15.8515625,21.109375,-60.6484375,-50.8046875,388.4765625,34.390625,28.0390625,13.609375,41.2109375,6.828125,16.3203125,26.1171875,22.859375,9.84375,105.9140625,42.5234375,-38,68.4375,766.1953125,0.78125,26.1484375,91.3203125,287.3671875,56.125,45.4375,15.78125,39.59375,44.25,15.0625,68.3515625,28.265625,40.484375,276.375,11.296875,22.1953125,-1.4765625,64.3671875,52.671875,684.78125,26.765625,77.75,79.9453125,45.5859375,1.71875,4340.5859375,17.0703125,-16.0625,663.015625,27.0625,0.28125,138.2421875,31.0703125,788.5703125,117.5,116.0390625,250.9765625,2.4453125,289.78125,10.0703125,226.640625,39.4453125,-9.765625,679.1484375,826.9140625,9.1171875,-0.4140625,27.15625,125.5234375,148.1328125,501.8359375,37.375,71.140625,29.6953125,25.75,184.9765625,35.84375,50.265625,-0.671875,-7.9921875,27.03125,150.1875,180.2265625,168.4375,46.8203125,53.75,168.015625,313.46875,18.34375,41.8359375,55.609375,-39.25,45.2734375,291.0390625,249.984375,0.4140625,26.5859375,93.8828125,288.21875,-3.03125,-17.140625,56.0390625,154.84375,163.4921875,405.5703125,-8.765625,59.4375,146.234375,-112.15625,-2.5078125,-39.8125,559.9609375,84.796875,56.90625,21.6484375,96.2421875,72.234375,97.1328125,-12.03125,50.2109375,25.2265625,126.0234375,138.3125,58.515625,495.96875,338.015625,110,101.1015625,154.0078125,-6.171875,-135.1640625,60.2421875,28.25,55.25,16.34375,17.203125,14.125,23.421875,-14.9609375,26.5,169.078125,109.953125,154.421875,173.6640625,2881.40625,56.2421875,-13.1328125,132.8515625,-38.34375,-0.0703125,55.8984375,78.515625,11.9375,68.4609375,-59.2578125,87.4609375,175.71875,58.5625,190.15625,122.15625,25.5234375,27.46875,-21.265625,232.1640625,82.640625,49.109375,-0.890625,12.9375,111.984375,19.953125,46.625,-363.578125,42.5078125,82.265625,112.0078125,36.8984375,75.2421875,165.3203125,12.71875,172.4140625,83.9140625,24.4921875,53.8046875,29.5078125,32.2421875,96.453125,-19,-0.03125,103.828125,-8.421875,86.2578125,0.015625,545.9375,48.1953125,9.125,68.875,48.6015625,-0.03125,196.7890625,23.125,251.4765625,60.859375,234.4140625,144.7265625,15.953125,22.1640625,326.046875,54.734375,208.078125,-16.703125,37.6640625,54.84375,33.2578125,15.6484375,-16.4296875,111.609375,20.734375,15.640625,14.859375,27.703125,13.9296875,11.3046875,179.78125,130.890625,109.828125,-131.2578125,58.1796875,73.078125,-0.578125,14.75,60.7578125,26.734375,136.734375,-14.375,90.4921875,67.640625,137.7890625,67.6875,196.796875,-25.1796875,102.421875,303.3515625,-2.984375,44.7734375,-149.9453125,125.8046875,491.5703125,40.703125,78.9140625,40.296875,6.046875,232.9921875,23.8515625,1.4375,3.296875,-1.296875,-6.078125,-61.765625,3.7578125,66.7890625,78.171875,67.9609375,138.9140625,34.21875,372.734375,0.546875,-18.1640625,111.65625,30.8828125,75.859375,1324.90625,320.703125,273.7890625,39.1796875,72.125,-8.328125,15.2109375,197.53125,73.7109375,234.9921875,271.515625,-16.8203125,8.234375,24.3125,49.7890625,48.5234375,14.25,20.1640625,102.515625,20.578125,83.5625,22.03125,178.390625,-17.9140625,-6.53125,133.8984375,22.203125,95.875,419.53125,-6.15625,10.4609375,26.0546875,133.34375,58.8671875,51.8828125,21.3984375,12.2734375,106.6640625,91.578125,-6.9375,681.609375,11.8203125,127.421875,75.7578125,873.609375,14.59375,145.1484375,74.7890625,164.28125,6.390625,1217.9296875,-24.84375,70.4140625,7.359375,188.25,158.7265625,-12.65625,125.25,282.4296875,227.2265625,8.515625,114.9296875,160.2109375,274.1015625,112.0859375,538.0078125,13.9609375,15.0078125,38.9140625,79.5078125,144.2890625,86.8828125,206.71875,192.375,166.140625,11.2578125,160.03125,16.265625,5.0703125,161.6328125,1.0078125,12.96875,2.4375,26.5703125,-6.1875,47.25,-2.234375,14.2578125,143.8046875,-90.4140625,37.0390625,66.9296875,119.4453125,17.421875,63.3984375,-5.8203125,22.4453125,26.1484375,114.1640625,79.546875,-50.828125,5.0546875,144.859375,80.375,51.21875,46.609375,29.609375,125,52.1796875,83.5,9.375,1.6015625,59.84375,-60.4375,-1.2578125,23.5703125,75.453125,55.34375,103.78125,-7.359375,10.4375,167.5625,109.1015625,12.546875,-12.78125,62.3515625,154.921875,52.46875,50.8203125,1142.4296875,-16.3359375,33.9296875,625.65625,207.2578125,167.5703125,117.234375,37.859375,-19.8359375,64.3828125,41.6484375,43.9765625,28.7265625,-39.796875,30.8359375,29.7578125,14.203125,34.4375,107.75,14.1640625,152.4921875,60.78125,621.703125,104.3671875,29.5546875,-8.546875,33.7421875,7.6328125,578.59375,257.8125,64.8359375,-5.5234375,2207.375,53.7109375,-0.8984375,26.3671875,80.7421875,245.609375,41.609375,298.703125,9.328125,-27.8125,-8.953125,74.0234375,173.2109375,-41.1953125,3.4375,30.3828125,129.59375,440.6484375,196.859375,48.3828125,35.984375,45.8828125,-37.7265625,36.7578125,1385.71875,137.9453125],"xaxis":"x","y":[74.33,21.29,69.48,25.53,5.45,4.54,191.97,66.44,13.95,108.04,265.21,72.16,29.57,16.01,133.5,324.25,281.25,53.02,33.09,263.51,4.56,37.34,233.43,20.85,9.83,46.7,6.07,49.22,466.95,3.88,45.83,11.84,1936.34,193.94,9.76,3.29,10.7,111.95,21.24,107.56,14.24,17.04,5.81,26.94,2.82,27.77,7.63,48.1,52.43,33.04,50.29,6.25,3.34,43.19,64.23,77.76,84.75,4.41,16.8,282.72,82.72,60.57,89.29,51.1,8.14,35.4,25.32,64.59,1.6,79.74,159.98,42.8,5.62,13.29,4.53,6.79,3.15,75.03,6.05,500.93,96.43,58.79,39.48,4.99,26.46,65.02,5.94,6.18,102.27,15.2,95.77,6.17,3.95,61.98,33.52,1148.5,37.82,79.98,138.68,34.48,4,3.35,4.39,4.78,105.01,126.03,56.48,25.9,570.98,18.33,387.49,16.44,8.65,13.87,29.78,1.08,12.29,1638.46,89.57,54.25,12.72,24.39,192.12,7.44,193.34,45.44,17.43,38.39,60.5,122.73,10.65,10.37,25.22,23.58,4.86,116.21,25.21,6.3,331.61,11.8,15.26,7.9,23.85,138.79,4.6,47.26,304.62,23.84,4.89,30.79,12.25,13.23,59.73,17.22,14.35,29.41,10.66,25.14,46.46,27.47,69.02,52.87,76.68,4.75,22.46,5.07,8.65,5.99,45.33,52.49,54.24,10.84,20.22,3.4,2937.83,170.01,30.26,82.58,229.2,569.61,2061.48,145.67,15.1,8.49,53.31,250.87,20.29,119.98,2.93,22.37,0.17,17.02,120.1,1.55,4.13,51.93,8.42,5.43,602.12,4.6,61.33,16.01,4.43,42.92,72.78,47.12,97.39,321.72,39.27,3.63,12.99,350.87,96.2,13.36,69.18,4.82,60.98,17.23,1.73,5.53,7.06,9.57,380.17,10.65,18.88,9.45,70.31,41.8,157.77,1.63,56.07,64.13,73.9,45.79,106.37,22.97,61.15,53.88,227.18,1.97,127.74,1266.06,29.93,33.28,22.38,249.46,6.12,4.44,4.55,4.2,5.28,293.56,30.19,515.71,3.52,5.67,46.79,11824.89,51.83,48.95,188.77,64.56,38.97,13.39,13.56,34.61,53.87,9.04,16.99,280.16,117.64,3.75,30.57,8.86,467.9,56.07,17.24,272.54,40.65,22.78,32.62,6.66,6.7,103.42,114.33,185.33,239.17,67.17,19.95,15.9,5.49,4.68,23.62,38.88,2.99,5.53,61.88,6.38,27.92,23.15,89.29,14.83,73.14,118.2,42.8,3.7,12.68,66.43,16.89,8.25,29.33,21.3,7.48,65.46,10.21,190.61,175.3,49.58,5.64,3.14,229.84,4.2,62.41,147.14,5.82,30.35,6.41,134.8,175.46,67.57,17.12,309.6,4.5,73.61,28.19,6.89,9.76,60.12,4.92,132.1,72.5,30.69,73.31,105.97,23.12,35.6,16.14,103.99,3.48,367.3,3.54,5.8,24.56,100.02,93.21,17.49,30.86,41.9,54.04,40.78,32.89,4.38,4.32,193.47,35.11,58.59,84.93,0,33.45,6.71,458.44,12.28,168.36,75,398.19,5.46,39.41,166.84,3.59,7.07,3.06,262.98,144.57,5.22,358.88,4.73,105.32,23.5,16.83,4.64,1.06,266.05,10.83,11.81,21.44,78.36,11.17,27.27,127.27,295.42,457.51,198.59,61.18,110.74,14.97,4.11,197.3,5.96,25.11,9.95,505.01,71.87,22.66,17.92,35.43,44.15,93.63,109.9,278.47,80.09,452.16,33.17,300.14,7.36,126.21,178.21,119.2,77.75,35.48,46.26,8.42,6.22,5.28,84.95,6.41,31.27,29.66,48.95,72.5,3.21,141.15,31.13,4.25,3.49,52,3.96,125.59,14.7,19.57,143.33,804.2,9.25,375.56,14.37,16.18,78.29,29.97,3.93,22.93,32.05,126.92,5.74,105.41,7.69,9.79,34.66,4.08,5.59,26.16,143.82,6,11.87,20.37,422.39,4.55,11.29,181.67,9.43,5.71,21.78,2.15,7.57,229.12,4.08,41.16,24.25,28.3,155.54,110.62,40.56,18.3,79.21,10.9,4.85,113.97,8.36,91.53,9.28,47.5,3.63,20.06,168.62,49.28,51.4,11.27,15.5,53.65,35.2,8.06,4,27.93,286.43,91.09,85.28,49.89,16.72,18.74,19.11,119.46,28.94,8.75,6.29,94.82,35.72,33.37,18.76,19.69,402.04,328.23,152.28,5.23,86.9,158.55,49.63,23.42,10.01,607.41,235.69,12.05,7.21,62.23,39.5,1265.29,290.4,45.33,463.12,70.84,93.83,53.71,9.53,184.09,28.37,13.1,48.99,18.47,5.15,6.37,31.15,35.8,17.14,379.34,161.39,127.56,6.9,84.59,4.33,351.9,27.99,124.54,30.1,59.14,21.57,25.12,6.01,7.97,78.92,13.71,1.55,72.9,70.16,4.04,7.47,10.02,75.99,77.04,11.18,107.16,38.13,39.57,227.13,4.43,98.88,104.61,20.02,5.63,130.28,138.35,8.08,1.08,6.96,16.46,5.94,64.79,9.67,45.38,30.19,5.79,32.42,191.97,636.96,30.02,1.73,85.59,8.61,199.48,70.17,3.69,41.74,33.12,39.73,4.35,180.14,20.5,74.98,23.63,23.9,5.79,334.15,132.32,10.29,9.39,4.14,553.27,28.63,249.98,41.96,147.36,128.91,0.78,209.92,7.34,22.15,180.89,4.12,9.43,1.59,549.74,25.93,523.95,128.22,124.77,10.48,5.53,63.77,1623.34,39.1,232.12,9.56,6.22,4.2,36.42,13.27,16.47,4.24,48.6,109.16,47.14,11.1,19.61,6.16,95.26,6.15,24.12,40.46,28.6,192.51,159.16,28.55,44.26,4.02,47.25,60.88,75.95,115.34,7.13,38.38,216.18,5.69,60.08,4.86,122.92,11.59,13.18,2.99,51.47,117.19,13.34,63.41,178.86,4.09,100.71,13.12,1874.42,11140.56,8.69,1060.13,64.06,13.71,75.58,27.38,158.48,213.9,20.09,39.57,53.48,6.04,54.76,4.19,4.39,3.44,21.71,9.16,9.82,3.65,6.08,34.03,28.21,27.63,3.83,45.07,13.21,218.49,1789.69,44.74,14.67,94.23,5.77,31.76,18.42,134.38,69.44,40.35,14.73,25.15,752.57,17.39,56.86,36.97,33.35,177.51,7.21,113.3,6.35,150.23,8.37,3.16,21.51,38.59,6.51,5.42,5.6,248.15,477.85,691.57,23.79,174.79,68.05,64.86,191.2,50.52,5.46,6.72,1699.45,5.07,35.8,376.46,21.12,58.88,348.21,29.53,0.92,9.67,13.18,92.21,4.8,426.39,20.66,5.95,48.34,71.03,59.84,234.75,723.4,133.7,158.6,250.05,69.42,25.39,2.79,298.61,94.02,39.67,5.67,1597.56,11.38,35.14,1019.53,10.04,2.69,63.37,4.31,6.73,20.76,86.81,28.05,49.6,110.41,27.33,8.64,61.86,59.76,60.67,1.78,11.97,14.49,43.15,7.52,12.62,6.26,225.35,675.34,5.54,3.78,17.11,84.83,119.01,11.54,393.2,22.74,40.25,46.52,34.03,5.84,3.54,51.89,125.13,106.74,21.38,7.39,33.58,3.27,18.91,15.7,54.02,6.86,22.81,17.08,1894.2,20.33,44.93,22.96,13.29,14.24,23.22,11.27,4.46,26.63,2.72,5.68,0.09,9.56,181.93,40.69,3.47,101.34,259.49,9.86,6.87,10.24,171.64,5.87,49.53,469.57,83.68,9.95,285.62,3.43,12.36,36.08,6.43,14.09,377.09,27.14,23.03,4.99,32.77,238.77,3243.48,93.31,5.94,455.39,67.24,29.62,9.96,5.91,234.15,39.44,314.92,154.96,5.46,86.99,20.09,260.13,6.67,39.57,147.82,188.13,5.03,9.73,5.79,45.6,21.92,740.97,59.05,120.84,47.92,108.79,84.64,6.35,15.43,9.72,12.71,26.21,47.24,22.17,68.3,46.6,6.85,2.85,84.18,12.92,15.34,18.1,4.84,12.75,34.14,47.71,129.42,45.02,7.85,202.65,4.45,4.31,148.25,15.09,123.18,2452.86,38.86,15.9,21.77,301.02,5.45,17.75,47.99,16.3,6.74,11.94,176.79,4.74,51.98,4.16,5.9,64.57,46.78,313.83,179.33,1088.2,237.36,319.73,161.2,19.39,85.73,273.91,42.18,4.62,6.33,67.45,15.37,47.64,4.89,24.89,5.6,4.19,254.16,136.94,221.51,834.96,1.14,8.16,202.99,9.71,71.55,59.07,9.36,138.61,18.9,481.97,42.99,134.01,54.41,21.44,50.46,85.95,33.92,7.69,98.95,99.96,78.7,0,15.6,51.39,3.98,25.48,58.28,62.17,88.51,9.44,24.51,36.78,88.74,5.65,215,32.48,92.45,22.53,9.11,25.38,45.32,128.39,9.46,69.59,3.28,82.07,8.45,94.59,65.35,86.86,103.77,1.85,3.02,434.48,5,133.8,8.8,587.16,214.65,15.11,3.5,433.6,75.48,19.07,4.26,4.19,41.96,3.76,5.97,10.23,61.62,4.82,28.61,86.9,36.72,16.89,9.62,103.39,303.69,37.51,225.66,7.6,92.19,3.67,38.85,9.29,23.78,14.86,3.41,176.75,59.74,6.32,111.04,30.03,17.57,14.04,399.73,3.77,13.09,87.07,154.6,28.88,6.3,60.52,6.73,13.35,65.05,30.78,103.4,231.39,4.92,0,565.79,103.07,9.57,21.05,18.86,99.39,5.91,206.01,6.03,9.15,10.6,8.63,106.73,509.36,5.44,193.72,37.74,125.42,38.56,3.05,132.71,148.7,130.73,218.53,4.52,4.51,8.28,16.32,68.14,3.56,78.06,66.23,70.39,15.43,178.65,355.05,0.92,3.04,284.01,61.07,6.98,67.78,18.65,11.62,7.73,319.88,7.49,38.67,7.85,5.04,11.39,33.8,19.7,359.09,3.3,55.69,117.11,1990.5,4.51,44.96,0,95.54,14.3,481.06,31.47,2.36,62.23,205.52,54.4,10.95,28.78,107.84,222.02,124.96,54.53,117.04,21.08,27.02,142.16,4.15,51.24,6.18,3,210.3,1.24,138.76,39.75,21.51,71.23,46.3,49.46,9.4,85.76,7.74,8.16,101.31,5.35,4.13,47.08,4.48,8.36,74.07,235.32,12.63,7.74,86.46,15.97,16.58,3.99,7.56,48.35,14.65,33.43,35.39,44.55,37.49,7.78,25.12,82.9,6.31,1.67,4.15,10.38,14.3,137.15,1.68,120.91,130.69,79.58,9.73,34.2,38.78,62.98,8.47,25.2,12.05,77.34,0.31,53.12,127.14,243.13,6.09,567.6,4.43,52.67,139.72,80.75,1185.15,63.33,30.95,4.65,303.81,6.25,8.85,23.95,5.17,41.22,3.81,1.86,72.5,51.28,41.3,37.56,22.1,36.09,49.17,6.71,3.5,67.06,4.41,394.6,38.41,271.68,138.03,1129.67,2.34,12.26,20.86,87.14,238.61,43.97,107.83,21.66,5.88,17.45,16.63,103.57,23.8,48.16,45.58,10.97,10.29,28.09,26.56,40.05,9.63,5.02,5.82,1596.08,293.55],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données d'émissions prédites par le modèle de régression linéaire<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predLR"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle régression linéaire\n","pipeLR = make_pipeline(scaler, LinearRegression())\n","\n","pipeLR.fit(BEBM_train, TotalGHGEmissions_train)\n","\n","TotalGHGEmissions_predLR = pipeLR.predict(BEBM_test)\n","\n","LRr2 = metrics.r2_score(TotalGHGEmissions_test, TotalGHGEmissions_predLR)\n","print(\"r2 :\", LRr2)\n","LRrmse = metrics.mean_squared_error(TotalGHGEmissions_test,\n","                                    TotalGHGEmissions_predLR,\n","                                    squared=False)\n","print(\"rmse :\", LRrmse)\n","\n","fig = px.scatter(\n","    x=TotalGHGEmissions_predLR.squeeze(),\n","    y=TotalGHGEmissions_test.squeeze(),\n","    labels={\n","        'x': f'{TotalGHGEmissions_predLR=}'.partition('=')[0],\n","        'y': f'{TotalGHGEmissions_test=}'.partition('=')[0]\n","    },\n","    title=\n","    \"Visualisation des données d'émissions prédites par le modèle de régression linéaire<br>vs les données test\"\n",")\n","fig.show()\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.1.2 Modèle Ridge"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre    Ridge()\n","0  ridge__alpha  44.120929\n","         Ridge()\n","R²     -0.228390\n","RMSE  565.274608\n","MAE   121.669174\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predRidge=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[40.19197020458446,93.48629960767609,210.0492770049503,37.006429771755606,24.029253325407044,73.00966184412974,413.0316058592465,38.62392565525174,99.38600193539541,-43.11165929017355,317.23444074187466,78.07755044577041,4.912175683139225,107.72934534936209,-60.993802390246714,1238.417867459667,20.062972961923066,25.355456640779686,-61.60768885339776,30.673263240394476,13.57479739820397,13.904143010402251,92.91476253667352,-63.91896942532236,3.669319159166818,3.2150752983327067,-8.030045399299773,-95.5353993841214,128.22237087262937,9.17424732071482,9.057711707144279,93.36817632096731,24.8103085013572,19.54060699945491,48.78290275271525,1.6856544681274386,38.70358361179554,12.119405860343328,-67.45066509890725,-0.06516212087707629,2.7255135743553325,13.946467424511937,7.2291897491547985,53.96177391230541,93.84413651896091,167.20020658983043,23.667042603864928,68.68152680420681,112.87520741816132,14.125889689829009,233.16123695178297,42.71048450370039,41.755264099005565,6.668602283775087,161.61143131258,18.808711395870716,227.07954602759125,22.36226909027704,18.94357116472054,157.9470156721678,129.38669758813774,79.45188892088224,156.2446405824815,42.06194689393304,101.06776163215525,43.54477468967681,58.84419395434735,136.80431022189265,-4.752213876191128,31.980883648338576,8.886971270567415,156.8101198572771,-3.491477941267675,49.50818390446673,-30.83462173720774,32.422589833852065,-23.670542984416386,35.88737062996106,28.57631416133549,857.1960183419746,244.77104765960883,54.79516888571345,30.8849992886069,12.608181817643434,165.9914924345865,16.429316852985284,0.7216536270381511,30.238037082499147,21.330010829202344,35.89531762576138,442.2866097210839,-50.285820159368214,22.13903473682135,78.410531006485,39.86993333609843,1436.7531670572093,-8.9992893690888,-19.263319319689657,-31.379115017329653,-12.018791584744172,9.96971976527189,6.057594485134082,-16.15162921772366,15.750251502931498,90.38025925223363,202.3254214876328,268.58256310655315,58.37921010933503,121.73541793108453,51.64272604164654,440.7771651861714,74.09841982803339,60.05170695811995,152.87737818712026,88.0106316395492,2.1262602083957063,11.251662986450995,1014.5005586646096,11.311116119722271,12.55594332633359,65.46606448749321,61.34946317417635,62.12360279958656,38.177024290046695,146.715355724319,35.78105483256958,-1.7872128168737902,47.783301383050755,54.07294574533769,6.547587441646279,39.48493399413914,27.25072373957243,15.341619238027484,65.77923269459322,23.500292725611374,21.012791795387038,2.3702654441276962,30.014948621645757,235.43957360939615,94.27585370511471,183.9466545466928,39.26280397864073,25.52619136513364,764.6602981216539,-5.7240197631113645,34.873377448422175,144.5411076181348,234.46485645432608,13.936175878306003,75.67538596529225,106.81396797052798,82.37955811041107,-10.47186000567936,41.18742536612739,0.4369607841439418,119.14867493974272,-15.933646305107544,13.938397110935057,167.86170215296875,3.847376381318618,84.3018571304592,141.58019912469854,143.24705514654016,0.2610806228392022,23.43275893613609,-15.441895477630304,84.64949049745911,42.74830884532994,70.7222723371967,365.4772328571995,26.67034580656391,96.86643755442354,23.59770757368598,13.177143538959612,1162.8714295885027,86.16296739742359,70.07491415277585,-2.674885039240536,174.3146996032613,1225.228790767752,1331.2966284146958,227.44399715724072,121.97096216882431,54.027108560059645,17.82800398269404,56.70724945420392,195.52537271822763,66.59682319799371,20.346320131104846,37.868433181503214,2.517498092090122,-2.6948902010212095,48.571328902451754,2.557704643813409,-64.33225151055078,97.8199735589808,100.41713950461565,28.1240240094561,361.55287475694274,15.64444986921783,16.547777452011843,-3.0160705859759958,0.8338625126161077,16.65792888707891,93.14910476309987,52.37708929832041,623.5759922927267,1858.6337340198309,191.06936924253208,-1.2058207023830647,32.80823570839502,275.1247888254294,118.44113335604735,-24.694175226255794,136.57024317919468,-23.1092895860275,141.50326750169768,119.23358917522378,60.66477405235407,5.9073676752489845,-12.65909708474954,-19.640507926435674,53.499289506572836,-23.358657758819163,-16.345244667198465,60.09238095728345,-52.29922705580435,141.4662342398147,31.831943892500703,3.465382027960814,16.947150599979558,15.026675085469961,101.58765022713455,32.39327761539908,-6.742786318816258,14.846074495082462,14.140968595796451,69.74049612154256,114.18925646489402,51.164891047070185,33.490017415457004,46.40407223171841,42.86026089619682,-1.8891465316178255,51.43583401814039,176.47046918602766,25.28385820798609,-43.80180515151555,13.918722027754939,433.9963449960219,36.82946982627494,409.06983317380275,77.40369026067992,-0.987760765253789,-61.639079822506034,46.72483504379055,28.664210799956198,4211.8347859852975,32.607856684790114,18.746370536755705,-60.7241226679092,141.45472590695562,25.07434798850037,149.3591633281735,88.52283015138656,-17.190090890597403,71.48805106618244,16.027480526405853,8.145110199354363,84.88111447865059,10.316750397058712,27.72945912999695,118.58784814307566,193.3995074334798,793.2721359603668,58.758258613486845,118.09304438862017,95.9627188342013,124.53951573111974,63.07400240108035,81.30855350776484,11.297036445833378,39.3354149740507,22.956909142194313,75.86067256359134,15.511074290287372,112.54851946145318,206.47643855031347,-23.302747471807507,21.142973121261203,-10.069654122788876,18.479277920192878,6.544131171506443,13.832544577608672,-8.658977242739049,43.68364287332314,19.389684476742154,-5.376774037108703,15.016759850200984,101.81979682990891,92.94437719793157,152.10674142672337,28.01393876235329,628.3520591726578,31.134149650491317,-17.31578113934313,13.229326348948781,-2.298850043260373,4.0784770371793115,92.22832165419445,-49.36900119145983,10.549747910637635,19.466504303221445,148.8854527648456,14.167857493955552,-195.6655562404721,270.1274049584849,15.750251502931498,24.79335494240611,15.663089993270916,139.4903147635716,7.545710813695365,266.4386789022868,373.5572574055767,-6.284451421256115,3.1946521536182004,35.54171059860664,39.19424323369626,553.1423123478274,120.58871202686669,62.17790270252852,216.2522655445539,30.02587748762329,-27.348395124828798,56.92239476135374,51.17314097026144,225.9452887453082,64.10324905085301,19.32379543694656,23.76522572534842,93.85205458207898,54.842532788851635,-36.916901123142864,62.27084583497319,60.44439531237727,238.16644882379043,7.43347486085603,45.15898689778864,80.4811850140894,161.58394749538508,16.217323146810656,50.67819960754234,59.737008084121456,147.63264732797612,-12.520392399808898,180.17020154715271,71.42341496983846,91.92311630149075,140.67252174055113,139.4089469025573,52.09291688751022,-4.2105443883492555,-0.24857517245546035,202.44045477305454,51.4277433332439,11.212012378168382,35.30403110526734,104.43806081679267,-11.11111420059676,-63.7427058667153,304.1969947241523,0.8524604466003254,-5.273783154680281,-3.4941855322285846,584.8606035485808,0.8595187051244437,50.87492779838797,709.9022817345192,9.764190275140862,54.56683154510121,46.258716569988266,966.6807864005727,175.20578430389273,-4.759315858485884,58.66971791084336,-24.016966970174906,101.14525590391449,102.72747421405631,149.73215249242799,14.595402657987606,61.572451436501474,479.74580374508923,14.986440252475688,19.915657579187396,20.738695422496015,-7.246724079099735,102.71620741712867,58.829330699974044,329.61641985113465,1183.8119476685602,67.51073405979878,197.24598457613686,93.64361696883475,133.5176089849495,77.849076192067,12.337238846573243,20.448284383602314,27.17564971272344,187.7903951994734,61.607975752071994,-69.58217320426579,73.18944597929715,25.961128291971505,-16.803258856705895,214.84105891799058,32.87047890569077,11.019916634905414,15.359599274575377,61.55702351382679,68.26762225699578,2079.8964462773397,79.65294502448117,-36.36601612212294,18.442490417480098,318.5534973379947,-47.56782387993891,250.3768824284965,35.82207728369825,97.54976447852232,65.45254879700106,47.30762508702962,55.45865922877866,37.71618887856482,12.798193043134113,-41.458870579320106,231.92443340228544,17.07876520985957,51.71697646804676,-21.986506632943218,-47.71394406605857,-31.08767213183725,84.75079225966262,1.7341506587517372,3.92390129921057,-105.81470191801546,-5.545827911848924,76.80842468711825,-8.458191908397666,10.028553719301826,214.09978320869678,600.441247917951,93.84413651896091,365.53989990689354,30.905226597092856,26.042831154792953,16.705815202138304,42.684068532750786,-24.384804076364162,39.1812029649568,-19.73685252368972,21.366902736195996,28.737774965443034,333.31298780121926,42.56459324477604,-52.79788698079771,42.96962427808045,14.048411831145316,12.591659102383375,24.74855179275493,387.6075595931918,21.309491025145427,10.34398560230359,6.261134607798368,249.07171290204724,15.80288227068905,96.04137374657643,511.962987624781,16.321593249527474,48.85800653559858,42.552790048085896,95.8589749239026,32.94663174016085,-33.7455192965523,21.805120491310092,85.0315331302719,-24.210424855954926,294.92126411215975,212.82247928791102,18.458241654210383,-24.988064939450226,32.27761353654378,-2.56867022114335,12.74809731478274,-19.342824118547313,100.45449088913918,56.44636845656356,19.353908738432438,78.62870468477956,37.58895981796347,-12.196344711689207,-15.273707539506049,195.42058050174418,53.28476668246782,169.67664509649924,59.51685300691009,976.631375467277,6.576378185105185,25.667079203515627,33.556900249627176,18.53050328321904,17.33991461441786,23.902631117209793,37.390488592299675,158.02732379761198,120.41687376430183,-25.381311417197765,14.944667145314376,40.60173389050813,215.60070282932332,72.92884888362924,58.97811023984578,36.260389319879735,119.34881074019475,-22.75401132335523,14.459347796279502,9.59799080079398,8.858899962897297,222.51631306405045,48.478428843288654,269.7969359149523,24.467665633333915,2.5643404965609875,256.3045456147449,29.11882170239793,257.9361615344246,108.25890653873184,981.4684987559211,-138.55678581300887,91.46318810436381,51.91847520720272,179.70419174101534,255.9870489376825,1331.6809585248193,1052.6234638018186,-25.89574232359763,539.3193142143616,68.90826211114181,88.86950276109404,6.067044141457181,128.11912410273607,141.0477078838932,101.97543901630266,10.375140526234176,13.05687933829035,209.93291688283344,26.442361212968045,-27.85521851529134,370.8322719929597,-24.977962196165123,-12.300722166522988,133.94559984677664,17.78894506236111,50.66167689228227,53.45534735291888,56.874378686763805,-9.389848840560596,4.824754306821717,36.28966840190681,-22.31545329566424,9.629087676828014,116.25003012075695,5.900965611049784,148.3973633965694,-4.71681283765632,3.479629290496831,305.4025575990381,13.705978229060392,44.39205974317215,93.31156300775065,60.86913518188452,14.705554093054673,56.73656725657639,95.20991110478721,-22.264921354829454,59.96145779773711,58.609175622762685,100.23757851976708,89.81083167145636,225.74239057233578,-21.016582256012157,14.748542711925616,75.39312564217583,265.7826351908159,-34.66077647871634,33.77102627990355,-123.2411762148138,41.407307665947016,40.11749016302687,3.0339375925431114,48.06868255160876,143.08873349444121,-2.929481977873465,126.36634772251223,21.69325292402042,65.38952305066636,57.6170113561236,29.69609245052651,-20.05742159804859,-24.77371174401236,1497.9552635055522,97.02123914819586,10.99336495271259,174.42884189625434,59.144029573972546,471.8099542091336,127.31139137727946,13.596790947370351,140.4471835706039,25.69141518916322,66.29720043481376,-9.342759976643194,554.8670914029966,27.442826335300587,130.76509597061494,13.855362917805508,81.97256560460784,22.158311237958085,22.40964604494347,68.31546948701242,-2.3403804835738526,68.22503931175355,1.4077900682857774,99.23777552295286,79.96485112171868,69.73461812338337,6.598398568922541,112.76584557897573,180.83401010267536,41.00943567062637,115.09693384904142,32.6809587978467,3.555438474449332,298.6191827233604,-27.985050558572723,30.77777656639934,45.29973712731957,106.11944200202956,103.40775096668648,120.82774054693712,804.4793106577117,352.3410041999823,-5.202009958631244,-36.42616227440503,14.968604192472228,214.9121979252939,99.49151951789545,5.398236562158225,58.338562733770715,28.277770621373985,12.028874220924962,36.86018112667068,17.095556332037553,16.245932960733306,55.92247987189059,-21.11003049572399,69.32101548093678,195.51160205865787,24.144813553615222,48.78466687954469,8.84876297067619,336.1502005664114,27.954017945746074,474.15200086637617,32.24159900167783,33.553417803106576,1184.7196250527077,624.2650072561241,54.71603775593375,67.7738494200594,6.78059916329142,-9.933542404193133,9.708722909062473,68.83281873739514,287.14338234640223,60.583626798487785,41.20276570040994,13.36509710717499,16.07050721453657,38.683780283894706,11.736972917997235,356.02660695869076,50.89647735244372,15.8941176366231,-0.8844688844323798,264.3585542997114,78.97717523142757,3.162810612391908,41.275821454491364,119.67717858363572,11.970985324975238,11.923698245207135,50.07546379506213,775.2926510390812,28295.754312131157,68.82738975810182,278.3557677119545,39.59145766804211,87.76229091030439,37.36297562570629,4.75505376546603,291.8244750422406,272.7838349395695,168.48109491745467,88.90315428730895,153.38934555492762,1.6853111736154744,91.95183832246018,23.751681305397984,38.79393171896177,4.701050003088383,38.53400389139828,12.829153955881992,133.93723375814028,34.860954769869615,40.06618079398639,32.806969841136194,126.82669295656247,37.95194250880313,15.988758919046482,26.280363369533816,-12.608095112571561,256.2825352401526,478.4556293340195,175.37717887846927,139.3291465046732,75.41928387876132,95.41272251351072,364.5666709249599,27.32165975672682,-11.599670349806495,256.29243526802117,164.0583751474757,1.2557219041690644,42.487926303671884,403.26860910478354,36.467995215786374,-4.719044679616033,51.24259567248491,55.465189474187596,471.4875019604707,308.23051873286363,23.67152091223975,-44.88503615002554,140.1746026460311,77.18676229269545,-12.674016304367605,-40.705519828473385,40.5463520991445,37.09276131528001,1.13933290938958,27.660032125046083,328.70429526211353,71.43151188469812,660.9844116780027,32.19276367805586,225.0273614045863,74.09712336344455,1.554464183456325,103.19570687413983,97.45406590131155,-47.24766323912891,67.88115937168092,786.2488463669513,8.776653100569831,122.22166070203296,160.00915351312221,49.038968350275816,21.449646385653637,439.86948780202397,279.05972433376735,47.68685948482758,82.92879857191906,12.101373525607912,-103.81815167326818,23.143754154789924,510.2786536689523,111.42139392682697,50.54056970624802,-43.53804048543067,201.21933252247945,18.48200709259475,181.78043263219732,463.22154690970524,7.455264825793684,9.422309174743546,328.95616644921677,327.4953354579892,32.76644028949394,82.08439887039569,241.22340685547312,23.331720242631118,-17.931500605619235,12.585348920384988,255.04746838818036,47.49038751750666,82.70065378429568,-64.8719020113696,20.499179582537643,45.20126279334444,-9.019406638045936,-12.902607161725008,25.537875776744762,3.073562549972465,81.75299139511763,45.42173118513382,-1.3617333075743332,199.47107884540074,496.17418343103566,23.283597098902465,223.46933274326815,117.27496541313984,-31.879086493775077,10.085687568565177,-56.32949743394691,46.21127501360035,49.956423805880604,-32.10549258181278,-13.193494091198374,42.075372528804465,65.11180394982108,221.77970377132308,12.94193828743694,19.31290943008741,25.135153770645545,-14.839849779094507,26.20734500703531,57.87431957693998,258.97390999873346,0.5994033925450708,96.14765762060546,413.39505865749686,22.9117766130116,53.25023561635834,-21.95007282168615,-26.785677411611402,102.98540916493859,315.46896829689956,213.6297125113905,-8.668912699264936,80.40087612361744,42.66294148315297,166.28881358795542,52.27646737117069,20.14055625059268,22.033318918351746,-22.63797265164267,-42.87105526180741,372.0154915240839,35.08109969655773,28.246931811290175,9.243135803269695,41.320165930490646,7.308541933789208,18.301554033159135,26.829744647418075,23.751681305397984,11.11018339480924,104.37179603238319,43.76504854335079,-37.84474823020561,67.32933024118971,739.9139927606964,5.207127156299336,26.951580494096255,87.94297049277502,328.04848906506936,54.31949258969232,44.68877471831012,10.193097050059976,42.74446012893449,43.88843559081359,23.878213189086352,62.80755017895274,37.5695861349985,37.5933573889842,277.99622874892896,12.109746573964962,24.978488892051875,4.102329537765613,63.5982936728048,44.63148141207418,661.995905282103,26.69003022835149,76.63966594356853,81.56695762342142,45.862171403746835,26.266443511127406,4323.474783697907,28.914020363939244,-12.895890850580898,679.6648985994675,37.51264509703029,4.299449772151931,134.81186571176758,30.84824690213324,746.2041317823282,114.08175178065918,104.74483224958803,248.54226894474027,10.48155152854467,268.3921790983991,11.096353371347945,215.46049982999833,40.94279116424059,-7.656728507002988,653.8259267372142,797.4330470274856,10.111167295602115,2.070699829362347,26.752354740898674,99.68006239793041,149.11610099992117,462.31386952555783,37.27384191503626,69.93991925227918,28.88552623270671,28.148970316703625,188.54448379728055,35.145894093009126,50.49591970709462,4.731135289907741,-1.1766697772911385,27.29412189796005,139.47232881652232,175.24606462228638,163.88082942596637,47.51039651718581,52.7133067629459,163.74427657345828,299.92983516945003,19.007980195039988,41.20798937019082,55.50669864287477,-32.497141633779705,45.87211459209766,282.96119729816576,242.64011806153513,3.109034742264811,26.729252498729373,91.69691219579704,279.3801166655387,-0.5108515747059101,-16.243516714270356,56.509811474939575,153.0144188108708,167.96957385714418,388.9074127355643,-1.7337496296937474,67.62262352262157,142.28187423257498,-41.41855138449654,0.45474775434490056,-34.67524404376792,540.5764231663474,82.87890159740039,54.706038556451304,21.760753509100276,94.29749862599618,72.07438720867606,99.60799755980871,-9.435278865761845,50.9966151953499,26.71772099553806,100.58773978207782,151.40354812588592,61.05532227728372,476.96005736982386,327.3895051237565,110.22580631993566,99.44325247434897,149.54018453539024,-2.8154985511552724,-127.38389703452026,59.75192407119192,29.126155162622815,53.343672893649966,18.11893701723013,18.36252234474176,14.910346810545121,23.929664824765588,-7.95600348505787,28.987582996318057,160.6630009475387,107.74073435457746,146.96934677482056,166.85643488660452,2762.448602889502,56.06219429095717,-3.6967628432607142,130.01650914073946,-32.021141064219165,1.059276946449927,55.926550403663015,76.57210759847246,12.257095154177165,77.47323138990106,-40.34455466458243,90.37144870455263,179.4075225215211,58.52474813281051,209.30814125874045,121.82966639909162,26.395315747007572,27.679651920694788,-18.72227480516949,245.67872504375626,89.69755957403126,47.268378499937114,2.0182276516118733,16.688994274834556,110.09248575396956,21.366010156659087,52.62123639419299,-334.4951270095097,42.55661578911846,80.97259803686225,105.43115383608588,39.01302455714725,74.55178193506866,171.99388376489364,14.990463910134316,166.99817443090387,82.07226664109986,30.354466435914624,53.56692099217615,29.68440803891539,27.68638795536326,93.76364953205288,-13.395638118576855,2.992574401174032,114.56089890317102,-5.488764997196881,86.25352298986797,3.192810494229356,530.8231851574959,52.591964610663496,11.487017456655316,67.74790569444457,48.52791912260117,1.383725606137233,187.13991055574783,25.356244742794647,239.95755027827974,59.96424832404667,226.27120244099862,136.65964012123405,16.59290998119456,22.57964047708962,319.77724978858555,54.483931689109475,201.68277681575077,-15.335839330122944,37.70410177990223,54.75689978541243,35.76863215401703,20.928791916107812,-12.703316745974604,107.64719570426382,24.528348489310307,20.353498619896172,23.283795222873408,31.734383100994975,14.062909421172634,13.10638001317151,165.2473396719216,127.90432674829663,105.32979257891553,-138.1625318701687,59.70295551040907,72.1301915868535,0.830908891201787,14.165528159253597,59.449067017297004,27.007728166785682,132.0313300295076,-12.660815222071534,89.06356423705023,66.419911633782,134.01984733444058,65.79553090042032,183.66913944963565,-20.975497427615636,99.18243086924812,289.19141076645707,-1.9878958913592228,44.96443720795025,-130.41107098807257,123.5558525999607,462.149602185546,41.1366748393697,78.09590435835152,40.243092358198105,8.830543852642407,225.43176951072223,27.082556432682868,8.002850737060193,14.505564916290197,0.9564726275595348,-0.9121450174841925,-35.26358471639617,5.099185972030597,65.60479101428571,75.87163342318021,67.20366865347896,137.6564665449384,36.837860317655284,362.76707987990005,4.170424997607995,-14.287031224595893,109.29803956435835,34.59454365522919,86.74527832339834,1235.0058365347477,311.02065727668764,260.043500251967,41.850109372016576,70.04737629400908,-0.8260722455463352,16.887915015252744,199.8452405572148,73.26050840723745,246.3424181948558,255.3748578560052,-15.60087204238831,9.334599678379305,26.631038082248676,49.69234426369209,48.65451028269696,15.081081534899077,40.642118366880325,109.4170382800838,25.234981213462888,82.88024298875526,38.808799365483566,220.11157964920497,-13.27966527502619,-2.810478806853453,134.80851912862863,23.085265123242227,94.75181390310833,404.4337768884867,-3.0222702266011225,11.16760850962958,25.855289813206497,128.02313318169902,58.30000964607892,52.3435114022878,23.535182337347873,13.515859201790846,101.26863345418116,90.1402945148308,-6.445546565054812,656.8618952379007,12.641227248163567,125.51302591615163,80.71293633139041,846.1348569316165,15.503080042135018,146.50296678984805,79.49620440868617,159.6405625309096,6.4008645496417955,1373.097763399523,-15.981225292092766,70.13977244531243,8.801045524915068,187.99303941066566,155.3586159803354,-2.142715475952322,122.70448177463669,274.5438833401816,221.3670144663918,14.155732140770994,111.96753003401392,150.26080291570227,265.86386565492796,109.99104852105216,513.4381980338397,14.62395307937075,17.294969954644856,35.8558493454738,80.65928023927401,135.75196273708664,86.06086567631678,205.67724439707,190.16169185838467,184.1100795682633,12.167576077375166,148.34297420075632,18.671074185816863,15.281826782238689,157.1523179666289,3.0886213635545374,13.955452379442733,1.655807582155461,27.22904359953619,-4.638150527701512,46.85832368823253,1.1687580069866144,18.155238560616823,143.68953724333014,-61.631800052056604,39.08810114265278,67.8388015166056,116.97869166892151,18.035893780573133,59.09821418027566,-4.998796417183186,22.562105199213175,25.821575114581965,120.10741256488414,78.68246075008365,-46.17568684835133,12.342960163317905,141.44057161144025,78.68090222950768,50.75980062122626,47.42549697004363,30.568313912789094,122.34582034256431,52.29293069389123,82.02112118777164,10.89619372005994,15.690562589679274,58.57887068026718,-59.57940029685946,-1.913098914017496,23.594774902966922,73.31887391550094,55.056719151458225,101.03943244797387,-4.065543078810599,12.954383572237148,163.00163406951646,106.69228885691783,13.705870427281525,-0.33729226444443583,61.330915333683535,147.87702415896797,52.83093150245957,50.74429046858258,1055.6089678603093,-10.709500830121137,38.43558787093727,588.0808181356383,201.34651594140624,156.7330417416962,115.44643672679084,41.651563744141725,-16.96516202272251,63.11326451400865,41.93492944909395,44.347923656638244,31.771947740598,-34.665031901647566,37.037835705145824,30.755571352403095,21.87674781645307,35.47187388013343,103.9647443744191,15.081750803003487,148.88472410420167,65.83085861667803,596.9607194948658,97.96860999241885,30.922626005793163,-6.182759184036797,34.693109418751675,8.36731412338051,557.5029323876622,250.1975784622918,64.02094189815605,-4.193806864139141,2052.8542583164985,53.58929457370154,6.938910337285385,28.832877100353485,79.46905051049785,240.2138067013133,42.103253682083206,289.60802070602784,9.947012098157408,-21.580256396366295,-4.19799952179288,73.28611337403078,173.15876161185997,-25.202639361268467,4.315519980353649,31.842119027213144,125.99864798210977,426.3076334549779,200.92377408032382,48.43223152007417,43.28812749144741,48.79714996359783,-33.57720471041375,38.54757671686305,1332.5886359089668,135.94360479146405],"xaxis":"x","y":[74.33,21.29,69.48,25.53,5.45,4.54,191.97,66.44,13.95,108.04,265.21,72.16,29.57,16.01,133.5,324.25,281.25,53.02,33.09,263.51,4.56,37.34,233.43,20.85,9.83,46.7,6.07,49.22,466.95,3.88,45.83,11.84,1936.34,193.94,9.76,3.29,10.7,111.95,21.24,107.56,14.24,17.04,5.81,26.94,2.82,27.77,7.63,48.1,52.43,33.04,50.29,6.25,3.34,43.19,64.23,77.76,84.75,4.41,16.8,282.72,82.72,60.57,89.29,51.1,8.14,35.4,25.32,64.59,1.6,79.74,159.98,42.8,5.62,13.29,4.53,6.79,3.15,75.03,6.05,500.93,96.43,58.79,39.48,4.99,26.46,65.02,5.94,6.18,102.27,15.2,95.77,6.17,3.95,61.98,33.52,1148.5,37.82,79.98,138.68,34.48,4,3.35,4.39,4.78,105.01,126.03,56.48,25.9,570.98,18.33,387.49,16.44,8.65,13.87,29.78,1.08,12.29,1638.46,89.57,54.25,12.72,24.39,192.12,7.44,193.34,45.44,17.43,38.39,60.5,122.73,10.65,10.37,25.22,23.58,4.86,116.21,25.21,6.3,331.61,11.8,15.26,7.9,23.85,138.79,4.6,47.26,304.62,23.84,4.89,30.79,12.25,13.23,59.73,17.22,14.35,29.41,10.66,25.14,46.46,27.47,69.02,52.87,76.68,4.75,22.46,5.07,8.65,5.99,45.33,52.49,54.24,10.84,20.22,3.4,2937.83,170.01,30.26,82.58,229.2,569.61,2061.48,145.67,15.1,8.49,53.31,250.87,20.29,119.98,2.93,22.37,0.17,17.02,120.1,1.55,4.13,51.93,8.42,5.43,602.12,4.6,61.33,16.01,4.43,42.92,72.78,47.12,97.39,321.72,39.27,3.63,12.99,350.87,96.2,13.36,69.18,4.82,60.98,17.23,1.73,5.53,7.06,9.57,380.17,10.65,18.88,9.45,70.31,41.8,157.77,1.63,56.07,64.13,73.9,45.79,106.37,22.97,61.15,53.88,227.18,1.97,127.74,1266.06,29.93,33.28,22.38,249.46,6.12,4.44,4.55,4.2,5.28,293.56,30.19,515.71,3.52,5.67,46.79,11824.89,51.83,48.95,188.77,64.56,38.97,13.39,13.56,34.61,53.87,9.04,16.99,280.16,117.64,3.75,30.57,8.86,467.9,56.07,17.24,272.54,40.65,22.78,32.62,6.66,6.7,103.42,114.33,185.33,239.17,67.17,19.95,15.9,5.49,4.68,23.62,38.88,2.99,5.53,61.88,6.38,27.92,23.15,89.29,14.83,73.14,118.2,42.8,3.7,12.68,66.43,16.89,8.25,29.33,21.3,7.48,65.46,10.21,190.61,175.3,49.58,5.64,3.14,229.84,4.2,62.41,147.14,5.82,30.35,6.41,134.8,175.46,67.57,17.12,309.6,4.5,73.61,28.19,6.89,9.76,60.12,4.92,132.1,72.5,30.69,73.31,105.97,23.12,35.6,16.14,103.99,3.48,367.3,3.54,5.8,24.56,100.02,93.21,17.49,30.86,41.9,54.04,40.78,32.89,4.38,4.32,193.47,35.11,58.59,84.93,0,33.45,6.71,458.44,12.28,168.36,75,398.19,5.46,39.41,166.84,3.59,7.07,3.06,262.98,144.57,5.22,358.88,4.73,105.32,23.5,16.83,4.64,1.06,266.05,10.83,11.81,21.44,78.36,11.17,27.27,127.27,295.42,457.51,198.59,61.18,110.74,14.97,4.11,197.3,5.96,25.11,9.95,505.01,71.87,22.66,17.92,35.43,44.15,93.63,109.9,278.47,80.09,452.16,33.17,300.14,7.36,126.21,178.21,119.2,77.75,35.48,46.26,8.42,6.22,5.28,84.95,6.41,31.27,29.66,48.95,72.5,3.21,141.15,31.13,4.25,3.49,52,3.96,125.59,14.7,19.57,143.33,804.2,9.25,375.56,14.37,16.18,78.29,29.97,3.93,22.93,32.05,126.92,5.74,105.41,7.69,9.79,34.66,4.08,5.59,26.16,143.82,6,11.87,20.37,422.39,4.55,11.29,181.67,9.43,5.71,21.78,2.15,7.57,229.12,4.08,41.16,24.25,28.3,155.54,110.62,40.56,18.3,79.21,10.9,4.85,113.97,8.36,91.53,9.28,47.5,3.63,20.06,168.62,49.28,51.4,11.27,15.5,53.65,35.2,8.06,4,27.93,286.43,91.09,85.28,49.89,16.72,18.74,19.11,119.46,28.94,8.75,6.29,94.82,35.72,33.37,18.76,19.69,402.04,328.23,152.28,5.23,86.9,158.55,49.63,23.42,10.01,607.41,235.69,12.05,7.21,62.23,39.5,1265.29,290.4,45.33,463.12,70.84,93.83,53.71,9.53,184.09,28.37,13.1,48.99,18.47,5.15,6.37,31.15,35.8,17.14,379.34,161.39,127.56,6.9,84.59,4.33,351.9,27.99,124.54,30.1,59.14,21.57,25.12,6.01,7.97,78.92,13.71,1.55,72.9,70.16,4.04,7.47,10.02,75.99,77.04,11.18,107.16,38.13,39.57,227.13,4.43,98.88,104.61,20.02,5.63,130.28,138.35,8.08,1.08,6.96,16.46,5.94,64.79,9.67,45.38,30.19,5.79,32.42,191.97,636.96,30.02,1.73,85.59,8.61,199.48,70.17,3.69,41.74,33.12,39.73,4.35,180.14,20.5,74.98,23.63,23.9,5.79,334.15,132.32,10.29,9.39,4.14,553.27,28.63,249.98,41.96,147.36,128.91,0.78,209.92,7.34,22.15,180.89,4.12,9.43,1.59,549.74,25.93,523.95,128.22,124.77,10.48,5.53,63.77,1623.34,39.1,232.12,9.56,6.22,4.2,36.42,13.27,16.47,4.24,48.6,109.16,47.14,11.1,19.61,6.16,95.26,6.15,24.12,40.46,28.6,192.51,159.16,28.55,44.26,4.02,47.25,60.88,75.95,115.34,7.13,38.38,216.18,5.69,60.08,4.86,122.92,11.59,13.18,2.99,51.47,117.19,13.34,63.41,178.86,4.09,100.71,13.12,1874.42,11140.56,8.69,1060.13,64.06,13.71,75.58,27.38,158.48,213.9,20.09,39.57,53.48,6.04,54.76,4.19,4.39,3.44,21.71,9.16,9.82,3.65,6.08,34.03,28.21,27.63,3.83,45.07,13.21,218.49,1789.69,44.74,14.67,94.23,5.77,31.76,18.42,134.38,69.44,40.35,14.73,25.15,752.57,17.39,56.86,36.97,33.35,177.51,7.21,113.3,6.35,150.23,8.37,3.16,21.51,38.59,6.51,5.42,5.6,248.15,477.85,691.57,23.79,174.79,68.05,64.86,191.2,50.52,5.46,6.72,1699.45,5.07,35.8,376.46,21.12,58.88,348.21,29.53,0.92,9.67,13.18,92.21,4.8,426.39,20.66,5.95,48.34,71.03,59.84,234.75,723.4,133.7,158.6,250.05,69.42,25.39,2.79,298.61,94.02,39.67,5.67,1597.56,11.38,35.14,1019.53,10.04,2.69,63.37,4.31,6.73,20.76,86.81,28.05,49.6,110.41,27.33,8.64,61.86,59.76,60.67,1.78,11.97,14.49,43.15,7.52,12.62,6.26,225.35,675.34,5.54,3.78,17.11,84.83,119.01,11.54,393.2,22.74,40.25,46.52,34.03,5.84,3.54,51.89,125.13,106.74,21.38,7.39,33.58,3.27,18.91,15.7,54.02,6.86,22.81,17.08,1894.2,20.33,44.93,22.96,13.29,14.24,23.22,11.27,4.46,26.63,2.72,5.68,0.09,9.56,181.93,40.69,3.47,101.34,259.49,9.86,6.87,10.24,171.64,5.87,49.53,469.57,83.68,9.95,285.62,3.43,12.36,36.08,6.43,14.09,377.09,27.14,23.03,4.99,32.77,238.77,3243.48,93.31,5.94,455.39,67.24,29.62,9.96,5.91,234.15,39.44,314.92,154.96,5.46,86.99,20.09,260.13,6.67,39.57,147.82,188.13,5.03,9.73,5.79,45.6,21.92,740.97,59.05,120.84,47.92,108.79,84.64,6.35,15.43,9.72,12.71,26.21,47.24,22.17,68.3,46.6,6.85,2.85,84.18,12.92,15.34,18.1,4.84,12.75,34.14,47.71,129.42,45.02,7.85,202.65,4.45,4.31,148.25,15.09,123.18,2452.86,38.86,15.9,21.77,301.02,5.45,17.75,47.99,16.3,6.74,11.94,176.79,4.74,51.98,4.16,5.9,64.57,46.78,313.83,179.33,1088.2,237.36,319.73,161.2,19.39,85.73,273.91,42.18,4.62,6.33,67.45,15.37,47.64,4.89,24.89,5.6,4.19,254.16,136.94,221.51,834.96,1.14,8.16,202.99,9.71,71.55,59.07,9.36,138.61,18.9,481.97,42.99,134.01,54.41,21.44,50.46,85.95,33.92,7.69,98.95,99.96,78.7,0,15.6,51.39,3.98,25.48,58.28,62.17,88.51,9.44,24.51,36.78,88.74,5.65,215,32.48,92.45,22.53,9.11,25.38,45.32,128.39,9.46,69.59,3.28,82.07,8.45,94.59,65.35,86.86,103.77,1.85,3.02,434.48,5,133.8,8.8,587.16,214.65,15.11,3.5,433.6,75.48,19.07,4.26,4.19,41.96,3.76,5.97,10.23,61.62,4.82,28.61,86.9,36.72,16.89,9.62,103.39,303.69,37.51,225.66,7.6,92.19,3.67,38.85,9.29,23.78,14.86,3.41,176.75,59.74,6.32,111.04,30.03,17.57,14.04,399.73,3.77,13.09,87.07,154.6,28.88,6.3,60.52,6.73,13.35,65.05,30.78,103.4,231.39,4.92,0,565.79,103.07,9.57,21.05,18.86,99.39,5.91,206.01,6.03,9.15,10.6,8.63,106.73,509.36,5.44,193.72,37.74,125.42,38.56,3.05,132.71,148.7,130.73,218.53,4.52,4.51,8.28,16.32,68.14,3.56,78.06,66.23,70.39,15.43,178.65,355.05,0.92,3.04,284.01,61.07,6.98,67.78,18.65,11.62,7.73,319.88,7.49,38.67,7.85,5.04,11.39,33.8,19.7,359.09,3.3,55.69,117.11,1990.5,4.51,44.96,0,95.54,14.3,481.06,31.47,2.36,62.23,205.52,54.4,10.95,28.78,107.84,222.02,124.96,54.53,117.04,21.08,27.02,142.16,4.15,51.24,6.18,3,210.3,1.24,138.76,39.75,21.51,71.23,46.3,49.46,9.4,85.76,7.74,8.16,101.31,5.35,4.13,47.08,4.48,8.36,74.07,235.32,12.63,7.74,86.46,15.97,16.58,3.99,7.56,48.35,14.65,33.43,35.39,44.55,37.49,7.78,25.12,82.9,6.31,1.67,4.15,10.38,14.3,137.15,1.68,120.91,130.69,79.58,9.73,34.2,38.78,62.98,8.47,25.2,12.05,77.34,0.31,53.12,127.14,243.13,6.09,567.6,4.43,52.67,139.72,80.75,1185.15,63.33,30.95,4.65,303.81,6.25,8.85,23.95,5.17,41.22,3.81,1.86,72.5,51.28,41.3,37.56,22.1,36.09,49.17,6.71,3.5,67.06,4.41,394.6,38.41,271.68,138.03,1129.67,2.34,12.26,20.86,87.14,238.61,43.97,107.83,21.66,5.88,17.45,16.63,103.57,23.8,48.16,45.58,10.97,10.29,28.09,26.56,40.05,9.63,5.02,5.82,1596.08,293.55],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Ridge()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predRidge"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression ridge\n","# réglage des paramètre pour la gridsearch\n","alphasridge = np.logspace(-3, 5, 1000)\n","param_gridRidge = {'ridge__alpha': alphasridge}\n","\n","GridRidge, \\\n","BestParametresRidge, \\\n","ScoresRidge, \\\n","TotalGHGEmissions_predRidge, \\\n","figRidge = reg_modelGrid(model=Ridge(),\n","                            scaler=scaler,\n","                            X_train=BEBM_train,\n","                            X_test=BEBM_test,\n","                            y_train=TotalGHGEmissions_train,\n","                            y_test=TotalGHGEmissions_test,\n","                            y_test_name='TotalGHGEmissions_test',\n","                            y_pred_name='TotalGHGEmissions_predRidge',\n","                            score=score,\n","                            param_grid=param_gridRidge)\n","\n","print(BestParametresRidge)\n","print(ScoresRidge)\n","figRidge.show()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[386.03905759131175,386.0390571224373,386.03905664483744,386.0390561583497,386.0390556628088,386.0390551580461,386.0390546438901,386.03905412016604,386.0390535866957,386.0390530432979,386.0390524897878,386.0390519259772,386.03905135167463,386.0390507666846,386.03905017080837,386.0390495638434,386.0390489455832,386.0390483158177,386.0390476743328,386.0390470209103,386.0390463553282,386.03904567736015,386.0390449867758,386.03904428334033,386.0390435668145,386.03904283695476,386.0390420935131,386.03904133623683,386.0390405648683,386.03903977914536,386.03903897880093,386.039038163563,386.0390373331544,386.03903648729283,386.03903562569064,386.0390347480552,386.03903385408796,386.03903294348504,386.03903201593704,386.0390310711284,386.03903010873825,386.0390291284392,386.0390281298981,386.0390271127755,386.0390260767258,386.0390250213967,386.0390239464295,386.03902285145864,386.0390217361122,386.03902060001076,386.0390194427684,386.0390182639916,386.03901706327974,386.0390158402247,386.0390145944106,386.0390133254142,386.03901203280407,386.0390107161409,386.03900937497696,386.0390080088567,386.0390066173155,386.0390051998805,386.03900375607,386.0390022853931,386.03900078735006,386.0389992614317,386.03899770711934,386.0389961238846,386.03899451118957,386.03899286848616,386.03899119521583,386.03898949081014,386.0389877546896,386.0389859862642,386.0389841849331,386.03898235008387,386.03898048109306,386.0389785773255,386.0389766381341,386.03897466285986,386.03897265083145,386.03897060136524,386.0389685137646,386.0389663873202,386.03896422130936,386.03896201499583,386.03895976763016,386.0389574784484,386.0389551466727,386.03895277151094,386.0389503521556,386.03894788778484,386.0389453775611,386.0389428206316,386.03894021612723,386.03893756316324,386.0389348608381,386.03893210823355,386.0389293044143,386.0389264484278,386.0389235393036,386.0389205760533,386.03891755767006,386.0389144831283,386.0389113513836,386.03890816137186,386.0389049120093,386.0389016021919,386.0388982307954,386.0388947966743,386.03889129866195,386.03888773557003,386.0388841061881,386.03888040928325,386.0388766435996,386.0388728078582,386.0388689007558,386.03886492096535,386.0388608671349,386.0388567378876,386.0388525318209,386.03884824750605,386.03884388348797,386.03883943828436,386.0388349103856,386.0388302982538,386.03882560032247,386.03882081499626,386.03881594064995,386.0388109756281,386.038805918245,386.0388007667827,386.0387955194922,386.03879017459167,386.038784730266,386.0387791846667,386.0387735359107,386.03876778208013,386.03876192122135,386.03875595134434,386.03874987042246,386.0387436763911,386.03873736714763,386.03873094054995,386.0387243944166,386.0387177265252,386.03871093461254,386.03870401637295,386.03869696945833,386.03868979147677,386.03868247999196,386.0386750325224,386.0386674465406,386.03865971947187,386.0386518486942,386.03864383153643,386.03863566527804,386.038627347148,386.038618874324,386.0386102439311,386.03860145304105,386.03859249867145,386.0385833777844,386.0385740872856,386.0385646240235,386.038554984788,386.0385451663094,386.0385351652573,386.0385249782397,386.03851460180175,386.03850403242416,386.03849326652266,386.0384823004465,386.03847113047715,386.038459752827,386.0384481636387,386.0384363589827,386.0384243348572,386.038412087186,386.03839961181706,386.03838690452193,386.03837396099357,386.038360776845,386.03834734760795,386.03833366873147,386.0383197355804,386.0383055434335,386.0382910874822,386.03827636282887,386.03826136448504,386.03824608737017,386.0382305263093,386.03821467603177,386.0381985311695,386.03818208625484,386.03816533571904,386.0381482738903,386.0381308949917,386.0381131931398,386.0380951623418,386.0380767964944,386.03805808938137,386.0380390346712,386.038019625916,386.0379998565478,386.0379797198776,386.03795920909295,386.03793831725517,386.0379170372971,386.0378953620217,386.0378732840986,386.03785079606166,386.03782789030737,386.03780455909134,386.03778079452667,386.03775658858024,386.0377319330711,386.03770681966705,386.0376812398823,386.0376551850743,386.0376286464411,386.03760161501833,386.0375740816764,386.0375460371174,386.03751747187164,386.0374883762955,386.03745874056693,386.0374285546833,386.0373978084576,386.0373664915153,386.03733459329027,386.03730210302257,386.0372690097536,386.0372353023234,386.0372009693664,386.0371659993081,386.03713038036096,386.03709410052085,386.0370571475625,386.0370195090366,386.03698117226435,386.0369421243346,386.03690235209854,386.0368618421662,386.03682058090163,386.0367785544187,386.03673574857623,386.03669214897394,386.03664774094733,386.03660250956284,386.0365564396134,386.0365095156131,386.0364617217925,386.0364130420932,386.036363460163,386.03631295935,386.03626152269806,386.0362091329409,386.03615577249593,386.03610142345997,386.036046067602,386.0359896863585,386.03593226082666,386.0358737717587,386.03581419955566,386.0357535242611,386.03569172555467,386.03562878274585,386.0355646747669,386.03549938016636,386.0354328771027,386.03536514333666,386.0352961562243,386.03522589271046,386.03515432932073,386.0350814421543,386.03500720687646,386.03493159871107,386.0348545924322,386.0347761623572,386.0346962823375,386.03461492575104,386.03453206549426,386.0344476739727,386.03436172309324,386.0342741842551,386.03418502834086,386.03409422570724,386.0340017461767,386.03390755902706,386.0338116329828,386.03371393620534,386.03361443628285,386.0335131002206,386.0334098944311,386.03330478472327,386.0331977362927,386.03308871371064,386.03297768091295,386.03286460119006,386.03274943717537,386.0326321508338,386.0325127034509,386.0323910556207,386.03226716723447,386.0321409974684,386.0320125047714,386.0318816468531,386.03174838067133,386.0316126624192,386.03147444751227,386.03133369057554,386.03119034543033,386.0310443650806,386.0308957016995,386.03074430661536,386.03059013029787,386.0304331223435,386.0302732314617,386.0301104054594,386.02994459122687,386.0297757347226,386.0296037809574,386.02942867397985,386.02925035686013,386.02906877167436,386.02888385948813,386.0286955603408,386.02850381322844,386.02830855608755,386.0281097257779,386.0279072580655,386.02770108760535,386.02749114792357,386.02727737139986,386.02705968924954,386.0268380315053,386.0266123269988,386.0263825033421,386.0261484869089,386.02591020281506,386.02566757490024,386.0254205257075,386.0251689764644,386.0249128470627,386.02465205603875,386.0243865205527,386.0241161563684,386.02384087783287,386.0235605978553,386.0232752278861,386.0229846778957,386.0226888563531,386.0223876702049,386.0220810248526,386.021768824132,386.0214509702897,386.0211273639626,386.02079790415416,386.02046248821296,386.0201210118096,386.0197733689139,386.01941945177293,386.0190591508869,386.0186923549874,386.0183189510132,386.01793882408793,386.0175518574966,386.01715793266175,386.0167569291216,386.01634872450484,386.0159331945089,386.0155102128757,386.01507965136864,386.01464137974943,386.01419526575444,386.01374117507197,386.01327897131915,386.01280851601854,386.01232966857606,386.01184228625755,386.0113462241668,386.01084133522306,386.0103274701388,386.0098044773983,386.0092722032358,386.0087304916143,386.00817918420455,386.0076181203651,386.00704713712133,386.0064660691464,386.0058747487417,386.00527300581814,386.00466066787845,386.00403755999866,386.0034035048123,386.00275832249315,386.00210183074057,386.00143384476365,386.0007541772683,386.0000626384434,385.9993590359492,385.99864317490585,385.99791485788313,385.9971738848916,385.99642005337455,385.99565315820166,385.9948729916633,385.99407934346664,385.9932720007331,385.9924507479972,385.99161536720766,385.9907656377286,385.9899013363446,385.98902223726566,385.98812811213577,385.98721873004206,385.9862938575272,385.98535325860314,385.9843966947684,385.98342392502605,385.9824347059059,385.9814287914888,385.98040593343313,385.979365881005,385.9783083811115,385.9772331783363,385.9761400149799,385.97502863110174,385.9738987645673,385.97275015109864,385.97158252432763,385.9703956158558,385.96918915531535,385.9679628704377,385.96671648712356,385.96544972952046,385.96416232010347,385.96285397976214,385.9615244278919,385.9601733824918,385.95880056026755,385.9574056767409,385.95598844636487,385.9545485826457,385.95308579827145,385.9515998052473,385.9500903150383,385.9485570387196,385.9469996871338,385.9454179710573,385.94381160137357,385.94218028925616,385.9405237463599,385.93884168502115,385.9371338184678,385.9353998610388,385.93363952841366,385.9318525378527,385.9300386084473,385.9281974613818,385.92632882020666,385.9244324111234,385.92250796328136,385.92055520908696,385.91857388452655,385.91656372950104,385.91452448817574,385.91245590934307,385.9103577468002,385.90822975974106,385.9060717131646,385.903883378297,385.90166453303175,385.8994149623848,385.89713445896757,385.8948228234766,385.8924798652018,385.89010540255174,385.88769926359953,385.88526128664546,385.88279132080095,385.8802892265924,385.8777548765844,385.875188156025,385.8725889635117,385.8699572116803,385.86729282791407,385.86459575507797,385.861865952274,385.85910339562133,385.85630807906045,385.85348001518105,385.8506192360751,385.84772579421525,385.8447997633588,385.84184123947745,385.83885034171357,385.83582721336336,385.8327720228865,385.8296849649432,385.8265662614595,385.8234161627195,385.8202349484862,385.81702292915026,385.81378044690797,385.8105078769662,385.80720562877815,385.803874147306,385.8005139143131,385.79712544968544,385.7937093127804,385.7902661038055,385.786796465225,385.783301083194,385.7797806890219,385.77623606066214,385.7726680242298,385.7690774555457,385.7654652817067,385.7618324826815,385.7581800929312,385.7545092030543,385.7508209614545,385.74711657603086,385.7433973158892,385.73966451307365,385.7359195643166,385.73216393280677,385.7283991499725,385.72462681727984,385.720848608043,385.71706626924635,385.71328162337437,385.7094965702491,385.7057130888715,385.7019332392661,385.69815916432407,385.69439309164426,385.6906373353679,385.68689429800514,385.68316647224935,385.6794564427766,385.6757668880259,385.6721005819584,385.6684603957896,385.6648492996924,385.66127036446557,385.6577267631642,385.6542217726877,385.6507587753197,385.6473412602158,385.6439728248346,385.6406571763058,385.63739813273105,385.6341996244103,385.6310656949909,385.6280005025304,385.6250083204692,385.6220935385059,385.6192606633696,385.61651431948184,385.61385924950287,385.61130031475375,385.60884249550963,385.6064908911545,385.6042507201938,385.6021273201137,385.6001261470838,385.5982527754927,385.5965128973112,385.594912321275,385.59345697187905,385.59215288817757,385.5910062223806,385.59002323824114,385.5892103092254,385.58857391645785,385.588120646437,385.58785718851004,385.5877903321058,385.58792696371404,385.5882740636077,385.5888387023023,385.5896280367443,385.5906493062256,385.5919098280178,385.5934169927215,385.5951782593264,385.597201149979,385.59949324445296,385.60206217432165,385.6049156168277,385.60806128845053,385.61150693816853,385.61526034041697,385.619329287741,385.6237215831455,385.628445032142,385.63350743449706,385.63891657568416,385.64468021804316,385.65080609165364,385.6573018849263,385.6641752349211,385.6714337173985,385.6790848366128,385.68713601485933,385.69559458178264,385.704467763461,385.71376267127835,385.72348629059763,385.73364546925075,385.74424690586176,385.75529713801836,385.7668025303124,385.7787692622667,385.7912033161675,385.8041104648273,385.8174962592951,385.8313660165408,385.84572480713683,385.86057744295977,385.87592846494056,385.89178213088735,385.9081424034085,385.9250129379644,385.9423970710761,385.96029780871777,385.9787178149256,385.99765940065004,386.0171245128835,386.037114724092,386.05763122198334,386.0786747996396,386.1002458460485,386.12234433705936,386.144969826798,386.1681214395682,386.19179786227016,386.2159973373647,386.2407176564123,386.26595615421417,386.2917097035826,386.3179747107676,386.3447471115629,386.3720223681182,386.3997954664785,386.4280609148735,386.45681274277666,386.4860445007528,386.51574926111357,386.54591961939286,386.57654769666124,386.6076251426874,386.6391431399604,386.6710924085804,386.70346321202476,386.7362453637955,386.7694282349499,386.8030007625174,386.83695145879994,386.8712684215538,386.90593934504886,386.9409515319964,386.97629190633904,387.01194702688963,387.0479031018077,387.08414600389773,387.12066128671427,387.157434201452,387.19444971460416,387.2316925263648,387.2691470897513,387.3067976304228,387.34462816716666,387.3826225330246,387.4207643970286,387.45903728651723,387.4974246099967,387.5359096805162,387.57447573952317,387.61310598116097,387.65178357697533,387.6904917009917,387.7292135551271,387.7679323948977,387.8066315553852,387.845294477424,387.8839047339692,387.9224460566089,387.9609023621821,387.99925777946225,388.0374966758732,388.07560368419394,388.11356372922154,388.1513620543511,388.1889842480402,388.2264162701206,388.2636444779256,388.3006556521973,388.33743702274336,388.3739762938114,388.4102616691497,388.44628187672777,388.48202619308597,388.5174844672891,388.55264714445843,388.5875052888572,388.62205060650604,388.65627546730923,388.6901729266674,388.7237367465606,388.7569614160822,388.7898421714081,388.8223750151838,388.8545567353169,388.88638492316215,388.91785799108476,388.948975189394,388.97973662263564,389.01014326523455,389.04019697648084,389.06990051485155,389.0992575516617,389.128272684041,389.15695144723077,389.1853003261965,389.21332676655425,389.2410391848074,389.26844697789204,389.295560532028,389.3223912308755,389.3489514629944,389.37525462860657,389.40131514565843,389.4271484551843,389.45277102596765,389.4782003585002,389.5034549882365,389.52855448814245,389.5535194705355,389.57837158821377,389.6031335348706,389.6278290447923,389.6524828918324,389.6771208876599,389.70176987927493,389.7264577457853,389.75121339443666,389.7760667558903,389.80104877873725,389.82619142324137,389.8515276542996,389.87709143361064,389.90291771103557,389.9290424151439,389.9555024429254,389.98233564865717,390.00958083190955,390.0372777246745,390.0654669776017,390.09419014532267,390.12348967084796,390.15340886901686,390.18399190898316,390.21528379571623,390.2473303505005,390.28017819041145,390.3138747067521,390.34846804242795,390.38400706824433,390.4205413581054,390.4581211630989,390.4967973844482,390.5366215453158,390.57764576144336,390.61992271061325,390.66350560092,390.7084481378391,390.7548044900848,390.80262925424825,390.8519774182112,390.90290432333234,390.95546562540324,391.0097172543795,391.06571537288835,391.1235163335226,391.1831766349316,391.2447528767233,391.30830171319593,391.3738798059217,391.4415437752076,391.5113501504642,391.5833553195164,391.6576154768955,391.7341865711547,391.8131242512586,391.89448381209667,391.9783201391806,392.06468765258717,392.1536402502146,392.2452312504243,392.3395133341466,392.43653848653213,392.53635793823594,392.6390221064272,392.7445805356203,392.8530818384289,392.9645736363483,393.07910250067533,393.1967138936793,393.3174521101406,393.4413602193767,393.56848000787863,393.69885192268265,393.83251501560443,393.9695068884639,394.10986363943164,394.25361981062485,394.40080833708515,394.5514604972654,394.7056058651559,394.8632722641745,395.02448572294566,395.18927043308815,395.3576487091301,395.5296409506616,395.70526560683487,395.8845391433129,396.06747601176176,396.2540886219773,396.4443873167255,396.6383803493723,396.8360738643668,397.0374718806353,397.24257627793156,397.4513867861838,397.6639009778636,397.8801142633955,398.10001988961466,398.3236089412699,398.55087034555726,398.78179087966305,399.0163551812795,399.25454576205163,399.4963430238983,399.74172527814756,399.9906687674112,400.2431476901182,400.4991342276164,400.7585985737476,401.02150896678876,401.2878317236508,401.5575312762176,401.83057020970324,402.10690930290184,402.38650757020196,402.66932230523014,402.95530912599213,403.24442202137476,403.5366133988726,403.8318341334036,404.1300336170789,404.43115980979326,404.7351592905056,405.0419773090807,405.3515578385694,405.66384362780553,405.9787762542037,406.29629617664904,406.6163427883721,406.93885446970904,407.2637686406563,407.59102181312926,407.9205496428468,408.2522869807672,408.58616792400653,408.92212586618086,409.26009354711675,409.60000310188224,409.9417861090998,410.28537363850074,410.630696297699,410.97768427815527,411.32626740031736,411.6763751579223,412.0279367614538,412.3808811807515,412.735137186772,413.0906333925092,413.4472982930779,413.805060304978,414.16384780454536,414.5235891656128,414.8842127963952,415.24564717561844,415.60782088791393,415.97066265849935,416.3341013871682,416.6980661816084,417.0624863900745,417.4272916334323,417.79241183659735,418.15777725938904,418.5233185268151,418.88896665880713,419.254653099422,419.6203097455226,419.98586897495113,420.351263674205,420.71642726562567,421.0812937341042,421.44579765331207,421.8098742114568,422.17345923656484,422.5364892212914,422.89890134725204,423.26063350887364,423.6216243367564,423.9818132205404,424.34114033126554,424.6995466432142,425.05697395522554,425.41336491146575,425.7686630216407,426.1228126806367,426.4757591875706,426.8274487642342,427.1778285729162,427.5268467335819,427.8744523403956,428.22059547756646,428.5652272345002,428.9082997202407,429.2497660771834,429.5895804940452,429.9276982180745,430.26407556648746,430.5986699371157,430.9314398182535,431.262344797692,431.59134557093046,431.91840394855456,432.24348286277393,432.5665463731131,432.8875596712496,433.20648908499527,433.5233020814203,433.837967269117,434.15045439960556,434.46073436788464,434.76877921212764,435.07456211253503,435.3780573893441,435.67924050000966,435.97808803556126,436.27457771615127,436.56868838580476,436.86040000638593,437.1496936507962,437.43655149542,437.7209568118361,438.0028939578145,438.28234836761555,438.55930654161637,438.8337560352817,439.1056854475046,439.3750844083397,439.6419435661519,439.90625457420583,440.16801007672063,440.42720369441605,440.6838300095748,440.93788455064816,441.1893637764307,441.4382650598309,441.6845866712641,441.9283277616946,442.1694883453529,442.40806928215596,442.6440722598556,442.8774997759416,443.1083551193257,443.3366423518317,443.5623662895165,443.7855324838483,444.0061472027654,444.22421741163805,444.4397507541603,444.6527555331903,444.8632406915658,445.0712157929117,445.27669100246413,445.4796770679309,445.68018530040524,445.87822755535564,446.07381621370615,446.2669641630281,446.4576847788574]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[463.43605248541814,463.4360538294653,463.43605519852525,463.43605659306365,463.43605801355466,463.43605946048115,463.43606093433516,463.43606243561777,463.43606396483926,463.43606552251987,463.43606710918914,463.43606872538635,463.43607037166134,463.43607204857364,463.43607375669353,463.4360754966017,463.4360772688897,463.43607907416015,463.43608091302696,463.4360827861152,463.43608469406183,463.43608663751553,463.4360886171372,463.4360906335998,463.43609268758894,463.43609477980294,463.43609691095344,463.4360990817648,463.4361012929751,463.4361035453363,463.4361058396141,463.4361081765886,463.43611055705446,463.4361129818211,463.4361154517127,463.4361179675692,463.4361205302461,463.43612314061454,463.4361257995623,463.43612850799315,463.43613126682817,463.43613407700536,463.43613693948004,463.4361398552255,463.43614282523345,463.4361458505134,463.436148932094,463.43615207102283,463.43615526836754,463.43615852521486,463.43616184267256,463.4361652218681,463.4361686639508,463.43617217009074,463.4361757414801,463.4361793793333,463.4361830848871,463.43618685940146,463.4361907041595,463.4361946204688,463.43619860966083,463.43620267309177,463.43620681214327,463.43621102822266,463.4362153227635,463.4362196972257,463.4362241530969,463.43622869189164,463.4362333151537,463.4362380244547,463.4362428213958,463.43624770760806,463.4362526847527,463.436257754522,463.4362629186397,463.4362681788616,463.4362735369762,463.4362789948053,463.4362845542045,463.436290217064,463.4362959853092,463.43630186090127,463.436307845838,463.4363139421542,463.43632015192264,463.4363264772544,463.4363329203006,463.43633948325146,463.4363461683384,463.4363529778346,463.43635991405506,463.4363669793581,463.436374176146,463.4363815068656,463.4363889740091,463.4363965801158,463.43640432777136,463.43641221961013,463.4364202583151,463.43642844661963,463.4364367873077,463.436445283215,463.43645393723017,463.4364627522955,463.43647173140795,463.4364808776206,463.43649019404296,463.43649968384256,463.43650935024596,463.4365191965396,463.4365292260711,463.43653944225053,463.4365498485512,463.43656044851116,463.4365712457344,463.43658224389196,463.43659344672307,463.4366048580365,463.43661648171184,463.43662832170133,463.4366403820302,463.43665266679886,463.43666518018426,463.4366779264404,463.43669090990113,463.4367041349806,463.43671760617497,463.4367313280645,463.4367453053144,463.4367595426766,463.4367740449918,463.4367888171902,463.4368038642946,463.43681919142034,463.43683480377854,463.4368507066771,463.4368669055229,463.436883405823,463.4369002131873,463.4369173333298,463.43693477207125,463.4369525353403,463.43697062917624,463.4369890597303,463.43700783326875,463.4370269561739,463.4370464349473,463.437066276211,463.43708648671077,463.4371070733175,463.43712804303016,463.4371494029778,463.43717116042234,463.4371933227604,463.4372158975268,463.43723889239624,463.4372623151862,463.4372861738595,463.43731047652767,463.4373352314526,463.4373604470501,463.43738613189265,463.4374122947121,463.4374389444029,463.43746609002494,463.43749374080664,463.4375219061483,463.43755059562477,463.4375798189893,463.4376095861769,463.43763990730673,463.43767079268684,463.4377022528169,463.43773429839183,463.4377669403054,463.4378001896546,463.4378340577423,463.43786855608204,463.43790369640146,463.4379394906463,463.4379759509846,463.43801308981114,463.4380509197508,463.43808945366357,463.43812870464865,463.43816868604904,463.43820941145606,463.4382508947135,463.43829314992314,463.43833619144885,463.43838003392204,463.4384246922459,463.43847018160113,463.4385165174509,463.438563715546,463.4386117919302,463.43866076294574,463.43871064523887,463.43876145576576,463.43881321179754,463.4388659309271,463.43891963107455,463.4389743304929,463.43903004777553,463.439086801861,463.4391446120404,463.43920349796383,463.4392634796469,463.4393245774772,463.43938681222176,463.4394502050346,463.43951477746197,463.43958055145197,463.43964754936025,463.43971579395907,463.43978530844333,463.43985611644024,463.439928242016,463.44000170968457,463.44007654441606,463.44015277164453,463.44023041727775,463.4403095077049,463.4403900698063,463.44047213096184,463.4405557190613,463.44064086251234,463.44072759025164,463.4408159317538,463.4409059170417,463.4409975766963,463.4410909418677,463.4411860442851,463.4412829162679,463.44138159073634,463.4414821012232,463.4415844818844,463.4416887675116,463.4417949935429,463.4419031960758,463.442013411879,463.4421256784048,463.44224003380197,463.44235651692884,463.4424751673659,463.4425960254299,463.4427191321872,463.4428445294677,463.44297225987924,463.44310236682156,463.4432348945014,463.4433698879477,463.4435073930262,463.44364745645566,463.4437901258231,463.4439354496004,463.4440834771607,463.44423425879495,463.4443878457288,463.44454429014036,463.4447036451775,463.4448659649765,463.44503130467933,463.44519972045333,463.4453712695098,463.44554601012334,463.4457240016517,463.44590530455605,463.44608998042116,463.4462780919761,463.44646970311635,463.4466648789243,463.4468636856921,463.4470661909438,463.44727246345843,463.44748257329263,463.4476965918052,463.44791459168084,463.4481366469547,463.4483628330377,463.44859322674176,463.4488279063056,463.4490669514222,463.4493104432645,463.44955846451364,463.44981109938664,463.45006843366514,463.4503305547238,463.4505975515609,463.450869514827,463.4511465368571,463.4514287117007,463.4517161351542,463.45200890479276,463.452307120004,463.4526108820208,463.4529202939562,463.4532354608376,463.4535564896426,463.453883489335,463.4542165709018,463.45455584739034,463.4549014339465,463.4552534478538,463.45561200857304,463.4559772377821,463.4563492594173,463.45672819971526,463.4571141872557,463.45750735300413,463.45790783035653,463.45831575518406,463.45873126587907,463.4591545034016,463.45958561132676,463.4600247358935,463.4604720260532,463.46092763352044,463.461391712824,463.46186442135826,463.46234591943704,463.46283637034696,463.4633359404025,463.4638447990019,463.46436311868445,463.4648910751881,463.46542884750863,463.4659766179598,463.4665345722348,463.4671028994683,463.46768179229997,463.4682714469397,463.4688720632329,463.46948384472756,463.47010699874284,463.4707417364385,463.4713882728853,463.472046827138,463.47271762230787,463.47340088563783,463.4740968485786,463.4748057468659,463.4755278205997,463.47626331432457,463.47701247711103,463.4777755626395,463.47855282928447,463.47934454020117,463.48015096341356,463.48097237190365,463.48180904370287,463.4826612619844,463.4835293151581,463.4844134969666,463.48531410658285,463.4862314487107,463.48716583368497,463.4881175775763,463.4890870022952,463.4900744356999,463.491080211705,463.4921046703921,463.4931481581239,463.49421102765785,463.49529363826457,463.4963963558456,463.49751955305635,463.49866360942826,463.499828911495,463.501015852921,463.5022248346306,463.5034562649418,463.50471055970024,463.5059881424176,463.50728944441045,463.5086149049431,463.5099649713723,463.5113400992952,463.51274075269833,463.5141674041114,463.5156205347622,463.5171006347349,463.51860820313095,463.520143748233,463.52170778767174,463.52330084859545,463.5249234678427,463.5265761921179,463.52825957817043,463.52997419297634,463.5317206139234,463.5334994289998,463.53531123698525,463.53715664764695,463.5390362819366,463.5409507721937,463.5429007623498,463.5448869081385,463.546909877307,463.5489703498326,463.5510690181434,463.5532065873411,463.5553837754294,463.55760131354486,463.559859946193,463.5621604314873,463.5645035413934,463.5668900619765,463.56932079365345,463.5717965514489,463.57431816525644,463.5768864801032,463.57950235641914,463.58216667031166,463.5848803138441,463.5876441953187,463.5904592395649,463.5933263882324,463.59624660008825,463.59922085132035,463.6022501358441,463.60533546561595,463.608477870951,463.6116784008457,463.6149381233064,463.61825812568355,463.6216395150095,463.6250834183446,463.62859098312583,463.63216337752345,463.63580179080236,463.63950743368804,463.6432815387416,463.6471253607363,463.6510401770445,463.6550272880264,463.65908801742864,463.663223712786,463.66743574583165,463.6717255129119,463.6760944354086,463.68054396016663,463.68507555992943,463.68969073377986,463.69439100758774,463.6991779344644,463.7040530952241,463.7090180988511,463.7140745829753,463.71922421435255,463.72446868935396,463.72980973446033,463.73524910676497,463.74078859448264,463.74643001746557,463.75217522772726,463.7580261099727,463.7639845821356,463.77005259592374,463.77623213737,463.782525227392,463.788933922358,463.7954603146606,463.802106533297,463.8088747444567,463.81576715211713,463.8227859986447,463.82993356540476,463.8372121733777,463.8446241837821,463.85217199870544,463.85985806174125,463.8676848586332,463.8756549179262,463.88377081162446,463.8920351558556,463.9004506115417,463.9090198850769,463.9177457290109,463.92663094273956,463.9356783732002,463.9448909155735,463.9542715139926,463.96382316225424,463.97354890453886,463.9834518361338,463.99353510416233,464.0038019083162,464.0142555015947,464.0248991910454,464.0357363385103,464.04677036137593,464.05800473332516,464.06944298509273,464.0810887052235,464.0929455408326,464.1050171983667,464.1173074443675,464.12982010623574,464.14255907299497,464.15552829605554,464.1687317899781,464.1821736332353,464.19585796897223,464.20978900576347,464.2239710183679,464.2384083484787,464.2531054054702,464.26806666713776,464.2832966804328,464.29880006219025,464.3145814998487,464.33064575216184,464.3469976498999,464.36364209654084,464.38058406895095,464.3978286180509,464.41538086947014,464.4332460241852,464.4514293591433,464.46993622786727,464.4887720610436,464.5079423670903,464.52745273270324,464.54730882338094,464.56751638392524,464.58808123891674,464.6090092931631,464.6303065321198,464.6519790222803,464.67403291153454,464.6964744294945,464.7193098877851,464.74254568029664,464.7661882834009,464.7902442561241,464.81472024027966,464.83962296055483,464.864959224552,464.89073592277987,464.91696002859607,464.9436385980953,464.9707787699431,464.99838776515236,465.02647288680055,465.0550415196852,465.0841011299146,465.113659264434,465.14372355048056,465.1743016949699,465.2054014838073,465.23703078112317,465.2691975284316,465.30190974370686,465.33517552037773,465.36900302623536,465.40340050225467,465.4383762613239,465.47393868688323,465.5100962314671,465.54685741515004,465.5842308238932,465.6222251077892,465.6608489792028,465.70011121080694,465.74002063350946,465.7805861342719,465.82181665381586,465.8637211842165,465.9063087663816,465.949588487414,465.9935694778578,466.03826090882444,466.0836719890003,466.12981196153373,466.1766901008005,466.2243157090486,466.27269811291995,466.32184665985267,466.37177071435787,466.42247965417977,466.47398286633205,466.52628974301524,466.5794096774175,466.63335205939603,466.6881262710457,466.74374168215365,466.800207645544,466.8575334923147,466.91572852697044,466.9748020224541,467.0347632150825,467.0956212993873,467.1573854228704,467.22006468067457,467.28366811017736,467.3482046855135,467.4136833120319,467.480112820694,467.54750196242105,467.61585940239763,467.68519371433916,467.7555133747326,467.82682675705837,467.8991421260039,467.97246763167743,468.04681130383267,468.1221810461162,468.19858463034575,468.27602969083364,468.35452371876613,468.4340740566503,468.5146878928412,468.59637225616376,468.67913401063925,468.76297985033386,468.8479162943402,468.9339496819061,469.02108616772705,469.10933171741397,469.1986921031522,469.28917289956786,469.38077947981276,469.4735170118864,469.5673904552082,469.6624045574548,469.7585638516775,469.85587265371555,469.9543350599178,470.0539549451885,470.15473596137105,470.2566815359843,470.3597948713222,470.46407894393457,470.5695365044953,470.67617007807723,470.783981964839,470.89297424113875,471.00314876108393,471.1145071585264,471.2270508495119,471.34078103519244,471.45569870520825,471.5718046415463,471.68909942287974,471.80758342939316,471.9272568480975,472.0481196786361,472.1701717395833,472.2934126752355,472.4178419628935,472.5434589206338,472.67026271556455,472.79825237256256,472.92742678348327,473.05778471683675,473.1893248279227,473.32204566941,473.4559457023538,473.59102330763335,473.7272767977991,473.8647044293116,474.0033044151548,474.14307493780825,474.2840141625545,474.4261202511036,474.569391375512,474.71382573237076,474.8594215572406,475.006177139307,475.1540908362274,475.30316108914445,475.4533864378352,475.60476553596516,475.75729716641916,475.91098025667463,476.0658138941859,476.22179734174574,476.37893005279216,476.5372116866227,476.6966421234835,476.85722147949906,477.01895012140295,477.1818286810377,477.34585806958484,477.51103949149115,477.6773744580521,477.84486480061986,478.01351268339687,478.18332061578053,478.35429146422393,478.52642846357725,478.6997352278739,478.87421576053174,479.04987446393034,479.22671614833723,479.40474604014736,479.58396978940857,479.7643934765996,479.9460236186357,480.1288671740714,480.31293154747596,480.49822459295564,480.68475461679816,480.8725303792197,481.0615610951901,481.2518564343202,481.44342651979076,481.63628192630927,481.83043367707694,482.0258932397582,482.22267252143524,482.4207838625439,482.6202400297802,482.82105420797404,483.02323999092505,483.2268113711982,483.43178272888304,483.63816881931365,483.8459847597578,484.0552460150791,484.2659683823805,484.47816797464066,484.69186120335405,484.9070647601885,485.12379559767817,485.342070908969,485.5619081066364,485.7833248005975,486.006338775143,486.2309679651131,486.4572304312468,486.68514433473223,486.9147279109924,487.14599944273704,487.3789772323173,487.61367957342,487.85012472213856,488.088330867463,488.32831610122884,488.57009838756875,488.81369553191246,489.0591251495794,489.3064046340129,489.55555112470466,489.8065814748579,490.05951221884226,490.3143595394911,490.57113923529556,490.8298666875477,491.0905568274896,491.3532241035224,491.61788244853267,491.8845452473914,492.1532253046863,492.42393481273984,492.696685319977,492.97148769969675,493.2483521193071,493.5272880100833,493.8083040375048,494.0914080722336,494.3766071617867,494.66390750296557,494.9533144150959,495.2448323141368,495.5384646877141,495.8342140711354,496.13208202443764,496.43206911052494,496.7341748744466,497.0383978238679,497.344735410784,497.6531840145245,497.9637389260994,498.2763943339268,498.59114331099335,498.90797780348413,499.22688862092616,499.54786542788185,499.87089673723017,500.1959699050692,500.5230711272706,500.85218543771856,501.1832967082546,501.5163876503573,501.85143981857374,502.1884336157227,502.5273482998832,502.8681619931794,503.21085169237006,503.55539328124667,503.9017615448392,504.24993018542807,504.5998718403516,504.9515581016001,505.3049595371762,505.66004571420626,506.0167852237707,506.37514570742906,506.7350938854006,507.0965955863625,507.4596157788203,507.82411860400055,508.1900674102103,508.5574247886041,508.9261526102913,509.29621206471495,509.6675636992263,510.04016745977276,510.413982732616,510.78896838698745,511.1650828185892,511.54228399383715,511.9205294947454,512.2997765643436,512.6799821525149,513.0611029621405,513.4430954954341,513.8259161003431,514.2095210168968,514.5938664233761,514.9789084821786,515.3646033852548,515.7509073989874,516.1377769083867,516.5251684604814,516.913038806775,517.3013449446527,517.6900441576157,518.07909405423,518.468452605678,518.8580781818055,519.2479295855649,519.6379660857554,520.028147447975,520.4184339636978,520.8087864774038,521.1991664116933,521.5895357903246,521.9798572591268,522.3700941047429,522.7602102711709,523.1501703740807,523.5399397128875,523.9294842805817,524.3187707713146,524.7077665857572,525.0964398342525,525.4847593377933,525.8726946268662,526.2602159382112,526.6472942095545,527.0339010723781,527.4200088428004,527.8055905106468,528.1906197267947,528.5750707888877,528.9589186255125,529.3421387789421,529.724707386551,530.1066011610119,530.4877973693834,530.8682738112074,531.2480087957276,531.6269811183491,532.0051700364542,532.3825552446918,532.7591168498592,533.1348353454847,533.5096915862317,533.88366676223,534.2567423734415,534.6289002041707,535.0001222978129,535.3703909319446,535.7396885938444,536.107997956535,536.4753018554285,536.8415832656553,537.206825280148,537.5710110885523,537.9341239570224,538.296147208965,538.657064206778,539.0168583346359,539.375512982358,539.7330115304012,540.0893373360021,540.4444737204984,540.7984039578481,541.1511112643597,541.5025787896502,541.8527896088275,542.2017267159072,542.549373018455,542.8957113334499,543.2407243843559,543.584394799387,543.9267051109493,544.2676377562328,544.6071750789333,544.9452993320733,545.28199268189,545.6172372127596,545.9510149331194,546.2833077823492,546.6140976385763,546.9433663273552,547.2710956311845,547.597267299812,547.9218630612856,548.2448646336982,548.566253737583,548.8860121089075,549.2041215126179,549.5205637566851,549.8353207066,550.1483743002715,550.4597065632743,550.7692996243993,551.0771357314555,551.3831972672756,551.6874667658775,551.9899269287334,552.2905606411005,552.5893509883671,552.8862812723717,553.1813350276466,553.4744960375516,553.7657483502483,554.0550762944821,554.3424644951322,554.6278978884892,554.9113617372317,555.1928416450637,555.4723235709841,555.7497938431555,556.0252391723478,556.2986466649268,556.5700038353652,556.8392986182528,557.1065193797848,557.3716549287087,557.6346945267131,557.8956278982429,558.1544452397267,558.411137228205,558.6656950293491,558.9181103048611,559.1683752192506,559.4164824459814,559.6624251729845,559.9061971075379,560.1477924805099,560.3872060499701,560.6244331041692,560.8594694638913,561.0923114841858,561.3229560554854,561.5514006041165,561.7776430922122,562.0016820170395,562.22351640975,562.4431458335665,562.6605703814207,562.875790673053,563.0888078515912,563.2996235796245,563.5082400347844,563.7146599048559,563.9188863824293,564.1209231591158,564.3207744193418,564.5184448337411,564.7139395521638,564.9072641963207,565.098424852083,565.2874280614535,565.4742808142329,565.6589905393971,565.8415650962052,566.0220127650587,566.2003422381293,566.3765626097749,566.5506833667607,566.722714378308,566.892665885983,567.060548493447,567.2263731560857,567.3901511705317,567.5518941641001,567.7116140841505,567.8693231873938,568.0250340291578,568.1787594526261,568.3305125780656,568.4803067920561,568.6281557367356,568.7740732990746]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[308.64206269720535,308.64206041540933,308.64205809114964,308.64205572363574,308.6420533120629,308.64205085561105,308.64204835344503,308.6420458047143,308.6420432085521,308.6420405640759,308.6420378703865,308.6420351265681,308.6420323316879,308.6420294847956,308.6420265849232,308.64202363108507,308.64202062227673,308.64201755747524,308.6420144356386,308.64201125570537,308.64200801659456,308.64200471720477,308.6420013564144,308.64199793308086,308.64199444604003,308.6419908941066,308.64198727607277,308.6419835907089,308.6419798367615,308.6419760129544,308.6419721179878,308.6419681505374,308.64196410925433,308.64195999276455,308.6419557996686,308.6419515285412,308.6419471779298,308.64194274635554,308.64193823231176,308.6419336342637,308.6419289506483,308.64192417987306,308.64191932031616,308.6419143703255,308.64190932821816,308.64190419228,308.641898960765,308.64189363189445,308.64188820385687,308.64188267480665,308.6418770428642,308.64187130611504,308.6418654626087,308.64185951035864,308.64185344734113,308.6418472714951,308.64184098072104,308.6418345728804,308.6418280457944,308.64182139724454,308.6418146249702,308.6418077266692,308.6418006999967,308.64179354256356,308.64178625193665,308.6417788256377,308.6417712611418,308.64176355587756,308.64175570722546,308.6417477125176,308.64173956903585,308.6417312740122,308.64172282462647,308.6417142180064,308.64170545122647,308.64169652130613,308.6416874252099,308.6416781598457,308.6416687220637,308.6416591086557,308.6416493163537,308.6416393418292,308.6416291816912,308.64161883248624,308.64160829069607,308.64159755273727,308.64158661495975,308.6415754736454,308.64156412500705,308.6415525651873,308.64154079025616,308.64152879621156,308.6415165789762,308.64150413439756,308.64149145824535,308.6414785462107,308.6414653939048,308.641451996857,308.6414383505135,308.64142445023595,308.6414102912995,308.64139586889155,308.64138117810995,308.6413662139611,308.64135097135926,308.6413354451231,308.6413196299756,308.64130352054127,308.6412871113448,308.641270396809,308.6412533712528,308.6412360288895,308.64121836382503,308.64120037005534,308.64118204146484,308.6411633718244,308.64114435478854,308.6411249838942,308.6411052525579,308.6410851540739,308.6410646816116,308.64104382821324,308.6410225867917,308.64100095012833,308.6409789108701,308.640956461527,308.64093359446997,308.640910301928,308.6408865759855,308.6408624085796,308.64083779149814,308.6408127163752,308.6407871746899,308.640761157763,308.64073465675347,308.64070766265627,308.6406801662985,308.6406521583373,308.6406236292554,308.6405945693589,308.6405649687737,308.6405348174419,308.640504105119,308.6404728213696,308.64044095556443,308.6404084968765,308.64037543427776,308.6403417565349,308.6403074522059,308.640272509636,308.64023691695377,308.640200662067,308.6401637326589,308.64012611618335,308.6400877998616,308.64004877067663,308.64000901536986,308.6399685204365,308.63992727212036,308.63988525640957,308.639842459032,308.63979886545025,308.6397544608567,308.6397092301683,308.639663158022,308.63961622876934,308.63956842647053,308.63951973488986,308.6394701374901,308.6394196174266,308.6393681575416,308.6393157403585,308.6392623480761,308.63920796256247,308.63915256534864,308.6390961376228,308.63903866022315,308.6389801136324,308.6389204779705,308.63885973298784,308.63879785805926,308.638734832176,308.6386706339392,308.63860524155234,308.6385386328143,308.63847078511174,308.638401675411,308.6383312802509,308.6382595757346,308.6381865375212,308.6381121408183,308.63803636037267,308.6379591704624,308.63788054488805,308.63780045696365,308.6377188795079,308.6376357848348,308.6375511447445,308.63746493051383,308.63737711288604,308.63728766206174,308.6371965476882,308.63710373884953,308.6370092040564,308.6369129112346,308.63681482771483,308.63671492022206,308.6366131548634,308.63650949711706,308.6364039118216,308.63629636316267,308.63618681466136,308.63607522916277,308.63596156882244,308.63584579509427,308.63572786871714,308.63560774970193,308.6354853973181,308.63536077008007,308.63523382573254,308.6351045212376,308.6349728127589,308.63483865564785,308.6347020044285,308.63456281278144,308.63442103352975,308.6342766186215,308.63412951911494,308.6339796851615,308.6338270659889,308.63367160988423,308.63351326417745,308.63335197522207,308.6331876883789,308.6330203479964,308.63284989739304,308.63267627883755,308.6324994335301,308.63231930158213,308.6321358219974,308.6319489326497,308.63175857026437,308.6315646703951,308.6313671674036,308.63116599443737,308.63096108340744,308.63075236496525,308.63053976848016,308.6303232220154,308.6301026523041,308.62987798472534,308.62964914327847,308.62941605055875,308.62917862773077,308.62893679450286,308.62869046909964,308.6284395682354,308.6281840070868,308.6279236992631,308.6276585567796,308.6273884900265,308.6271134077405,308.626833216974,308.6265478230641,308.6262571296015,308.6259610383989,308.62565944945766,308.62535226093564,308.6250393691126,308.6247206683566,308.6243960510891,308.624065407749,308.62372862675653,308.6233855944771,308.62303619518303,308.622680311016,308.6223178219477,308.6219486057413,308.62157253790974,308.6211894916767,308.62079933793325,308.6204019451965,308.6199971795663,308.6195849046809,308.61916498167284,308.6187372691235,308.6183016230166,308.6178578966907,308.61740594079254,308.6169456032271,308.6164767291085,308.61599916071,308.6155127374115,308.61501729564844,308.6145126688582,308.61399868742575,308.61347517862924,308.61294196658366,308.6123988721833,308.6118457130451,308.61128230344895,308.6107084542773,308.6101239729553,308.60952866338755,308.6089223258959,308.60830475715466,308.60767575012545,308.60703509399093,308.606382574087,308.6057179718343,308.605041064668,308.604351625967,308.6036494249816,308.60293422675966,308.6022057920722,308.6014638773372,308.60070823454254,308.5999386111665,308.5991547500994,308.5983563895606,308.5975432630167,308.5967150990982,308.59587162151234,308.5950125489578,308.5941375950358,308.5932464681606,308.59233887146763,308.59141450272176,308.59047305422206,308.58951421270683,308.5885376592558,308.5875430691913,308.5865301119778,308.5854984511196,308.5844477440569,308.58337764206055,308.5822877901253,308.58117782685963,308.58004738437637,308.57889608817993,308.5777235570515,308.57652940293457,308.57531323081525,308.5740746386042,308.57281321701436,308.57152854943797,308.5702202118209,308.5688877725356,308.56753079225217,308.56614882380694,308.5647414120693,308.563308093807,308.56184839754815,308.5603618434432,308.5588479431223,308.55730619955324,308.55573610689447,308.5541371503489,308.5525088060131,308.550850540726,308.54916181191413,308.54744206743567,308.54569074542195,308.54390727411595,308.5420910717103,308.5402415461808,308.5383580951195,308.5364401055649,308.5344869538285,308.53249800532217,308.53047261437905,308.52841012407595,308.5263098660512,308.52417116031967,308.5219933150884,308.5197756265658,308.51751737877163,308.5152178433431,308.51287627933874,308.5104919330407,308.5080640377529,308.5055918135987,308.5030744673152,308.5005111920446,308.49790116712484,308.49524355787617,308.4925375153859,308.4897821762912,308.4869766625598,308.4841200812663,308.4812115243694,308.4782500684836,308.475234774651,308.47216468810996,308.4690388380607,308.4658562374309,308.4626158826365,308.45931675334265,308.45595781222033,308.45253800470397,308.44905625874344,308.44551148455736,308.4419025743823,308.4382284022214,308.4344878235902,308.4306796752618,308.42680277500995,308.42285592135016,308.41883789327983,308.4147474500173,308.410583330738,308.4063442543121,308.40202891903806,308.3976360023775,308.3931641606872,308.3886120289528,308.3839782205192,308.37926132682196,308.37445991711803,308.36957253821646,308.364597714208,308.3595339461959,308.35437971202657,308.3491334660206,308.3437936387036,308.33835863653945,308.33282684166306,308.32719661161514,308.32146627907764,308.31563415161116,308.3096985113949,308.3036576149672,308.29750969297004,308.2912529498944,308.2848855638309,308.2784056862207,308.2718114416123,308.2651009274209,308.25827221369263,308.2513233428719,308.24425232957503,308.23705716036847,308.2297357935524,308.2222861589499,308.2147061577037,308.2069936620785,308.19914651527057,308.19116253122553,308.18303949446397,308.1747751599151,308.1663672527606,308.1578134682868,308.14911147174735,308.1402588982371,308.1312533525767,308.1220924092084,308.11277361210483,308.1032944746917,308.09365247978167,308.0838450795246,308.07386969537157,308.0637237180553,308.0534045075862,308.04290939326575,308.0322356737172,308.02138061693597,308.0103414603574,307.99911541094673,307.98769964530845,307.97609130981823,307.9642875207767,307.9522853645879,307.9400818979608,307.92767414813727,307.91505911314516,307.90223376207956,307.8891950354115,307.8759398453267,307.8624650760926,307.8487675844595,307.83484420009074,307.8206917260284,307.80630693919227,307.79168659091454,307.7768274075111,307.7617260908896,307.74637931919824,307.73078374751196,307.71493600856127,307.6988327135026,307.6824704527324,307.665845796745,307.64895529703693,307.63179548705745,307.614362883207,307.59665398588504,307.5786652805882,307.56039323905964,307.541834320491,307.52298497277803,307.5038416338309,307.4844007329409,307.4646586922037,307.44461192800014,307.42425685253704,307.40358987544806,307.3826074054537,307.3613058520861,307.3396816274742,307.31773114819345,307.2954508371806,307.27283712571204,307.24988645544914,307.2265952805501,307.2029600698471,307.17897730909283,307.1546435032734,307.12995517898946,307.1049088869062,307.0795012042702,307.0537287374957,307.02758812481875,307.00107603901824,306.9741891902058,306.9469243286808,306.91927824785313,306.89124778723055,306.86282983547005,306.83402133349375,306.80481927766476,306.77522072302514,306.7452227865915,306.7148226507076,306.6840175664521,306.6528048570973,306.6211819216189,306.58914623825257,306.5566953680933,306.5238269587365,306.49053874795595,306.4568285674142,306.42269434640343,306.3881341156101,306.3531460108995,306.3177282771162,306.28187927189407,306.24559746947017,306.20888146449715,306.1717299758478,306.1341418504055,306.09611606683205,306.0576517393084,306.01874812123754,305.97940460890476,305.93962074508437,305.89939622258527,305.8587308877272,305.8176247437369,305.77607795405515,305.7340908455447,305.6916639115891,305.64879781507045,305.60549339121656,305.5617516503047,305.51757378021233,305.4729611487996,305.4279153061157,305.38243798641156,305.3365311099511,305.29019678460384,305.24343730720835,305.19625516469216,305.14865303493366,305.1006337873548,305.05220048322747,305.0033563756818,304.9541049094022,304.90444971999574,304.85439463302134,304.8039436626622,304.7531010100318,304.701871061096,304.65025838420013,304.59826772718714,304.5459040140925,304.4931723414055,304.4400779738819,304.38662633989895,304.33282302633813,304.2786737729876,304.2241844664518,304.1693611335608,304.11420993426816,304.05873715403067,304.0029491956625,303.9468525706566,303.89045388996846,303.8337598542591,303.77677724359,303.7195129065731,303.6619737489707,303.60416672174716,303.5460988085747,303.48777701279425,303.42920834383835,303.37039980311954,303.3113583693929,303.25209098360244,303.19260453321954,303.1329058360884,303.07300162379056,303.012898524545,302.9526030456603,302.89212155555975,302.8314602653975,302.770625210291,302.70962223019313,302.64845695042897,302.5871347619276,302.5256608011763,302.4640399299294,302.4022767147058,302.3403754061068,302.27833991799474,302.2161738065665,302.15388024936215,302.09146202425137,302.0289214884366,301.9662605575177,301.90348068466267,301.840582839929,301.77756748978277,301.71443457686183,301.6511835000321,301.5878130947839,301.52432161401975,301.46070670927975,301.39696541245723,301.3330941180525,301.2690885660139,301.20494382521747,301.14065427763217,301.0762136032201,301.01161476561896,300.94684999865547,300.8819107937327,300.81678788813895,300.7514712543209,300.6859500901638,300.6202128103178,300.5542470386122,300.48803960159336,300.4215765232212,300.3548430207599,300.2878235018916,300.2205015630841,300.15285998923815,300.0848807546395,300.0165450252372,299.9478331622664,299.8787247272357,299.8091984882883,299.7392324279528,299.6688037522895,299.5978889014383,299.5264635615745,299.45450267826726,299.3819804712447,299.3088704505549,299.23514543412153,299.16077756667664,299.08573834006387,299.00999861489436,298.93352864353744,298.85629809442645,298.7782760776587,298.69943117186324,298.6197314523115,298.5391445202423,298.4576375333707,298.37517723754894,298.2917299995473,298.207261840919,298.12173847291297,298.03512533239854,297.94738761876306,297.85849033174327,297.76839831015053,297.6770762714511,297.58448885215785,297.4906006489939,297.3953762607869,297.29878033105064,297.2007775912146,297.10133290445754,297.00041131010585,296.8979780685549,296.7939987066718,296.68843906364157,296.5812653372155,296.4724441303232,296.36194249801076,296.2497279946671,296.1357687215012,296.0200333742358,295.90249129098186,295.78311250025797,295.6618677691261,295.5387286514051,295.41366753593513,295.28665769486025,295.15767333189956,295.02668963057727,294.89368280238426,294.7586301348422,294.62151003944257,294.4823020994356,294.3409871174413,294.19754716285587,294.05196561903017,293.9042272301922,293.7543181480886,293.60222597832103,293.44793982634906,293.2914503431349,293.13274977040385,292.97183198549254,292.8086925457566,292.643328732511,292.4757395944718,292.30592599067097,292.1338906328092,291.95963812701876,291.78317501499646,291.6045098144761,291.42365305899955,291.2406173369486,291.05541732979674,290.8680698495374,290.6785938752442,290.48701058871615,290.2933434091585,290.0976180268481,289.8998624357283,289.7001069648799,289.49838430880686,289.29472955647753,289.0891802190588,288.88177625627503,288.67256010132564,288.46157668429106,288.2488734539521,288.03450039795007,287.81851006120786,287.600957562535,287.3819006093313,287.1613995103108,286.9395171861541,286.7163191780072,286.4918736537358,286.26625141184417,286.0395258829698,285.81177312885865,285.58307183873035,285.3535033229378,285.12315150382955,284.89210290371835,284.66044662986553,284.42827435638526,284.1956803029792,283.9627612104093,283.72961631262075,283.49634730542687,283.26305831167326,283.029855842797,282.79684875670483,282.5641482118934,282.3318676177424,282.1001225809139,281.8690308477964,281.6387122429394,281.4092886034273,281.18088370915183,280.9536232089461,280.72763454255187,280.5030468584017,280.27999092720296,280.0585990513225,279.83900496998,279.62134376026717,279.4057517340218,279.19236633059677,278.981326005576,278.7727701155003,278.56683879868115,278.3636728521909,278.16341360513314,277.96620278831097,277.77218240042265,277.58149457093214,277.39428141977373,277.2106849140667,277.0308467220283,276.85490806429266,276.68300956285395,276.51529108786775,276.3518916025631,276.1929490065256,276.0385999776315,275.8889798129238,275.7442222687347,275.6044594003712,275.469821401692,275.34043644491624,275.2164305210119,275.0979272810218,274.985047878694,274.87791081478736,274.77663178342914,274.68132352090663,274.5920956572735,274.50905457115476,274.4323032481333,274.36194114309416,274.29806404690396,274.24076395778957,274.19012895777877,274.14624309454814,274.1091862690171,274.0790341290101,274.0558579692935,274.03972463827665,274.03069645164555,274.0288311131798,274.03418164297835,274.0467963132956,274.0667185921654,274.0939870949638,274.1286355440339,274.1706927364664,274.2201825201026,274.27712377779704,274.3415304199436,274.4134113852453,274.49277064967146,274.57960724352154,274.67391527648203,274.7756839705379,274.88489770056987,275.00153604244304,275.1255738283666,275.25698120928394,275.39572372402296,275.54176237492356,275.69505370963486,275.8555499087597,276.023198879009,276.1979443515162,276.37972598494775,276.56847947304226,276.7641366561985,276.9666256367333,277.1758708974238,277.3917934229504,277.6143108238589,277.843337462662,278.0787845817073,278.320560432447,278.5685704057518,278.8227171629227,279.08290076706635,279.34901881451424,279.6209665659765,279.89863707714187,280.1819213284456,280.47070835374905,280.76488536769,281.0643378914781,281.3689498769332,281.67860382857816,281.9931809236165,282.3125611296473,282.63662331997904,282.965245386433,283.2983043495325,283.6356764659988,283.97723733348647,284.32286199250643,284.67242502550084,285.0258006530456,285.3828628271703,285.74348532179613,286.1075418203058,286.4749060002632,286.84545161531844,287.2190525743354,287.595583017787,287.97491739147205,288.35693051761166,288.741497663387,289.128494606984,289.51779770121567,289.90928393479123,290.30283099130475,290.69831730601845,291.0956221205108,291.4946255352651,291.8952085602677,292.29725316369,292.70064231871777,293.105260048598,293.5109914699657,293.91772283451013,294.3253415690412,294.73373631400614,295.1427969605118,295.55241468589776,295.96248198790414,296.37289271747585,296.7835421102385,297.1943268166815,297.60514493107553,298.0158960191528,298.4264811445736,298.836802894198,299.2467654021809,299.65627437290624,300.0652371027695,300.47356250082174,300.8811611082808,301.28794511691564,301.69382838630906,302.0987264600007,302.50255658051117,302.9052377032497,303.306690509303,303.70683741710627,304.1056025929935,304.50291196062716,304.8986932093046,305.29287580114186,305.6853909771312,306.0761717620761,306.46515296840033,306.8522711988347,307.23746484798335,307.6206741027725,308.00184094178553,308.3809091334915,308.75782423337284,309.1325335799606,309.5049862897879,309.87513325127077,310.2429271175322,310.6083222981784,310.9712749500492,311.3317429669533,311.6896859684113,312.0450652874237,312.39784395728645,312.747986697476,313.09545989862784,313.4402316066328,313.78227150587895,314.1215509016646,314.4580427018121,314.79172139751046,315.12256304341787,315.45054523705494,315.77564709751937,316.09784924355574,316.41713377101206,316.73348422971634,317.04688559980787,317.35732426755527,317.6647880006976,317.969265923341,318.27074849044527,318.5692274619357,318.8646958764728,319.1571480249148,319.44657942350597,319.73298678682454,320.0163680005221,320.29672209388855,320.57404921227226,320.8483505893887,321.11962851954786,321.3878863298292,321.6531283522349,321.9153598958489,322.17458721903154,322.4308175016729,322.6840588175345,322.9343201067039,323.1816111481844,323.42594253264565,323.6673256353562,323.9057725893206,324.1412962586401]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[385.416994352438,385.4169915824723,385.4169887609579,385.4169858869353,385.4169829594276,385.4169799774393,385.41697693995644,385.41697384594636,385.41697069435713,385.4169674841173,385.4169642141352,385.4169608832991,385.41695749047693,385.41695403451456,385.41695051423744,385.41694692844845,385.41694327592853,385.41693955543604,385.4169357657056,385.416931905449,385.41692797335384,385.41692396808315,385.41691988827523,385.41691573254286,385.4169114994733,385.4169071876274,385.4169027955388,385.41689832171477,385.4168937646338,385.41688912274685,385.4168843944757,385.41687957821244,385.4168746723201,385.41686967513044,385.4168645849446,385.4168594000319,385.41685411862983,385.41684873894235,385.4168432591409,385.416837677362,385.41683199170836,385.4168262002467,385.4168203010082,385.41681429198695,385.41680817114036,385.41680193638734,385.41679558560804,385.4167891166435,385.4167825272943,385.41677581532036,385.4167689784397,385.41676201432773,385.4167549206172,385.4167476948961,385.4167403347081,385.41673283755085,385.4167252008755,385.41671742208587,385.4167094985372,385.41670142753594,385.4166932063382,385.41668483214863,385.4166763021207,385.4166676133545,385.4166587628958,385.41664974773596,385.4166405648101,385.4166312109964,385.4166216831148,385.4166119779263,385.4166020921314,385.41659202236946,385.416581765217,385.416571317187,385.41656067472786,385.41654983422114,385.4165387919819,385.4165275442562,385.4165160872204,385.4165044169794,385.4164925295662,385.41648042093937,385.4164680869826,385.41645552350326,385.41644272623034,385.41642969081335,385.4164164128211,385.4164028877393,385.4163891109711,385.41637507783264,385.41636078355356,385.4163462232748,385.4163313920465,385.4163162848273,385.41630089648174,385.4162852217787,385.4162692553899,385.4162529918876,385.41623642574393,385.41621955132723,385.41620236290146,385.41618485462396,385.41616702054336,385.41614885459734,385.416130350611,385.41611150229454,385.416092303241,385.4160727469246,385.4160528266978,385.41603253578944,385.41601186730264,385.4159908142118,385.4159693693611,385.4159475254614,385.4159252750877,385.4159026106775,385.415879524527,385.41585600878943,385.415832055472,385.4158076564331,385.41578280338024,385.41575748786556,385.4157317012852,385.4157054348747,385.41567867970707,385.41565142668804,385.41562366655586,385.4155953898753,385.41556658703604,385.41553724824894,385.4155073635432,385.41547692276185,385.4154459155596,385.41541433139844,385.4153821595445,385.41534938906403,385.41531600882035,385.4152820074697,385.41524737345674,385.41521209501155,385.4151761601458,385.41513955664743,385.41510227207755,385.4150642937664,385.4150256088081,385.41498620405673,385.41494606612235,385.4149051813656,385.41486353589386,385.4148211155558,385.4147779059374,385.41473389235625,385.4146890598569,385.41464339320623,385.4145968768876,385.41454949509574,385.4145012317319,385.4144520703978,385.41440199438995,385.414350986695,385.4142990299829,385.4142461066015,385.4141921985703,385.41413728757476,385.41408135495993,385.4140243817237,385.41396634851134,385.41390723560755,385.41384702293135,385.413785690028,385.4137232160625,385.4136595798132,385.41359475966306,385.41352873359426,385.41346147917903,385.41339297357314,385.4133231935078,385.41325211528135,385.41317971475223,385.41310596732984,385.4130308479671,385.4129543311509,385.4128763908945,385.412797000728,385.41271613368974,385.4126337623173,385.41254985863776,385.4124643941586,385.41237733985804,385.4122886661749,385.412198342999,385.41210633966085,385.4120126249211,385.4119171669601,385.4118199333675,385.41172089113047,385.41162000662314,385.4115172455954,385.4114125731607,385.41130595378485,385.4111973512736,385.41108672876067,385.4109740486947,385.41085927282813,385.41074236220265,385.41062327713604,385.4105019772101,385.4103784212561,385.41025256734076,385.41012437275276,385.4099937939876,385.4098607867335,385.409725305856,385.4095873053833,385.4094467384897,385.40930355748117,385.4091577137783,385.4090091579005,385.40885783944896,385.4087037070893,385.4085467085354,385.4083867905306,385.4082238988306,385.4080579781848,385.40788897231766,385.40771682391005,385.4075414745795,385.40736286486106,385.407180934187,385.40699562086644,385.4068068620647,385.40661459378214,385.4064187508324,385.40621926682104,385.4060160741228,385.40580910385887,385.40559828587425,385.4053835487131,385.4051648195961,385.4049420243952,385.4047150876088,385.4044839323366,385.40424848025407,385.4040086515851,385.40376436507717,385.40351553797177,385.4032620859787,385.40300392324656,385.4027409623348,385.40247311418386,385.4022002880857,385.40192239165316,385.40163933078946,385.4013510096566,385.401057330643,385.4007581943314,385.400453499465,385.4001431429146,385.3998270196428,385.39950502267084,385.3991770430405,385.3988429697803,385.3985026898665,385.39815608818645,385.39780304750036,385.3974434484013,385.3970771692766,385.3967040862664,385.3963240732237,385.3959370016706,385.39554274075743,385.3951411572183,385.39473211532686,385.39431547685183,385.39389110101047,385.3934588444225,385.39301856106187,385.392570102209,385.39211331640183,385.3916480493844,385.3911741440573,385.3906914404248,385.39019977554244,385.389698983463,385.3891888951815,385.38866933858054,385.388140138372,385.3876011160403,385.3870520897836,385.3864928744538,385.38592328149485,385.3853431188826,385.3847521910601,385.384150298874,385.3835372395094,385.382912806423,385.3822767892758,385.3816289738642,385.38096914204925,385.3802970716866,385.3796125365529,385.37891530627223,385.37820514624156,385.37748181755336,385.3767450769185,385.37599467658697,385.375230364267,385.3744518830437,385.3736589712953,385.37285136260846,385.37202878569167,385.3711909642884,385.37033761708665,385.36946845762856,385.36858319421833,385.3676815298277,385.3667631620008,385.36582778275596,385.3648750784879,385.3639047298662,385.3629164117333,385.3619097930006,385.3608845365416,385.3598402990857,385.35877673110707,385.3576934767143,385.35659017353663,385.35546645260894,385.3543219382545,385.3531562479658,385.3519689922835,385.3507597746733,385.3495281914007,385.3482738314035,385.3469962761624,385.34569509956964,385.3443698677951,385.34302013915016,385.3416454639496,385.3402453843712,385.33881943431254,385.33736713924634,385.3358880160725,385.3343815729683,385.3328473092363,385.3312847151489,385.32969327179177,385.32807245090345,385.326421714713,385.32474051577464,385.3230282968012,385.32128449049287,385.31950851936364,385.317699795567,385.31585772071645,385.31398168570445,385.3120710705181,385.31012524405236,385.30814356392017,385.3061253762595,385.30407001553755,385.30197680435197,385.29984505322926,385.29767406041964,385.29546311168946,385.29321148010985,385.2909184258425,385.28858319592246,385.28620502403777,385.28378313030504,385.2813167210428,385.2788049885408,385.2762471108271,385.2736422514298,385.27098955913743,385.26828816775503,385.2655371958567,385.2627357465348,385.25988290714616,385.2569777490543,385.2540193273675,385.25100668067535,385.2479388307795,385.24481478242205,385.24163352301025,385.23839402233745,385.23509523230007,385.2317360866117,385.22831550051353,385.2248323704796,385.2212855739209,385.21767396888356,385.2139963937452,385.2102516669056,385.2064385864763,385.20255592996386,385.1986024539516,385.19457689377674,385.19047796320325,385.1863043540933,385.18205473607276,385.17772775619443,385.1733220385975,385.1688361841636,385.16426877016886,385.1596183499333,385.1548834524662,385.15006258210894,385.14515421817356,385.14015681457835,385.13506879948136,385.1298885749092,385.1246145163834,385.11924497254483,385.1137782647733,385.108212686806,385.10254650435274,385.0967779547081,385.09090524636184,385.0849265586064,385.0788400411425,385.0726438136821,385.0663359655501,385.0599145552837,385.05337761022986,385.0467231261417,385.0399490667736,385.0330533634745,385.0260339147802,385.0188885860063,385.0116152088384,385.0042115809231,384.9966754654592,384.98900459078806,384.9811966499849,384.9732493004506,384.96516016350387,384.9569268239757,384.94854682980457,384.9400176916326,384.9313368824061,384.92250183697644,384.9135099517045,384.90435858406875,384.89504505227626,384.8855666348785,384.8759205703906,384.8661040569159,384.85611425177666,384.84594827114904,384.83560318970575,384.8250760402648,384.814363813446,384.8034634573346,384.7923718771552,384.7810859349522,384.76960244928284,384.75791819491775,384.7460299025553,384.7339342585455,384.7216279046275,384.70910743767956,384.6963694094845,384.68341032650744,384.67022664969136,384.65681479426803,384.6431711295858,384.62929197895727,384.61517361952383,384.6008122821416,384.58620415128894,384.5713453649941,384.5562320147885,384.5408601456815,384.52522575616325,384.50932479823103,384.4931531774461,384.476706753017,384.4599813379146,384.4429726990174,384.4256765572896,384.4080885879939,384.39020442093874,384.3720196407621,384.353529787255,384.3347303557224,384.3156167973869,384.29618451983475,384.27642888750563,384.25634522222833,384.23592880380454,384.21517487064114,384.1940786204332,384.1726352109005,384.15083976057707,384.1286873496578,384.10617302090253,384.08329178060023,384.06003859959435,384.0364084143723,384.01239612822036,383.98799661244493,383.9632047076651,383.9380152251751,383.9124229483805,383.8864226343107,383.8600090152071,383.8331768001933,383.8059206770248,383.77823531392295,383.7501153614955,383.7215554547442,383.69255021516193,383.6630942529237,383.6331821691692,383.60280855838323,383.5719680108745,383.54065511535276,383.50886446161036,383.47659064330634,383.4438282608575,383.41057192443736,383.3768162570849,383.34255589792406,383.30778550549866,383.27249976122056,383.2366933729356,383.20036107860693,383.1634976501192,383.12609789720256,383.08815667148053,383.04966887063983,383.01062944272604,382.97103339056497,382.9308757763088,382.89015172611175,382.848856434931,382.8069851714577,382.7645332831748,382.7214962015438,382.6778694473204,382.6336486359962,382.5888294833701,382.5434078112442,382.4973795532471,382.45074076078146,382.40348760909376,382.35561640346583,382.30712358552785,382.258005739686,382.2082595996662,382.1578820551712,382.1068701586443,382.05522113214124,382.0029323743019,381.95000146742217,381.89642618461784,381.8422044970782,381.78733458140306,381.7318148270185,381.6756438436659,381.618820468956,381.5613437759841,381.5032130809978,381.4444279511107,381.3849882120534,381.32489395595485,381.2641455491453,381.2027436399701,381.1406891666087,381.07798336488463,381.0146277760601,380.9506242546029,380.88597497591456,380.8206824440092,380.75474949913064,380.6881793252965,380.6209754577564,380.5531417903515,380.48468258276205,380.4156024676307,380.34590645754605,380.27559995187426,380.2046887434231,380.13317902492486,380.06107739532337,379.9883908658491,379.9151268658697,379.84129324849846,379.7668982959466,379.6919507246043,379.6164596898351,379.5404347904675,379.4638860729703,379.3868240352952,379.30925963037186,379.23120426924174,379.152669823816,379.07366862924073,378.994213485861,378.91431766076477,378.83399488889705,378.75325937373094,378.6721257874834,378.5906092708652,378.5087254323532,378.42649034697695,378.34392055460796,378.26103305774643,378.17784531879516,378.09437525681665,378.01064124376603,377.9266621001965,377.8424570904346,377.75804591722147,377.6734487158203,377.5886860475905,377.50377889302985,377.41874864428695,377.33361709714944,377.24840644251265,377.16313925733624,377.0778384950971,376.9925274757489,376.9072298751987,376.82196971431455,376.7367713474783,376.6516594506986,376.5666590093023,376.48179530522265,376.397093903904,376.312580640846,376.2282816078084,376.14422313870193,376.0604317951907,375.97693435203263,375.89375778218573,375.81092924171065,375.7284760544974,375.6464256968496,375.56480578195675,375.48364404428884,375.4029683239447,375.32280655099066,375.2431867298224,375.16413692358594,375.08568523869326,375.0078598094672,374.93068878295253,374.85420030392766,374.7784225001527,374.7033834678897,374.6291112577284,374.555633860753,374.4829791950817,374.4111750928131,374.3402492874106,374.27022940155507,374.2011429354955,374.13301725592555,374.065879585413,373.9997569924069,373.93467638184603,373.87066448639115,373.80774785829914,373.745952861959,373.68530566710535,373.6258322427221,373.5675583516492,373.5105095459028,373.4547111627131,373.40018832128914,373.3469659203092,373.29506863614,373.244520921781,373.19534700652963,373.1475708963618,373.10121637501544,373.0563070057702,373.0128661339038,372.97091688981436,372.9304821927854,372.8915847553775,372.85424708841936,372.8184915065778,372.784340134477,372.75181491334,372.7209376081217,372.6917298151018,372.6642129699041,372.63840835590673,372.61433711300896,372.59202024671475,372.571478637497,372.5527330504023,372.53580414485634,372.52071248462875,372.5074785479183,372.49612273751455,372.4866653909966,372.4791267909259,372.4735271749923,372.46988674607246,372.46822568215873,372.4685641461194,372.4709222952498,372.47532029057584,372.4817783058726,372.49031653635905,372.5009552070366,372.5137145806337,372.52861496512514,372.54567672079475,372.5649202668101,372.5863660872827,372.610034736785,372.63594684530125,372.6641231225879,372.6945843619239,372.7273514432302,372.7624453355426,372.7998870988226,372.8396978850931,372.8818989388887,372.9265115970106,372.97355728758043,373.02305752838845,373.07503392453197,373.1295081653452,373.1865020206211,373.24603733612804,373.30813602842744,373.3728200789982,373.440111527679,373.51003246543513,373.5826050264665,373.6578513796669,373.73579371945164,373.81645425596946,373.8998552047168,373.98601877557326,374.07496716127844,374.1667225253719,374.26130698961674,374.3587426209328,374.45905141785835,374.56225529656956,374.6683760764793,374.7774354654403,374.8894550445802,375.0044562527922,375.12246037090813,375.24348850557857,375.3675615728876,375.4947002817255,375.62492511694694,375.75825632233807,375.8947138834182,376.03431751009987,376.17708661923155,376.3230403170464,376.47219738153905,376.6245762447932,376.78019497528186,376.93907126016046,377.10122238757333,377.2666652289927,377.4354162216105,377.6074913507987,377.782906132658,377.9616755966705,378.1438142684721,378.3293361527623,378.5182547163633,378.71058287144547,378.9063329589317,379.10551673209466,379.308145340359,379.51422931332274,379.72377854500843,379.9368022783569,380.1533090899747,380.3733068751483,380.59680283313367,380.82380345273486,381.05431449818195,381.28834099531923,381.52588721811486,381.7669566755036,382.01155209857393,382.2596754281099,382.51132780250146,382.76650954603167,383.0252201575567,383.287458299587,383.55322178778385,383.8225075808833,384.0953117710596,384.37162957474055,384.65145532388897,384.93478245776066,385.2216035151534,385.51191012715964,385.80569301043477,386.1029419609934,386.4036458485476,386.70779261139745,387.0153692518861,387.3263618324316,387.64075547214503,387.9585343440466,388.27968167288776,388.6041797335907,388.93200985031035,389.2631523961292,389.5975867933892,389.9352915146665,390.2762440843944,390.62042108113553,390.96779814050706,391.31834995875795,391.6720502969981,392.02887198607715,392.38878693210916,392.7517661226377,393.1177796334354,393.4867966359289,393.8587854052406,394.23371332883545,394.61154691576047,394.9922518064616,395.3757927831651,395.7621337808042,396.15123789847297,396.5430674113895,396.93758378334627,397.33474767962593,397.7345189803611,398.1368567943122,398.54171947304144,398.9490646254547,399.3588491326889,399.77102916331523,400.18556018883464,400.6023969994367,401.02149371999633,401.4428038262803,401.8662801613398,402.29187495205923,402.71953982583943,403.14922582738774,403.5808834355913,404.0144625804528,404.44991266006275,404.8871825575908,405.3262206582737,405.7669748663843,406.2093926221625,406.65342091869445,407.0990063187259,407.54609497139796,407.99463262889515,408.44456466299806,408.89583608153316,409.34839154471695,409.8021753813892,410.2571316051379,410.7132039303138,411.1703357879393,411.62847034151565,412.08755050273516,412.54751894710546,413.00831812949673,413.4698902996215,413.93217751746005,414.39512166864426,414.8586644798159,415.32274753397303,415.78731228582245,416.25230007715425,416.7176521522572,417.18330967339193,417.64921373634024,418.1153053860502,418.58152563239224,419.0478154660482,419.5141158745466,419.98036785846455,420.44651244780977,420.91249071859926,421.3782438096488,421.8437129395857,422.3088394240973,422.7735646934236,423.2378303101053,423.7015779869922,424.1647496055177,424.6272872342433,425.0891331476735,425.5502298453422,426.0105200711656,426.4699468330596,426.9284534228131,427.3859834362087,427.8424807933808,428.2978897593968,428.75215496504734,429.2052214278271,429.6570345730895,430.1075402553506,430.5566847797238,431.004414923457,431.4506779575478,431.89542166840886,432.3385943795536,432.7801449732723,433.22002291226636,433.65817826120855,434.09456170819425,434.52912458605084,434.96181889346866,435.39259731591875,435.82141324632124,436.24822080542737,436.6729748618804,437.0956310519175,437.51614579867936,437.9344763310882,438.3505807022639,438.76441780743966,439.1759474013465,439.58513011503413,439.9919274720946,440.3963019042606,440.79821676634924,441.197636350522,441.59452589983766,441.9888516210709,442.38058069677527,442.7696812965694,443.1561225876268,443.539874744351,443.92090895722174,444.29919744079814,444.6747134408657,445.04743124071916,445.4173261665726,445.7843745920903,446.14855394203494,446.50984269503135,446.8682203854458,447.22366760438115,447.57616599979406,447.9256982757375,448.2722481907365,448.6158005553076,448.9563412286298,449.2938571143818,449.62833615575795,449.95976732967864,450.28814064021134,450.6134471112215,450.93567877826985,451.25482867977956,451.5708908474928,451.8838602962391,452.1937330130393,452.50050594556996,452.804176990011,453.1047449783042,453.4022096648475,453.69657171265163,453.9878326789874,454.27599500054816,454.5610619781586,454.8430377610544,455.1219273307627,455.39773648460965,455.67047181888455,455.940140711687,456.20675130548426,456.47031248940834,456.73083388131704,456.98832580964773,457.24279929508884,457.4942660320961,457.74273837027766,457.98822929567444,458.2307524119582,458.4703219215724,458.706952606839,458.940659811053,459.17145941958603,459.39936784102105,459.62440198833923,459.8465792601757,460.0659175221663,460.2824350884013,460.49615070300433,460.70708352185295],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[387.9542224974144,387.9542218485393,387.9542211875888,387.95422051433826,387.95421982855885,387.95421913001724,387.95421841847605,387.9542176936934,387.9542169554228,387.95421620341324,387.9542154374091,387.95421465715,387.9542138623706,387.95421305280064,387.95421222816503,387.95421138818335,387.95421053256996,387.9542096610341,387.95420877327933,387.9542078690039,387.95420694790045,387.9542060096557,387.9542050539508,387.95420408046067,387.9542030888546,387.9542020787953,387.95420104993934,387.95420000193707,387.95419893443216,387.95419784706166,387.95419673945594,387.9541956112384,387.9541944620256,387.95419329142675,387.9541920990439,387.95419088447176,387.9541896472973,387.95418838710003,387.9541871034515,387.9541857959153,387.95418446404705,387.9541831073937,387.95418172549444,387.95418031787915,387.9541788840696,387.954177423578,387.9541759359082,387.9541744205543,387.95417287700116,387.95417130472407,387.95416970318854,387.9541680718502,387.9541664101544,387.9541647175363,387.9541629934206,387.95416123722106,387.9541594483407,387.9541576261714,387.9541557700939,387.9541538794772,387.9541519536785,387.95414999204326,387.9541479939046,387.9541459585834,387.95414388538774,387.95414177361283,387.9541396225409,387.95413743144076,387.95413519956765,387.95413292616274,387.95413061045343,387.9541282516527,387.9541258489584,387.9541234015541,387.95412090860793,387.9541183692723,387.95411578268426,387.95411314796456,387.95411046421765,387.95410773053135,387.9541049459765,387.9541021096067,387.9540992204577,387.9540962775476,387.95409327987625,387.9540902264246,387.954087116155,387.9540839480103,387.9540807209136,387.95407743376825,387.9540740854569,387.95407067484166,387.9540672007632,387.95406366204094,387.95406005747225,387.9540563858319,387.95405264587225,387.95404883632216,387.95404495588696,387.95404100324777,387.95403697706155,387.95403287595985,387.9540286985491,387.9540244434094,387.954020109095,387.9540156941327,387.9540111970225,387.954006616236,387.9540019502165,387.95399719737844,387.9539923561068,387.95398742475646,387.95398240165184,387.95397728508584,387.95397207332,387.95396676458347,387.9539613570723,387.9539558489491,387.9539502383422,387.95394452334557,387.9539387020172,387.9539327723791,387.95392673241685,387.9539205800781,387.95391431327266,387.95390792987143,387.9539014277057,387.95389480456623,387.9538880582032,387.95388118632434,387.9538741865954,387.9538670566384,387.9538597940311,387.95385239630633,387.9538448609511,387.9538371854058,387.95382936706295,387.9538214032669,387.95381329131243,387.9538050284442,387.9537966118555,387.95378803868783,387.953779306029,387.953770410913,387.9537613503189,387.95375212116926,387.9537427203298,387.95373314460755,387.9537233907508,387.95371345544686,387.953703335322,387.9536930269392,387.9536825267982,387.9536718313335,387.953660936913,387.95364983983774,387.9536385363395,387.95362702258046,387.9536152946513,387.9536033485702,387.9535911802813,387.95357878565335,387.9535661604786,387.9535533004709,387.9535402012647,387.95352685841317,387.9535132673873,387.9534994235733,387.95348532227246,387.95347095869846,387.9534563279759,387.9534414251395,387.95342624513114,387.9534107827992,387.9533950328962,387.9533789900775,387.9533626488989,387.9533460038157,387.95332904917973,387.95331177923833,387.95329418813196,387.95327626989246,387.95325801844075,387.953239427585,387.9532204910188,387.9532012023182,387.95318155494067,387.9531615422218,387.9531411573739,387.95312039348323,387.95309924350784,387.9530777002753,387.95305575647984,387.9530334046808,387.9530106372989,387.95298744661477,387.95296382476585,387.95293976374364,387.9529152553915,387.9528902914013,387.9528648633111,387.952838962502,387.95281258019565,387.9527857074509,387.9527583351607,387.9527304540496,387.9527020546703,387.95267312740026,387.9526436624389,387.9526136498042,387.95258307932914,387.9525519406585,387.95252022324536,387.95248791634737,387.95245500902365,387.9524214901306,387.9523873483184,387.952352572027,387.95231714948284,387.9522810686944,387.9522443174481,387.9522068833046,387.9521687535946,387.9521299154143,387.9520903556213,387.9520500608304,387.9520090174088,387.95196721147175,387.95192462887786,387.95188125522446,387.95183707584266,387.9517920757927,387.9517462398586,387.95169955254374,387.95165199806496,387.95160356034785,387.9515542230212,387.9515039694117,387.95145278253807,387.95140064510605,387.95134753950185,387.9512934477872,387.95123835169244,387.95118223261164,387.95112507159536,387.9510668493449,387.95100754620603,387.95094714216214,387.9508856168278,387.9508229494424,387.9507591188626,387.95069410355575,387.95062788159305,387.95056043064176,387.95049172795837,387.9504217503807,387.95035047432066,387.95027787575657,387.9502039302244,387.9501286128111,387.9500518981453,387.94997376038987,387.9498941732326,387.9498131098784,387.94973054304,387.9496464449293,387.94956078724846,387.94947354118017,387.94938467737865,387.94929416596017,387.94920197649316,387.94910807798834,387.94901243888853,387.94891502705883,387.9488158097758,387.9487147537171,387.94861182495066,387.94850698892367,387.9484002104513,387.94829145370613,387.94818068220536,387.9480678587999,387.94795294566256,387.9478359042754,387.9477166954175,387.94759527915284,387.9474716148168,387.9473456610038,387.9472173755537,387.9470867155388,387.9469536372495,387.94681809618106,387.94668004701913,387.9465394436254,387.9463962390233,387.94625038538237,387.94610183400437,387.94595053530674,387.94579643880763,387.94563949311004,387.9454796458853,387.9453168438572,387.9451510327848,387.9449821574459,387.9448101616196,387.9446349880688,387.9444565785226,387.9442748736579,387.94408981308135,387.94390133531067,387.94370937775534,387.9435138766977,387.9433147672732,387.9431119834506,387.9429054580114,387.94269512253044,387.9424809073532,387.9422627415762,387.9420405530249,387.94181426823195,387.9415838124146,387.941349109453,387.94111008186644,387.9408666507908,387.9406187359545,387.94036625565474,387.9401091267333,387.93984726455193,387.9395805829667,387.9393089943035,387.93903240933156,387.9387507372374,387.93846388559876,387.938171760357,387.93787426579013,387.937571304485,387.93726277730923,387.93694858338273,387.93662862004857,387.93630278284377,387.9359709654697,387.9356330597619,387.9352889556592,387.9349385411731,387.93458170235624,387.93421832327067,387.9338482859556,387.9334714703945,387.9330877544825,387.9326970139923,387.93229912254094,387.9318939515548,387.931481370235,387.93106124552264,387.9306334420623,387.9301978221669,387.9297542457804,387.92930257044134,387.9288426512455,387.9283743408081,387.9278974892252,387.9274119440356,387.92691755018166,387.9264141499698,387.9259015830307,387.925379686279,387.92484829387257,387.9243072371717,387.92375634469744,387.9231954420898,387.9226243520657,387.92204289437603,387.92145088576325,387.9208481399175,387.9202344674334,387.91960967576597,387.91897356918605,387.91832594873637,387.9176666121856,387.9169953539842,387.9163119652182,387.9156162335639,387.91490794324176,387.91418687497,387.91345280591844,387.9127055096621,387.9119447561337,387.91117031157756,387.9103819385022,387.90957939563316,387.9087624378656,387.90793081621797,387.90708427778327,387.90622256568327,387.9053454190199,387.9044525728294,387.90354375803435,387.902618701397,387.90167712547276,387.9007187485634,387.89974328467105,387.898750443452,387.8977399301716,387.8967114456584,387.89566468625986,387.8945993437981,387.893515105526,387.8924116540843,387.89128866745943,387.8901458189411,387.88898277708233,387.8877992056589,387.88659476363057,387.8853691051027,387.8841218792899,387.8828527304795,387.8815612979978,387.8802472161761,387.87891011431964,387.87754961667684,387.87616534241124,387.874756905574,387.87332391507914,387.87186597468065,387.87038268295095,387.86887363326247,387.86733841377065,387.8657766074003,387.86418779183305,387.8625715394996,387.8609274175728,387.8592549879649,387.8575538073274,387.85582342705453,387.8540633932902,387.8522732469379,387.8504525236755,387.848600753973,387.8467174631156,387.8448021712296,387.8428543933145,387.8408736392785,387.83885941398063,387.8368112172757,387.8347285440674,387.83261088436507,387.8304577233476,387.82826854143275,387.8260428143533,387.8237800132401,387.8214796047116,387.81914105097104,387.81676380991144,387.81434733522764,387.81189107653785,387.8093944795119,387.80685698601053,387.804278034232,387.80165705886935,387.798993491277,387.79628675964807,387.7935362892021,387.79074150238375,387.7879018190732,387.7850166568085,387.78208543101954,387.7791075552756,387.7760824415456,387.773009500472,387.7698881416592,387.7667177739756,387.76349780587174,387.76022764571337,387.7569067021312,387.7535343843862,387.750110102753,387.74663326892073,387.7431032964117,387.7395196010192,387.73588160126485,387.7321887188756,387.7284403792812,387.72463601213286,387.7207750518445,387.71685693815465,387.71288111671373,387.7088470396927,387.70475416641784,387.70060196402926,387.69638990816594,387.692117483677,387.6877841853594,387.68338951872425,387.67893300079083,387.6744141609102,387.66983254161823,387.6651876995197,387.66047920620326,387.6557066491884,387.6508696329051,387.64596777970723,387.6410007309196,387.6359681479197,387.63086971325566,387.62570513179975,387.62047413193886,387.61517646680306,387.60981191553174,387.6043802845791,387.5988814090589,387.59331515412964,387.5876814164205,387.58198012549815,387.5762112453764,387.5703747760675,387.5644707551771,387.558499259542,387.55246040691253,387.5463543576789,387.54018131664225,387.5339415348311,387.5276353113631,387.521262995352,387.5148249878611,387.5083217439023,387.5017537744814,387.49512164868923,387.4884259958394,387.4816675076515,387.4748469404805,387.4679651175917,387.4610229314804,387.4540213462373,387.4469613999578,387.4398442071946,387.4326709614545,387.4254429377365,387.4181614951117,387.4108280793438,387.4034442255488,387.39601156089253,387.3885318073255,387.38100678435234,387.3734384118356,387.3658287128308,387.3581798164527,387.35049396076755,387.342773495714,387.3350208860448,387.3272387142917,387.3194296837477,387.3115966214652,387.30374248126697,387.29587034676666,387.28798343439587,387.2800850964332,387.27217882403363,387.2642682502525,387.2563571530611,387.24844945834957,387.24054924291244,387.23266073741263,387.22478832931904,387.21693656581306,387.20911015665865,387.2013139770302,387.1935530702951,387.1858326507417,387.17815810625115,387.1705350009032,387.1629690775131,387.15546626009194,387.1480326562236,387.14067455935435,387.13339845098403,387.12621100275817,387.1191190784484,387.1121297358184,387.10525022836674,387.098488006939,387.0918507212051,387.0853462209903,387.0789825574573,387.0727679841292,387.06671095774794,387.0608201389611,387.0551043928288,387.04957278914645,387.04423460257414,387.03909931256726,387.0341766031027,387.0294763621926,387.0250086811813,387.0207838538191,387.0168123751074,387.0131049399098,387.00967244132516,387.00652596881713,387.00367680609696,387.001136428755,386.99891650163863,386.99702887597306,386.9954855862232,386.9942988466947,386.99348104787344,386.993044752502,386.99300269139366,386.99336775898576,386.99415300863126,386.99537164763285,386.9970370320215,386.99916266108283,387.0017621716356,387.0048493320683,387.0084380361379,387.0125422965389,387.0171762382502,387.0223540916664,387.0280901855244,387.03439893963514,387.0412948574295,387.04879251833216,387.05690656997496,387.06565172026177,387.0750427293017,387.08509440122225,387.0958215758811,387.1072391204897,387.1193619211691,387.13220487445255,387.14578287875486,387.16011082582816,387.17520359222146,387.19107603076634,387.2077429621072,387.2252191662984,387.24351937448955,387.2626582607191,387.28265043383954,387.30351042959575,387.325252702878,387.34789162017177,387.3714414522279,387.39591636697173,387.4213304226774,387.44769756142375,387.47503160285703,387.50334623827865,387.53265502507895,387.562971381536,387.5943085819989,387.62667975247257,387.6600978666224,387.6945757422142,387.7301260380049,387.7667612510988,387.804493714781,387.84333559684205,387.88329889840213,387.92439545324567,387.9666369276739,388.0100348208811,388.0546004658606,388.1003450308428,388.1472795212688,388.19541478229746,388.2447615018475,388.2953302141688,388.34713130394044,388.4001750108883,388.4544714349136,388.51003054172503,388.56686216896026,388.6249760327857,388.68438173496014,388.7450887703454,388.8071065348468,388.87044433376377,388.9351113905311,389.00111685582755,389.06846981702927,389.1371793079835,389.20725431907596,389.2787038075655,389.3515367081582,389.42576194379143,389.5013884365974,389.5784251190172,389.6568809450324,389.73676490148176,389.81808601943305,389.9008533855731,389.98507615358614,390.07076355548526,390.15792491286294,390.2465696480283,390.3367072949955,390.42834751029244,390.5215000835531,390.6161749478645,390.7123821898324,390.8101320593365,390.90943497894307,391.0103015529435,391.1127425759919,391.2167690413104,391.3223921484354,391.4296233104797,391.5384741608821,391.64895655962323,391.7610825988825,391.8748646081159,391.99031515853346,392.1074470669581,392.2262733990487,392.34680747187105,392.4690628558027,392.5930533757591,392.7187931117296,392.84629639861436,392.9755778253541,393.10665223334615,393.2395347141427,393.3742406064281,393.51078549227424,393.64918519267366,393.78945576235293,393.9316134838696,394.0756748609974,394.2216566114072,394.3695756586502,394.51944912345476,394.6712943143458,394.8251287176007,394.9809699865534,395.13883593026407,395.298744501567,395.4607137845164,395.6247619812475,395.7909073982716,395.9591684322256,396.12956355509715,396.3021112989465,396.47683024014856,396.65373898317733,396.8328561439577,397.0142003328069,397.1977901369931,397.3836441029333,397.57178071805924,397.7622183923737,397.9549754397257,398.15007005882893,398.3475203140513,398.54734411599924,398.7495592019255,398.9541831159834,399.16123318935746,399.37072652029036,399.5826799540374,399.79711006276943,400.01403312545006,400.23346510771205,400.4554216417572,400.6799180063012,400.90696910658835,401.1365894544983,401.368793148767,401.6035938553429,401.84100478790043,402.0810386885306,402.3237078086297,402.56902389000487,402.81699814621646,403.0676412441752,403.3209632860132,403.57697379124534,403.8356816792396,404.0970952520127,404.361222177367,404.6280694723855,404.89764348730057,405.16994988975097,405.44499364944363,405.7227790232334,406.00330954063617,406.2865879897893,406.5726164038732,406.86139604800815,407.15292740663847,407.44721017141956,407.7442432296185,408.04402465304275,408.3465516875087,408.6518207428623,408.9598273835653,409.2705663198569,409.58403139950497,409.90021560015634,410.21911102229865,410.5407088828442,410.8649995093461,411.1919723348568,411.52161589344104,411.85391781634866,412.18886482885955,412.52644274780744,412.866636479791,413.2094300200786,413.5548064522145,413.90274794833033,414.25323577016877,414.60625027082216,414.9617708971898,415.31977619315563,415.6802438034883,416.04315047846274,416.40847207920314,416.77618358374474,417.1462590938103,417.51867184229883,417.8933942014775,418.27039769187223,418.6496529918469,419.0311299478612,419.41479758539623,419.8006241205354,420.1885769721847,420.578622774919,420.9707273924358,421.3648559315989,421.7609727570525,422.1590415063826,422.559025105808,422.9608857863724,423.3645851006162,423.7700839397018,424.177342550965,424.58632055586537,424.99697696830754,425.4092702133039,425.8231581459483,426.23859807067186,426.65554676074885,427.07396047802246,427.49379499281844,427.91500560401516,428.337547159241,428.7613740751642,429.1864403578476,429.61269962313645,430.04010511704956,430.46860973614474,430.8981660478304,431.32872631059524,431.7602424941304,432.1926662993176,432.62594917806035,433.0600423529354,433.4948968366414,433.9304634512275,434.3666928470812,434.8035355216607,435.2409418379554,435.67886204266233,436.1172462840663,436.5560446296144,436.9952070831779,437.43468360199336,437.87442411328163,438.31437853053944,438.75449676950586,439.1947287638019,439.63502448024883,440.0753339338672,440.51560720256424,440.95579444151696,441.3958458972594,441.83571192148383,442.2753429845694,442.71468968884847,443.1537027816256,443.5923331679636,444.03053192325143,444.4682503055703,444.90543976787393,445.34205197000034,445.77803879053204,446.2133523385211,446.6479449650971,447.0817692749751,447.5147781378789,447.9469246998981,448.378162394792,448.8084449552586,449.23772642418027,449.6659611658616,450.09310387727004,450.5191095992933,450.94393372802153,451.36753202606417,451.78986063390994,452.2108760813355,452.63053529886804,453.0487956293055,453.46561483929645,453.8809511309803,454.2947631536863,454.70701001568966,455.1176512960204,455.5266470563188,455.9339578527312,456.33954474783815,456.743369322603,457.14539368833135,457.54558049862896,457.94389296134267,458.3402948504706,458.73475051802586,459.127224905834,459.51768355725073,459.90609262877604,460.29241890154844,460.6766297926975,461.05869336653507,461.4385783455615,461.8162541212685,462.1916907647146,462.5648590368514,462.9357303985793,463.30427702051105,463.67047179242047,464.0342883323554,464.3957009953947,464.7546848820269,465.1112158461326,465.4652705025505,465.8168262342073,466.1658611987971,466.51235433498846,466.85628536814914,467.19763481556885,467.53638399116846,467.87251500968324,468.2060107903076,468.53685505979223,468.8650323549836,469.19052802479933,469.5133282316311,469.83341995217245,470.1507909776655,470.465429913565,470.77732617861903,471.0864700033648,471.3928524280425,471.6964652999296,471.9973012700987,472.2953537896037,472.59061710510315,472.8830862539241,473.1727570585793,473.4596261207435,473.7436908147019,474.0249492802817,474.30340041527813,474.57904386739045,474.8518800256797,475.12191001156447,475.3891356693705,475.6535595564492,475.9151849328827,476.17401575079253,476.4300566432702,476.6833129129463,476.93379052021965,477.18149607116146,477.4264368051169,477.6686205820222,477.90805586945544,478.1447517294426,478.3787178050368,478.6099643066913,478.8385019984444,479.0643421839366,479.28749669227915,479.5079778637916,479.7257985356295,479.9409720273175,480.15351212620885,480.36343307288746,480.57074954653046,480.77547665024787,480.977629896417,481.1772251920258,481.3742788240432,481.56880744483016,481.7608280576061,481.95035800198553,482.1374149395981,482.3220168398059],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[384.48870294042536,384.48870599145397,384.4887090992625,384.4887122649077,384.4887154894655,384.48871877403263,384.48872211972576,384.4887255276825,384.48872899906144,384.48873253504297,384.48873613682935,384.4887398056451,384.4887435427376,384.48874734937783,384.4887512268595,384.48875517650146,384.4887591996462,384.488763297662,384.48876747194197,384.4887717239053,384.488776054998,384.4887804666924,384.4887849604883,384.48878953791416,384.48879420052583,384.4887989499088,384.4888037876778,384.4888087154777,384.4888137349841,384.4888188479036,384.48882405597425,384.4888293609673,384.4888347646862,384.4888402689682,384.48884587568494,384.48885158674244,384.48885740408275,384.48886332968345,384.4888693655596,384.48887551376316,384.48888177638463,384.4888881555531,384.4888946534378,384.4889012722477,384.4889080142333,384.48891488168704,384.48892187694366,384.4889290023814,384.4889362604232,384.4889436535366,384.4889511842353,384.4889588550797,384.4889666686779,384.4889746276864,384.4889827348114,384.488990992809,384.4889994044873,384.48900797270596,384.48901670037793,384.48902559047104,384.4890346460076,384.48904387006667,384.4890532657841,384.4890628363545,384.4890725850319,384.4890825151306,384.4890926300268,384.48910293315964,384.48911342803177,384.4891241182118,384.48913500733414,384.4891460991008,384.4891573972832,384.4891689057223,384.4891806283311,384.48919256909517,384.489204732074,384.48921712140293,384.48922974129454,384.4892425960392,384.48925569000716,384.48926902765027,384.4892826135035,384.4892964521857,384.4893105484016,384.48932490694375,384.48933953269426,384.48935443062516,384.4893696058017,384.4893850633833,384.4894008086252,384.4894168468805,384.4894331836019,384.4894498243436,384.489466774763,384.4894840406231,384.48950162779397,384.4895195422547,384.4895377900958,384.4895563775214,384.48957531085074,384.4895945965207,384.48961424108757,384.48963425123065,384.4896546337525,384.48967539558276,384.48969654377976,384.4897180855335,384.4897400281676,384.4897623791418,384.48978514605477,384.48980833664683,384.48983195880163,384.48985602055023,384.489880530073,384.48990549570203,384.4899309259249,384.48995682938727,384.489983214895,384.4900100914184,384.49003746809433,384.49006535423035,384.4900937593058,384.490122692978,384.49015216508263,384.49018218563964,384.4902127648541,384.4902439131221,384.4902756410322,384.4903079593707,384.4903408791242,384.49037441148386,384.4904085678493,384.4904433598315,384.49047879925814,384.49051489817697,384.4905516688594,384.49058912380553,384.490627275748,384.4906661376561,384.4907057227407,384.49074604445843,384.4907871165162,384.49082895287575,384.490871567759,384.4909149756522,384.4909591913108,384.4910042297655,384.4910501063259,384.4910968365871,384.4911444364337,384.4911929220462,384.4912423099062,384.49129261680145,384.4913438598324,384.4913960564176,384.4914492242997,384.49150338155084,384.49155854658034,384.49161473813933,384.49167197532773,384.49173027760094,384.4917896647765,384.4918501570401,384.4919117749534,384.49197453946033,384.4920384718946,384.4921035939863,384.49216992787007,384.4922374960927,384.4923063216194,384.49237642784345,384.49244783859297,384.492520578139,384.4925946712045,384.49267014297203,384.4927470190925,384.4928253256943,384.4929050893913,384.49298633729285,384.49306909701204,384.493153396676,384.49323926493497,384.49332673097143,384.4934158245113,384.4935065758334,384.4935990157792,384.49369317576384,384.4937890877871,384.49388678444336,384.493986298934,384.4940876650767,384.494190917319,384.4942960907485,384.49440322110564,384.4945123447955,384.4946234988997,384.49473672118984,384.49485205013974,384.4949695249388,384.495089185505,384.4952110724986,384.4953352273358,384.49546169220304,384.49559051007117,384.49572172470994,384.4958553807029,384.49599152346275,384.49613019924584,384.4962714551695,384.49641533922664,384.4965619003019,384.4967111881893,384.49686325360847,384.4970181482222,384.49717592465305,384.49733663650227,384.4975003383673,384.49766708586066,384.49783693562875,384.4980099453703,384.4981861738572,384.4983656809533,384.4985485276353,384.4987347760133,384.4989244893517,384.49911773209055,384.49931456986764,384.4995150695406,384.4997192992096,384.49992732823995,384.50013922728664,384.5003550683174,384.50057492463674,384.50079887091204,384.5010269831973,384.50125933896015,384.50149601710757,384.5017370980122,384.5019826635402,384.5022327970786,384.50248758356355,384.50274710950885,384.5030114630357,384.5032807339018,384.5035550135327,384.50383439505157,384.5041189733113,384.5044088449268,384.504704108307,384.5050048636885,384.5053112131697,384.50562326074464,384.5059411123383,384.50626487584293,384.5065946611539,384.5069305802071,384.50727274701654,384.5076212777131,384.5079762905836,384.50833790610994,384.508706247012,384.5090814382864,384.50946360725004,384.50985288358316,384.5102493993723,384.5106532891558,384.5110646899685,384.5114837413878,384.5119105855811,384.5123453673543,384.51278823419904,384.5132393363434,384.5136988268031,384.51416686143125,384.5146435989723,384.5151292011155,384.51562383254816,384.5161276610122,384.51664085736013,384.51716359561266,384.51769605301644,384.5182384101054,384.5187908507596,384.51935356226846,384.51992673539235,384.5205105644282,384.5211052472732,384.52171098549263,384.5223279843865,384.522956453059,384.5235966044888,384.5242486556002,384.52491282733547,384.5255893447299,384.5262784369867,384.52698033755365,384.5276952842021,384.52842351910505,384.5291652889201,384.52992084487107,384.5306904428319,384.5314743434128,384.5322728120469,384.53308611907966,384.5339145398586,384.5347583548253,384.5356178496097,384.53649331512537,384.5373850476657,384.53829334900405,384.5392185264934,384.5401608931697,384.5411207678552,384.542098475266,384.54309434611883,384.5441087172422,384.545141931688,384.54619433884613,384.5472662945598,384.5483581612446,384.54947030800895,384.550603110776,384.55175695240916,384.55293222283893,384.5541293191918,384.55534864592283,384.5565906149484,384.5578556457843,384.55914416568294,384.5604566097759,384.5617934212172,384.56315505133006,384.564541959756,384.56595461460626,384.56739349261716,384.568859079306,384.57035186913254,384.5718723656602,384.5734210817243,384.57499853959774,384.57660527116525,384.5782418180973,384.5799087320272,384.5816065747335,384.58333591832366,384.5850973454211,384.586891449356,384.58871883436103,384.5905801157669,384.59247592020455,384.5944068858094,384.5963736624301,384.5983769118398,384.6004173079517,384.602495537039,384.60461229795766,384.60676830237355,384.6089642749935,384.6112009538002,384.61347909029223,384.6157994497258,384.618162811364,384.6205699687274,384.62302172985125,384.62551891754447,384.6280623696571,384.63065293934795,384.63329149535946,384.63597892229797,384.6387161209151,384.641504008399,384.6443435186656,384.6472356026595,384.6501812286557,384.65318138256896,384.6562370682691,384.65934930789825,384.66251914219737,384.66574763083565,384.6690358527468,384.67238490646963,384.67579591049656,384.6792700036244,384.6828083453164,384.68641211606376,384.6900825177581,384.69382077406874,384.6976281308248,384.7015058564058,384.70545524213674,384.70947760269075,384.71357427649804,384.7177466261622,384.72199603888095,384.7263239268767,384.73073172783256,384.7352209053346,384.73979294932303,384.74444937654926,384.7491917310402,384.754021584571,384.7589405371442,384.7639502174774,384.7690522834967,384.7742484228405,384.77954035336944,384.7849298236843,384.79041861365226,384.7960085349414,384.80170143156306,384.8074991804225,384.8134036918783,384.8194169103092,384.82554081469124,384.8317774191809,384.8381287737095,384.84459696458543,384.85118411510416,384.8578923861685,384.86472397691796,384.87168112536546,384.87876610904624,384.8859812456719,384.8933288937976,384.9008114534965,384.9084313670437,384.9161911196109,384.92409323996867,384.93214030119947,384.94033492142137,384.94867976451883,384.95717754088554,384.9658310081754,384.97464297206494,384.98361628702304,384.9927538570944,385.0020586366882,385.01153363138064,385.02118189872505,385.03100654907274,385.04101074640334,385.0511977091656,385.0615707111282,385.0721330822385,385.0828882094939,385.09383953782117,385.1049905709655,385.11634487239024,385.1279060661856,385.1396778379863,385.15166393590016,385.16386817144405,385.1762944204915,385.1889466242269,385.20182879011,385.21494499285,385.2282993753862,385.2418961498796,385.25573959870974,385.26983407548346,385.2841840060483,385.2987938895155,385.3136682992888,385.3288118841028,385.34422936906526,385.35992555670845,385.37590532804484,385.39217364363003,385.40873554463076,385.4255961538975,385.44276067704243,385.4602344035221,385.47802270772223,385.4961310500474,385.51456497801377,385.533330127342,385.55243222305495,385.57187708057324,385.5916706068142,385.61181880128873,385.63232775719797,385.6532036625286,385.6744528011458,385.69608155388204,385.71809639962345,385.74050391639076,385.7633107824141,385.7865237772013,385.8101497825989,385.8341957838443,385.85866887060786,385.8835762380245,385.9089251877133,385.93472312878276,385.96097757882325,385.98769616488295,386.01488662442557,386.04255680627114,386.07071467151536,386.09936829442876,386.12852586333094,386.15819568144207,386.1883861677061,386.2191058575866,386.2503634038335,386.28216757721515,386.31452726722057,386.34745148272214,386.3809493526025,386.4150301263409,386.4497031745584,386.4849779895165,386.52086418557093,386.5573714995761,386.59450979123693,386.6322890434075,386.6707193623335,386.7098109778339,386.74957424342193,386.7900196363609,386.83115775765117,386.8729993319488,386.9155552074083,386.9588363554508,387.0028538704498,387.047618969337,387.0931429911203,387.1394373963108,387.18651376626076,387.23438380240117,387.28305932538234,387.33255227410945,387.3828747046723,387.43403878916337,387.486056814383,387.53894118042564,387.5927043991443,387.64735909249015,387.7029179907229,387.759393930487,387.81679985275247,387.87514880061366,387.934453916944,387.9947284419021,388.05598571028577,388.11823914872963,388.18150227274367,388.24578868358606,388.311112064972,388.3774861796084,388.4449248655561,388.5134420324129,388.5830516573168,388.6537677807614,388.725604502228,388.79857597562255,388.87269640452087,388.9479800372169,389.0244411615714,389.10209409965995,389.1809532022152,389.26103284286603,389.34234741216613,389.4249113114163,389.50873894627347,389.5938447201514,389.68024302740673,389.76794824631423,389.8569747318292,389.9473368081372,390.0390487609941,390.132124829854,390.2265791997887,390.32242599320267,390.41967926134004,390.5183529755925,390.61846101860857,390.7200171752086,390.8230351231088,390.92752842346135,391.03351051121473,391.14099468530014,391.2499940986516,391.36052174806537,391.47259046390906,391.58621289968596,391.7014015214659,391.8181685971915,391.9365261858707,392.0564861266655,392.1780600278905,392.30125925593217,392.42609492410224,392.5525778814404,392.6807187014779,392.8105276709803,392.94201477868336,393.0751897040389,393.2100618059871,393.34664011177534,393.4849333058383,393.62494971876066,393.7666973163432,393.9101836887872,394.055416040024,394.2024011772056,394.3511455003823,394.50165499238415,394.6539352089334,394.8079912690076,394.9638278454777,395.12144915604193,395.2808589544831,395.4420605222676,395.60505666051387,395.7698496823511,395.93644140569313,396.10483314645006,396.2750257122011,396.4470193963512,396.6208139727953,396.79640869110926,396.9738022722938,397.1529929050878,397.33397824287476,397.51675540120135,397.7013209559259,397.88767094201694,398.07580085301765,398.2657056411928,398.45737971837355,398.6508169575144,398.8460106949735,399.04295373353074,399.24163834615007,399.44205628049826,399.6441987642245,399.8480565110088,400.053619727381,400.2608781203154,400.4698209055989,400.6804368169744,400.89271411605483,401.10664060300365,401.3222036279762,401.5393901033125,401.7581865164731,401.9785789437034,402.20055306441657,402.42409417627454,402.64918721095455,402.87581675057834,403.1039670447843,403.3336220284204,403.5647653398314,403.79738033971654,404.03145013052756,404.2669575763794,404.503885323441,404.7422158207759,404.9819313415966,405.22301400489897,405.4654457974419,405.7092085960302,405.95428419006896,406.20065430434465,406.44830062199566,406.69720480763095,406.9473485305559,407.1987134880626,407.45128142874347,407.7050341757853,407.9599536502013,408.21602189395844,408.47322109295794,408.7315335998262,408.9909419564751,409.25142891638876,409.51297746659793,409.7755708492989,410.0391925830809,410.30382648371994,410.5694566845042,410.8360676560526,411.1036442255922,411.3721715956595,411.6416353621937,411.91202153199,412.18331653948246,412.4555072628301,412.72858103927683,413.00252567976213,413.2773294827595,413.55298124731837,413.8294702852939,414.1067864327433,414.38492006047454,414.6638620837329,414.94360397101394,415.2241377519914,415.50545602455276,415.7875519609361,416.0704193129635,416.3540524163682,416.63844619421656,416.92359615942325,417.20949841636616,417.4961496616025,417.7835471836957,418.0716888621597,418.3605731655309,418.6501991485807,418.9405664486795,419.2316752813295,419.52352643488047,419.8161212644475,420.1094616850479,420.40355016397905,420.69838971245764,420.993983876542,421.2903367273627,421.58745285068306,421.88533733581716,422.1839957639298,422.4834341957452,422.7836591586922,423.08467763351297,423.3864970403628,423.6891252244323,423.99257044111636,424.296841340763,424.6019469530288,424.90789667087023,425.2147002342016,425.52236771324624,425.83090949161175,426.1403362491165,426.45065894439665,426.7618887973207,427.07403727123926,427.38711605509684,427.7011370454324,428.0161123282936,428.3320541610898,428.6489749544089,428.9668872538202,429.285803721687,429.6057371190105,429.92670028732476,430.24870613066497,430.57176759762564,430.8958976635282,431.2211093127153,431.5474155209867,431.8748292381926,432.20336337099894,432.5330307658364,432.8638441920465,433.19581632523546,433.5289597308449,433.86328684795046,434.1988099732947,434.53554124556234,434.8734926299042,435.2126759027158,435.55310263667286,435.8947841860319,436.237731672195,436.58195596954323,436.9274676915396,437.2742771771044,437.62239447726085,437.9718293420537,438.3225912077391,438.6746891842447,439.0281320429009,439.3829282044391,439.73908572725793,440.09661229595406,440.4555152101167,440.8158013733832,441.17747728275396,441.5405490181661,441.90502223232136,442.2709021407699,442.63819351224515,443.0069006592519,443.3770274289038,443.74857719401075,444.121552844417,444.4959567785875,444.8717908954439,445.2490565864531,445.6277547279653,446.0078856738066,446.3894492481267,446.77244473850413,447.1568708893114,447.5427258953447,447.9300073957196,448.3187124680386,448.7088376228326,449.1003787982835,449.49333135523005,449.8876900724634,450.28344914231724,450.68060216655806,451.0791421525799,451.47906150991105,451.880352047036,452.28300496853893,452.6870108725753,453.0923597486742,453.49904097587876,453.9070433212273,454.3163549385813,454.7269633678026,455.1388555342848,455.55201774884114,455.96643570795095,456.3820944943677,456.7989785780895,457.21707181769284,457.63635746202937,458.0568181522865,458.47843592440756,458.9011922118732,459.3250678488384,459.7500430736226,460.17609753254925,460.6032102841284,461.0313598035779,461.460523987676,461.89068015993837,462.32180507611207,462.7538749299783,463.1868653594532,463.62075145298064,464.05550775620316,464.4911082789033,464.92752650220405,465.36473538601683,465.80270737672623,466.24141441509977,466.68082794441176,467.12091891876884,467.561657811627,468.0030146244877,468.4449588957626,468.88745970979573,469.3304857060328,469.7740050883288,470.2179856343818,470.6623947052873,471.10719925520186,471.55236584111054,471.99786063269073,472.4436494222659,472.8896976348441,473.33597033823787,473.7824322532613,474.2290477640013,474.6757809281627,475.1225954874859,475.56945487823697,476.0163222417726,476.463160435181,476.9099320420019,477.35659938303064,477.8031245272094,478.2494693026129,478.6955953075331,479.1414639216712,479.5870363174433,480.0322734714084,480.4771361758277,480.921585050362,481.36558055391873,481.8090829966561,482.2520525521543,482.6944492697641,483.136233087139,483.5773638429638,484.0178012898856,484.4575051076557,484.8964349164919,485.3345502906656,485.7718107723227,486.2081758855419,486.6436051506362,487.0780580987012,487.5114942864122,487.943873311072,488.37515482591084,488.8052985556346,489.2342643122229,489.6620120109701,490.0885016867667,490.51369351061203,490.937547806354,491.3600250676432,491.7810859750922,492.2006914136276,492.61880249002195,493.0353805505878,493.4503871990206,493.86378431437174,494.27553406912966,494.68559894739394,495.0939417631156,495.5005256783848,495.9053142217396,496.3082713064727,496.7093612489097,497.1085487866325,497.50579909662224,497.9010778132913,498.29435104637975,498.68558539868457,499.07474798359493,499.4618064424039,499.84672896136794,500.2294842884842,500.6100417499587,500.9883712663352,501.36444336825815,501.73822921184063,502.1097005936119,502.47882996501664,502.8455904464423,503.20995584074774,503.5719006462711,503.93140006929434,504.288430035941,504.6429672034898,504.9949889710819,505.3444734898079,505.69139967215284,506.0357472007905,506.37749653670807,506.7166289266525,507.05312640988706,507.38697182424767,507.71814881149464,508.0466418219505,508.3724361184223,508.695517779404,509.0158737015582,509.3334916014769,509.6483600167246,509.9604683061646,510.2698066495757,510.57636604656255,510.8801383147702,511.1811160874087,511.4792928100998,511.7746627370556,512.0672209266024,512.356963236063,512.643886316012,512.9279876039187,513.2092653171968,513.4877184456735,513.7633467435014,514.0361507205284,514.3061316331458,514.5732914746362,514.8376329650387,515.0991595405569,515.3578753425273,515.6137852059697,515.8668946477474,516.1172098543502,516.364737669332,516.6094855804187,516.8514617063119,517.0906747832115,517.327134151077,517.5608497396527,517.7918320542766,518.0200921614975,518.2456416745182,518.4684927384902,518.6886580156789,518.9061506705183,519.1209843545798,519.333173191471,519.5427317616839,519.7496750874149,519.9540186173708,520.1557782115812,520.354970126232,520.5516109985402,520.745717831682,520.9373079797925,521.1263991330513,521.3130093028666,521.4971568071729,521.6788602558554],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[508.52981585765764,508.5298177545735,508.529819686791,508.52982165496707,508.5298236597712,508.52982570188476,508.5298277820021,508.5298299008306,508.5298320590904,508.52983425751546,508.5298364968535,508.5298387778655,508.5298411013272,508.5298434680286,508.52984587877444,508.52984833438427,508.52985083569297,508.5298533835509,508.5298559788247,508.52985862239655,508.529861315165,508.52986405804614,508.52986685197226,508.52986969789316,508.52987259677667,508.5298755496084,508.52987855739235,508.5298816211509,508.52988474192597,508.5298879207786,508.5298911587895,508.5298944570598,508.5298978167107,508.5299012388845,508.5299047247448,508.5299082754767,508.5299118922877,508.5299155764072,508.5299193290879,508.5299231516058,508.5299270452603,508.52993101137554,508.52993505129973,508.52993916640656,508.52994335809535,508.5299476277908,508.529951976945,508.52995640703625,508.5299609195711,508.5299655160837,508.529970198137,508.52997496732235,508.52997982526176,508.5299847736066,508.5299898140395,508.52999494827407,508.5300001780559,508.53000550516316,508.53001093140676,508.530016458632,508.5300220887178,508.5300278235784,508.53003366516367,508.5300396154594,508.53004567648907,508.53005185031304,508.5300581390306,508.53006454477946,508.530071069738,508.5300777161243,508.5300844861981,508.53009138226116,508.53009840665806,508.5301055617771,508.53011285005056,508.53012027395675,508.5301278360195,508.53013553880993,508.5301433849467,508.53015137709735,508.53015951797926,508.53016781036007,508.530176257059,508.5301848609479,508.5301936249519,508.53020255205024,508.53021164527854,508.530220907728,508.53023034254755,508.53023995294507,508.53024974218755,508.53025971360347,508.5302698705828,508.53028021657843,508.5302907551078,508.530301489754,508.5303124241662,508.5303235620621,508.5303349072279,508.5303464635209,508.53035823486994,508.5303702252769,508.530382438818,508.53039487964554,508.5304075519888,508.53042046015645,508.5304336085364,508.53044700159865,508.5304606438967,508.53047454006816,508.530488694837,508.53050311301587,508.53051779950573,508.5305327592999,508.5305479974839,508.53056351923817,508.5305793298393,508.53059543466236,508.5306118391819,508.53062854897513,508.53064556972225,508.5306629072097,508.5306805673315,508.5306985560909,508.5307168796032,508.53073554409747,508.5307545559186,508.53077392152966,508.53079364751375,508.5308137405767,508.53083420754876,508.5308550553875,508.5308762911802,508.5308979221453,508.5309199556362,508.5309423991426,508.53096526029407,508.53098854686147,508.531012266761,508.5310364280554,508.53106103895783,508.5310861078342,508.53111164320615,508.5311376537534,508.53116414831777,508.5311911359049,508.5312186256888,508.53124662701316,508.5312751493963,508.53130420253336,508.5313337962996,508.53136394075426,508.53139464614367,508.53142592290453,508.53145778166794,508.5314902332629,508.53152328871954,508.53155695927296,508.53159125636796,508.5316261916617,508.5316617770282,508.5316980245627,508.5317349465849,508.5317725556441,508.5318108645229,508.5318498862416,508.5318896340627,508.5319301214953,508.53197136229994,508.532013370493,508.53205616035166,508.53209974641834,508.5321441435065,508.5321893667046,508.5322354313818,508.53228235319324,508.5323301480853,508.53237883230076,508.53242842238456,508.5324789351891,508.5325303878806,508.53258279794437,508.5326361831907,508.5326905617613,508.5327459521351,508.5328023731349,508.5328598439333,508.53291838405954,508.5329780134057,508.5330387522345,508.53310062118453,508.53316364127886,508.533227833931,508.5332932209529,508.5333598245619,508.53342766738876,508.5334967724845,508.53356716332894,508.53363886383875,508.5337118983748,508.533786291751,508.5338620692428,508.5339392565953,508.5340178800325,508.5340979662653,508.5341795425019,508.53426263645576,508.53434727635585,508.5344334909558,508.53452130954304,508.53461076195094,508.53470187856584,508.5347946903395,508.534889228799,508.53498552605737,508.5350836148245,508.5351835284182,508.53528530077523,508.53538896646336,508.53549456069277,508.53560211932756,508.5357116788988,508.53582327661604,508.53593695038023,508.5360527387971,508.53617068118905,508.53629081760937,508.5364131888556,508.5365378364829,508.53666480281913,508.5367941309779,508.536925864874,508.53706004923794,508.53719672963126,508.5373359524618,508.5374777649993,508.5376222153915,508.53776935268064,508.5379192268194,508.53807188868865,508.5382273901138,508.53838578388303,508.5385471237645,508.53871146452497,508.53887886194804,508.53904937285307,508.5392230551144,508.53939996768105,508.5395801705961,508.5397637250176,508.53995069323867,508.54014113870886,508.5403351260555,508.5405327211054,508.540733990907,508.5409390037531,508.5411478292044,508.5413605381117,508.54157720264095,508.54179789629677,508.5420226939482,508.54225167185274,508.542484907683,508.5427224805521,508.54296447104116,508.54321096122555,508.54346203470305,508.5437177766219,508.5439782737096,508.54424361430165,508.54451388837157,508.5447891875611,508.54506960521115,508.5453552363929,508.54564617794017,508.5459425284815,508.5462443884733,508.5465518602341,508.54686504797843,508.5471840578521,508.54750899796744,508.5478399784401,508.54817711142533,508.54852051115637,508.5488702939823,508.54922657840683,508.5495894851286,508.54995913708115,508.55033565947446,508.5507191798361,508.5511098280554,508.55150773642515,508.55191303968735,508.55232587507726,508.55274638236966,508.55317470392595,508.553610984741,508.554055372492,508.5545080175878,508.5549690732189,508.5554386954089,508.5559170430661,508.5564042780368,508.556900565159,508.5574060723181,508.55792097050147,508.55844543385655,508.5589796397481,508.5595237688179,508.5600780050441,508.56064253580286,508.56121755193004,508.56180324778535,508.5623998213167,508.563007474125,508.56362641153237,508.5642568426491,508.5648989804437,508.56555304181273,508.5662192476532,508.56689782293597,508.56758899677845,508.5682930025225,508.5690100778099,508.56974046466195,508.5704844095582,508.57124216351883,508.5720139821868,508.57280012591167,508.5736008598365,508.57441645398376,508.575247183345,508.5760933279712,508.57695517306365,508.5778330090693,508.57872713177454,508.5796378424024,508.580565447712,508.58151026009824,508.5824725976937,508.5834527844737,508.58445115036045,508.5854680313325,508.5865037695328,508.587558713381,508.5886332176864,508.58972764376387,508.59084235955044,508.59197773972556,508.59313416583205,508.5943120264004,508.5955117170736,508.59673364073586,508.597978207643,508.59924583555363,508.6005369498658,508.6018519837521,508.6031913783007,508.6045555826561,508.60594505416367,508.60736025851685,508.60880166990535,508.61026977116853,508.61176505394775,508.61328801884525,508.6148391755826,508.61641904316343,508.6180281500391,508.61966703427555,508.62133624372495,508.6230363361987,508.62476787964476,508.62653145232593,508.62832764300384,508.6301570511229,508.63202028700016,508.6339179720163,508.6358507388113,508.63781923148167,508.63982410578245,508.6418660293321,508.6439456818196,508.64606375521737,508.64822095399495,508.65041799533765,508.65265560936933,508.65493453937705,508.65725554204033,508.6596193876645,508.66202686041646,508.6644787585661,508.6669758947294,508.6695190961171,508.6721092047868,508.67474707789773,508.67743358797236,508.6801696231584,508.6829560874991,508.68579390120357,508.68868400092373,508.6916273400358,508.69462488892367,508.6976776352701,508.70078658434807,508.7039527593209,508.7071772015437,508.71046097087043,508.713805145966,508.71721082462153,508.72067912407636,508.7242111813425,508.72780815353553,508.73147121820887,508.7352015736937,508.7390004394446,508.7428690563874,508.7468086872742,508.75082061704313,508.75490615318137,508.75906662609583,508.763303389486,508.7676178207245,508.77201132124037,508.77648531690915,508.7810412584472,508.7856806218116,508.7904049086048,508.7952156464846,508.80011438957985,508.8051027189098,508.81018224281104,508.8153545973666,508.82062144684363,508.825984484133,508.8314454311968,508.8370060395187,508.8426680905617,508.84843339622836,508.8543037993295,508.8602811740545,508.86636742644896,508.8725644948967,508.8788743506062,508.8852989981031,508.8918404757259,508.89850085612846,508.9052822467853,508.9121867905028,508.91921666593447,508.9263740881012,508.9336613089147,508.9410806177065,508.94863434176096,508.95632484685115,508.96415453778,508.97212585892515,508.98024129478614,508.98850337053614,508.99691465257774,509.0054777490995,509.0141953106379,509.0230700306404,509.032104646032,509.04130193778406,509.0506647314836,509.06019589790725,509.06989835359394,509.07977506142083,509.0898290311794,509.1000633201518,509.1104810336885,509.121085325786,509.13187939966224,509.1428665083342,509.15404995519185,509.16543309457234,509.17701933233093,509.1888121264104,509.20081498740734,509.2130314791343,509.22546521917934,509.23811987946,509.25099918677364,509.2641069233396,509.2774469273386,509.29102309344245,509.3048393733381,509.3188997762416,509.33320836940607,509.34776927861674,509.3625866886775,509.3776648438864,509.3930080484978,509.40862066717256,509.42450712541427,509.44067190999084,509.4571195693387,509.47385471395387,509.49088201676125,509.50820621346793,509.5258321028953,509.54376454729027,509.56200847261397,509.5805688688082,509.5994507900353,509.6186593548933,509.63819974660305,509.65807721316696,509.6782970674974,509.69886468751275,509.7197855162019,509.7410650616519,509.7627088970412,509.78472266059305,509.80711205549125,509.82988284975306,509.85304087605823,509.87659203153595,509.900542277501,509.9248976391447,509.94966420517204,509.9748481273885,510.00045562023047,510.026492960239,510.0529664854749,510.07988259487234,510.107247747529,510.1350684619301,510.16335131510465,510.192102941712,510.2213300330537,510.2510393360118,510.2812376519092,510.3119318352889,510.3431287926115,510.37483548086675,510.40705890609803,510.439806121835,510.47308422743487,510.50690036632653,510.5412617241572,510.57617552683763,510.6116490384828,510.64768955924825,510.68430442305527,510.72150099520525,510.75928666987915,510.7976688675207,510.83665503209943,510.87625262825105,510.9164691382935,510.95731205911545,510.9987888989353,511.04090717392694,511.0836744047128,511.1270981127178,511.1711858163864,511.21594502725634,511.261383245891,511.3075079576663,511.3543266284095,511.4018466998911,511.45007558516585,511.4990206637625,511.54868927672095,511.5990887214756,511.6502262465841,511.7021090463016,511.7547442549981,511.8081389414209,511.8623001028008,511.9172346588023,511.97294944531933,512.0294512081152,512.0867465963114,512.1448421557217,512.2037443220376,512.2634594138644,512.3239936256132,512.385353020245,512.4475435218798,512.5105709082648,512.5744408031103,512.6391586682978,512.7047297959613,512.7711593004514,512.8384521101841,512.9066129593814,512.9756463797112,513.0455566918306,513.1163479968432,513.188024167675,513.2605888403798,513.3340454053806,513.4083969986574,513.4836464928915,513.5597964885756,513.6368493050998,513.7148069718271,513.7936712191673,513.873443469663,513.9541248291,514.0357160776559,514.1182176610982,514.2016296820483,514.2859518913257,514.3711836793851,514.4573240678642,514.5443717012556,514.6323248387212,514.7211813460626,514.8109386878679,514.9015939198463,514.9931436813741,515.0855841882645,515.1789112257808,515.273120141911,515.3682058409204,515.4641627772014,515.5609849494372,515.6586658950985,515.7571986852898,515.8565759199645,515.956789723523,516.0578317408181,516.1596931335756,516.2623645772543,516.3658362583589,516.4700978722208,516.5751386212646,516.6809472137734,516.787511863169,516.8948202878184,517.0028597113834,517.1116168637201,517.2210779823465,517.3312288144832,517.4420546196801,517.5535401730368,517.6656697690252,517.7784272259208,517.8917958908478,518.0057586454434,518.1202979121457,518.2353956611045,518.3510334177186,518.4671922707995,518.5838528813581,518.7009954920123,518.8185999370137,518.9366456528836,519.0551116896562,519.1739767227164,519.2932190652276,519.412816681135,519.5327471987322,519.6529879247822,519.7735158591722,519.8943077100904,520.0153399097061,520.1365886303349,520.2580298010697,520.3796391248557,520.5013920959901,520.6232640180218,520.7452300220259,520.8672650852332,520.9893440499837,521.1114416429821,521.2335324948243,521.3555911597714,521.4775921357397,521.5995098844785,521.7213188519075,521.8429934885846,521.9645082702706,522.0858377185658,522.2069564215847,522.3278390546396,522.448460400904,522.568795372023,522.6888190286447,522.808506600839,522.9278335083782,523.0467753808475,523.1653080775606,523.2834077072479,523.4010506474971,523.5182135639125,523.6348734289738,523.7510075405662,523.8665935401586,523.9816094306093,524.0960335935746,524.2098448065021,524.3230222591882,524.4355455698803,524.5473948009108,524.6585504738405,524.7689935841032,524.8787056151341,524.9876685519711,525.095864894319,525.2032776690658,525.3098904422441,525.4156873304295,525.5206530115738,525.6247727352621,525.7280323324011,525.8304182243246,525.9319174313284,526.0325175806256,526.1322069137298,526.2309742932682,526.3288092092283,526.4257017846452,526.5216427807333,526.6166236014741,526.7106362976651,526.8036735704408,526.8957287742757,526.9867959194812,527.0768696742078,527.1659453659644,527.2540189826684,527.3410871732418,527.4271472477654,527.5121971772068,527.5962355927367,527.6792617846501,527.7612757009064,527.8422779453056,527.9222697753148,528.0012530995626,528.0792304750156,528.1562051038537,528.2321808300602,528.3071621357404,528.3811541371848,528.4541625806918,528.526193838163,528.5972549024858,528.6673533827156,528.736497499072,528.8046960777579,528.8719585456158,528.9382949246304,529.0037158262861,529.068232445792,529.1318565561772,529.1946005022692,529.2564771945569,529.3175001029467,529.3776832504149,529.4370412065601,529.4955890810576,529.5533425170178,529.6103176842502,529.6665312724301,529.7220004841693,529.7767430279857,529.830777111173,529.8841214325602,529.9367951751627,529.9888179987142,530.0402100320747,530.0909918655085,530.1411845428227,530.1908095533576,530.2398888238213,530.2884447099586,530.3364999880417,530.3840778461773,530.4312018754136,530.4778960606419,530.5241847712772,530.5700927517098,530.6156451115162,530.6608673154171,530.7057851729738,530.7504248280104,530.7948127477555,530.8389757116878,530.8829408000843,530.926735382255,530.9703871044621,531.0139238775148,531.0573738640314,531.1007654653682,531.1441273082074,531.1874882308011,531.2308772688721,531.2743236411667,531.317856734664,531.361506089439,531.405301383186,531.4492724154036,531.4934490912491,531.5378614050667,531.5825394235997,531.6275132688943,531.6728131009062,531.7184690998238,531.764511448117,531.8109703123314,531.8578758246392,531.9052580641655,531.9531470381074,532.0015726626663,532.0505647438102,532.1001529578903,532.1503668321297,532.2012357250114,532.252788806583,532.3050550387052,532.3580631552686,532.4118416424002,532.4664187186845,532.5218223154291,532.5780800569906,532.6352192411958,532.693266819873,532.7522493795252,532.8121931221661,532.8731238463429,532.9350669283676,532.9980473037841,533.062089449086,533.1272173637132,533.1934545523421,533.260824007492,533.3293481924652,533.3990490246381,533.4699478591199,533.5420654727943,533.6154220487596,533.6900371611769,533.7659297605439,533.8431181594004,533.921620018477,534.0014523332976,534.0826314212408,534.1651729090671,534.2490917209212,534.334402066807,534.4211174315477,534.5092505642266,534.5988134681152,534.6898173910882,534.7822728165281,534.8761894547162,534.9715762347134,535.0684412967294,535.1667919849767,535.2666348410128,535.3679755975649,535.4708191728389,535.5751696653101,535.6810303489934,535.7884036691946,535.8972912387386,536.0076938346766,536.1196113954712,536.2330430186588,536.3479869589914,536.464440627058,536.5824005883873,536.7018625630344,536.8228214256554,536.945271206069,537.0692050903164,537.1946154222156,537.3214937054233,537.4498306060036,537.5796159555132,537.7108387546097,537.8434871771871,537.9775485750506,538.1130094831336,538.2498556252673,538.38807192051,538.5276424900459,538.6685506646579,538.8107789927853,538.9543092491737,539.0991224441204,539.2451988333293,539.3925179283742,539.5410585077813,539.6907986287323,539.8417156393957,539.9937861918842,540.1469862558475,540.3012911326931,540.4566754704416,540.6131132792118,540.7705779473314,540.9290422580732,541.0884784070053,541.2488580199529,541.4101521715586,541.5723314044334,541.7353657488828,541.8992247431988,542.0638774544957,542.2292925000769,542.3954380693111,542.562281946,542.7297915312084,542.897933866541,543.0666756578335,543.235983299234,543.4058228976442,543.5761602974922,543.7469611058052,543.9181907175463,544.0898143411869,544.2617970244753,544.4341036803694,544.6066991130947,544.7795480442915,544.9526151392132,545.1258650329382,545.2992623565573,545.4727717632943,545.6463579545281,545.8199857056705,545.9936198918662,546.1672255134721,546.3407677212853,546.5142118414739,546.6875234001811,546.8606681477637,547.033612082629,547.2063214746395,547.3787628880505,547.5509032039481,547.7227096421595,547.894149782605,548.0651915860645,548.2358034143319,548.4059540497344,548.5756127139912,548.7447490863922,548.9133333212767,549.081336064794,549.2487284709279,549.4154822167745,549.5815695170585,549.7469631378742,549.9116364096487,550.075563239314,550.238718121685,550.4010761500398,550.5626130258989,550.7233050680039,550.8831292204962,551.0420630602983,551.2000848037031,551.3571733121757,551.5133080973762,551.6684693254099,551.8226378203178,551.9757950668177,552.1279232123047,552.2790050681326,552.4290241101818,552.5779644787355,552.725810977679,552.872549073038,553.0181648908779,553.1626452145766,553.3059774814993,553.4481497790854,553.5891508403779,553.7289700390077,553.8675973836606,554.0050235120467,554.1412396843913,554.276237776475,554.4100102722416,554.5425502559954,554.673851404214,554.8039079769965,554.9327148091678,555.0602673010648,555.1865614090246,555.3115936355961,555.4353610194945,555.5578611253267,555.679092033099,555.799052327534,555.9177410872163,556.0351578735815,556.1513027197745,556.2661761193906,556.3797790151191,556.4921127873072,556.6031792424616,556.7129806017041],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[263.8055523086233,263.80554843514756,263.80554448958713,263.8055404706002,263.8055363768207,263.80553220685664,263.8055279592901,263.80552363267714,263.80551922554673,263.8055147364004,263.80551016371186,263.80550550592653,263.80550076146073,263.80549592870153,263.80549100600547,263.8054859916994,263.8054808840782,263.8054756814054,263.8054703819121,263.80546498379664,263.80545948522365,263.8054538843236,263.8054481791925,263.8054423678905,263.80543644844187,263.80543041883425,263.80542427701727,263.8054180209034,263.80541164836507,263.80540515723595,263.80539854530923,263.8053918103371,263.8053849500296,263.8053779620541,263.8053708440352,263.8053635935531,263.80535620814237,263.8053486852923,263.80534102244513,263.80533321699596,263.8053252662908,263.8053171676267,263.8053089182504,263.8053005153573,263.8052919560906,263.8052832375403,263.8052743567426,263.8052653106779,263.80525609627097,263.8052467103893,263.80523714984156,263.8052274113779,263.8052174916875,263.8052073873979,263.8051970950737,263.80518661121624,263.80517593226114,263.8051650545782,263.805153974469,263.8051426881673,263.8051311918355,263.80511948156555,263.80510755337684,263.8050954032137,263.8050830269458,263.80507042036595,263.805057579188,263.80504449904686,263.80503117549574,263.80501760400546,263.80500377996214,263.80498969866636,263.80497535533124,263.80496074508073,263.804945862948,263.80493070387405,263.80491526270566,263.8048995341937,263.8048835129912,263.8048671936519,263.8048505706284,263.8048336382699,263.8048163908202,263.8047988224165,263.80478092708654,263.80476269874714,263.80474413120197,263.80472521813925,263.8047059531298,263.80468632962544,263.8046663409547,263.8046459803238,263.80462524081116,263.8046041153675,263.80458259681143,263.8045606778284,263.8045383509681,263.804515608641,263.804492443117,263.8044688465216,263.8044448108343,263.80442032788505,263.8043953893523,263.80436998675873,263.80434411147075,263.80431775469293,263.8042909074667,263.8042635606667,263.8042357049983,263.8042073309936,263.8041784290082,263.80414898921913,263.80411900162005,263.804088456019,263.80405734203384,263.80402564908997,263.8039933664155,263.8039604830387,263.8039269877834,263.80389286926584,263.8038581158903,263.8038227158457,263.80378665710043,263.80374992740036,263.8037125142626,263.8036744049723,263.8036355865782,263.8035960458881,263.80355576946465,263.80351474362016,263.80347295441305,263.80343038764175,263.80338702884103,263.8033428632766,263.80329787594,263.80325205154406,263.8032053745166,263.8031578289969,263.8031093988283,263.8030600675544,263.8030098184123,263.8029586343278,263.8029064979093,263.8028533914412,263.80279929687924,263.80274419584305,263.80268806961095,263.80263089911307,263.80257266492475,263.8025133472607,263.8024529259671,263.80239138051604,263.8023286899978,263.80226483311384,263.8021997881698,263.80213353306794,263.80206604529957,263.8019973019382,263.8019272796304,263.8018559545891,263.80178330258525,263.80170929893876,263.80163391851164,263.80155713569803,263.80147892441653,263.8013992581009,263.801318109691,263.80123545162394,263.80115125582495,263.8010654936966,263.8009781361113,263.8008891533989,263.8007985153388,263.80070619114855,263.80061214947375,263.80051635837725,263.80041878532893,263.80031939719385,263.8002181602218,263.80011504003505,263.80001000161826,263.7999030093042,263.7997940267641,263.799683016994,263.7995699423024,263.79945476429816,263.7993374438767,263.7992179412073,263.79909621571954,263.79897222608963,263.7988459302255,263.7987172852548,263.79858624750796,263.79845277250513,263.7983168149401,263.7981783286657,263.79803726667814,263.79789358110054,263.79774722316796,263.79759814320903,263.7974462906314,263.7972916139026,263.7971340605348,263.79697357706533,263.79681010903914,263.7966436009909,263.79647399642573,263.79630123780055,263.79612526650425,263.79594602283913,263.7957634459987,263.79557747404886,263.7953880439067,263.795195091319,263.7949985508402,263.794798355812,263.79459443833844,263.7943867292654,263.79417515815567,263.79395965326626,263.79374014152404,263.7935165485005,263.79328879838755,263.79305681397216,263.792820516609,263.79257982619623,263.7923346611465,263.79208493836063,263.79183057319995,263.7915714794567,263.7913075693261,263.79103875337745,263.7907649405218,263.7904860379841,263.79020195127055,263.7899125841372,263.7896178385578,263.789317614691,263.7890118108468,263.78870032345293,263.7883830470189,263.7880598741026,263.787730695273,263.7873953990735,263.7870538719857,263.78670599839086,263.7863516605305,263.7859907384689,263.7856231100511,263.78524865086325,263.7848672341915,263.7844787309779,263.78408300977986,263.78367993672407,263.7832693754634,263.78285118713086,263.78242523029326,263.781991360905,263.7815494322586,263.78109929493763,263.7806407967661,263.7801737827575,263.77969809506465,263.77921357292615,263.77872005261355,263.778217367377,263.7777053473897,263.7771838196923,263.77665260813455,263.7761115333183,263.77556041253723,263.77499905971655,263.77442728535175,263.77384489644584,263.7732516964459,263.7726474851781,263.77203205878124,263.77140520964036,263.77076672631756,263.77011639348336,263.7694539918447,263.76877929807426,263.76809208473554,263.7673921202091,263.76667916861635,263.7659529897429,263.7652133389585,263.76445996713784,263.7636926205791,263.7629110409202,263.7621149650555,263.761304125049,263.7604782480473,263.759637056191,263.75878026652316,263.7579075908985,263.7570187358888,263.7561134026881,263.7551912870149,263.7542520790147,263.7532954631588,263.75232111814154,263.7513287167785,263.75031792589823,263.7492884062364,263.7482398123259,263.74717179238536,263.746083988206,263.7449760350369,263.7438475614672,263.74269818930753,263.7415275334682,263.74033520183684,263.73912079515213,263.7378839068766,263.7366241230674,263.73534102224403,263.7340341752539,263.73270314513695,263.7313474869859,263.729966747806,263.7285604663718,263.72712817308013,263.7256693898034,263.7241836297384,263.7226703972526,263.72112918772905,263.7195594874078,263.7179607732247,263.7163325126488,263.7146741635147,263.7129851738547,263.7112649817262,263.70951301503766,263.70772869137113,263.70591141780125,263.7040605907137,263.7021755956168,263.70025580695403,263.6983005879113,263.6963092902216,263.69428125396684,263.69221580737593,263.69011226662053,263.68796993560665,263.685788105764,263.68356605583114,263.6813030516375,263.67899834588246,263.6766511779105,263.67426077348324,263.6718263445477,263.6693470890009,263.666822190452,263.66425081797905,263.66163212588356,263.6589652534404,263.65624932464493,263.6534834479558,263.6506667160335,263.64779820547625,263.64487697655096,263.64190207292125,263.63887252137016,263.6357873315204,263.6326454955503,263.62944598790415,263.62618776500153,263.6228697649386,263.61949090719,263.6160500923017,263.61254620158365,263.6089780967965,263.60534461983275,263.60164459239775,263.597876815682,263.5940400700318,263.59013311461496,263.586154687082,263.5821035032238,263.57797825662345,263.5737776183055,263.5695002363801,263.5651447356826,263.5607097174087,263.5561937587464,263.5515954125023,263.54691320672435,263.54214564432067,263.5372912026717,263.5323483332428,263.5273154611868,263.52219098494703,263.51697327585424,263.51166067771834,263.5062515064191,263.500744049489,263.4951365656954,263.4894272846155,263.4836144062114,263.4776961003973,263.47167050660494,263.4655357333459,263.4592898577683,263.4529309252116,263.4464569487573,263.43986590877626,263.433155752473,263.4263243934262,263.4193697111279,263.4122895505168,263.4050817215119,263.39774399854196,263.3902741200722,263.3826697881293,263.3749286678251,263.367048386876,263.3590265351231,263.350860664049,263.34254828629435,263.3340868751725,263.3254738641835,263.3167066465281,263.3077825746201,263.2986989595993,263.28945307084484,263.28004213548866,263.270463337929,263.2607138193469,263.25079067722135,263.24069096484925,263.2304116908647,263.219949818763,263.20930226642577,263.1984659056515,263.1874375616874,263.17621401276693,263.16479198965203,263.15316817518055,263.14133920381784,263.12930166121663,263.1170520837827,263.1045869582475,263.09190272124874,263.0789957589204,263.06586240649125,263.0524989478919,263.03890161537475,263.02506658914325,263.0109899969933,262.9966679139681,262.9820963620252,262.96727130971834,262.9521886718943,262.93684430940567,262.92123402883976,262.9053535822646,262.8891986669963,262.8727649253818,262.85604794460477,262.83904325651207,262.821746337463,262.8041526082013,262.7862574337532,262.76805612334874,262.7495439303726,262.7307160523413,262.71156763091005,262.69209375191,262.67228944541745,262.6521496858548,262.6316693921285,262.61084342779867,262.58966660128954,262.5681336661354,262.54623932126776,262.5239782113432,262.50134492711453,262.47833400584443,262.4549399317669,262.4311571365937,262.40698000007166,262.3824028505883,262.3574199658298,262.33202557349443,262.30621385205666,262.27997893159295,262.2533148946623,262.22621577724766,262.19867556975976,262.17068821810324,262.1422476248075,262.1133476502255,262.08398211379824,262.05414479539184,262.02382943670386,261.99302974274303,261.96173938338507,261.92995199500206,261.8976611821717,261.8648605194644,261.83154355331175,261.79770380395667,261.76333476748584,261.72842991794914,261.6929827095634,261.6569865790026,261.62043494777794,261.5833212247064,261.5456388084685,261.50738109025895,261.4685414565269,261.42911329181,261.3890899816608,261.3484649156659,261.3072314905592,261.2653831134284,261.22291320501427,261.1798152031045,261.1360825660185,261.0917087761859,261.0466873438157,261.0010118106553,260.95467575384,260.90767278982764,260.8599965784218,260.8116408268772,260.76259929408644,260.7128657948479,260.6624342042069,260.611298461872,260.5594525767008,260.5068906312497,260.4536067863881,260.3995952859671,260.34485046154145,260.2893667371396,260.23313863407213,260.17616077577776,260.1184278926974,260.05993482716843,260.00067653833514,259.94064810706425,259.87984474085755,259.81826177875655,259.7558946962229,259.692739109992,259.62879078288347,259.5640456285628,259.49849971623814,259.4321492752826,259.36499069977094,259.2970205529132,259.2282355713778,259.1586326694833,259.0882089432503,259.0169616742941,258.94488833354427,258.871986584774,258.7982542879236,258.7236895022001,258.64829048893426,258.57205571417927,258.49498385102913,258.4170737816413,258.33832459893983,258.25873560798334,258.17830632697314,258.0970364878853,258.01492603670147,257.93197513322025,257.84818415042645,257.7635536733956,257.6780844977151,257.59177762739563,257.5046342722554,257.4166558447518,257.32784395624145,257.23820041264474,257.1477272094954,257.05642652635225,256.9643007205542,256.8713523202966,256.77758401701,256.6829986570217,256.5875992324829,256.49138887154095,256.39437082774384,256.29654846865685,256.1979252636797,256.0985047710492,255.998290624015,255.89728651617793,255.79549618598054,255.6929234003424,255.5895719374338,255.48544556858232,255.3805480393112,255.27488304950626,255.16845423271496,255.06126513457906,254.95331919040746,254.8446197018973,254.73516981301196,254.62497248503152,254.51403047078742,254.4023462881027,254.28992219245646,254.1767601488958,254.06286180322257,253.94822845248333,253.83286101479382,253.71675999853366,253.5999254709485,253.4823570262002,253.3640537529084,253.2450142012305,253.12523634952805,253.00471757067396,252.8834545980526,252.76144349131224,252.6386796019304,252.51515753865306,252.39087113287553,252.26581340403055,252.13997652505515,252.01335178800738,251.88592956990763,251.7576992988798,251.6286494206705,251.49876736562516,251.36803951620078,251.23645117509753,251.10398653409123,250.9706286436494,250.8363593834149,250.70115943363993,250.56500824765564,250.42788402545872,250.28976368849922,250.15062285575263,250.01043582115426,249.86917553248034,249.7268135717503,249.58332013723032,249.43866402711018,249.29281262492864,249.1457318868156,248.9973863306204,248.8477390269892,248.69675159245443,248.54438418459475,248.39059549931892,248.23534277032704,248.07858177079518,247.92026681732685,247.76035077621188,247.59878507202706,247.4355196986098,247.2705032324308,247.10368284838907,246.9350043380453,246.76441213030776,246.59184931457813,246.41725766636233,246.24057767534416,246.06174857591768,245.880708380168,245.69739391328602,245.51174085140005,245.32368376179912,245.13315614552351,244.94009048229069,244.74441827772162,244.54607011282965,244.34497569572952,244.14106391552187,243.93426289830418,243.72450006525682,243.51170219275005,243.2957954744155,243.07670558512137,242.85435774679252,242.62867679600947,242.39958725332247,242.16701339421445,241.9308793216434,241.69110904009614,241.4476265310832,241.20035583000367,240.94922110430875,240.69414673289276,240.4350573866392,240.1718781100495,239.90453440388382,239.63295230874107,239.35705848950786,239.07678032060494,238.7920459719619,238.5027844956494,238.2089259131025,237.91040130286424,237.60714288878623,237.2990841286171,236.9861598029154,236.6683061042235,236.34546072643718,236.01756295431173,235.68455375304103,235.34637585785012,235.00297386354194,234.65429431393935,234.30028579116396,233.94089900469527,233.57608688015142,233.20580464773707,232.8300099302999,232.44866283094163,232.061726020126,231.6691648222271,231.2709473014637,230.86704434716015,230.45742975827787,230.04208032715957,229.62097592242537,229.1940995709639,228.7614375389549,228.32297941186124,227.8787181733289,227.42865028292633,226.97277575265915,226.51109822218933,226.0436250326897,225.57036729925946,225.091339981827,224.60656195446174,224.1160560730159,223.61984924101355,223.11797247370262,222.6104609601813,222.09735412351085,221.57869567871913,221.05453368860225,220.52492061722228,219.98991338100376,219.44957339732105,218.90396663047227,218.35316363492922,217.79723959575026,217.23627436604193,216.67035250135154,216.09956329087032,215.52400078532438,214.94376382142931,214.35895604278122,213.76968591705517,213.17606674938125,212.57821669176494,211.97625874841924,211.3703207768739,210.76053548472598,210.14704042189592,209.5299779682533,208.90949531647553,208.28574445000407,207.65888211596345,207.0290697929088,206.39647365327028,205.76126452036308,205.12361781983725,204.4837135254404,203.84173609897343,203.19787442432164,202.55232173544758,201.90527553823952,201.2569375261124,200.60751348926607,199.9572132175123,199.30625039658943,198.65484249789236,198.0032106615545,197.35157957282718,196.70017733171517,196.0492353158339,195.39898803647156,194.74967298784725,194.10153048957315,193.4548035223418,192.8097375568746,192.16658037618612,191.52558189123204,190.886993950029,190.25107014035274,189.61806558613824,188.98823673772839,188.3618411561355,187.7391372915028,187.1203842559752,186.5058415912082,185.8957690307708,185.29042625771717,184.69007265762875,184.09496706744858,183.50536752045667,182.92153098775694,182.34371311666965,181.77216796644828,181.20714774175894,180.64890252438815,180.09768000366063,179.55372520607352,179.01728022467128,178.48858394870336,177.96787179412627,177.45537543552447,176.95132254003983,176.45593650391123,175.9694361922347,175.49203568256397,175.02394401297514,174.5653649352221,174.11649667361078,173.67753169021543,173.24865645705756,172.83005123585704,172.42188986595517,172.0243395609947,171.63756071492335,171.26170671786824,170.89692378240335,170.54335078070707,170.20111909307528,169.8703524682246,169.55116689578463,169.2436704913398,168.94796339434237,168.66413767917476,168.39227727959562,168.13245792675923,167.88474710094832,167.6492039971141,167.4258795042676,167.2148161987152,167.01604835108415,166.8296019470318,166.65549472148408,166.49373620620045,166.34432779041487,166.2072627942569,166.08252655461305,165.97009652304607,165.86994237535086,165.78202613228805,165.70630229100254,165.64271796660213,165.59121304334505,165.5517203348578,165.52416575278608,165.5084684832616,165.5045411705548,165.51229010727204,165.53161543044868,165.5624113228866,165.60456621908418,165.65796301510986,165.72247928177754,165.79798748049134,165.88435518114017,165.98144528143766,166.08911622712142,166.20722223244508,166.3356135004205,166.47413644229007,166.62263389573627,166.78094534136235,166.94890711700768,167.12635262948854,167.31311256338776,167.5090150865459,167.7138860519365,167.92754919564163,168.1498263306714,168.38053753640298,168.61950134344409,168.86653491375475,169.12145421588872,169.38407419524341,169.65420893923283,169.93167183732265,170.21627573588967,170.50783308788982,170.8061560973398,171.11105685863498,171.42234749074572,171.73984026634756,172.0633477359573,172.39268284715877,172.727659059013,173.0680904517581,173.4137918319121,173.76457883289933,174.12026801132635,174.48067693903735,174.84562429108345,175.21492992974134,175.58841498471745,175.96590192967597,176.34721465522497,176.73217853849613,177.12062050944976,177.51236911403348,177.90725457432112,178.30510884575128,178.70576567158372,179.10906063468408,179.51483120674425,179.92291679503893,180.33315878681444,180.74540059139872,181.15948768011788,181.57526762409645,181.99259013001563,182.411307073895,182.83127253296138,183.25234281566017,183.67437648986134,184.09723440930722,184.5207797383434,184.94487797497197,185.3693969722602,185.7942069581357,186.2191805535947,186.64419278934744,187.0691211209223,187.49384544224657,187.91824809772177,188.34221389280745,188.7656301031267,189.18838648210712,189.61037526716575,190.03149118445157,190.45163145215344,190.87069578238422,191.28858638165156,191.70520794992493,192.12046767830998,192.53427524534172,192.94654281190824,193.3571850148189,193.7661189590298,194.17326420854351,194.57854277599816,194.98187911096454,195.38320008697053,195.78243498727258,196.17951548939837,196.57437564848163,196.96695187941654,197.35718293785692,197.7450099000881,198.13037614180232,198.51322731580623,198.89351132869416,199.27117831651987,199.6461806195008,200.01847275579158,200.38801139436177,200.75475532701677,201.11866543959925,201.47970468241093,201.83783803989394,202.1930324996127,202.54525702057637,202.89448250094392,203.2406817451523,203.58382943051095,203.92390207330203,204.2608779944307,204.5947372846649,204.92546176950728,205.2530349737404,205.57744208568528,205.8986699212142,206.21670688755762,206.5315429469439,206.84316958011146,207.15157974972988,207.456767863768,207.75872973884512,208.05746256359984,208.35296486211254,208.64523645741292,208.93427843510688,209.22009310715217,209.50268397581544,209.78205569783728,210.05821404883514,210.33116588797023,210.60091912290383,210.8674826750685],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Ridge en fonction de alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Ridge pour les paramètres de GridSearchCV\n","FigRMSEGRidRidge = visuRMSEGrid(Ridge(), 'Ridge', alphasridge, 'alpha',\n","                                GridRidge)\n","FigRMSEGRidRidge.show()\n","if write_data is True:\n","    FigRMSEGRidRidge.write_image('./Figures/EmissionsGraphRMSERidge.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.1.3 Modèle Lasso"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre  Lasso()\n","0  lasso__alpha      1.0\n","         Lasso()\n","R²     -0.255703\n","RMSE  571.524475\n","MAE   123.408867\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predLasso=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[40.522630307089976,91.01249223469489,212.60728429161912,37.6663502969622,22.81703575651698,73.48430860527986,420.4099551188026,39.73318810464714,100.26513725952893,-42.951948233585924,323.6799810969778,79.01466156514974,-1.6449382213442831,106.2632976550484,-68.56889516341512,1186.0515442450615,20.814428702689796,25.996110633831783,-82.17422482499843,29.5544889339658,13.523109138262996,13.411156967739359,93.03305114995935,-71.84375953581288,2.5758448235586684,1.0699845784779924,-8.585033548878329,-104.41745687085292,115.69973091800041,9.37296761142587,6.632928329759167,95.5913509540182,25.307784258546846,12.746294101316174,45.28289682301691,2.0065560941694685,37.825717689704426,11.932815375987595,-69.91541340267209,-0.765153101126387,2.1411545291009446,12.534881178110368,6.145610605791781,56.731164844929054,95.51096930219157,168.33482049128907,23.578571778809692,68.9599635782354,114.19173512267736,14.22021225607675,222.57606730279002,42.85718109587056,42.58275125226325,2.210294289006292,151.52599079043898,23.814118066013606,231.29536529430607,21.144294521640173,18.542722299501634,142.8723791449936,131.597496841007,81.67755270482976,159.39138440591267,42.78203241778827,103.43170978877632,44.10195298544379,59.02582756207158,139.0103751084726,-8.3755662391142,33.18173674696606,7.918810390196384,158.81691221777064,-4.375038301335394,50.36441091892773,-32.40729525106899,32.150703682241385,-25.157864621733594,35.60892227624258,28.351583245132495,784.2020361992336,237.35877976925428,56.08708559600961,30.589469873057077,13.01577406512817,165.63430529166106,14.868508820137631,-0.7932833150484626,31.47132175442384,20.558355424179283,34.72529764692803,456.7116078858926,-53.303684866601415,22.72088205049935,80.29076453815102,40.54994170517473,1442.288791486074,-12.031490261447345,-25.678719412939998,-38.70343978356834,-10.947598794193851,9.21010414255177,5.534156742998405,-16.04384250414173,16.21529102881164,91.51152354234692,205.3903995437425,279.00384746595313,58.47723074504068,126.20656907203963,51.56178142636697,448.0332932984228,74.47996384414941,59.90508940258819,153.7000536340214,91.5711729718611,2.455217543941046,8.742405021498925,947.8774629709122,10.421660601354404,13.25679850341404,62.1754198498305,62.42218738153858,62.8419659371628,38.0148168667765,146.03475800832456,35.283459164298094,-2.228061632427462,48.72100071195049,52.73342578416745,4.648343848123368,38.061722121524156,25.672626273080926,14.874914947619104,66.00867484565003,22.289898039361148,20.21510086930863,1.8724010338121815,30.38830799167031,240.353083032175,95.5913509540182,186.675608820146,40.38374720681591,26.169966945618263,776.8776757476527,-5.729484692487951,35.283459164298094,149.5685145116808,238.11744817051715,13.556784291726224,77.2360523318057,108.56649428183714,84.51483376559378,-12.982616454319981,40.87180898770071,-0.9185828340667328,121.27815718938481,-18.45868016972352,12.243890127651824,174.67798549088855,4.207801332111217,82.76828433128247,144.45381428726762,145.81713435782837,-0.4925123764416952,23.19251800600259,-16.14778913687381,86.06583258941573,42.66967940815648,72.74349829547214,370.3271422153218,27.04920536018455,98.00328154091815,22.80173440919367,12.670865575616254,1175.798866815618,85.64425617975017,71.75920321615374,-1.9816731294308738,178.1308493944921,1216.926933324687,1319.5142203781763,223.47903654344083,123.45378992639912,50.686843047363524,17.745785284590447,56.883834963731914,199.07684518956887,67.18023994161157,20.895390776741287,36.81256993961667,-3.1710237182536005,-2.228061632427462,50.088479897809314,0.04276393383456423,-66.49442687356427,97.10599816640953,102.54317458579419,28.702287649955416,371.3551084588646,15.719910572470127,16.103125666368754,-2.6681230328269194,-0.6515759496889402,16.21529102881164,96.74109173805142,51.972768891916765,636.7576949599461,1836.405918294464,191.46399583492934,-0.9377846699564287,29.43317614143176,279.1817503024264,122.23104806318462,-28.230786620983864,136.40990937976767,-23.836654255785902,143.11796690812636,118.13041061161407,61.896114074208256,3.298436675763355,-17.517420675172858,-23.071814487318264,52.996689629729666,-20.837390502357565,-16.515159818999123,60.55687528935883,-53.46656603625699,139.56406162327815,23.61664531570174,0.04276393383456423,16.592919941312374,14.667221379910853,100.28859798416539,32.545843896961394,-16.336872135572904,12.243890127651824,13.652312496991577,66.72598836560216,110.23138676235251,51.127135383539404,33.65538637282074,45.86371780678851,43.93396857532231,-6.771652350303711,52.32730476167835,179.76098951991952,25.923203148243907,-45.46469761400266,12.463833859122062,440.75347445027785,37.79291498157401,426.3679313599431,74.32656509838523,-1.1024471676536507,-65.74830256286432,47.75619094775273,29.817394770317755,4264.8008733798815,32.456835510542334,19.266188887258288,-79.39850286152638,143.51979797305466,23.28110043453939,152.493026968486,92.02852684307396,-19.74919877447381,73.76322083387598,14.291315850235087,9.322407607764589,83.26737755816696,6.1613300817428325,27.602668672518757,120.89442017770739,201.87690853281663,808.4048457654658,55.44669051678589,124.01434661326664,94.9462749820504,125.84328744081049,63.36699291293104,80.65921905533467,10.982401576472974,40.34467697714979,23.55370986663781,76.63679207923286,12.614277384328595,113.74606545415575,211.78139762932005,-27.69226743473334,19.70366063557233,-12.664324640582393,18.25233943336471,5.030001859263358,13.33824948215149,-9.837137114196857,44.772404659582925,18.996992017395357,-9.78798588307221,9.95063719809356,100.03982500052811,94.82077702878999,153.75196374170662,17.81115011622974,652.9958852915075,25.189915227974105,-18.311431432086223,10.554820315695778,-4.100789944046674,5.792166456310866,94.7054854117329,-52.358074942219616,10.334458961176395,21.18854102945032,150.97337395457367,13.470441668307231,-209.94451005932478,265.42568064071025,16.21529102881164,26.108503655747576,13.458979662635414,134.79735732251362,7.973753376131299,271.14809386464316,376.12181979614206,-9.78798588307221,1.066175457913019,36.368602525738176,39.974906575466015,577.2714171209009,122.83376292107573,65.02022455037688,224.54781664549517,30.751922001410378,-30.44832414551179,59.55427625988389,53.220834599963936,239.9316410242094,65.22632144261087,18.07398456697631,24.828840275670146,94.82077702878999,54.8100160090174,-44.75960758314055,64.71652141129006,59.728653671853635,243.4890431483983,7.048201488788557,47.390979080114235,79.63091812989481,165.16733763015492,15.766629579040078,50.857563219298676,63.43792324410218,130.9168975894484,-20.13234327846861,182.94311873766992,66.15701685682905,93.89408500850675,144.45381428726762,141.09758908585485,52.07212856212076,-4.695699606171068,-0.8873102568571127,224.9089525878561,51.420562381978385,11.594078096164516,29.607397238838402,106.47218619177195,-10.947598794193851,-70.78604888778708,276.03147175825666,-2.2793951344347434,-12.189284333274792,-5.606030121136271,532.1300276985359,1.1653158758477886,61.982761442732894,721.3987769865549,9.308521189201713,57.658733273365776,47.05553272686244,984.983971725249,176.69164161375534,-4.809636315206532,47.134048941356085,-23.836654255785902,100.68209566938623,100.03982500052811,151.66766190537646,15.152336572476358,61.896114074208256,489.9947950841266,15.550523609148623,19.53258162306016,20.384502871997007,-9.255571747209707,104.18602185120476,63.43792324410218,335.0086923827479,1225.193081940572,65.128370979545,199.1774834690125,91.53650153371147,133.89252159918004,78.29919443532985,11.815604686989225,12.746294101316174,26.812301292792704,188.67868450084302,61.98717130769459,-70.94347340505125,74.93067883922649,19.35233631730987,-17.68156731260001,209.52493498177506,32.6112505310902,10.587206321050672,15.34525888112367,62.35180085557341,69.67608431224724,2143.464787067365,82.14649289101759,-57.31020579395562,17.23366192780652,323.51138958563774,-76.325826212091,246.42057838623572,36.25521323199585,99.38664887971062,65.88233468608652,48.46264508395406,55.832621482336336,38.69584614923928,13.209259315342145,-45.65049670089118,239.11575025218593,17.848596723884615,50.93368487426975,-23.69294149943697,-50.41053568490061,-52.187011038167995,85.55311395694608,1.1316662671149373,1.7330636027942745,-123.27288241906653,-5.325481093273154,78.44104714427772,-10.65206976743714,9.577718059064651,214.92697750765285,589.0401227619577,95.51096930219157,372.2872258850206,30.345430239841136,25.771779908945994,17.188325548003732,39.604362246083,-41.06705892845268,40.18764546972974,-23.97210254383083,21.010360273245254,28.62900486998977,308.1643350059711,43.74590394604158,-56.95031263431888,42.78203241778827,14.482336179068966,12.998949260761727,24.453836900242017,388.6308368505811,20.474012785862755,8.742405021498925,-1.887872678596267,227.38273575972505,14.968524886598743,98.08740556275032,490.32532274718835,13.937878350926631,50.041372560337756,42.470585889820356,99.02338796686003,33.83908595546208,-39.64590601159624,22.146381336719646,81.88291624737347,-27.69226743473334,291.3815944294785,211.69419501104966,15.388554381251211,-27.162665635447624,31.237979219534115,-0.9831392715807183,15.419967740806143,-21.25506050626369,91.54325794677247,56.84420179249912,9.063958540916119,76.67181959749723,36.62482420917315,-13.169916660460906,-19.397401883934677,194.14251823720713,51.972768891916765,173.95216412169592,60.57600006690703,1042.6031108319257,6.986698186633859,26.878474627125108,33.53623947686627,18.122102190340797,18.06003594661943,25.307784258546846,33.00612835576317,160.09777069733596,123.02157798661621,-26.078694408262464,12.393392623485965,38.85634761436761,211.17362851904647,74.55230838817062,59.309223279370556,36.06313956027018,122.23104806318462,-30.598110368774158,10.912751721708297,8.772978696139774,7.032453633248416,220.8858298774989,50.21989890430193,276.1664440950904,24.16781522601265,1.623622069941284,260.35481185140696,25.494181448865213,263.2811842930131,108.24910898370993,995.295222807088,-157.5369929463983,86.90857510983507,53.04478778693506,185.31918812289882,260.6183828181278,1356.374621938312,1040.5158504652113,-27.162665635447624,543.6907378485112,69.42093070359722,90.8974181829721,3.7259013671198105,128.21962127392032,148.19796177317477,103.43170978877632,9.70321153489202,8.256179428316827,208.69479817077485,26.550393658762673,-27.1158110433782,359.61059236519407,-29.082748162711553,-18.565286619155117,142.6852347992292,19.12783812072169,50.84073841493225,53.22946215590001,58.07109898092655,-11.02968950611988,2.197496282416175,36.20596275019598,-26.657084610447527,8.282934707889446,118.28389041854273,5.4107811658635185,151.28762461660264,-4.874844697207237,4.022625590884388,309.07735615154894,10.270020780176708,45.326410929344775,93.06726009623924,59.49754503772885,15.264501934919249,57.38643095848128,99.2589214867094,-25.71623131874354,59.49754503772885,60.57600006690703,100.68209566938623,90.62488662930663,199.87485858908138,-23.02923025352362,14.384003839742547,75.79324757281034,258.7585716100699,-34.805140670532296,34.56554432446869,-127.27754578051163,37.85867111393476,41.14105105049431,2.455217543941046,49.124608369556,145.0706717560165,-2.9097959938841456,128.62779256345527,20.135208585528318,66.12536008758633,57.69722148863177,29.604843523242927,-22.184575525914525,-50.18111289888468,1530.9353839078194,97.81003764475793,9.536387613988566,176.7577619405111,59.90508940258819,480.9522784309384,123.94218668144788,14.022458193053112,143.19208958228694,22.075694975888748,66.12536008758633,-9.730016027856408,560.4488449496089,28.121644252124575,132.742103242354,14.62478407461699,83.42234736490893,22.740510988926857,22.65751827427136,68.5912823158976,-2.0930879031182172,69.06424273627437,1.0602071675172482,90.11537045357991,85.67511546647793,72.203747001542,5.698735373524094,109.41924190111853,168.1741725905145,40.16018865322661,110.23138676235251,32.61608539792745,4.850243761106697,296.2013479476379,-30.63098366138179,31.624816315904283,45.326410929344775,107.08643470629795,105.58846417611869,126.20656907203963,814.8064775829166,358.8470113303012,-14.36660607790214,-38.394911410319224,13.819908700865753,212.60633146213877,101.03173664783817,2.3473095105517388,57.90051902652066,27.8119171594606,12.538883627556991,35.31232400016225,18.037227579753072,16.720035159804652,57.12199871173417,-29.218681630296672,80.54974880858326,199.6022915473115,23.742926385542845,49.8536832254348,8.53274157937495,342.8122221803343,28.755190902738022,482.5151872335024,28.281979414689616,33.30667577823612,1225.193081940572,638.8288525798987,56.232525272807585,68.9599635782354,5.950245988003061,-12.63183735407128,8.824378887673063,66.72598836560216,292.34435558509966,61.981375392383555,41.90714259073373,13.200714592204193,15.338127581409815,39.568119489421655,12.241645417083319,358.84542157583684,51.30584937469247,15.550523609148623,-1.8155757318112933,263.773684681445,80.59820907103136,0.6746807110402244,42.433569205459754,121.80353602093858,11.44265485686661,11.85664087957771,50.94206253550863,792.3036685277449,28549.787410370907,69.45158853108013,283.7351010760411,39.568119489421655,91.94195263995562,38.11015742485295,4.207801332111217,296.4127788133378,280.0612197711505,183.86598577961036,90.62488662930663,153.77865115427016,1.3079514185847003,96.33573628688742,23.438740370133857,39.68028485186454,5.077082891043631,38.491332009969895,12.42952239917517,136.56311666191206,34.61237730496502,40.97579478807994,33.252740914669104,133.3070247132587,38.011637438337644,15.533886451971085,26.099333612168802,-13.716877270988796,263.766323692976,470.79849064837606,183.20227527102202,142.0536111534916,76.97526786412595,96.1839614768489,363.28803838051965,27.99826235343739,-16.958365990134716,261.26837897872997,166.6628476795429,-1.1065946571502892,39.43247139303555,410.68553653983344,37.19881385500445,-7.488485743674559,51.67552621594215,56.0003685258975,462.38062181472617,313.23178726446054,23.087977829755836,-49.01007166927688,145.50274422783875,79.78779269493992,-13.943887905607795,-44.74022480048434,41.35173639945251,37.136747611283084,0.5259733099233159,27.64456616736451,346.98042730995587,70.6366017301791,639.3256593524363,32.260170831889496,228.29088117015124,74.93067883922649,0.28759091348369026,102.980328730144,99.5259263360804,-47.36938704194691,69.41233065422512,802.6092530290236,9.227201301430611,124.25311467135383,159.30374003370108,48.9623438883916,18.347723607482536,448.0332932984228,284.56493711092963,47.58551406440509,83.58479949326224,10.42303713771868,-113.82657339209224,22.156231385826743,499.91279186654197,113.39816496177214,51.75469847165292,-47.50840472516617,201.95490852386484,18.996992017395357,183.29798267842915,482.1306488698541,4.648343848123368,7.673417430084029,303.72110025466424,333.66009154656103,33.52127858991082,82.61195262125904,242.82319014780376,22.325017026589908,-21.027279637678404,13.844112241188952,239.61061831944392,48.53574021673083,81.37288870999683,-96.07273922059586,20.003238269525866,45.978745247410686,-12.224413946874819,-13.051180443959979,25.370601091023772,3.419839660949897,83.42477620459222,44.4072223034409,-3.195779911520191,201.58735882635307,492.682907095403,23.999379535159427,227.61914554024025,118.28489930542446,-35.691982185936624,9.536387613988566,-61.41862834932343,46.64799505206251,50.461617375120824,-33.98829262269028,-14.936070864523685,42.09744841250887,65.44205211812454,232.07938911678775,12.570991968527466,16.89549646749412,25.771779908945994,-17.86488240527497,22.763860656560247,54.858480180343946,260.9654855397788,-0.7176126446974962,98.15121189339297,423.00168139274376,22.583479481506807,53.476624432340195,-23.738051938142817,-30.31136204796671,105.27140938002864,331.42774876060366,216.90136986444423,-11.046848036246345,80.65921905533467,42.58275125226325,161.6719091140579,53.40932521487446,18.560943152602796,21.688960716024745,-44.58108258366197,-46.650770328880824,378.4509992248198,35.673552572970024,28.94045139795768,11.580070107411913,42.139698070613825,7.033993171294313,18.00800951741324,27.384344641251445,23.438740370133857,10.638472808331088,105.3358987568284,43.69627874878365,-37.236170386057026,67.9261396018568,752.8078426781523,3.0471135597129972,26.69714414909985,90.43013462621272,303.72110025466424,55.82872996801318,45.79591359187431,13.374292581491687,41.01204819487383,44.86793286484828,19.63014850864908,65.60206483302231,31.59935928799557,39.84033068161786,275.82720134148497,12.508225800074094,23.439109295816166,1.066175457913019,64.82514262468862,49.11704679347153,673.1261321402887,27.46809336870584,77.2927210496847,80.65831851556956,46.651737422068024,11.87307742470665,4309.516297647394,22.473989374999206,-14.689722998002274,668.1071912657686,31.77351200289249,3.0471135597129972,136.5294670531792,31.702335800925,767.1493777286362,115.5199523954899,109.54919385313046,249.41639172154822,6.577078353107808,280.0376077518463,11.476304465599462,222.18323557469625,40.10759371161869,-8.003608043063522,666.4091051808873,811.6045503763381,9.661842080896818,0.628748104669242,27.64456616736451,113.52545089083257,148.31227765125476,482.1306488698541,38.13240285015264,70.58445869175333,29.929747779949533,27.449035477393295,185.58774477927506,36.19156678551284,50.53511045412273,2.6374281780227022,-5.266614609711226,27.97022101282667,145.01717021259128,178.56471610780812,166.12990620185826,47.54566400381882,53.967160245838954,165.93752331033343,307.35530123582714,19.53258162306016,42.251488138678944,56.464073009848796,-35.70776550377094,45.73758971815846,287.53971555266656,246.21616263327167,1.6161608892961254,26.696768854722077,93.55050429912424,283.74089699135214,-1.1064461175441807,-16.0244012129031,56.49266538852333,153.75196374170662,164.60360660465147,396.5971954845554,-4.859528718953726,63.02779475353037,144.2490604761223,-84.01840350305831,-1.0373901059460877,-37.51133606843548,550.0517499638818,84.30443794865388,56.01969032219895,22.44869339938645,96.08562913752249,72.41892530926128,97.64961917744279,-10.070578415450868,50.76203165654756,26.031703231448127,113.52545089083257,142.5755249984452,59.26681527759233,484.66408665588233,333.3168148961733,108.98784953126555,100.40119145751274,152.45134025194764,-4.742425953775481,-131.39841023245606,60.09718495053187,28.946106789798222,55.30664612342992,18.0698096926207,17.95105001261539,15.360030140184612,23.732986799357555,-11.59042934321684,27.696989523313658,166.03137441992305,108.90138675272405,151.51323236076058,170.64923207236234,2809.829830917168,57.03824998427978,-8.987979574210485,131.32565163811347,-34.97854802183643,0.7923312661242221,55.636440897478806,78.26217775096924,12.073022079041209,72.23292979911055,-50.88845286000209,88.51814968152351,175.83002997370522,58.73456344403962,195.37517067758967,121.47097760540531,26.130709068763252,28.362799781376793,-20.14057246450035,237.35877976925428,85.45102074258487,48.38383376706347,1.1012314521620041,14.156054678722185,111.94338254726405,20.748045969486643,48.830227962716926,-350.68064356471336,43.39875426403528,81.70584202515491,109.05460161991819,38.0555886751049,75.05462367948033,167.1534169985024,14.55771886622578,170.34152791439996,83.1381369135371,27.75665793128129,53.799099849363515,30.40420937783741,30.334372781920877,95.06978201865876,-16.4045589744889,1.4621151772482364,107.48365305179912,-6.314537236378747,87.00656963281062,1.9665613658820433,537.7242990079426,50.42297958226395,10.890875416119464,68.35236797913979,49.53110600943401,0.6299083714489484,191.28335345028222,24.830336904509462,246.33182956971234,61.18050725216912,230.87263625266576,141.41406545508676,17.073356051499765,23.16954350027092,321.75521491246434,54.73287649170058,204.84901402276665,-16.0244012129031,37.759265372841135,55.70909043933151,34.61237730496502,18.81890377352847,-14.933979361826367,110.31254296526026,22.485846366019445,17.931729803600305,19.105373041299075,29.730332805831914,14.723116413943409,12.187194831349103,172.84031101114266,130.27933515895353,108.32786553705495,-136.10417700681836,59.065673651379385,72.70176755505179,0.21191029508321435,15.053628938772157,60.82607215922177,27.67859107047515,134.96139317147313,-12.705832473260287,90.64299257217007,67.81135775254172,136.64724068374423,67.40157888524739,191.3449032695487,-22.701882443416224,101.3989002416878,296.8234585329297,-1.7341587433009522,45.73758971815846,-141.7250529454418,124.83815221804055,477.85936954160104,41.839843373267996,78.66354421608037,40.34467697714979,7.487228510953081,229.73047084103882,25.694785909696087,4.479374250042902,8.127299221106945,-0.09690773109734607,-3.012050439901728,-51.085122886286044,5.098541327499966,66.98133407046433,77.6619054147109,67.68517171979347,138.23261614991037,35.968572109801535,368.13549307381976,2.2411123388560767,-16.43536958964569,110.54916497734446,33.0853552092219,80.61966639342728,1280.506427830795,316.47838379719855,267.4424141691267,40.66741712897459,71.73116187554301,-4.859528718953726,16.079223425890525,197.5531468020023,73.41744302241045,238.31892975594974,263.766323692976,-15.483015691927257,8.87107627567444,25.303655333506704,49.8536832254348,49.49512935999532,15.533886451971085,28.208667145450136,104.7783257414549,22.84240157191872,83.42234736490893,28.801539443091272,194.84157415699264,-15.845528952495584,-4.787018381730569,134.83090527039673,22.76013992735436,95.51096930219157,412.3785906982192,-3.894978740426339,10.850607275602577,25.919836072616164,130.25912050523237,59.42998869118554,52.32730476167835,22.297350746511253,13.01577406512817,105.10390636540032,91.73940899004933,-6.047292223607911,669.0485459211119,13.049423673861043,126.60306382946501,78.4936770511328,858.1887301191668,15.152336572476358,143.62376168526362,77.47596322249545,162.8575707366623,7.033993171294313,1271.407652776491,-20.34312232353706,70.49214600773917,8.824378887673063,187.41314442970594,157.60269662814883,-7.779418107732106,124.08771187139034,278.28666758701104,224.70338505457642,10.6372919188411,114.19173512267736,156.04336310609807,270.35987529982265,111.14202935771252,523.5930846743274,15.068400197833085,16.530961843032895,37.70699260823021,80.65831851556956,141.41406545508676,86.66112220544743,205.8909384670057,191.46399583492934,173.42192111703457,12.567112615356592,154.27988657345207,17.570980664402185,9.911091376506633,159.39138440591267,2.686906494230186,13.576413230153733,2.541207463889826,27.790944080106925,-5.325481093273154,48.02647993214367,-0.4925123764416952,16.475015480111498,141.76059185487821,-79.39850286152638,38.134220984597704,68.14589130917022,119.06849954925984,18.542722299501634,62.351046108790584,-4.687099556798962,23.264696411158482,26.696768854722077,116.27741591561444,79.26082477108876,-48.190963026889165,8.465942932565461,143.97762241376498,79.57651217325453,51.05366495638485,47.260281504044,30.26699445603378,124.53375683428975,52.38880806383304,83.58479949326224,10.198421998334538,6.775541356658252,59.93996579592293,-59.58756208760555,-1.0929502792598527,24.31624668406059,75.06247314009688,55.31613637640361,103.37722216408929,-6.002133276372561,12.348786470388532,165.2927616609378,108.70703654640886,13.209259315342145,-7.430198882604685,62.62931800768014,151.51323236076058,52.82363649048813,50.924862436764414,1100.5499463775186,-13.159951895988385,35.80710809744606,606.0190076110575,204.3885971847514,160.85688685938942,116.4063392295347,39.43273306732483,-17.716119049714294,64.52752880777228,41.95443757539417,45.00600543774633,29.871531486552463,-37.3382344294897,33.38016902060109,30.457675572186673,18.337450859186227,35.013120182710594,106.38165882102436,14.7233040611323,150.7466143900046,63.65299594645495,609.7843693182726,101.90983571683489,30.38830799167031,-7.347926835876642,34.54854075239324,8.11901188567019,567.4007397525469,254.8360707370079,64.52752880777228,-4.380535584509808,2132.711352881007,54.52013759743687,2.6659268632680266,27.851199552327472,80.29410369857001,239.60126139151646,42.824094428704356,294.8540555697594,10.418958277386338,-24.31897339224541,-5.941273967489948,74.3146395918622,173.08026021379476,-33.766647306406014,4.6845041224935144,31.595130027590233,128.09635231319965,433.12871025529495,197.5299444956193,49.34202458711397,39.44950057098629,47.681319826130554,-35.79153377567371,37.65264751712739,1356.374621938312,137.24342807983032],"xaxis":"x","y":[74.33,21.29,69.48,25.53,5.45,4.54,191.97,66.44,13.95,108.04,265.21,72.16,29.57,16.01,133.5,324.25,281.25,53.02,33.09,263.51,4.56,37.34,233.43,20.85,9.83,46.7,6.07,49.22,466.95,3.88,45.83,11.84,1936.34,193.94,9.76,3.29,10.7,111.95,21.24,107.56,14.24,17.04,5.81,26.94,2.82,27.77,7.63,48.1,52.43,33.04,50.29,6.25,3.34,43.19,64.23,77.76,84.75,4.41,16.8,282.72,82.72,60.57,89.29,51.1,8.14,35.4,25.32,64.59,1.6,79.74,159.98,42.8,5.62,13.29,4.53,6.79,3.15,75.03,6.05,500.93,96.43,58.79,39.48,4.99,26.46,65.02,5.94,6.18,102.27,15.2,95.77,6.17,3.95,61.98,33.52,1148.5,37.82,79.98,138.68,34.48,4,3.35,4.39,4.78,105.01,126.03,56.48,25.9,570.98,18.33,387.49,16.44,8.65,13.87,29.78,1.08,12.29,1638.46,89.57,54.25,12.72,24.39,192.12,7.44,193.34,45.44,17.43,38.39,60.5,122.73,10.65,10.37,25.22,23.58,4.86,116.21,25.21,6.3,331.61,11.8,15.26,7.9,23.85,138.79,4.6,47.26,304.62,23.84,4.89,30.79,12.25,13.23,59.73,17.22,14.35,29.41,10.66,25.14,46.46,27.47,69.02,52.87,76.68,4.75,22.46,5.07,8.65,5.99,45.33,52.49,54.24,10.84,20.22,3.4,2937.83,170.01,30.26,82.58,229.2,569.61,2061.48,145.67,15.1,8.49,53.31,250.87,20.29,119.98,2.93,22.37,0.17,17.02,120.1,1.55,4.13,51.93,8.42,5.43,602.12,4.6,61.33,16.01,4.43,42.92,72.78,47.12,97.39,321.72,39.27,3.63,12.99,350.87,96.2,13.36,69.18,4.82,60.98,17.23,1.73,5.53,7.06,9.57,380.17,10.65,18.88,9.45,70.31,41.8,157.77,1.63,56.07,64.13,73.9,45.79,106.37,22.97,61.15,53.88,227.18,1.97,127.74,1266.06,29.93,33.28,22.38,249.46,6.12,4.44,4.55,4.2,5.28,293.56,30.19,515.71,3.52,5.67,46.79,11824.89,51.83,48.95,188.77,64.56,38.97,13.39,13.56,34.61,53.87,9.04,16.99,280.16,117.64,3.75,30.57,8.86,467.9,56.07,17.24,272.54,40.65,22.78,32.62,6.66,6.7,103.42,114.33,185.33,239.17,67.17,19.95,15.9,5.49,4.68,23.62,38.88,2.99,5.53,61.88,6.38,27.92,23.15,89.29,14.83,73.14,118.2,42.8,3.7,12.68,66.43,16.89,8.25,29.33,21.3,7.48,65.46,10.21,190.61,175.3,49.58,5.64,3.14,229.84,4.2,62.41,147.14,5.82,30.35,6.41,134.8,175.46,67.57,17.12,309.6,4.5,73.61,28.19,6.89,9.76,60.12,4.92,132.1,72.5,30.69,73.31,105.97,23.12,35.6,16.14,103.99,3.48,367.3,3.54,5.8,24.56,100.02,93.21,17.49,30.86,41.9,54.04,40.78,32.89,4.38,4.32,193.47,35.11,58.59,84.93,0,33.45,6.71,458.44,12.28,168.36,75,398.19,5.46,39.41,166.84,3.59,7.07,3.06,262.98,144.57,5.22,358.88,4.73,105.32,23.5,16.83,4.64,1.06,266.05,10.83,11.81,21.44,78.36,11.17,27.27,127.27,295.42,457.51,198.59,61.18,110.74,14.97,4.11,197.3,5.96,25.11,9.95,505.01,71.87,22.66,17.92,35.43,44.15,93.63,109.9,278.47,80.09,452.16,33.17,300.14,7.36,126.21,178.21,119.2,77.75,35.48,46.26,8.42,6.22,5.28,84.95,6.41,31.27,29.66,48.95,72.5,3.21,141.15,31.13,4.25,3.49,52,3.96,125.59,14.7,19.57,143.33,804.2,9.25,375.56,14.37,16.18,78.29,29.97,3.93,22.93,32.05,126.92,5.74,105.41,7.69,9.79,34.66,4.08,5.59,26.16,143.82,6,11.87,20.37,422.39,4.55,11.29,181.67,9.43,5.71,21.78,2.15,7.57,229.12,4.08,41.16,24.25,28.3,155.54,110.62,40.56,18.3,79.21,10.9,4.85,113.97,8.36,91.53,9.28,47.5,3.63,20.06,168.62,49.28,51.4,11.27,15.5,53.65,35.2,8.06,4,27.93,286.43,91.09,85.28,49.89,16.72,18.74,19.11,119.46,28.94,8.75,6.29,94.82,35.72,33.37,18.76,19.69,402.04,328.23,152.28,5.23,86.9,158.55,49.63,23.42,10.01,607.41,235.69,12.05,7.21,62.23,39.5,1265.29,290.4,45.33,463.12,70.84,93.83,53.71,9.53,184.09,28.37,13.1,48.99,18.47,5.15,6.37,31.15,35.8,17.14,379.34,161.39,127.56,6.9,84.59,4.33,351.9,27.99,124.54,30.1,59.14,21.57,25.12,6.01,7.97,78.92,13.71,1.55,72.9,70.16,4.04,7.47,10.02,75.99,77.04,11.18,107.16,38.13,39.57,227.13,4.43,98.88,104.61,20.02,5.63,130.28,138.35,8.08,1.08,6.96,16.46,5.94,64.79,9.67,45.38,30.19,5.79,32.42,191.97,636.96,30.02,1.73,85.59,8.61,199.48,70.17,3.69,41.74,33.12,39.73,4.35,180.14,20.5,74.98,23.63,23.9,5.79,334.15,132.32,10.29,9.39,4.14,553.27,28.63,249.98,41.96,147.36,128.91,0.78,209.92,7.34,22.15,180.89,4.12,9.43,1.59,549.74,25.93,523.95,128.22,124.77,10.48,5.53,63.77,1623.34,39.1,232.12,9.56,6.22,4.2,36.42,13.27,16.47,4.24,48.6,109.16,47.14,11.1,19.61,6.16,95.26,6.15,24.12,40.46,28.6,192.51,159.16,28.55,44.26,4.02,47.25,60.88,75.95,115.34,7.13,38.38,216.18,5.69,60.08,4.86,122.92,11.59,13.18,2.99,51.47,117.19,13.34,63.41,178.86,4.09,100.71,13.12,1874.42,11140.56,8.69,1060.13,64.06,13.71,75.58,27.38,158.48,213.9,20.09,39.57,53.48,6.04,54.76,4.19,4.39,3.44,21.71,9.16,9.82,3.65,6.08,34.03,28.21,27.63,3.83,45.07,13.21,218.49,1789.69,44.74,14.67,94.23,5.77,31.76,18.42,134.38,69.44,40.35,14.73,25.15,752.57,17.39,56.86,36.97,33.35,177.51,7.21,113.3,6.35,150.23,8.37,3.16,21.51,38.59,6.51,5.42,5.6,248.15,477.85,691.57,23.79,174.79,68.05,64.86,191.2,50.52,5.46,6.72,1699.45,5.07,35.8,376.46,21.12,58.88,348.21,29.53,0.92,9.67,13.18,92.21,4.8,426.39,20.66,5.95,48.34,71.03,59.84,234.75,723.4,133.7,158.6,250.05,69.42,25.39,2.79,298.61,94.02,39.67,5.67,1597.56,11.38,35.14,1019.53,10.04,2.69,63.37,4.31,6.73,20.76,86.81,28.05,49.6,110.41,27.33,8.64,61.86,59.76,60.67,1.78,11.97,14.49,43.15,7.52,12.62,6.26,225.35,675.34,5.54,3.78,17.11,84.83,119.01,11.54,393.2,22.74,40.25,46.52,34.03,5.84,3.54,51.89,125.13,106.74,21.38,7.39,33.58,3.27,18.91,15.7,54.02,6.86,22.81,17.08,1894.2,20.33,44.93,22.96,13.29,14.24,23.22,11.27,4.46,26.63,2.72,5.68,0.09,9.56,181.93,40.69,3.47,101.34,259.49,9.86,6.87,10.24,171.64,5.87,49.53,469.57,83.68,9.95,285.62,3.43,12.36,36.08,6.43,14.09,377.09,27.14,23.03,4.99,32.77,238.77,3243.48,93.31,5.94,455.39,67.24,29.62,9.96,5.91,234.15,39.44,314.92,154.96,5.46,86.99,20.09,260.13,6.67,39.57,147.82,188.13,5.03,9.73,5.79,45.6,21.92,740.97,59.05,120.84,47.92,108.79,84.64,6.35,15.43,9.72,12.71,26.21,47.24,22.17,68.3,46.6,6.85,2.85,84.18,12.92,15.34,18.1,4.84,12.75,34.14,47.71,129.42,45.02,7.85,202.65,4.45,4.31,148.25,15.09,123.18,2452.86,38.86,15.9,21.77,301.02,5.45,17.75,47.99,16.3,6.74,11.94,176.79,4.74,51.98,4.16,5.9,64.57,46.78,313.83,179.33,1088.2,237.36,319.73,161.2,19.39,85.73,273.91,42.18,4.62,6.33,67.45,15.37,47.64,4.89,24.89,5.6,4.19,254.16,136.94,221.51,834.96,1.14,8.16,202.99,9.71,71.55,59.07,9.36,138.61,18.9,481.97,42.99,134.01,54.41,21.44,50.46,85.95,33.92,7.69,98.95,99.96,78.7,0,15.6,51.39,3.98,25.48,58.28,62.17,88.51,9.44,24.51,36.78,88.74,5.65,215,32.48,92.45,22.53,9.11,25.38,45.32,128.39,9.46,69.59,3.28,82.07,8.45,94.59,65.35,86.86,103.77,1.85,3.02,434.48,5,133.8,8.8,587.16,214.65,15.11,3.5,433.6,75.48,19.07,4.26,4.19,41.96,3.76,5.97,10.23,61.62,4.82,28.61,86.9,36.72,16.89,9.62,103.39,303.69,37.51,225.66,7.6,92.19,3.67,38.85,9.29,23.78,14.86,3.41,176.75,59.74,6.32,111.04,30.03,17.57,14.04,399.73,3.77,13.09,87.07,154.6,28.88,6.3,60.52,6.73,13.35,65.05,30.78,103.4,231.39,4.92,0,565.79,103.07,9.57,21.05,18.86,99.39,5.91,206.01,6.03,9.15,10.6,8.63,106.73,509.36,5.44,193.72,37.74,125.42,38.56,3.05,132.71,148.7,130.73,218.53,4.52,4.51,8.28,16.32,68.14,3.56,78.06,66.23,70.39,15.43,178.65,355.05,0.92,3.04,284.01,61.07,6.98,67.78,18.65,11.62,7.73,319.88,7.49,38.67,7.85,5.04,11.39,33.8,19.7,359.09,3.3,55.69,117.11,1990.5,4.51,44.96,0,95.54,14.3,481.06,31.47,2.36,62.23,205.52,54.4,10.95,28.78,107.84,222.02,124.96,54.53,117.04,21.08,27.02,142.16,4.15,51.24,6.18,3,210.3,1.24,138.76,39.75,21.51,71.23,46.3,49.46,9.4,85.76,7.74,8.16,101.31,5.35,4.13,47.08,4.48,8.36,74.07,235.32,12.63,7.74,86.46,15.97,16.58,3.99,7.56,48.35,14.65,33.43,35.39,44.55,37.49,7.78,25.12,82.9,6.31,1.67,4.15,10.38,14.3,137.15,1.68,120.91,130.69,79.58,9.73,34.2,38.78,62.98,8.47,25.2,12.05,77.34,0.31,53.12,127.14,243.13,6.09,567.6,4.43,52.67,139.72,80.75,1185.15,63.33,30.95,4.65,303.81,6.25,8.85,23.95,5.17,41.22,3.81,1.86,72.5,51.28,41.3,37.56,22.1,36.09,49.17,6.71,3.5,67.06,4.41,394.6,38.41,271.68,138.03,1129.67,2.34,12.26,20.86,87.14,238.61,43.97,107.83,21.66,5.88,17.45,16.63,103.57,23.8,48.16,45.58,10.97,10.29,28.09,26.56,40.05,9.63,5.02,5.82,1596.08,293.55],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Lasso()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predLasso"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression lasso\n","# réglage des paramètre pour la gridsearch\n","alphaslasso = np.linspace(0.1, 1, 5)\n","param_gridLasso = {'lasso__alpha': alphaslasso}\n","\n","GridLasso, \\\n","BestParametresLasso, \\\n","ScoresLasso, \\\n","TotalGHGEmissions_predLasso, \\\n","figLasso = reg_modelGrid(model=Lasso(),\n","                            scaler=RobustScaler(quantile_range=(10, 90)),\n","                            X_train=BEBM_train,\n","                            X_test=BEBM_test,\n","                            y_train=TotalGHGEmissions_train,\n","                            y_test=TotalGHGEmissions_test,\n","                            y_test_name='TotalGHGEmissions_test',\n","                            y_pred_name='TotalGHGEmissions_predLasso',\n","                            score=score,\n","                            param_grid=param_gridLasso)\n","\n","print(BestParametresLasso)\n","print(ScoresLasso)\n","figLasso.show()\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[386.008883004169,385.95613426868715,385.9063969186644,385.87055792151233,385.84505389214326]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[463.5707401363162,463.9015995948705,464.24258948353713,464.58991167669893,464.94223564725786]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[308.44702587202175,308.0106689425038,307.5702043537917,307.1512041663257,306.74787213702865]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[385.2858980338354,384.98205547148666,384.66477905100277,384.36834918212793,384.0819080249847],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[387.90853262809634,387.80562570917203,387.7033327525289,387.6101691285135,387.5223375504507],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[384.71160848641915,385.2678525159772,385.8380363001807,386.40730429280745,386.98601547965313],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[508.69338383255655,509.0953102838749,509.5134360022618,509.93791823630625,510.3655758262804],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[263.44499203993746,262.62982736292525,261.8124004873477,261.02904876780633,260.2694325793472],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Lasso en fonction de alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Lasso pour les paramètres de GridSearchCV\n","FigRMSEGRidLasso = visuRMSEGrid(Lasso(), 'Lasso', alphaslasso, 'alpha',\n","                                GridLasso, None, None)\n","FigRMSEGRidLasso.show()\n","if write_data is True:\n","    FigRMSEGRidLasso.write_image('./Figures/EmissionsGraphRMSELasso.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.1.4 Modèle ElasticNet"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.020e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.029e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.005e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.447e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.974e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.502e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.969e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.406e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.962e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.945e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.464e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.942e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.440e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.380e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.942e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.922e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.366e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.355e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.843e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.020e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.912e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.417e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.030e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.502e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.868e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.964e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.305e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.005e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.400e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.955e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.448e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.978e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.932e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.965e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.433e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.458e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.939e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.836e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.373e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.937e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.348e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.021e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.300e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.903e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.503e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.411e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.392e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.006e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.919e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.361e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.910e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.971e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.451e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.030e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.893e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.894e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.364e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.889e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.958e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.356e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.448e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.340e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.948e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.864e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.425e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.859e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.404e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.932e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.954e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.930e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.951e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.828e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.030e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.021e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.938e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.416e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.940e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.819e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.331e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.448e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.923e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.294e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.923e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.007e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.396e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.904e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.504e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.288e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.383e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.353e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.881e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.350e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.022e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.443e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.884e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.962e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.505e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.370e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.853e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.894e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.877e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.885e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.007e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.344e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.866e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.432e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.847e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.951e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.340e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.023e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.031e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.321e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.927e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.809e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.404e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.387e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.008e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.941e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.031e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.913e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.449e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.281e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.914e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.928e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.912e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.921e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.389e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.449e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.308e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.798e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.505e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.024e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.903e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.376e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.273e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.354e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.936e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.008e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.883e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.336e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.871e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.417e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.861e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.506e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.849e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.032e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.332e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.832e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.839e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.871e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.323e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.302e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.450e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.397e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.863e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.829e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.328e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.371e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.894e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.831e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.912e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.293e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.032e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.025e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.890e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.883e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.363e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.785e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.865e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.450e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.870e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.009e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.890e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.347e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.264e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.275e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.770e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.507e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.863e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.821e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.875e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.347e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.889e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.254e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.025e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.794e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.508e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.303e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.854e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.857e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.318e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.276e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.010e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.264e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.822e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.804e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.371e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.852e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.033e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.744e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.335e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.840e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.307e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.860e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.844e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.451e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.774e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.026e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.242e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.840e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.760e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.812e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.254e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.836e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.033e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.819e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.317e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.011e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.328e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.800e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.451e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.277e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.675e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.227e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.233e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.857e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.735e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.803e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.208e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.509e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.834e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.211e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.305e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.285e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.679e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.799e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.819e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.028e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.295e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.832e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.012e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.198e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.737e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.130e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.800e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.593e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.510e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.793e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.819e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.034e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.214e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.140e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.281e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.691e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.787e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.723e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.452e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.225e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.746e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.029e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.195e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.713e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.806e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.762e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.034e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.276e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.680e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.013e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.194e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.674e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.155e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.551e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.368e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.453e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.154e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.688e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.699e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.770e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.241e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.017e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.175e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.511e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.030e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.513e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.474e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.803e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.761e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.110e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.264e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.612e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.014e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.062e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.443e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.771e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.634e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.847e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.310e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.035e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.785e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.245e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.721e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.561e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.956e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.955e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.031e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.453e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.753e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.089e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.561e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.036e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.574e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.059e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.658e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.015e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.381e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.103e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.454e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.620e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.432e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.726e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.152e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.925e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.652e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.196e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.514e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.038e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.510e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.623e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.668e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.585e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.180e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.032e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.138e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.076e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.671e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.515e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.764e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.720e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.016e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.223e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.171e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.805e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.756e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.732e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.036e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.345e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.736e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.607e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.739e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.103e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.197e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.589e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.227e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.455e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.345e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.708e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.564e+07, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.034e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.356e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.733e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.954e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.037e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.582e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.658e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.922e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.017e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.452e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.594e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.064e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.096e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.456e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.538e+06, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.844e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.134e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.533e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.495e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.517e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.967e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.035e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.233e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.256e+07, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.059e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.524e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.519e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.709e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.059e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.018e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.274e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.731e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.166e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.182e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.790e+06, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.679e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.038e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.179e+07, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.414e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.673e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.385e+07, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.129e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.378e+07, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.456e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.645e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.085e+07, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.961e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.459e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.037e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.475e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.039e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.030e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.696e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.412e+06, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.806e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.602e+07, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.020e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.016e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.361e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.457e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.838e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.383e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.313e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.494e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.179e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.404e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.520e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.039e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.662e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.267e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.963e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.522e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.943e+07, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.021e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.658e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.630e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.085e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.040e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.065e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.605e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.458e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.236e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.031e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.578e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.041e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.522e+07, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.216e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.556e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.465e+05, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.041e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.923e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.459e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.419e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.317e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.898e+07, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.023e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.900e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.302e+07, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.524e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.210e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.558e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.075e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.043e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.822e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.526e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.513e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.785e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.965e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.025e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.388e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.496e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.616e+07, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.433e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.042e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.434e+06, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.883e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.423e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.277e+07, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.461e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.033e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.045e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.076e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.043e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.580e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.026e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.725e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.462e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.907e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.376e+07, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.772e+07, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.528e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.047e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.604e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.302e+07, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.332e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.781e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.028e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.531e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.332e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.566e+07, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.044e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.203e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.652e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.463e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.217e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.050e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.046e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.691e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.030e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.088e+06, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.464e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.450e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.413e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.253e+08, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.533e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.052e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.036e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.536e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.486e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.072e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.032e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.817e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.270e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.047e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.884e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.055e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.053e+08, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.035e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.048e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.984e+07, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.017e+07, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.058e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.735e+07, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.538e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.526e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.541e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.985e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.037e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.639e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.136e+08, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.050e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.603e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.469e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.316e+08, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.061e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.051e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.244e+07, tolerance: 8.776e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.040e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.471e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.862e+07, tolerance: 8.421e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.300e+07, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.545e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.548e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.064e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.095e+08, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.929e+07, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.197e+06, tolerance: 9.395e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.042e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.592e+07, tolerance: 1.120e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.067e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.055e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.738e+07, tolerance: 9.193e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.053e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.475e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.045e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.473e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.551e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.071e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.555e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.048e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.059e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.057e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.074e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.477e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.051e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.559e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.078e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.563e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.055e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.061e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.083e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.063e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.058e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.484e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.087e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.567e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.572e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.062e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.066e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.068e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.092e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.486e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.489e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.066e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.582e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.097e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.577e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.070e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.073e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.102e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.071e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.495e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.075e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.492e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.107e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.587e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.592e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.079e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.076e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.079e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.113e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.498e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.501e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.598e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.118e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.604e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.089e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.086e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.082e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.125e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.508e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.505e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.094e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.131e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.617e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.100e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.093e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.138e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.089e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.516e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.105e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.512e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.145e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.624e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.631e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.111e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.100e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.096e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.152e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.525e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.520e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.117e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.638e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.159e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.646e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.124e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.108e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.104e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.167e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.534e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.529e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.654e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.130e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.175e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.662e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.137e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.117e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.113e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.183e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.544e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.539e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.192e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.679e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.144e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.671e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.152e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.122e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.126e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.549e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.555e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.201e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.698e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.688e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.167e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.159e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.131e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.136e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.567e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.561e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.219e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.228e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.717e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.707e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.175e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.183e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.147e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.142e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.238e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.579e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.738e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.248e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.573e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.192e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.728e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.200e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.153e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.158e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.258e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.586e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.593e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.269e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.760e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.749e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.218e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.164e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.279e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.608e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.600e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.228e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.290e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.771e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.783e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.237e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.177e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.183e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.301e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.615e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.623e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.313e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.807e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.247e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.795e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.257e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.197e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.190e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.324e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.640e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.832e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.631e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.336e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.267e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.820e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.277e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.211e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.204e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.348e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.658e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.360e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.859e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.288e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.846e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.298e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.226e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.372e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.218e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.384e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.677e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.887e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.309e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.667e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.320e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.873e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.242e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.234e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.397e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.697e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.686e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.916e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.410e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.902e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.331e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.343e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.259e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.250e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.423e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.436e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.947e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.718e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.707e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.355e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.367e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.932e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.277e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.268e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.449e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.742e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.730e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.379e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.980e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.463e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.963e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.391e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.296e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.477e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.286e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.766e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.491e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.404e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.014e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.416e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.997e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.316e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.306e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.505e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.519e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.793e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.050e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.429e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.779e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.443e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.337e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.326e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.534e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.807e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.821e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.549e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.087e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.456e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.470e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.068e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.360e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.348e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.564e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.580e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.851e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.127e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.836e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.107e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.484e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.499e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.384e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.372e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.595e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.883e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.513e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.866e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.168e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.147e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.528e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.409e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.627e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.396e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.644e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.543e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.212e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.559e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.190e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.436e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.660e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.422e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.952e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.574e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.257e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.677e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.934e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.234e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.590e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.694e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.464e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.712e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.304e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.606e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.990e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.450e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.623e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.494e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.971e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.280e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.729e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.479e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.029e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.640e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.009e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.352e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.747e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.328e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.657e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.524e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.765e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.070e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.509e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.674e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.784e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.402e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.377e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.049e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.692e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.557e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.540e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.802e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.092e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.113e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.821e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.453e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.710e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.728e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.428e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.590e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.573e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.840e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.158e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.135e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.859e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.506e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.746e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.479e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.764e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.624e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.607e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.204e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.878e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.898e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.559e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.783e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.181e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.532e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.802e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.660e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.642e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.251e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.918e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.613e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.937e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.227e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.586e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.821e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.840e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.678e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.696e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.275e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.299e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.957e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.667e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.977e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.640e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.859e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.878e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.732e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.714e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.017e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.347e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.721e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.997e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.323e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.694e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.898e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.751e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.769e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.372e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.396e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.037e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.775e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.057e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.748e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.937e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.956e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.788e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.806e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.421e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.077e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.097e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.828e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.446e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.995e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.976e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.802e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.843e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.825e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.495e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.117e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.470e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.136e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.881e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.855e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.015e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.034e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.880e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.861e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.175e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.156e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.543e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.932e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.519e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.072e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.053e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.906e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.898e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.916e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.567e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.194e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.213e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.982e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.590e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.109e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.091e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.957e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.951e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.933e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.232e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.637e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.250e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.614e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.030e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.128e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.146e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.006e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.968e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.985e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.268e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.660e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.682e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.286e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.077e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.054e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.181e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.164e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.018e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.002e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.303e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.320e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.121e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.725e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.704e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.215e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.198e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.099e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.050e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.034e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.337e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.767e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.353e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.746e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.164e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.232e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.143e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.248e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.369e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.080e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.065e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.264e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.384e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.787e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.204e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.806e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.279e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.108e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.184e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.399e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.094e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.844e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.294e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.241e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.825e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.223e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.308e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.427e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.135e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.122e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.322e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.879e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.441e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.276e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.862e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.259e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.336e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.454e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.160e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.147e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.349e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.309e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.911e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.895e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.361e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.293e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.172e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.183e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.479e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.927e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.339e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.942e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.374e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.501e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.490e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.324e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.396e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.512e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.385e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.204e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.224e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.407e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.996e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.194e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.970e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.522e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.418e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.532e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.214e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.380e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.353e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.956e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.392e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.427e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.983e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.233e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.367e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.541e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.008e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.559e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.550e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.454e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.437e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.446e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.242e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.258e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.567e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.574e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.250e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.019e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.040e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.470e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.463e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.030e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.582e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.404e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.415e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.273e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.426e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.266e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.478e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.286e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.050e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.060e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.280e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.077e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.472e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.436e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.589e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.446e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.068e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.595e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.464e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.485e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.455e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.492e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.292e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.601e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.085e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.613e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.607e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.498e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.509e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.623e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.504e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.618e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.298e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.309e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.520e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.303e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.515e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.106e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.627e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.093e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.318e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.313e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.100e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.494e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.525e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.480e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.119e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.327e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.113e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.487e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.322e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.507e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.524e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.130e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.632e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.501e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.125e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.518e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.529e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.513e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.636e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.640e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.533e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.643e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.330e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.537e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.541e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.135e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.650e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.334e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.647e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.337e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.653e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.548e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.140e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.545e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.144e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.551e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.656e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.344e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.529e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.533e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.341e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.554e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.346e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.148e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.152e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.349e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.352e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.156e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.556e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.163e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.542e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.160e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.538e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.545e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.659e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.549e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.557e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.552e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.661e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.664e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.559e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.354e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.668e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.562e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.666e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.564e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.356e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.672e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.166e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.566e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.670e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.358e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.570e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.169e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.360e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.673e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.568e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.364e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.171e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.559e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.174e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.572e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.179e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.561e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.362e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.564e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.365e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.569e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.367e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.176e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.183e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.181e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.575e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.675e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.676e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.566e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.573e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.571e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.573e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.575e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.368e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.678e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.184e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.679e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.576e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.578e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.680e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.681e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.369e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.682e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.371e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.579e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.186e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.580e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.188e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.581e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.372e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.683e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.578e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.373e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.374e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.577e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.582e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.189e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.191e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.376e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.192e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.375e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.580e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.586e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.193e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.194e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.581e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.684e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.583e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.584e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.585e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.583e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.685e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.686e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.584e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.377e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.585e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.688e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.687e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.688e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.377e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.586e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.195e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.587e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.586e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.379e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.689e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.196e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.379e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.198e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.378e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.689e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.588e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.587e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.589e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.199e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.197e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.380e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.588e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.590e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.200e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.380e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.381e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.588e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.690e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.593e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.200e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.591e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.201e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.591e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.589e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.691e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.592e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.691e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.589e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.381e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.691e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.692e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.692e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.590e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.590e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.202e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.591e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.382e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.693e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.591e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.382e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.383e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.592e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.202e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.693e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.383e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.204e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.383e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.203e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.592e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.593e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.595e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.203e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.204e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.594e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.384e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.594e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.384e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.693e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.595e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.205e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.596e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.205e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.592e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.596e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.596e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.694e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.694e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.592e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.694e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.384e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.593e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.593e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.205e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.695e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.594e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.385e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.694e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.384e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.385e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.207e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.598e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.593e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.206e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.206e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.594e+08, tolerance: 9.193e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.695e+08, tolerance: 9.395e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.385e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.597e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.385e+08, tolerance: 8.776e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.207e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.597e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.598e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.206e+08, tolerance: 8.421e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.597e+08, tolerance: 1.120e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"name":"stdout","output_type":"stream","text":["              paramètre  ElasticNet()\n","0     elasticnet__alpha     19.116441\n","1  elasticnet__l1_ratio      1.000000\n","      ElasticNet()\n","R²        0.183257\n","RMSE    460.929134\n","MAE     108.819280\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predEN=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[38.04669334869829,88.00015621408377,197.0613387684212,33.959985420675,28.606044717276177,58.806693355339846,375.6973538804863,43.64514024733333,90.55441273284544,31.34315673797255,261.54885734992365,72.5461756556785,25.786569564628408,102.44473066982619,16.84596227431669,1310.496306632162,64.8453389227016,23.313345746516326,23.68621596523551,72.7158190111754,12.188093934242659,12.742080512528695,122.36058354374353,-19.134970835435666,20.304035376981545,3.8304610555913285,7.454579066927735,-35.617422714183164,165.03943968732776,9.986202543775235,25.5495723358799,94.96194865424383,53.509795992046065,56.59143036150226,64.29914494748436,7.558679657482557,45.07015614003014,24.689415902285916,-26.022546741886615,15.288809942204665,7.671741317739105,10.257540173459546,17.592902005251567,51.91512960484906,72.91254356563223,158.4408285742804,30.07538556529171,50.627619238814724,97.39699862962219,21.244966966245627,259.6211767524122,41.87289354534616,32.84962052143973,15.136627081246544,192.06683474912228,11.20450902363639,191.36677910363667,23.06837881596047,17.052556309809752,211.12102731157455,99.66104802132823,63.76498447888349,139.76073681496695,28.6206381601279,101.54779036418799,35.445327804829745,69.89849310784942,135.8299752924446,0.8357351520801899,33.18929212485759,23.600250064348756,130.48543050673862,7.031058576285311,34.98977835458034,-5.91496225840956,34.67817368625364,-12.528199700730063,32.85034295762519,25.29192480100599,1297.794245877048,248.50531565579175,52.98591256806311,22.775355502329855,12.409961885525071,182.36096136393112,22.84225549544736,8.236102261054022,26.546461609130457,21.979812062561923,25.567973333200985,358.27971331785045,-25.134040499815427,20.562178680273554,72.63508694014975,26.7456989608734,1487.803509408358,-4.2840685190716385,-1.832043762257669,17.731467207865016,8.070356232304057,10.732566812644961,10.521837336706362,-3.2071406727308585,15.097531767873534,87.13854767727577,178.39955920764368,232.964380710585,41.78761067750554,89.56888675014343,36.221679361125695,368.28613098587493,55.24665915054595,53.96247748106337,149.56809179790912,71.53273575264836,7.935551858337732,32.05491197774923,1154.6879169936117,15.775901729412851,21.10818263619391,83.7938068946861,45.118218752563145,47.075598745754704,29.01259042648299,159.85063311316037,47.54200398164154,12.794387062794485,38.00570331938958,36.41382775021477,10.055918723467727,53.337822163077334,40.23609071038501,13.971626067818704,48.130840908149196,19.19978049671639,30.127205492909297,20.842964002307795,23.649227918062795,256.128094315393,94.96194865424383,162.67927752947222,44.19160493857333,23.45938372434771,666.671367835183,-0.9297773380017844,47.54200398164154,116.599778379471,205.8900308087733,17.260783378247943,66.35450653543256,95.1791109050552,90.05413559607302,-2.5824853547448257,31.456601471563086,17.191067198555444,94.55678515099595,8.128452754233997,21.021491675065803,122.88108649651764,9.407708892928255,99.82831094778626,138.56550392216505,115.16933908651863,5.830993567704667,27.558235626406315,-7.400022465568007,80.0029384668684,32.92263951035542,50.40385136955696,317.2736624135521,20.554164968539677,88.19519793295775,45.49058762802492,12.120241381117651,1066.1720053083454,100.02311240797857,70.54674903462673,21.794105574147466,168.69087575576046,1469.734349551079,1449.6716539620636,270.00920784897664,105.17705412602618,60.760691636193656,29.572266354615767,44.862421923698406,159.79571857028196,62.30407773420728,19.02877991304407,26.981244086407894,20.531557813954073,12.794387062794485,61.13626053420445,3.662276658493994,-32.907983891937235,85.8001647415917,92.00867351536104,21.190136807482784,467.41220291433183,21.257984158568135,15.003313717659744,8.02836181526402,23.764670916903242,15.097531767873534,80.17253095725323,46.94699628467859,564.4481633056502,2000.6597848576246,192.09935033137748,5.085455839370468,40.25610662621175,227.19460079071916,210.28259021398594,-9.470724512502713,145.71912682312694,-0.9181027878553181,112.89868407636621,116.63727184124667,48.819580032677734,21.738496214658483,3.65710502066365,-7.778610824035539,49.18797778647938,-0.6736334938774178,-6.741835218649676,56.74050186908278,-17.301674362066166,158.58127896782042,60.39521485941012,3.662276658493994,15.052778194021975,18.1935420753645,86.4328337713116,26.6123855972866,117.76043901671216,21.021491675065803,12.944649320488352,87.61765164007862,140.72224754979584,35.63046109603414,58.552830688320135,51.709257987666206,42.77738682739858,35.3805495284426,36.638594233321726,148.0782959329092,23.252104013877364,-17.522626044748932,16.13628577148061,367.82917344233806,37.61894857819338,315.51564987435677,75.83759789662828,0.1722232663070855,-37.55539274256806,41.591647487992276,44.10870340931662,3739.0834537594756,28.740305438830838,17.660262733688718,96.9969258504664,130.82510211015648,41.38626249345763,138.36254612725998,76.33388040342328,-4.565775311745178,94.21199368440486,23.95262039468263,20.26587844474831,96.61783088675125,124.11027486580221,28.193840747590833,113.83617956332202,136.50266110617684,698.1162326145983,87.22269059930512,95.91867852350086,84.29172858020316,98.39145979469735,50.30822522605567,84.70179634643739,19.494698526282697,39.76240922055718,21.261747703110977,74.17084115863459,31.58192701074456,92.62626247958104,178.3689383413242,-7.778610824035539,31.638457840872835,11.88423552026353,20.039268501587003,8.699178800389106,12.680838779889733,-4.29113418574542,43.48166675274668,17.434139413175615,-0.9944504733227433,24.08639449358908,102.13852200663135,81.12554135770004,136.34438513674908,88.31204902508571,533.6391067076602,62.4119677566335,-5.287951489168201,10.284000838841905,1.5786496554816551,37.490677045881384,83.24403939422291,18.902131389556843,23.346808686739358,18.242950897147015,132.68967214642174,13.70310462470939,-88.145693125209,281.8505176070694,15.097531767873534,23.74419585065399,10.59672515422919,165.39893058715577,12.57107992885637,216.05001354371487,415.929154838586,-0.9944504733227433,12.666706072357663,32.02615994003689,30.65905085396903,435.89372689739866,91.44994494519611,54.1402189447049,188.19929002127745,27.30819107558117,-7.118137114571134,49.8591934892755,45.83925667065359,160.87169905865488,47.473670007907984,24.410986031507022,39.918355626058144,81.12554135770004,38.99404548866656,19.93853874835135,99.80196613598638,70.58746326674427,236.78039181666188,16.190000415033893,54.25284874008731,65.4457630861891,157.80162460230127,14.720659567018366,44.196776576403664,54.8156374581464,150.6243590849644,8.574339883855828,163.94039130904957,86.43943938975801,86.93764635370611,138.56550392216505,115.6013340242146,36.424248169085345,10.721589958091023,5.127853961966686,153.90212473493693,35.76472181758879,11.215748099065245,85.8423703746239,81.36390266980952,8.070356232304057,-13.440454376611058,420.0919776705865,5.1259592460309875,7.9020889181147,53.68690189606146,759.6434614781091,6.852044280879099,63.100855654334026,662.0373016544874,13.692274726400512,41.561026621672795,32.210345873273425,846.4413536149182,226.58556393553837,0.012482820149067209,112.97016250061142,-0.9181027878553181,83.08809298872194,102.13852200663135,124.22699652128739,18.601035142539157,48.819580032677734,402.5164959865141,18.935509220798124,17.884030602946474,22.56431224731667,-2.2442477319430054,102.18140675187576,54.8156374581464,287.27793803470354,960.1168392911476,62.92778569422547,176.84872492365892,115.32341666341244,145.3517837289147,58.45478376032562,11.401828748237484,56.59143036150226,19.602562661380357,171.8198364934977,53.54556260886733,29.72816484623226,73.21076440442174,45.32503772771385,-0.340556494976191,233.16805106160223,24.473635857433482,14.766360498837756,31.9522194804819,47.14956509263824,50.30021151432179,2015.1862180964918,113.99160311027632,53.05847082165915,27.464926439566877,278.63107647633257,108.06738608610917,247.2658875601607,38.55641817782063,75.72259691325863,68.79394291587512,46.58144060478049,52.79747646763552,38.37740388241442,12.572488022143865,-16.257287985309105,206.40528889902177,25.114389383650547,38.758500363132086,51.57694375670435,-27.184643488380914,66.21332247262667,73.34077495878535,6.823778865814965,4.6430917386852855,59.1813136542539,2.312827628866657,57.366565280356305,-0.3205901211452655,13.918398046913616,236.20049007252746,604.2684489832136,72.91254356563223,309.79886822237506,22.859204389554424,23.12490964608874,15.914873353478193,38.5229552375976,33.568033154319906,39.630503950257875,-9.930085178133226,19.125353414513206,29.92133387572645,427.3505310815914,47.01579099373179,-24.6642290104146,28.6206381601279,13.64186289207042,12.395829177993,22.017847556076667,392.4999151871883,27.68234921547701,32.05491197774923,42.39910653325591,291.43331227268874,16.985111147187965,88.26586147061809,573.546784882399,29.77337592965559,47.907559661539636,32.755402471225935,74.3903588607013,34.29776230815716,-0.5712053590012545,18.207649424015134,111.53118578634052,-7.778610824035539,300.5669332820394,216.59274057774059,44.343171175907145,-2.4477380105419684,46.22530672990378,8.978556029135845,24.5955701019,-10.045481733969943,151.55545184768374,53.62188440700621,105.09702055800157,72.5201305639575,41.06732921601822,0.2059081962638203,23.113158277140574,206.46706248377174,46.94699628467859,158.7862032270354,44.34422264752254,663.6088458364679,11.741961086974989,46.03640989415654,34.04337357257992,16.699238621508023,25.439857494773072,53.509795992046065,72.45986207661494,140.29447689196238,113.60675343358571,-3.976425875260759,41.950295163538414,36.80677863041906,211.1879014173635,68.49655908450887,51.29610666001301,27.373196352762978,210.28259021398594,4.272445056317771,23.12160683686555,18.862490231882433,9.463041323853595,232.16086528228246,70.03940554408828,218.19206609279118,21.777591528031493,9.994932091435103,213.93576567438308,37.74048468550347,240.2165085755618,96.23570990485584,846.4427358208771,16.23775850012018,117.89556943424901,46.03402855557264,162.43620531485206,211.6015134803363,1174.3041668227806,1210.411159883027,-2.4477380105419684,513.0418477510366,59.78986388678649,86.62270385235524,7.142225520606161,120.57794179143859,97.0488525791508,101.54779036418799,38.84609535839454,22.214758103557788,195.63109323377867,27.4730666834828,-12.607525603691698,398.2390223941363,17.834041372844723,27.619597263934896,90.53454263926749,28.56648866858352,44.182643868871594,38.23243292357621,85.42710785993648,2.338547297859087,10.429335421068856,31.889543767226883,-7.03899912985726,11.80932181541214,92.22724385945992,14.285848442747415,128.55726328657903,8.242707879500408,8.427841170704795,260.40646349108135,27.792005940894715,30.01035440078135,89.78041662780484,49.242613900671955,18.69525319275295,57.02786692223485,78.76008778873674,-6.096818627719323,49.242613900671955,44.34422264752254,83.08809298872194,97.5067833679841,300.7595936623295,13.089708036720374,17.95564149857467,66.94607551732724,300.6828370161995,-7.492668648587809,30.511604782850156,-43.12586868037056,57.6873932737314,40.431357377075116,7.935551858337732,42.74110770060055,127.73144725392086,5.596885248257273,100.73042289125478,37.4503034457764,50.01755736368041,41.149283387307094,30.741030912586453,-8.720791326173469,81.47458731145397,1303.839392423895,68.42912121286182,18.341935504451225,145.55560763843488,53.96247748106337,418.6621928712635,117.36461965650003,13.255568886193863,121.7570755123985,37.26867379530226,50.01755736368041,-3.6254804161034784,616.6700989684825,25.098777798067715,106.63884199762748,26.950649107416965,62.75819320384064,20.578666839060972,17.096362525693447,50.30021151432179,4.115009922168397,72.67937977868165,9.652675823484117,115.79353366918502,76.50410147007842,101.69480158731578,26.164155596641404,141.61812062405068,232.5942683132659,29.14354833881445,140.72224754979584,23.616251600487967,11.607239630100686,305.7767307235425,-4.4107762642120605,37.35137449305192,30.01035440078135,82.63584634769573,94.56669357866554,89.56888675014343,684.7762363985214,298.5091903555073,35.42697081743519,-10.60442985349232,20.91408827528779,222.65331665774286,74.52602767554345,4.133366909562952,63.18831931292945,34.77827436424751,16.40575457255777,55.25922342434687,34.21345268561295,15.521512993835607,49.458854680844034,0.763947005507859,64.86157715158797,182.33034661749457,45.21838096397194,43.35352502699021,11.496046798451275,302.62559767921084,30.027329182216967,399.11192156457287,70.68611897322836,25.057787768759002,960.1168392911476,537.3775725132995,61.90083483422367,50.627619238814724,8.091310596075893,3.46019447318254,10.259434889395237,87.61765164007862,238.25108898330782,57.93707110679796,27.88573736846031,5.3393579522995225,15.575688372708534,34.713729822385346,16.156076739491212,330.38694617470503,53.366087578141475,18.935509220798124,5.57067879797151,238.49700327183155,69.17869259059103,34.10809437973838,45.91343980623041,95.05378536587372,11.088553731276619,28.11424202755731,35.47500131318138,661.1785067636002,23424.040644726403,64.2119932510366,244.20851183072313,34.713729822385346,63.376268790708835,29.09267576916471,9.407708892928255,250.46128755569583,421.0810440068808,168.5671607664872,97.5067833679841,142.80162438561712,15.764611095784325,91.99851104397936,21.16517420164184,34.807947872599144,10.137898782085152,33.809236540332925,16.31389197359932,116.1887887447633,25.40309174532685,35.89616635256845,30.850789763619815,104.83547486008055,37.80267377611028,14.525157112824736,38.56622461348517,0.30248169773295785,266.4951208313925,493.5957029222293,142.2752216991427,120.8007623027285,66.13544956868549,73.477851866915,351.77820711777906,24.995137942832542,29.96892952348155,225.33663637290027,149.7158211780221,12.789480756266826,47.17076415393635,377.2241729165979,28.327154111177638,3.6170623493227936,34.04759785244241,42.45986164324667,478.48123295200105,268.9855035856955,28.249048118032952,-19.642071439239643,102.65011566436083,67.94437485430149,-7.965470964191219,3.790185337816979,31.81557742034334,37.06777298444269,6.315001394660484,33.49078988554173,267.0277356331169,89.10407577495056,798.6554701890838,37.36786265183933,188.80169268656613,73.21076440442174,3.2415982417551135,87.41359759562157,89.47420796461,-16.33122844486408,64.17901693346178,746.0045031263517,13.623966639995515,117.28125739192366,138.0713916422365,33.81205272690792,56.09115886737371,368.28613098587493,249.3019448027465,32.65552616053361,67.29102877709195,35.05972815836303,-30.900312581154125,31.890491125194735,635.0392462617135,87.9874315998392,49.34674037855533,-6.949561443801173,180.68104411610497,17.434139413175615,237.60380170977749,478.4693921246527,10.055918723467727,7.282605237959018,389.42820071853055,281.7487465804413,29.727239514820326,62.07746779104598,249.50430323959193,37.0192558660483,-5.232170789903847,14.285848442747415,310.60210298777633,42.24646293697814,88.71905546961216,95.13799047807518,22.84225549544736,31.305852591221015,-3.6952057052354377,5.570218062651861,27.18429951701575,8.745827090176348,71.55298745597861,42.695406768781154,3.4168489926184833,173.35699880527002,389.8824947567483,26.032483853152122,188.27878250787958,106.54557130538154,-13.667238962397633,18.341935504451225,-19.280998858090186,53.849902443455015,57.053316150724,-7.69624037825443,0.09407900202089081,36.838346854706394,52.051259155010854,325.03730638917784,20.34171362023899,23.616251600487967,23.12490964608874,29.427662357400436,29.537856056424882,70.95800564634422,272.5111792669556,5.4062035574449325,105.72844490053694,338.8689002578733,20.44676156876166,46.396768048895744,-9.668714681619718,-14.88124371686807,98.6967469872529,240.14952647775695,188.06868661083425,3.686414138755481,84.70179634643739,32.84962052143973,176.284363980344,46.34023721876747,21.432748286783298,19.695372618306664,63.44176069053884,-18.96958110662638,371.56377232546333,22.64956922782873,25.786569564628408,16.75863607615328,32.47745922309525,20.574442559198488,20.109932039247347,20.083074717470705,21.16517420164184,11.845097647251286,81.03885039657193,47.62490551089825,-11.89467073034389,62.930627768129014,638.2260423735523,5.714822059867188,23.90220856035254,79.26803767520082,389.42820071853055,61.561649853454014,44.34140646094755,16.665182474582352,62.2044950364503,39.16553269498709,29.36024985430618,113.7349469339993,82.47867141605428,43.587201323917554,315.8144223640372,11.983625208307657,31.49241986304146,12.666706072357663,51.53305987883499,40.15470064260379,558.1048813003495,28.94617705601369,66.40632646305015,71.5379073904787,31.87116089250377,72.96625820918553,4022.3315870778656,71.75877672428891,6.0156958805438805,658.1196970535466,76.47024820649935,5.714822059867188,116.16052332969916,32.50290845158439,671.0626100395731,115.69580188215447,199.01034786387342,213.93576567438308,25.60991072047755,252.13040691242455,11.116819146340752,176.75921777595582,43.69836826823841,1.3143783795633865,578.8408919464464,696.4075830965055,13.98906158457396,19.860740828829,33.49078988554173,162.5762278461995,128.3999087877906,478.4693921246527,33.50773877964879,65.16359555819592,39.80670205908909,35.48255428959561,201.556487121587,40.670205311832795,39.633320136832864,4.1804759346698575,23.855247128513916,24.971583430279097,127.24419710800113,141.96495256732152,141.02466678111927,60.480497727250736,51.20519141902241,136.20682160597124,238.16629273811543,17.884030602946474,41.36411607419167,40.15057205504088,-1.6869272550655836,31.103283783261357,236.59285129954503,203.90013523332655,16.93506462939898,32.69464736123517,80.05852193902882,239.8170033327925,8.759959797708419,1.205566886497877,40.47326887702312,136.34438513674908,215.68313647384537,353.28147164133725,6.75122061221893,95.2493137073959,127.04130003610483,148.96776092411883,8.774092505240496,-9.03122426923079,476.7052227054711,88.11936275746707,48.802631138570675,24.729919308946428,77.79163047341925,53.5154024778675,114.21583083034321,-1.9891018006028958,36.167503982252754,26.67779439634493,162.5762278461995,205.60682391945662,83.93244079675073,497.75596823647356,272.7089853977475,146.40522778265844,81.41666995539495,129.53477555754716,17.07188654250071,-51.90087196449794,51.95798846276491,20.75202287398864,39.51554757406562,14.194446579108615,16.555556094931987,14.37911913499336,25.808715983894363,25.240440805774696,29.416319949114808,126.23855189335188,93.22313900515113,131.2090406647777,166.80275120694174,2797.7374953593458,40.595752342301054,13.420450474068012,170.98465043047526,-2.1391738960917905,37.42803453433961,68.45820487510267,71.61282109533009,24.80718846505316,106.57667879434922,18.883120229819948,92.73179705075191,219.7782839202632,46.417019752226,157.88844235735667,151.4659432089218,23.42640740677288,25.301346606027373,3.224756648270386,248.50531565579175,90.74661237781586,43.943238606247135,13.235425420063947,43.56641271778275,82.31926852151167,20.94493195204385,69.71527474148999,-214.86273751659562,33.53505683674508,70.10909583645224,86.45198226514401,27.687879463011345,60.12574605833296,214.2775222979902,17.554267427198198,148.95877396708855,69.95128060234413,34.40421834996732,46.6676449432604,27.01611511991841,29.042289822163177,94.52383472074969,-0.7115292213584397,6.354096708033495,170.81617476137544,0.10272907634829664,69.1438215570805,24.15943936983333,517.4687949747912,79.77655568014923,19.79524892899899,63.28865635894143,33.57179669886274,7.033414027540658,219.30627219855492,27.41418638207169,234.73114922451188,53.11687048039457,199.0450894607412,97.40923662121855,15.818299852009055,20.939050881128736,348.0283804939272,47.45201021129023,182.34117651580345,1.205566886497877,37.59068316312924,39.4792684472676,25.40309174532685,19.132880503598894,14.531151505106273,101.65048286145532,27.958295622056355,15.675078060752668,38.345835658127086,24.672901856169947,22.636870500912714,20.204315422231033,151.03915560321317,101.7456223823084,94.74498794706628,58.60904254851934,45.58930900363211,62.54574185553996,6.051190854061858,31.70725254992604,56.96662518959589,24.726616499723235,128.03248427928537,-3.3147477004555554,68.8234801863536,58.43783486621856,116.25945228242365,66.88637778382085,158.83943124794047,-1.365911432284875,95.44386880362167,270.6979425356597,4.41650768285254,31.103283783261357,-55.57571452734868,97.56234095281597,387.55940640240283,27.829206538332034,63.15721182396177,39.76240922055718,11.064999218723166,194.44865670359556,46.69639698097274,13.07796268407516,37.31481374673612,5.194882285318663,8.789633306060061,32.43616642978071,13.152194111776417,57.74062129463648,75.50497392712761,58.33183955972804,155.23027696428784,44.06817411532755,307.2069244835279,13.727606495230688,1.8450855480958523,94.33726744892924,44.25425476449979,113.32645473419969,1114.9290024853299,259.30646775483535,249.60111299950384,52.152491523551916,70.52319452207328,6.75122061221893,16.59559876627285,242.968648887101,69.93102889901387,351.0426398653776,266.4951208313925,-2.7360504216618864,13.32482433056672,34.54089363885209,43.35352502699021,34.259588465423434,14.525157112824736,98.85174603478599,129.81320542832606,53.41744677043941,62.75819320384064,73.97298325318562,317.9237928473559,4.806104497952283,7.473883412290142,116.36355287297847,20.595154997848383,72.91254356563223,365.79974770552724,2.535910223119515,19.38399231728149,36.43840676394597,147.20352793386553,47.00117166355152,36.638594233321726,32.6713242808822,12.409961885525071,77.00726520538942,69.74446162719343,9.586262453014811,563.4724939759607,12.438227300589212,100.0379202221834,74.49540680922397,701.6346105124627,18.601035142539157,201.88574474082344,72.67699844009775,127.97121665931783,20.574442559198488,1813.024511969526,7.77067027046359,51.70877136501801,10.259434889395237,207.866603985517,137.23003480875875,34.75425911637441,101.3131667092928,232.653128707321,189.20447485123012,34.05758573040839,97.39699862962219,106.30342056140067,230.93695210014246,90.43804826336569,514.8274653798778,14.134152204437498,35.50329261557407,32.58114645891138,71.5379073904787,97.40923662121855,65.47873940376392,191.7851974107967,192.09935033137748,214.0116267372023,12.033089684669896,134.40094272703323,14.371105423259465,40.06626243249667,139.76073681496695,11.38864339867326,17.277271537035357,29.989667849460005,20.424615149495704,2.312827628866657,31.456601471563086,5.830993567704667,39.51462610342633,171.25746837689724,96.9969258504664,32.632919005948004,53.77685756721077,92.70068956178423,17.052556309809752,45.162511591095054,6.332436911415741,25.41535562425177,32.69464736123517,122.13630316450903,63.65892294135021,-13.578653285333829,13.423266660643009,113.62416306301242,72.31945647193353,48.757877564719124,65.99555647398086,22.5044786079652,97.29146405845131,41.08663356138063,67.29102877709195,12.280843416646718,70.74914731821329,56.222302592906914,-13.405271363077603,26.937003022533094,26.298649845006086,73.32147061342295,47.94194407240196,77.1711994352957,11.47805266218706,16.28939010307802,140.06835357144928,88.39351657372637,12.572488022143865,22.84225549544736,54.0849609463413,131.2090406647777,37.05550910551776,44.253307406531945,964.9968363084628,5.900441973719786,47.6955690485586,649.5963907472805,162.9143360323585,179.82894348608224,96.12646304502347,52.026825776225785,-9.00006596766275,56.37543846277497,36.718218840683804,35.829266359450955,48.785682244463615,-22.79941873850376,50.02746579134999,22.664649293328647,25.028574995727016,33.697634867893,104.02572508481076,18.240651100471396,123.70644179385619,60.66419703783908,511.03638548744175,87.6315390873053,23.649227918062795,2.9624209457113153,53.791980565149565,12.061035696750324,495.6746190227309,211.1407923922566,56.37543846277497,18.601521765187357,1710.2811980087224,38.48055711500139,14.733844916582584,43.30614629124231,63.51524041477418,304.9913153915544,28.65596992895807,240.35921785684147,14.625033423517067,-3.083249114165426,-0.3740194351992159,67.71229327526878,181.6136220674496,14.059725122234312,9.808135606336876,42.17533866399815,96.94285727266028,356.50887470915075,206.79069602245195,33.81205272690792,75.101731027144,55.9325820884995,-13.795349598785897,36.81714779340829,1174.3041668227806,132.4715625376425],"xaxis":"x","y":[74.33,21.29,69.48,25.53,5.45,4.54,191.97,66.44,13.95,108.04,265.21,72.16,29.57,16.01,133.5,324.25,281.25,53.02,33.09,263.51,4.56,37.34,233.43,20.85,9.83,46.7,6.07,49.22,466.95,3.88,45.83,11.84,1936.34,193.94,9.76,3.29,10.7,111.95,21.24,107.56,14.24,17.04,5.81,26.94,2.82,27.77,7.63,48.1,52.43,33.04,50.29,6.25,3.34,43.19,64.23,77.76,84.75,4.41,16.8,282.72,82.72,60.57,89.29,51.1,8.14,35.4,25.32,64.59,1.6,79.74,159.98,42.8,5.62,13.29,4.53,6.79,3.15,75.03,6.05,500.93,96.43,58.79,39.48,4.99,26.46,65.02,5.94,6.18,102.27,15.2,95.77,6.17,3.95,61.98,33.52,1148.5,37.82,79.98,138.68,34.48,4,3.35,4.39,4.78,105.01,126.03,56.48,25.9,570.98,18.33,387.49,16.44,8.65,13.87,29.78,1.08,12.29,1638.46,89.57,54.25,12.72,24.39,192.12,7.44,193.34,45.44,17.43,38.39,60.5,122.73,10.65,10.37,25.22,23.58,4.86,116.21,25.21,6.3,331.61,11.8,15.26,7.9,23.85,138.79,4.6,47.26,304.62,23.84,4.89,30.79,12.25,13.23,59.73,17.22,14.35,29.41,10.66,25.14,46.46,27.47,69.02,52.87,76.68,4.75,22.46,5.07,8.65,5.99,45.33,52.49,54.24,10.84,20.22,3.4,2937.83,170.01,30.26,82.58,229.2,569.61,2061.48,145.67,15.1,8.49,53.31,250.87,20.29,119.98,2.93,22.37,0.17,17.02,120.1,1.55,4.13,51.93,8.42,5.43,602.12,4.6,61.33,16.01,4.43,42.92,72.78,47.12,97.39,321.72,39.27,3.63,12.99,350.87,96.2,13.36,69.18,4.82,60.98,17.23,1.73,5.53,7.06,9.57,380.17,10.65,18.88,9.45,70.31,41.8,157.77,1.63,56.07,64.13,73.9,45.79,106.37,22.97,61.15,53.88,227.18,1.97,127.74,1266.06,29.93,33.28,22.38,249.46,6.12,4.44,4.55,4.2,5.28,293.56,30.19,515.71,3.52,5.67,46.79,11824.89,51.83,48.95,188.77,64.56,38.97,13.39,13.56,34.61,53.87,9.04,16.99,280.16,117.64,3.75,30.57,8.86,467.9,56.07,17.24,272.54,40.65,22.78,32.62,6.66,6.7,103.42,114.33,185.33,239.17,67.17,19.95,15.9,5.49,4.68,23.62,38.88,2.99,5.53,61.88,6.38,27.92,23.15,89.29,14.83,73.14,118.2,42.8,3.7,12.68,66.43,16.89,8.25,29.33,21.3,7.48,65.46,10.21,190.61,175.3,49.58,5.64,3.14,229.84,4.2,62.41,147.14,5.82,30.35,6.41,134.8,175.46,67.57,17.12,309.6,4.5,73.61,28.19,6.89,9.76,60.12,4.92,132.1,72.5,30.69,73.31,105.97,23.12,35.6,16.14,103.99,3.48,367.3,3.54,5.8,24.56,100.02,93.21,17.49,30.86,41.9,54.04,40.78,32.89,4.38,4.32,193.47,35.11,58.59,84.93,0,33.45,6.71,458.44,12.28,168.36,75,398.19,5.46,39.41,166.84,3.59,7.07,3.06,262.98,144.57,5.22,358.88,4.73,105.32,23.5,16.83,4.64,1.06,266.05,10.83,11.81,21.44,78.36,11.17,27.27,127.27,295.42,457.51,198.59,61.18,110.74,14.97,4.11,197.3,5.96,25.11,9.95,505.01,71.87,22.66,17.92,35.43,44.15,93.63,109.9,278.47,80.09,452.16,33.17,300.14,7.36,126.21,178.21,119.2,77.75,35.48,46.26,8.42,6.22,5.28,84.95,6.41,31.27,29.66,48.95,72.5,3.21,141.15,31.13,4.25,3.49,52,3.96,125.59,14.7,19.57,143.33,804.2,9.25,375.56,14.37,16.18,78.29,29.97,3.93,22.93,32.05,126.92,5.74,105.41,7.69,9.79,34.66,4.08,5.59,26.16,143.82,6,11.87,20.37,422.39,4.55,11.29,181.67,9.43,5.71,21.78,2.15,7.57,229.12,4.08,41.16,24.25,28.3,155.54,110.62,40.56,18.3,79.21,10.9,4.85,113.97,8.36,91.53,9.28,47.5,3.63,20.06,168.62,49.28,51.4,11.27,15.5,53.65,35.2,8.06,4,27.93,286.43,91.09,85.28,49.89,16.72,18.74,19.11,119.46,28.94,8.75,6.29,94.82,35.72,33.37,18.76,19.69,402.04,328.23,152.28,5.23,86.9,158.55,49.63,23.42,10.01,607.41,235.69,12.05,7.21,62.23,39.5,1265.29,290.4,45.33,463.12,70.84,93.83,53.71,9.53,184.09,28.37,13.1,48.99,18.47,5.15,6.37,31.15,35.8,17.14,379.34,161.39,127.56,6.9,84.59,4.33,351.9,27.99,124.54,30.1,59.14,21.57,25.12,6.01,7.97,78.92,13.71,1.55,72.9,70.16,4.04,7.47,10.02,75.99,77.04,11.18,107.16,38.13,39.57,227.13,4.43,98.88,104.61,20.02,5.63,130.28,138.35,8.08,1.08,6.96,16.46,5.94,64.79,9.67,45.38,30.19,5.79,32.42,191.97,636.96,30.02,1.73,85.59,8.61,199.48,70.17,3.69,41.74,33.12,39.73,4.35,180.14,20.5,74.98,23.63,23.9,5.79,334.15,132.32,10.29,9.39,4.14,553.27,28.63,249.98,41.96,147.36,128.91,0.78,209.92,7.34,22.15,180.89,4.12,9.43,1.59,549.74,25.93,523.95,128.22,124.77,10.48,5.53,63.77,1623.34,39.1,232.12,9.56,6.22,4.2,36.42,13.27,16.47,4.24,48.6,109.16,47.14,11.1,19.61,6.16,95.26,6.15,24.12,40.46,28.6,192.51,159.16,28.55,44.26,4.02,47.25,60.88,75.95,115.34,7.13,38.38,216.18,5.69,60.08,4.86,122.92,11.59,13.18,2.99,51.47,117.19,13.34,63.41,178.86,4.09,100.71,13.12,1874.42,11140.56,8.69,1060.13,64.06,13.71,75.58,27.38,158.48,213.9,20.09,39.57,53.48,6.04,54.76,4.19,4.39,3.44,21.71,9.16,9.82,3.65,6.08,34.03,28.21,27.63,3.83,45.07,13.21,218.49,1789.69,44.74,14.67,94.23,5.77,31.76,18.42,134.38,69.44,40.35,14.73,25.15,752.57,17.39,56.86,36.97,33.35,177.51,7.21,113.3,6.35,150.23,8.37,3.16,21.51,38.59,6.51,5.42,5.6,248.15,477.85,691.57,23.79,174.79,68.05,64.86,191.2,50.52,5.46,6.72,1699.45,5.07,35.8,376.46,21.12,58.88,348.21,29.53,0.92,9.67,13.18,92.21,4.8,426.39,20.66,5.95,48.34,71.03,59.84,234.75,723.4,133.7,158.6,250.05,69.42,25.39,2.79,298.61,94.02,39.67,5.67,1597.56,11.38,35.14,1019.53,10.04,2.69,63.37,4.31,6.73,20.76,86.81,28.05,49.6,110.41,27.33,8.64,61.86,59.76,60.67,1.78,11.97,14.49,43.15,7.52,12.62,6.26,225.35,675.34,5.54,3.78,17.11,84.83,119.01,11.54,393.2,22.74,40.25,46.52,34.03,5.84,3.54,51.89,125.13,106.74,21.38,7.39,33.58,3.27,18.91,15.7,54.02,6.86,22.81,17.08,1894.2,20.33,44.93,22.96,13.29,14.24,23.22,11.27,4.46,26.63,2.72,5.68,0.09,9.56,181.93,40.69,3.47,101.34,259.49,9.86,6.87,10.24,171.64,5.87,49.53,469.57,83.68,9.95,285.62,3.43,12.36,36.08,6.43,14.09,377.09,27.14,23.03,4.99,32.77,238.77,3243.48,93.31,5.94,455.39,67.24,29.62,9.96,5.91,234.15,39.44,314.92,154.96,5.46,86.99,20.09,260.13,6.67,39.57,147.82,188.13,5.03,9.73,5.79,45.6,21.92,740.97,59.05,120.84,47.92,108.79,84.64,6.35,15.43,9.72,12.71,26.21,47.24,22.17,68.3,46.6,6.85,2.85,84.18,12.92,15.34,18.1,4.84,12.75,34.14,47.71,129.42,45.02,7.85,202.65,4.45,4.31,148.25,15.09,123.18,2452.86,38.86,15.9,21.77,301.02,5.45,17.75,47.99,16.3,6.74,11.94,176.79,4.74,51.98,4.16,5.9,64.57,46.78,313.83,179.33,1088.2,237.36,319.73,161.2,19.39,85.73,273.91,42.18,4.62,6.33,67.45,15.37,47.64,4.89,24.89,5.6,4.19,254.16,136.94,221.51,834.96,1.14,8.16,202.99,9.71,71.55,59.07,9.36,138.61,18.9,481.97,42.99,134.01,54.41,21.44,50.46,85.95,33.92,7.69,98.95,99.96,78.7,0,15.6,51.39,3.98,25.48,58.28,62.17,88.51,9.44,24.51,36.78,88.74,5.65,215,32.48,92.45,22.53,9.11,25.38,45.32,128.39,9.46,69.59,3.28,82.07,8.45,94.59,65.35,86.86,103.77,1.85,3.02,434.48,5,133.8,8.8,587.16,214.65,15.11,3.5,433.6,75.48,19.07,4.26,4.19,41.96,3.76,5.97,10.23,61.62,4.82,28.61,86.9,36.72,16.89,9.62,103.39,303.69,37.51,225.66,7.6,92.19,3.67,38.85,9.29,23.78,14.86,3.41,176.75,59.74,6.32,111.04,30.03,17.57,14.04,399.73,3.77,13.09,87.07,154.6,28.88,6.3,60.52,6.73,13.35,65.05,30.78,103.4,231.39,4.92,0,565.79,103.07,9.57,21.05,18.86,99.39,5.91,206.01,6.03,9.15,10.6,8.63,106.73,509.36,5.44,193.72,37.74,125.42,38.56,3.05,132.71,148.7,130.73,218.53,4.52,4.51,8.28,16.32,68.14,3.56,78.06,66.23,70.39,15.43,178.65,355.05,0.92,3.04,284.01,61.07,6.98,67.78,18.65,11.62,7.73,319.88,7.49,38.67,7.85,5.04,11.39,33.8,19.7,359.09,3.3,55.69,117.11,1990.5,4.51,44.96,0,95.54,14.3,481.06,31.47,2.36,62.23,205.52,54.4,10.95,28.78,107.84,222.02,124.96,54.53,117.04,21.08,27.02,142.16,4.15,51.24,6.18,3,210.3,1.24,138.76,39.75,21.51,71.23,46.3,49.46,9.4,85.76,7.74,8.16,101.31,5.35,4.13,47.08,4.48,8.36,74.07,235.32,12.63,7.74,86.46,15.97,16.58,3.99,7.56,48.35,14.65,33.43,35.39,44.55,37.49,7.78,25.12,82.9,6.31,1.67,4.15,10.38,14.3,137.15,1.68,120.91,130.69,79.58,9.73,34.2,38.78,62.98,8.47,25.2,12.05,77.34,0.31,53.12,127.14,243.13,6.09,567.6,4.43,52.67,139.72,80.75,1185.15,63.33,30.95,4.65,303.81,6.25,8.85,23.95,5.17,41.22,3.81,1.86,72.5,51.28,41.3,37.56,22.1,36.09,49.17,6.71,3.5,67.06,4.41,394.6,38.41,271.68,138.03,1129.67,2.34,12.26,20.86,87.14,238.61,43.97,107.83,21.66,5.88,17.45,16.63,103.57,23.8,48.16,45.58,10.97,10.29,28.09,26.56,40.05,9.63,5.02,5.82,1596.08,293.55],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle ElasticNet()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predEN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression elasticnet\n","# réglage des paramètre pour la gridsearch\n","alphasEN = np.logspace(-3, 3, 200)\n","l1ratioEN = np.linspace(0, 1, 6)\n","param_gridEN = {\n","    'elasticnet__alpha': alphasEN,\n","    'elasticnet__l1_ratio': l1ratioEN\n","}\n","\n","GridEN, \\\n","BestParametresEN, \\\n","ScoresEN, \\\n","TotalGHGEmissions_predEN, \\\n","figEN = reg_modelGrid(model=ElasticNet(),\n","                         scaler=scaler,\n","                         X_train=BEBM_train,\n","                         X_test=BEBM_test,\n","                         y_train=TotalGHGEmissions_train,\n","                         y_test=TotalGHGEmissions_test,\n","                         y_test_name='TotalGHGEmissions_test',\n","                         y_pred_name='TotalGHGEmissions_predEN',\n","                         score=score,\n","                         param_grid=param_gridEN)\n","\n","print(BestParametresEN)\n","print(ScoresEN)\n","figEN.show()\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"y":[386.0386526392207,386.0386038642117,386.0385694564325,386.03853847831607,386.0385206793784,386.0384498143611,386.0384116706328,386.0383612717295,386.0383095285857,386.0382552575572,386.0381930422242,386.03814259328465,386.0380556691168,386.03798550753197,386.037923356719,386.03783653271364,386.037739703525,386.03765289825566,386.0375456950294,386.03742692679407,386.0372510968856,386.03713838095445,386.03707341173566,386.0368947598348,386.0366873052174,386.0365065442778,386.03642093931785,386.03622203241383,386.03590843764596,386.0356723314306,386.03555506155584,386.0352335029863,386.03488177237796,386.03462072340807,386.0342993995078,386.03397070446164,386.03360242845565,386.03326090354676,386.0325444058528,386.0324104222351,386.03181101596977,386.0311459971446,386.03054760681505,386.0292027093499,386.03387236673905,386.0319654264103,386.0314001000402,386.0312597989782,386.0256437411099,386.02990766117824,386.0293138902483,386.0284526101572,386.02768733104654,386.02702615654675,386.02602303959094,386.02519113103443,386.0242559161575,386.0231046757468,386.0221985946722,386.0208290626646,386.01945549740884,386.0180949271224,386.01658883863564,386.01515747161955,386.0132301312591,386.0113770513701,386.0094157439448,386.00783152558677,386.00530459446236,386.00284538816305,386.0008970655165,385.9984542482306,385.995574229492,385.99372699129253,385.9909238411252,385.98858269122167,385.98581169757875,385.98242426204774,385.9793137753516,385.97544848975366,385.9716375063798,385.96771823996085,385.96275528211123,385.9579071356314,385.95245005210603,385.947926420983,385.94067334329156,385.93453943929774,385.9284662525634,385.9194482295266,385.91264015709055,385.905807950351,385.89907094998716,385.8921625695167,385.88594174989555,385.87636735210367,385.86938006896287,385.86227265175336,385.85597388375044,385.84870117660955,385.84249816905805,385.83564289902176,385.8292032221088,385.8234798689161,385.81901931031035,385.8174565192409,385.81791668513233,385.82032903141965,385.8246509012606,385.8318392067958,385.8420528878727,385.8562478083052,385.87522086629605,385.89744946844655,385.9280943076763,385.96139469367864,385.98072253022195,385.94238627647667,385.9078484566926,385.87606267492947,385.8556077232065,385.72097004892487,385.56713262020656,385.41299026433387,385.2605843997582,385.1112764704978,384.9683430235188,384.83536921889265,384.7157822429714,384.6130174622232,384.4950387463598,384.2710416555525,384.04208462876966,383.849261027846,383.70596809851884,383.5644642572828,383.4268659332748,383.2958797926917,383.17438687639805,383.06684988523136,382.97700693777585,382.91077861664735,382.87444482585465,382.875398061638,382.92319562026336,383.02987112852634,383.2078817054955,383.47354359575786,383.8159749725241,383.9978161569597,384.2090735320518,384.56337747084444,384.87952003042966,385.36359782367697,385.8811795350848,386.5720576092434,387.51452791535104,388.5459968589885,389.372440444893,390.25302359292766,391.3342156378225,392.24722176149737,393.19914212623837,394.31901253823406,395.6335731810157,397.17342174263695,398.9734422644059,401.0732327350564,403.51751196405644,406.3564821804694,409.64612188776147,413.44838550102776,417.8312937711654,422.8689125395875,428.6412356398,435.2340070611949,442.7385323297036,449.8934118577945,455.16409391966306,458.0268434436549,458.62272938915874,458.62272938915874,458.62272938915874,458.62272938915874,458.62272938915874,458.62272938915874,458.62272938915874,458.62272938915874,458.62272938915874,458.62272938915874,458.62272938915874,458.62272938915874,458.62272938915874,458.62272938915874,458.62272938915874,458.62272938915874,458.62272938915874,458.62272938915874,458.62272938915874,458.62272938915874]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"y":[463.436731185339,463.43676528377625,463.43683241839244,463.4369112027996,463.436975864298,463.43701428282594,463.4371099129762,463.43720432695613,463.4373093475324,463.4373511696589,463.4374654526962,463.4376059350159,463.43769937176444,463.4377824525363,463.4379530126589,463.438118005168,463.4382585893148,463.438374423155,463.4385741496262,463.4387450553891,463.43892561256513,463.4391970279339,463.4394190078281,463.43962231357636,463.4398843567145,463.4402103499891,463.4404361218718,463.4408079268918,463.44111372015334,463.44146371225384,463.44183961450193,463.4422728757663,463.44263635584645,463.4432460642115,463.443867585706,463.4441232989518,463.44481724860356,463.44562381607307,463.446008791995,463.446787945993,463.44742926155027,463.4482671050022,463.44929002144806,463.4489799758866,463.4553735988959,463.45309903803997,463.4544340144104,463.45562710573114,463.4475463330465,463.4761242146676,463.4790421564457,463.48201710964173,463.4853069682416,463.4891021138932,463.49276619184536,463.49686960850386,463.5014430589555,463.5060356669913,463.51126860913416,463.5165845436683,463.5224977138077,463.5285456947968,463.53543039257255,463.54265894952687,463.5500864669028,463.5584045007412,463.5671317369602,463.5772933216976,463.58726103677617,463.5981406112286,463.61071481491456,463.62367466986655,463.6370907950766,463.6531888495457,463.6698629322767,463.6886938476875,463.70853750126884,463.7292180543303,463.7519129591562,463.77565639618683,463.80154647102404,463.8295809236208,463.8594061555311,463.89152996353425,463.92580856425474,463.9640446466076,464.00232910817346,464.0448291263002,464.09114275862,464.1381626170901,464.1918978204142,464.2491534673891,464.3098352190978,464.3753397343835,464.4455267815371,464.5227137798606,464.60438735905734,464.69229483678225,464.7876233640126,464.88871405701076,464.99721360347417,465.11487242587,465.24184700620816,465.3787379179928,465.527416829565,465.6906325596491,465.8677434935644,466.05948446548484,466.26611577219364,466.4901800895186,466.7324754801808,466.99525646216546,467.2804905776077,467.5932880055231,467.92907515700165,468.28123539274986,468.6038672734027,468.78859352422296,468.9905619274533,469.20928934740687,469.4548501216762,469.5851040520041,469.70600376728413,469.837263121772,469.981228676774,470.13801425377017,470.30925114751295,470.497717413092,470.7040485891017,470.9305011623534,471.187141395515,471.5271247239938,471.90261701296157,472.35407593210925,472.89712338610263,473.4908520457721,474.14072038930806,474.85300639046795,475.6331390054154,476.4893483871847,477.42856945432845,478.45980613818284,479.5921221280403,480.8352757140898,482.20028651926515,483.700467019118,485.34808014818566,487.1563727146006,489.09958182150143,490.76109018001785,492.3668371604758,494.11932769689645,495.8043093053839,497.5971167493034,499.4553340110211,501.4690610705566,503.6130504244236,505.57297768277186,506.9552848122129,508.15022840351236,509.46333092311227,510.9798430131218,512.7288623377672,514.6653568589526,516.81004759379,519.1858920798645,521.8183384509769,524.7356289515537,527.9691687181909,531.5539781249347,535.5292474091073,539.9390090983262,544.8329355128288,550.2672547793593,556.3057605896574,563.0208718500157,570.4946836298061,576.5444760653263,579.4962449894756,578.5154740076523,577.9604333682457,577.9604333682457,577.9604333682457,577.9604333682457,577.9604333682457,577.9604333682457,577.9604333682457,577.9604333682457,577.9604333682457,577.9604333682457,577.9604333682457,577.9604333682457,577.9604333682457,577.9604333682457,577.9604333682457,577.9604333682457,577.9604333682457,577.9604333682457,577.9604333682457,577.9604333682457]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"y":[308.6405740931024,308.6404424446472,308.6403064944726,308.64016575383255,308.6400654944588,308.6398853458963,308.6397134282894,308.6395182165029,308.63930970963906,308.6391593454555,308.63892063175217,308.6386792515534,308.6384119664691,308.63818856252766,308.63789370077905,308.6375550602593,308.6372208177352,308.63693137335633,308.63651724043257,308.636108798199,308.63557658120607,308.635079733975,308.6347278156432,308.63416720609325,308.6334902537203,308.6328027385665,308.6324057567639,308.63163613793586,308.6307031551386,308.62988095060734,308.62927050860975,308.6281941302062,308.62712718890947,308.62599538260463,308.6247312133096,308.62381810997147,308.62238760830775,308.62089799102046,308.61908001971057,308.6180328984772,308.61619277038926,308.61402488928707,308.61180519218203,308.6094254428132,308.6123711345822,308.6108318147806,308.6083661856699,308.6068924922253,308.6037411491733,308.5836911076889,308.57958562405094,308.5748881106727,308.57006769385146,308.5649501992003,308.5592798873365,308.553512653565,308.54706877335946,308.54017368450235,308.53312858021025,308.5250735816609,308.51641328101,308.507644159448,308.49774728469873,308.4876559937122,308.47637379561536,308.46434960199906,308.45169975092944,308.43836972947594,308.42334815214855,308.4075501650975,308.3910793161184,308.3732338265946,308.3540576639074,308.33426513303937,308.3119847499737,308.28847153475584,308.2630858938887,308.23563046976517,308.20671459154704,308.1752405833205,308.1417285417356,308.10585555630087,308.0661044086914,308.0242843077285,307.9790915399573,307.93180819535837,307.8790175784097,307.82424975229526,307.7657897465068,307.70073384196314,307.6333824937669,307.56246243331293,307.4883066808765,307.4089854046499,307.326356718254,307.2300209243467,307.1343727788684,307.03225046672446,306.9243244034883,306.80868829620835,306.6877827346419,306.55641337217355,306.4165594380094,306.26822181983937,306.1106217910557,305.94428047883275,305.7680898767003,305.58117359735445,305.3831860303276,305.17349832407297,304.95163029556454,304.717239154445,304.4699511549844,304.20161093137,303.92711345835096,303.6415539946074,303.3575777870412,303.0961790287304,302.82513498593187,302.5428360024521,302.2563653247368,301.8568360458456,301.428261473129,300.9887174068957,300.53994012274245,300.0845386872254,299.62743489952464,299.1730210246933,298.72751589684106,298.295533762093,297.80293609720457,297.0149585871112,296.18155224457774,295.34444612358277,294.51481281093504,293.6380764687935,292.7130114772416,291.7387531949155,290.7156347473807,289.64435138327804,288.52544442122326,287.36175109511186,286.15676752366903,284.91552040918623,283.64610472126157,282.3592752379347,281.0676832628053,279.7907144769151,278.53236812354675,277.2345421339015,276.0513099036277,275.00742724479244,273.9547307554754,273.13007889805056,272.30702505914854,271.6750541479302,271.4160054062785,271.5190160352051,271.78959607757315,272.35581878234296,273.2051003525328,273.51460050987293,273.6694219147096,273.97266821751555,274.4570987682414,275.1609514054093,276.128546077835,277.41083651855905,279.065855209922,281.1589862360041,283.76299636641573,286.9577619037293,290.829652029502,295.47057029981573,300.97671068994254,307.4471422723741,314.98238102960113,323.24234765026273,330.8319428498505,337.5382128796575,339.2850254100717,339.2850254100717,339.2850254100717,339.2850254100717,339.2850254100717,339.2850254100717,339.2850254100717,339.2850254100717,339.2850254100717,339.2850254100717,339.2850254100717,339.2850254100717,339.2850254100717,339.2850254100717,339.2850254100717,339.2850254100717,339.2850254100717,339.2850254100717,339.2850254100717,339.2850254100717]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[385.41683075200694,385.4168296476806,385.41679377896287,385.4167925105835,385.4167524162214,385.4167077017427,385.4167061405993,385.4166561591015,385.4166004173029,385.4165984960834,385.4165361892798,385.41653398322256,385.41646433668257,385.4163866652236,385.4163839511774,385.4162971315052,385.41629401586573,385.4161969684807,385.41608874150864,385.41608491011,385.4159639359267,385.4159595391853,385.4158243138629,385.41567351472446,385.41566811129087,385.4154995514477,385.41549335389203,385.4153049385891,385.4150948292343,385.4150872194578,385.4148523669727,385.4148436452191,385.41458113287683,385.41457114047853,385.4142777080502,385.4142662649562,385.4139382685473,385.413925170866,385.4135585381774,385.41354355558497,385.41313373612326,385.4131166091991,385.41265851892217,385.4126389564949,385.41261806200555,385.4121045930974,385.41208076331213,385.4120553327856,385.38052512345877,385.3779473337857,385.3752142695574,385.3719217663256,385.36866464160687,385.365432093661,385.3614002357101,385.3576907489247,385.3534442380073,385.34853095502075,385.34387228758584,385.3383917033975,385.3325239624046,385.32684650055336,385.32002345856574,385.31331410882353,385.30519961362546,385.2970635439,385.2889015458368,385.27987961840273,385.26980452400454,385.2585588058791,385.2470211328373,385.23519763827915,385.22186702745324,385.2081015551502,385.19250526188966,385.1762588530493,385.159288025347,385.1399833552962,385.1196506928388,385.0981489374254,385.07547214499374,385.0497314425525,385.0223168942243,384.9914892084637,384.9600234850909,384.9272873786516,384.8902953578235,384.85173820747764,384.8111928347741,384.7548981190858,384.7086903227731,384.6589818457037,384.60613583820447,384.5495656469549,384.4893401228988,384.42515279773,384.35620813720277,384.2831773246436,384.20917145524834,384.1257012763344,384.0366885441555,383.9418034174643,383.8407026461807,383.73303016069707,383.6184179389744,383.49648720734154,383.36685004045125,383.2291114369401,383.08287196022115,382.9277310487827,382.7632911177072,382.5891625932212,382.4049700453887,382.2159135421237,382.0110435036902,381.7948231148406,381.5680444941765,381.3294961701168,381.07915498935165,380.8179499106597,380.5453613659392,380.2617668935906,379.96723150013,379.66443674494917,379.35185240244,379.0330143063471,378.7086672117677,378.38160978362856,378.0539065584277,377.72981200157744,377.3635677420934,376.9721446538458,376.5916187692196,376.41413226918206,376.4996082100825,376.6053535410543,376.7357346154499,376.8973688247585,377.09458258711766,377.33300715123175,377.61955990102064,377.96225850062365,378.37032228051464,378.85260342001203,379.4251549874871,380.09961330428104,380.89234578791013,381.821650453106,382.90862748080997,384.1772655846306,385.654828668342,387.37054367457,388.28418155851176,389.30719201747155,390.50088640551166,391.8920526697949,393.51053131099843,395.3910735009355,396.4352210060857,397.0348069002825,397.76438080014685,398.64567771730003,399.70363329556534,400.96678801438776,402.4677251989805,404.24354045547665,406.3363382230495,408.79374880432545,411.669456542466,415.02372686765,418.9239169365279,423.44495184191385,428.6697462964248,434.6895508112433,441.6042022698573,449.5222619733649,458.5610300891794,468.84643404661523,473.98489873371057,473.98489873371057,473.98489873371057,473.98489873371057,473.98489873371057,473.98489873371057,473.98489873371057,473.98489873371057,473.98489873371057,473.98489873371057,473.98489873371057,473.98489873371057,473.98489873371057,473.98489873371057,473.98489873371057,473.98489873371057,473.98489873371057,473.98489873371057,473.98489873371057,473.98489873371057,473.98489873371057,473.98489873371057],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[387.9540694345895,387.95405340881103,387.95404444523854,387.9540263539853,387.9540066642215,387.95399562580684,387.95397339746296,387.9539492060922,387.9539356127547,387.9539083013803,387.9538785793738,387.95386184005093,387.9538282835293,387.95379176676335,387.9537711538063,387.95372992450825,387.95368506013324,387.95365967782186,387.9536090220488,387.95355390266656,387.95352264850754,387.95346041217863,387.95339269508173,387.9533542122561,387.95327774974663,387.95319455786154,387.9531471768302,387.9530532391862,387.9529510396305,387.95289270647373,387.9527773044069,387.95271029964266,387.9525799482337,387.9524381843108,387.9523557025218,387.9521955743115,387.9521008411413,387.9519199146602,387.9517232220961,387.95160662935797,387.95138439366696,387.95125050308235,387.9509993351249,387.9508455921212,387.95056164670507,387.9503851232187,387.95019597936806,387.9498613788263,387.9414237523195,387.9404911729059,387.9394725747334,387.93843093276695,387.93727508508675,387.93607492991447,387.93482363255845,387.9334048659991,387.93190139927026,387.93030000809637,387.9285855623947,387.92674073416697,387.92474566690964,387.9225810858754,387.92038365433643,387.9179857307835,387.9153530060649,387.9124621275363,387.90947794727924,387.90640071383046,387.9029957942038,387.899215794715,387.895274486135,387.89116292583174,387.88657201818836,387.8817509823945,387.876361820178,387.87098772606015,387.8649610906661,387.8585560095036,387.85173352660956,387.84407690955624,387.83622880196475,387.82776324699057,387.8185573909694,387.80878002780975,387.79850882658127,387.79003854083277,387.7722354057006,387.7597576759221,387.74654243311033,387.7324238046349,387.71746003884596,387.701510667073,387.684682819171,387.6667617019134,387.64777846587094,387.6276867013194,387.60644165610813,387.5839102903201,387.56023604593344,387.5352936814507,387.5090535138564,387.4814993744251,387.4525268325313,387.422479768404,387.39315452609475,387.36538529761197,387.3366396042087,387.3069994228903,387.276575339173,387.24551199447785,387.2139944306108,387.18225547080414,387.15058429778657,387.1193364133974,387.08894519177954,387.0599352696344,387.04138807995116,387.0192946340257,386.99911825458076,386.9838883919269,386.9759066247624,386.974849210251,386.98276602147877,387.00127562625374,387.03543778891617,387.08574082526013,387.1550010485381,387.2487448906796,387.37174428978875,387.527426503943,387.72349049668344,387.9199410075667,388.12490951687334,388.3505061789424,388.60297073521093,388.8829919670914,389.1943282968429,389.5408768990703,389.927023800873,390.3577195024749,390.8374979959144,391.3751132093728,391.9766349420461,392.65028573994925,393.4053824648659,394.25249208294133,395.20347818646763,396.27185456495243,397.4720217074703,398.7948899364634,400.26793115153595,401.9329608423823,403.8124990338859,405.9023317857417,407.7246036971217,409.72623732924194,411.93145168884723,414.3625945476427,415.9421614123552,417.22206708972766,418.6358272089875,420.1988103854488,421.9282231987069,423.84332103211773,425.96563852521604,428.31924013365904,430.93099091596895,433.8308471845879,437.0521660635434,440.6320322861797,444.6115997537315,449.0364444826279,453.9569246364988,459.42854242996543,465.5123018901038,472.2750558744892,479.7898354955797,488.1361553208397,493.9202381626699,493.9202381626699,493.9202381626699,493.9202381626699,493.9202381626699,493.9202381626699,493.9202381626699,493.9202381626699,493.9202381626699,493.9202381626699,493.9202381626699,493.9202381626699,493.9202381626699,493.9202381626699,493.9202381626699,493.9202381626699,493.9202381626699,493.9202381626699,493.9202381626699,493.9202381626699,493.9202381626699,493.9202381626699],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[384.4898490308923,384.4898842836014,384.49000104420816,384.49012813049575,384.49026647094536,384.4903130083732,384.4904669620389,384.4906345293108,384.4908169342461,384.49087836904573,384.49108136440634,384.4913023084358,384.4915428148967,384.49162391649537,384.49189157758747,384.4921829031758,384.4925000208576,384.49260708602657,384.49296001581354,384.4933441467111,384.4934760074114,384.49390362612553,384.4943689940843,384.49487550097325,384.4950495790118,384.4956134415153,384.4962270781334,384.49689495844467,384.49712477471667,384.4978683033761,384.498677463925,384.4989605151018,384.4998615592499,384.50084202549544,384.5019090378192,384.5022827375338,384.5034709501142,384.5047638986723,384.50522418367746,384.5066643780782,384.50823134115404,384.5087982885503,384.5105441741996,384.5072090352184,384.53543183430736,384.5388995563522,384.54231946741385,384.54622325140224,384.5502985942754,384.55518816692285,384.56000315471266,384.5647951918037,384.5702674594376,384.57662947069366,384.5826589115761,384.5898878528004,384.59697677853626,384.6047957947319,384.61353560174217,384.62244146390844,384.6315484354093,384.64207995286506,384.65310956555373,384.6660471307967,384.67852864176535,384.69190414652286,384.7063431564687,384.72371697296404,384.7392478196888,384.7582080779827,384.7791295229496,384.802238702232,384.82584921789754,384.85381608583975,384.88092977833094,384.91264332764035,384.94411451205224,384.9788664361241,385.01705740731774,385.0573567012208,385.10009457043475,385.1468831967066,385.19661444188057,385.2507313109298,385.3073639432046,385.369826374937,385.435597011857,385.50671931788736,385.5832993683851,385.66578707471047,385.7540086199651,385.8489406815349,385.9490922852734,386.05671991905535,386.1724745455633,386.29699803846717,386.43098690368254,386.57519744886264,386.7304515042738,386.8976427601776,387.0777437940785,387.2718138703898,387.4810076054661,387.7079738832086,387.9519494809099,388.2194511990458,388.5105727371423,388.82509006923146,389.1645951209986,389.53163479098646,389.92775143041047,390.3563873212806,390.8205114232782,391.3231835783084,391.8670697367781,392.45653061709356,393.09534135597056,393.7894086297254,394.5423353994155,395.34553043853157,396.2395147909663,396.5938330376807,396.88041875283074,397.1884303658544,397.5210568222872,397.87744314668123,398.2621806284844,398.68084013014203,399.1334322372362,399.6227205883645,400.1519077295525,400.7245410534571,401.34452143278946,402.01613325449944,402.74410784625474,403.53365034141757,404.39047714406524,405.3207264879097,406.3288012833094,407.42823714716667,408.62456845517397,409.927242362416,411.34671099915397,412.89411230272435,414.57962763335047,416.4239264055227,418.4392826940289,420.64260650445186,422.90576632839515,424.385645093112,425.9972174401758,427.75738647188336,429.68332325758934,431.78759233575295,434.0908145261209,436.6135479358094,439.38026324569836,441.3410543450038,443.04727497796796,444.91568175294236,446.9744951491039,449.23457289535196,451.71706194352726,454.4457473250284,457.4470204263281,460.75014049812114,464.3875138174119,468.39498955490535,472.8121708331846,477.6827388350921,483.0547871617884,488.98116298435747,495.51981093399644,502.7341152003047,510.6932350333991,519.4724288590409,529.1533626010646,533.0337875452802,533.0337875452802,533.0337875452802,533.0337875452802,533.0337875452802,533.0337875452802,533.0337875452802,533.0337875452802,533.0337875452802,533.0337875452802,533.0337875452802,533.0337875452802,533.0337875452802,533.0337875452802,533.0337875452802,533.0337875452802,533.0337875452802,533.0337875452802,533.0337875452802,533.0337875452802,533.0337875452802,533.0337875452802,533.0337875452802],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[508.53011188698247,508.5301124190596,508.5301518504361,508.53019518959474,508.5302428242494,508.5302435272035,508.5302959342814,508.53035353528236,508.5304168455469,508.5304177744929,508.53048742763025,508.53056398419886,508.53056512940714,508.5306493571969,508.5307419324529,508.5308436834302,508.53084519780987,508.53095714470646,508.5310801867908,508.5310820549047,508.531217428679,508.5313662188515,508.5315297567072,508.5315322296678,508.53171215866405,508.5319099210225,508.5319129747397,508.53213056300956,508.532369716448,508.53237348961267,508.53263662456106,508.53292583814067,508.53293050374015,508.5332487270263,508.533598488131,508.53360426252107,508.5339891180923,508.534412115066,508.5344192697282,508.534884725602,508.53489298569394,508.53540518842544,508.53596814811124,508.5359784075426,508.53659793506375,508.5298866663979,508.5298994594632,508.5299132280506,508.52992805060455,508.57836737260067,508.5819357362992,508.5856499085158,508.5896544446677,508.59412424203543,508.5986603704271,508.60338045289257,508.6091276916155,508.6147272661251,508.6210398318127,508.6274507018622,508.63497095406916,508.64200338045964,508.6505644103572,508.65883922372626,508.66801528100854,508.6783662768056,508.6887981919022,508.7008658739382,508.71336107405745,508.72643383835845,508.74190915240445,508.75668302424896,508.7725171959476,508.7917121472384,508.81255568905124,508.83526494762094,508.8599856846162,508.88518104642844,508.91282445606737,508.94138022949124,508.97283955391794,509.0074005596778,509.0435630386394,509.08325283042626,509.1250857550442,509.170827583379,509.2193107453203,509.2708650823333,509.3271102982465,509.3870390487801,509.4521387902217,509.5216124451165,509.59592329544336,509.67629230082173,509.76290184761365,509.85551309984623,509.95569811217996,510.06304754657435,510.17824787376424,510.3008110857458,510.43278619133923,510.5749387571801,510.7281025075372,510.89210999807443,511.07003032526865,511.2641016247317,511.4738420697249,511.7005255339466,511.943745222193,512.2071150364395,512.4912207208846,512.7986646955436,513.1316293823047,513.4922245875119,513.8816350148991,514.2789047507043,514.5881680960508,514.6000218389869,514.6131087187938,514.6277266983984,514.6443829632772,514.6636961218037,514.686426841127,514.7107466120763,514.7385445400845,514.7696318920944,514.805227816129,514.8456626287381,514.8916046024161,514.9443687814678,515.0037958486917,515.0714214634548,515.1484410374309,515.2362050588476,515.336258540669,515.4503716977372,515.5805728419509,515.7291862236007,515.8991855385876,516.0929556256727,516.3143444723559,516.5673563253551,516.856577791399,517.1873306530958,517.565425529406,517.9978763563494,518.4925600564632,519.0583894081682,519.7056480330405,520.0367678235617,519.9810550968582,519.9547763510252,519.9654426181453,520.0212558996059,520.1326545657367,520.3108768271643,520.569416960018,520.9239912176029,521.3327242782686,521.8714133586409,522.5688961446747,522.9958233456238,523.4534097776029,524.001517847367,524.6550299746976,525.4310701098083,526.349313742206,527.4323342213163,528.7059876923022,530.1998384486665,531.9476257435805,533.987772028283,536.3639311440802,539.125573139823,542.328600096007,546.0359846234359,550.3184196386816,555.2549647483114,560.9336713457542,563.9670465484377,563.9670465484377,563.9670465484377,563.9670465484377,563.9670465484377,563.9670465484377,563.9670465484377,563.9670465484377,563.9670465484377,563.9670465484377,563.9670465484377,563.9670465484377,563.9670465484377,563.9670465484377,563.9670465484377,563.9670465484377,563.9670465484377,563.9670465484377,563.9670465484377,563.9670465484377,563.9670465484377],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[263.8024020916321,263.80213956190613,263.8018561633169,263.8015502069211,263.80133502125454,263.80098920867937,263.80061591878143,263.80021292886084,263.79977783307794,263.7994733467835,263.7989816504307,263.7984508505151,263.7978777810682,263.7974758319806,263.79682816857087,263.7961290209489,263.79537422295857,263.79484361424255,263.7939905089849,263.7930696195782,263.7920754639036,263.7910021084314,263.79025129894217,263.78903834155216,263.7877289273737,263.78631524954255,263.7853241129941,263.7837264628398,263.7820018282005,263.78013993823254,263.7788315479138,263.77672721682717,263.77445571778946,263.77200353972955,263.76935606101694,263.7675046829857,263.76451296438313,263.76128341846936,263.75779681558464,263.75535282255237,263.7514126232107,263.7471593964659,263.7425678577171,263.7393415553723,263.7341523556133,263.7285511929853,263.7225048306436,263.7182458038265,263.7260431848914,263.6975442596761,263.6899437159391,263.68146525137416,263.6725750244337,263.66287004642913,263.65257204768295,263.64159173455573,263.62982947335837,263.61716935475994,263.6039596898256,263.58912070998764,263.5734884682516,263.5569637158582,263.5388631043653,263.5196011639679,263.4990541138314,263.47708916208575,263.4535578782371,263.42829444879834,263.401113760357,263.37181042387977,263.34115103325604,263.3069889505612,263.271065687973,263.2332541858401,263.19226665617595,263.14775860173734,263.1007091752121,263.0495344628864,262.99530279392457,262.9362796710747,262.87355246058786,262.8068127538768,262.7327246448427,262.6552823005275,262.5712682506092,262.48165222711464,262.3859281957566,262.28361691286807,262.1741863283009,262.05709310042204,261.93090301364686,261.79799411232705,261.65952051184377,261.511473278838,261.3572137675312,261.17648612315566,260.9975655356411,260.806030648366,260.6017625395326,260.3840570793391,260.15621880186075,259.90815907564934,259.6436765188287,259.3618055341965,259.0615442803041,258.74185726747373,258.40167897413454,258.03991869408964,257.65546686371727,257.24720316329257,256.8140067397506,256.3547689606768,255.868409182722,255.33658922089165,254.79177809123468,254.21677971612,253.6106706249609,252.97371010952867,252.3055249213212,251.60521793513095,250.87287287108754,250.110704981298,249.31881998546623,248.5000619725357,247.65603044506324,246.7905521821062,245.91063841267493,245.019988661275,244.1282235269884,243.24075943576293,242.23243191477815,240.6671600994378,239.00093238753522,237.22932837775858,235.34689516037702,233.34995373911366,231.2332167680652,228.9912405281193,226.62234117210244,224.12232999961086,221.4890638644142,218.72192268546928,215.82197811615939,212.79265819240845,209.6403874862076,206.3754474935371,203.0117418026077,199.57321704811082,196.0878113129047,192.59451234703087,189.14433530334716,185.80122001436152,182.652153684016,179.79961707981258,176.95693848093342,174.31757328420665,172.1809763711931,170.71127068375748,170.10482054978766,170.22114886304507,170.72747888619992,170.1612244637621,169.19338241578922,168.33768847226946,167.63245177985613,167.12311751611963,166.86305462339308,166.9142439101469,167.34777868878604,168.24407446475882,169.69267984317895,171.79159616795658,174.6460558448264,178.36678111660066,183.06783890963302,188.86430397564362,195.8700138240129,204.1957176279259,213.94787381090075,225.22824622817598,228.20767595569532,228.20767595569532,228.20767595569532,228.20767595569532,228.20767595569532,228.20767595569532,228.20767595569532,228.20767595569532,228.20767595569532,228.20767595569532,228.20767595569532,228.20767595569532,228.20767595569532,228.20767595569532,228.20767595569532,228.20767595569532,228.20767595569532,228.20767595569532,228.20767595569532,228.20767595569532],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle EN pour le paramètre<br>elasticnet__l1_ratio=1.0<br>en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE ElasticNet pour tout le meilleur paramètre l1 ratio\n","FigRMSEGRidEN = visuRMSEGrid(ElasticNet(), 'EN', alphasEN, 'alpha', GridEN,\n","                             BestParametresEN, 'elasticnet__l1_ratio')\n","FigRMSEGRidEN.show()\n","if write_data is True:\n","    FigRMSEGRidEN.write_image('./Figures/EmissionsGraphRMSEEN.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.1.5 Modèle kNeighborsRegressor"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                          paramètre  KNeighborsRegressor()\n","0  kneighborsregressor__n_neighbors                      5\n","      KNeighborsRegressor()\n","R²                 0.602850\n","RMSE             321.416814\n","MAE               91.582383\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predkNN=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[24.133999999999997,41.269999999999996,116.202,43.397999999999996,36.084,37.988,660.9419999999999,25.375999999999998,62.55,163.43800000000002,95.492,46.054,91.032,34.294000000000004,288.422,2074.036,122.58000000000001,35.815999999999995,145.76,82.042,5.097999999999999,39.477999999999994,210.89600000000002,34.757999999999996,36.336,37.584,33.791999999999994,37.896,77.288,20.636,34.07000000000001,52.398,92.072,35.786,28.398000000000003,8.242,16.56,60.464,28.748,15.959999999999999,3.7020000000000004,69.426,24.456,22.756,30.43,40.672000000000004,21.51,35.47599999999999,69.45,10.048,122.79400000000001,13.65,36.712,92.26000000000002,428.592,120.826,182.124,30.912,33.322,126.524,76.99199999999999,146.27,54.843999999999994,22.118000000000002,52.398,27.407999999999998,30.921999999999997,151.38,48.544,25.555999999999997,28.428000000000004,88.576,8.056000000000001,58.912,35.32,26.196000000000005,5.314,36.236000000000004,15.738,430.88599999999997,135.21999999999997,32.274,16.94,40.769999999999996,156.338,20.312,31.351999999999997,42.141999999999996,45.366,19.264,88.69800000000001,24.362000000000002,26.329999999999995,11.97,39.806,579.812,35.702,30.088,28.924,20.658,10.936,8.440000000000001,7.686,16.302,48.616,108.55799999999999,76.268,25.278,74.346,12.664,221.08800000000002,74.852,19.625999999999998,325.662,42.718,8.572,24.394,536.938,61.669999999999995,9.16,42.23,37.08,46.482,30.48,156.964,48.214,18.22,33.111999999999995,37.552,63.120000000000005,53.862,44.028,11.642,77.63399999999999,15.074000000000002,27.242,56.751999999999995,12.166,168.856,47.263999999999996,97.026,28.014,35.742,187.608,4.822,20.351999999999997,114.63999999999999,97.862,24.05,11.936,41.24999999999999,51.50600000000001,18.47,11.01,21.546,79.314,40.622,19.932,61.376,9.463999999999999,126.26400000000001,101.18800000000002,105.756,37.010000000000005,19.412,22.522,44.498000000000005,51.602,31.009999999999998,84.05799999999999,37.54600000000001,47.888,82.44,30.332,1690.89,68.982,23.907999999999998,35.936,113.898,492.858,283.42400000000004,303.206,31.758000000000003,54.553999999999995,33.980000000000004,27.947999999999997,96.40599999999999,22.651999999999997,57.774,25.458,80.00999999999999,36.260000000000005,47.194,74.606,47.338,150.706,58.986000000000004,31.417999999999996,427.15200000000004,42.616,3.65,32.616,48.05,6.266,66.166,38.32,214.33599999999996,268.07599999999996,90.32600000000001,11.591999999999999,88.6,192.06800000000004,270.884,6.238000000000001,112.054,15.146,30.196000000000005,35.190000000000005,49.897999999999996,8.036,39.634,20.2,31.276,154.74,25.302,5.8100000000000005,28.748,110.02000000000001,43.604,65.694,20.967999999999996,26.624000000000002,159.468,29.514,99.3,54.98,50.436,71.054,101.87,9.074,80.47999999999999,27.068,21.386000000000003,143.246,63.04799999999999,101.41799999999999,35.815999999999995,22.88,28.536,230.516,22.139999999999997,160.308,106.61800000000001,27.809999999999995,14.9,35.424,54.608000000000004,5166.964,30.698,37.762,344.356,161.78000000000003,44.71,124.196,36.842,36.45399999999999,110.55999999999999,27.110000000000003,36.260000000000005,71.054,120.56199999999998,45.224,58.246,121.04,206.56199999999998,36.8,37.57600000000001,49.684,33.581999999999994,35.120000000000005,29.492,11.414,7.628,4.406000000000001,28.979999999999997,12.238,62.122,58.226,56.086,15.745999999999999,34.024,9.975999999999999,60.01800000000001,39.478,6.168,21.352,51.702,66.054,46.709999999999994,192.828,24.472,39.67,122.78800000000001,222.12399999999997,35.104,18.052,31.448,27.077999999999996,107.18000000000002,43.004000000000005,101.088,60.464,9.304,55.898,6.803999999999999,127.34200000000001,154.652,16.302,18.644,66.912,97.53399999999999,14.436000000000002,80.316,158.776,78.80199999999999,27.028,12.778000000000002,22.735999999999997,119.224,242.13399999999996,21.848000000000003,110.276,25.562,22.986000000000004,24.774,19.974000000000004,152.996,73.534,39.082,47.233999999999995,42.93,25.486,74.982,122.48599999999999,29.432,191.15400000000002,17.157999999999998,62.318,31.162,118.51200000000001,3.5400000000000005,52.94000000000001,48.983999999999995,49.836,141.472,155.73600000000002,42.838,73.038,60.553999999999995,51.513999999999996,12.664,15.642,4.524,135.83,9.074,16.689999999999998,171.542,35.001999999999995,8.802,42.524,197.382,12.906,24.054,133.844,247.42200000000003,5.617999999999999,216.046,326.954,11.943999999999999,42.644,20.672,235.19,439.1820000000001,5.8100000000000005,161.458,10.052,149.25,41.501999999999995,55.218,22.023999999999997,36.019999999999996,268.224,7.928,44.001999999999995,18.009999999999998,34.272000000000006,47.263999999999996,37.992000000000004,135.13,228.75,98.252,65.47999999999999,115.46799999999999,94.13199999999999,88.936,26.206,65.812,10.598,79.386,35.958000000000006,224.414,31.986,38.370000000000005,17.058,78.35800000000002,21.413999999999998,20.491999999999997,22.148,32.65400000000001,96.742,1253.9360000000001,308.72200000000004,88.554,22.932000000000002,83.54799999999999,292.91400000000004,157.26000000000002,10.764,27.953999999999997,85.49600000000001,33.91,43.525999999999996,15.124,52.331999999999994,29.086000000000002,82.69200000000001,28.094,27.451999999999998,130.704,14.87,185.312,70.812,5.66,37.584,87.42999999999999,5.638,55.132000000000005,39.872,8.068000000000001,178.838,237.244,30.43,130.35999999999999,22.599999999999998,14.722,8.526,84.00399999999999,136.91799999999998,7.628,63.872,32.510000000000005,9.475999999999999,435.302,21.928,6.424000000000001,35.652,38.63,40.769999999999996,19.996,367.32800000000003,13.846,17.534,68.18199999999999,155.068,41.767999999999994,42.506,282.788,16.56,46.686,51.602,48.528,30.381999999999998,72.838,14.186000000000002,125.378,56.378,193.368,205.81,57.822,33.339999999999996,58.54600000000001,32.498000000000005,48.484,12.258000000000001,154.47799999999998,43.525999999999996,159.448,25.574,40.844,23.364,74.03,146.83599999999998,37.298,151.38000000000002,81.49000000000001,132.73,14.344000000000003,83.936,23.240000000000002,54.056000000000004,28.094,44.942,54.104,39.67,59.646,30.227999999999998,40.866,36.648,160.21200000000002,66.166,13.26,17.592,200.07999999999998,97.01599999999999,50.306,23.787999999999997,37.065999999999995,90.248,45.836,89.558,8.225999999999999,5.084,141.50000000000003,91.91400000000002,136.67600000000002,56.534000000000006,432.646,102.44800000000001,171.08200000000002,36.558,51.602,142.18200000000002,2117.6299999999997,1588.3920000000003,40.824,439.174,19.504,59.739999999999995,27.091999999999995,29.728,60.298,47.263999999999996,85.51599999999999,51.116,72.396,7.470000000000001,6.22,466.854,111.44799999999998,25.380000000000003,124.08999999999999,55.69200000000001,52.94000000000001,23.852,128.6,15.778,168.55199999999996,7.13,56.086,30.432,164.05,8.956000000000001,209.298,18.396000000000004,5.708,76.74799999999999,48.914,22.118000000000002,42.134,99.686,21.624000000000002,30.421999999999997,9.716,56.378,122.152,83.824,277.77799999999996,80.89599999999999,234.30599999999998,39.116,26.419999999999998,43.275999999999996,195.4,54.784000000000006,15.592000000000002,96.39000000000001,12.33,7.728,5.912,64.388,32.362,12.053999999999998,36.646,57.208000000000006,97.294,86.89,5.970000000000001,20.054000000000002,185.31199999999998,3046.6340000000005,27.815999999999995,14.288000000000002,139.714,92.928,145.57999999999998,156.74,58.552,33.198,44.257999999999996,88.70599999999999,7.998,257.374,13.752,58.22800000000001,76.976,13.712,26.329999999999995,3.938,71.084,6.954000000000001,56.25,7.6800000000000015,125.30999999999999,68.84400000000001,121.73200000000001,26.692,136.376,133.536,37.56,88.71,15.169999999999998,47.31,355.928,5.144,35.962,37.544,138.49399999999997,51.074,92.484,244.858,94.42800000000001,78.946,16.992,31.070000000000004,212.32200000000003,62.04,89.426,64.72,28.964,36.196000000000005,54.29,42.36,17.992,54.708000000000006,45.81400000000001,164.262,129.594,29.52,61.592,26.8,123.91000000000001,6.154,194.17800000000003,146.226,16.014,336.446,321.4440000000001,112.638,40.007999999999996,7.014,13.475999999999999,29.375999999999998,86.94,122.60799999999999,35.794,24.098000000000003,18.198,18.116,24.592000000000002,36.196000000000005,102.28,57.45,4.25,3.7020000000000004,131.048,23.524,78.64200000000001,21.928,79.314,26.206,22.954,62.758,296.098,5541.147999999999,119.57400000000003,72.744,11.514000000000001,94.406,15.163999999999998,17.586,111.65,360.99000000000007,286.25,100.248,75.982,18.22,77.922,27.256,24.592000000000002,6.378,30.774,5.272,30.594,28.47,5.942,30.395999999999997,171.668,17.854,20.968,36.306000000000004,28.571999999999996,209.68400000000003,233.62399999999997,13.809999999999999,19.358,18.268,37.516000000000005,119.68999999999998,15.568000000000001,84.186,90.60799999999999,155.238,48.176,186.406,277.55999999999995,15.684000000000003,32.824,41.013999999999996,79.14600000000002,164.27599999999998,94.882,47.598,22.252000000000002,48.506,15.288,10.102,112.66799999999998,38.62,17.854,8.127999999999998,43.75,246.87800000000001,104.94800000000001,160.13600000000002,34.106,177.35600000000002,58.21,21.778,67.974,41.465999999999994,22.034,112.454,752.446,11.226,56.15599999999999,66.31800000000001,69.554,113.35999999999999,230.516,123.578,18.954,28.820000000000004,27.145999999999997,165.92400000000004,8.568,164.762,129.12,79.00799999999998,63.181999999999995,70.244,46.09,291.252,596.0379999999999,64.62800000000001,61.846000000000004,192.55,124.304,11.687999999999999,32.378,182.228,36.00599999999999,25.726,4.15,341.35200000000003,65.27799999999999,37.67,292.91400000000004,14.947999999999999,44.73,35.702,29.742,21.32,4.15,72.59599999999999,77.352,7.562,193.18,112.23799999999999,8.34,182.124,27.244,49.55800000000001,37.114,46.727999999999994,58.54600000000001,58.54600000000001,14.264,10.326,21.442,29.401999999999997,404.93199999999996,36.554,127.02799999999999,34.232000000000006,92.596,40.76,28.398000000000003,224.56599999999997,4.692,80.352,256.39,15.735999999999999,39.242,8.610000000000001,72.064,57.886,149.06400000000002,113.636,74.856,34.471999999999994,51.602,39.518,36.558,140.288,28.653999999999996,120.696,70.388,195.916,30.002,17.094,151.092,36.712,36.896,7.508,14.553999999999998,27.256,20.856,79.792,18.002,49.922000000000004,49.748,214.07799999999997,35.096000000000004,49.668,34.074000000000005,178.338,112.638,14.190000000000001,40.624,113.66,22.630000000000003,18.432,156.66000000000003,51.004,37.742,276.336,34.086,14.672,31.153999999999996,75.85600000000001,26.526,258.014,5.99,29.272000000000002,54.866,18.142,64.2,3172.976,65.592,55.096000000000004,264.126,76.044,21.778,37.368,7.168000000000001,512.6800000000001,92.054,270.884,55.1,21.95,164.294,16.689999999999998,53.676,40.844,9.964,268.492,278.3,8.068,52.236000000000004,63.477999999999994,221.292,131.328,631.51,24.204,120.52400000000003,27.676,28.183999999999997,561.368,27.106,40.477999999999994,61.84599999999999,31.622000000000003,15.568000000000001,78.634,226.12999999999997,239.706,42.926,98.602,94.58200000000001,186.58800000000002,37.862,11.719999999999999,75.922,21.146,15.514,122.60799999999999,210.31400000000002,10.998000000000001,21.744000000000003,17.692,78.32000000000001,7.152000000000001,29.892000000000003,36.62,55.29599999999999,328.51800000000003,392.786,78.514,56.976,32.362,519.926,10.01,24.821999999999996,132.898,52.398,10.108,8.856,73.258,109.894,101.51599999999999,24.720000000000002,19.922000000000004,103.96000000000001,130.98200000000003,181.45000000000002,32.257999999999996,635.848,84.32399999999998,131.28000000000003,70.10799999999999,209.298,52.23599999999999,66.782,25.582,10.598,15.395999999999997,14.844000000000003,45.38999999999999,12.604000000000001,25.056000000000004,48.556000000000004,50.682,37.562,66.04599999999999,56.156000000000006,135.18200000000002,584.7,99.04,81.87,144.20600000000002,29.786,199.794,87.538,7.566,60.464,40.556,57.144000000000005,62.26800000000001,205.06,29.596000000000004,137.48,203.11200000000002,18.446,10.315999999999999,14.290000000000001,155.344,70.772,38.779999999999994,6.075999999999999,31.426,31.197999999999997,15.681999999999999,24.520000000000003,140.51399999999998,48.268,61.477999999999994,61.605999999999995,40.858,69.094,193.47600000000003,27.095999999999997,29.51,28.848000000000003,92.81,48.272000000000006,25.561999999999998,34.738,47.263999999999996,9.866,15.508,378.65999999999997,14.662,14.916,9.724,225.658,111.59400000000001,36.553999999999995,67.20599999999999,26.054000000000002,5.912,235.63600000000002,49.27,148.85200000000003,44.690000000000005,98.64399999999999,72.7,8.526,4.244,314.51599999999996,29.868000000000002,81.248,44.122,17.854,40.488,19.264,40.574,36,57.886,32.532,27.744,33.066,52.596000000000004,10.477999999999998,14.690000000000001,95.42,36.646,56.42,491.01399999999995,144.44400000000002,13.974,14.419999999999998,42.36,36.339999999999996,15.568000000000001,80.67,14.392,28.944,24.226000000000003,30.594,23.908,79.35000000000001,28.839999999999996,68.31800000000001,336.114,6.953999999999999,44.73,154.248,60.30800000000001,349.348,24.098000000000003,75.846,11.162,30.432,72.538,23.776000000000003,58.61600000000001,42.48,6.238,22.718,165.24,6.9079999999999995,30.072000000000003,35.426,19.503999999999998,87.792,21.131999999999998,252.92,17.608,11.620000000000001,66.04599999999999,19.993999999999996,68.78399999999999,213.46599999999998,92.644,205.13600000000002,76.968,23.907999999999998,59.194,35.022000000000006,294.294,84.608,142.988,218.93800000000002,7.686,16.119999999999997,14.85,50.236000000000004,56.136,3.8340000000000005,90.97999999999999,169.81799999999998,72.978,32.378,47.638,258.816,18.060000000000002,13.25,53.568,13.154,37.516000000000005,660.9419999999999,17.288,11.414,28.534000000000002,264.65999999999997,78.83,9.814,16.56,39.122,65.466,40.906000000000006,13.447999999999999,257.276,40.769999999999996,92.724,172.954,372.34,4.546,115.824,177.496,124.24599999999998,77.774,775.338,26.998,31.528,44.31999999999999,156.428,39.67,28.496000000000002,117.75199999999998,93.57,175.154,168.858,74.7,72.7,133.72200000000004,54.448,331.11199999999997,20.57,17.534,16.04,55.802,63.452,71.748,53.239999999999995,82.308,171.29199999999997,22.876,86.192,77.74600000000001,25.386,59.96,7.696000000000001,24.05,44.792,33.802,9.17,24.816,16.712,21.730000000000004,532.116,306.38,44.344,81.11600000000001,164.05,29.746,42.4,30.2,5.204,63.65199999999999,53.924,67.866,12.751999999999999,43.592,48.88,58.922000000000004,59.01800000000001,72.978,18.4,83.686,28.57,28.290000000000003,24.52,84.13399999999999,36.339999999999996,55.162,44.792,8.34,31.986,24.888,39.444,13.324000000000002,31.322000000000003,220.85200000000003,71.84799999999998,39.478,91.03200000000001,45.926,36.446,9.814,52.94000000000001,377.406,48.284,224.298,333.06,55.016,140.114,190.53399999999996,130.03,14.886000000000001,121.43000000000002,21.442,27.407999999999998,65.136,7.640000000000001,53.814,21.663999999999998,26.25,78.60199999999999,76.456,7.468000000000001,55.218,95.54799999999999,168.83800000000002,182.182,6.196,5.273999999999999,22.032,8.47,165.844,154.53,92.986,42.36,936.682,30.008,92.26,33.536,70.002,373.906,22.118000000000002,122.60799999999999,53.69200000000001,22.066,29.294,206.91,251.58199999999997,34.041999999999994,11.425999999999998,53.39399999999999,101.47,186.224,192.14399999999998,23.722,81.946,8.186,6.424000000000001,30.644,3046.6340000000005,121.978],"xaxis":"x","y":[74.33,21.29,69.48,25.53,5.45,4.54,191.97,66.44,13.95,108.04,265.21,72.16,29.57,16.01,133.5,324.25,281.25,53.02,33.09,263.51,4.56,37.34,233.43,20.85,9.83,46.7,6.07,49.22,466.95,3.88,45.83,11.84,1936.34,193.94,9.76,3.29,10.7,111.95,21.24,107.56,14.24,17.04,5.81,26.94,2.82,27.77,7.63,48.1,52.43,33.04,50.29,6.25,3.34,43.19,64.23,77.76,84.75,4.41,16.8,282.72,82.72,60.57,89.29,51.1,8.14,35.4,25.32,64.59,1.6,79.74,159.98,42.8,5.62,13.29,4.53,6.79,3.15,75.03,6.05,500.93,96.43,58.79,39.48,4.99,26.46,65.02,5.94,6.18,102.27,15.2,95.77,6.17,3.95,61.98,33.52,1148.5,37.82,79.98,138.68,34.48,4,3.35,4.39,4.78,105.01,126.03,56.48,25.9,570.98,18.33,387.49,16.44,8.65,13.87,29.78,1.08,12.29,1638.46,89.57,54.25,12.72,24.39,192.12,7.44,193.34,45.44,17.43,38.39,60.5,122.73,10.65,10.37,25.22,23.58,4.86,116.21,25.21,6.3,331.61,11.8,15.26,7.9,23.85,138.79,4.6,47.26,304.62,23.84,4.89,30.79,12.25,13.23,59.73,17.22,14.35,29.41,10.66,25.14,46.46,27.47,69.02,52.87,76.68,4.75,22.46,5.07,8.65,5.99,45.33,52.49,54.24,10.84,20.22,3.4,2937.83,170.01,30.26,82.58,229.2,569.61,2061.48,145.67,15.1,8.49,53.31,250.87,20.29,119.98,2.93,22.37,0.17,17.02,120.1,1.55,4.13,51.93,8.42,5.43,602.12,4.6,61.33,16.01,4.43,42.92,72.78,47.12,97.39,321.72,39.27,3.63,12.99,350.87,96.2,13.36,69.18,4.82,60.98,17.23,1.73,5.53,7.06,9.57,380.17,10.65,18.88,9.45,70.31,41.8,157.77,1.63,56.07,64.13,73.9,45.79,106.37,22.97,61.15,53.88,227.18,1.97,127.74,1266.06,29.93,33.28,22.38,249.46,6.12,4.44,4.55,4.2,5.28,293.56,30.19,515.71,3.52,5.67,46.79,11824.89,51.83,48.95,188.77,64.56,38.97,13.39,13.56,34.61,53.87,9.04,16.99,280.16,117.64,3.75,30.57,8.86,467.9,56.07,17.24,272.54,40.65,22.78,32.62,6.66,6.7,103.42,114.33,185.33,239.17,67.17,19.95,15.9,5.49,4.68,23.62,38.88,2.99,5.53,61.88,6.38,27.92,23.15,89.29,14.83,73.14,118.2,42.8,3.7,12.68,66.43,16.89,8.25,29.33,21.3,7.48,65.46,10.21,190.61,175.3,49.58,5.64,3.14,229.84,4.2,62.41,147.14,5.82,30.35,6.41,134.8,175.46,67.57,17.12,309.6,4.5,73.61,28.19,6.89,9.76,60.12,4.92,132.1,72.5,30.69,73.31,105.97,23.12,35.6,16.14,103.99,3.48,367.3,3.54,5.8,24.56,100.02,93.21,17.49,30.86,41.9,54.04,40.78,32.89,4.38,4.32,193.47,35.11,58.59,84.93,0,33.45,6.71,458.44,12.28,168.36,75,398.19,5.46,39.41,166.84,3.59,7.07,3.06,262.98,144.57,5.22,358.88,4.73,105.32,23.5,16.83,4.64,1.06,266.05,10.83,11.81,21.44,78.36,11.17,27.27,127.27,295.42,457.51,198.59,61.18,110.74,14.97,4.11,197.3,5.96,25.11,9.95,505.01,71.87,22.66,17.92,35.43,44.15,93.63,109.9,278.47,80.09,452.16,33.17,300.14,7.36,126.21,178.21,119.2,77.75,35.48,46.26,8.42,6.22,5.28,84.95,6.41,31.27,29.66,48.95,72.5,3.21,141.15,31.13,4.25,3.49,52,3.96,125.59,14.7,19.57,143.33,804.2,9.25,375.56,14.37,16.18,78.29,29.97,3.93,22.93,32.05,126.92,5.74,105.41,7.69,9.79,34.66,4.08,5.59,26.16,143.82,6,11.87,20.37,422.39,4.55,11.29,181.67,9.43,5.71,21.78,2.15,7.57,229.12,4.08,41.16,24.25,28.3,155.54,110.62,40.56,18.3,79.21,10.9,4.85,113.97,8.36,91.53,9.28,47.5,3.63,20.06,168.62,49.28,51.4,11.27,15.5,53.65,35.2,8.06,4,27.93,286.43,91.09,85.28,49.89,16.72,18.74,19.11,119.46,28.94,8.75,6.29,94.82,35.72,33.37,18.76,19.69,402.04,328.23,152.28,5.23,86.9,158.55,49.63,23.42,10.01,607.41,235.69,12.05,7.21,62.23,39.5,1265.29,290.4,45.33,463.12,70.84,93.83,53.71,9.53,184.09,28.37,13.1,48.99,18.47,5.15,6.37,31.15,35.8,17.14,379.34,161.39,127.56,6.9,84.59,4.33,351.9,27.99,124.54,30.1,59.14,21.57,25.12,6.01,7.97,78.92,13.71,1.55,72.9,70.16,4.04,7.47,10.02,75.99,77.04,11.18,107.16,38.13,39.57,227.13,4.43,98.88,104.61,20.02,5.63,130.28,138.35,8.08,1.08,6.96,16.46,5.94,64.79,9.67,45.38,30.19,5.79,32.42,191.97,636.96,30.02,1.73,85.59,8.61,199.48,70.17,3.69,41.74,33.12,39.73,4.35,180.14,20.5,74.98,23.63,23.9,5.79,334.15,132.32,10.29,9.39,4.14,553.27,28.63,249.98,41.96,147.36,128.91,0.78,209.92,7.34,22.15,180.89,4.12,9.43,1.59,549.74,25.93,523.95,128.22,124.77,10.48,5.53,63.77,1623.34,39.1,232.12,9.56,6.22,4.2,36.42,13.27,16.47,4.24,48.6,109.16,47.14,11.1,19.61,6.16,95.26,6.15,24.12,40.46,28.6,192.51,159.16,28.55,44.26,4.02,47.25,60.88,75.95,115.34,7.13,38.38,216.18,5.69,60.08,4.86,122.92,11.59,13.18,2.99,51.47,117.19,13.34,63.41,178.86,4.09,100.71,13.12,1874.42,11140.56,8.69,1060.13,64.06,13.71,75.58,27.38,158.48,213.9,20.09,39.57,53.48,6.04,54.76,4.19,4.39,3.44,21.71,9.16,9.82,3.65,6.08,34.03,28.21,27.63,3.83,45.07,13.21,218.49,1789.69,44.74,14.67,94.23,5.77,31.76,18.42,134.38,69.44,40.35,14.73,25.15,752.57,17.39,56.86,36.97,33.35,177.51,7.21,113.3,6.35,150.23,8.37,3.16,21.51,38.59,6.51,5.42,5.6,248.15,477.85,691.57,23.79,174.79,68.05,64.86,191.2,50.52,5.46,6.72,1699.45,5.07,35.8,376.46,21.12,58.88,348.21,29.53,0.92,9.67,13.18,92.21,4.8,426.39,20.66,5.95,48.34,71.03,59.84,234.75,723.4,133.7,158.6,250.05,69.42,25.39,2.79,298.61,94.02,39.67,5.67,1597.56,11.38,35.14,1019.53,10.04,2.69,63.37,4.31,6.73,20.76,86.81,28.05,49.6,110.41,27.33,8.64,61.86,59.76,60.67,1.78,11.97,14.49,43.15,7.52,12.62,6.26,225.35,675.34,5.54,3.78,17.11,84.83,119.01,11.54,393.2,22.74,40.25,46.52,34.03,5.84,3.54,51.89,125.13,106.74,21.38,7.39,33.58,3.27,18.91,15.7,54.02,6.86,22.81,17.08,1894.2,20.33,44.93,22.96,13.29,14.24,23.22,11.27,4.46,26.63,2.72,5.68,0.09,9.56,181.93,40.69,3.47,101.34,259.49,9.86,6.87,10.24,171.64,5.87,49.53,469.57,83.68,9.95,285.62,3.43,12.36,36.08,6.43,14.09,377.09,27.14,23.03,4.99,32.77,238.77,3243.48,93.31,5.94,455.39,67.24,29.62,9.96,5.91,234.15,39.44,314.92,154.96,5.46,86.99,20.09,260.13,6.67,39.57,147.82,188.13,5.03,9.73,5.79,45.6,21.92,740.97,59.05,120.84,47.92,108.79,84.64,6.35,15.43,9.72,12.71,26.21,47.24,22.17,68.3,46.6,6.85,2.85,84.18,12.92,15.34,18.1,4.84,12.75,34.14,47.71,129.42,45.02,7.85,202.65,4.45,4.31,148.25,15.09,123.18,2452.86,38.86,15.9,21.77,301.02,5.45,17.75,47.99,16.3,6.74,11.94,176.79,4.74,51.98,4.16,5.9,64.57,46.78,313.83,179.33,1088.2,237.36,319.73,161.2,19.39,85.73,273.91,42.18,4.62,6.33,67.45,15.37,47.64,4.89,24.89,5.6,4.19,254.16,136.94,221.51,834.96,1.14,8.16,202.99,9.71,71.55,59.07,9.36,138.61,18.9,481.97,42.99,134.01,54.41,21.44,50.46,85.95,33.92,7.69,98.95,99.96,78.7,0,15.6,51.39,3.98,25.48,58.28,62.17,88.51,9.44,24.51,36.78,88.74,5.65,215,32.48,92.45,22.53,9.11,25.38,45.32,128.39,9.46,69.59,3.28,82.07,8.45,94.59,65.35,86.86,103.77,1.85,3.02,434.48,5,133.8,8.8,587.16,214.65,15.11,3.5,433.6,75.48,19.07,4.26,4.19,41.96,3.76,5.97,10.23,61.62,4.82,28.61,86.9,36.72,16.89,9.62,103.39,303.69,37.51,225.66,7.6,92.19,3.67,38.85,9.29,23.78,14.86,3.41,176.75,59.74,6.32,111.04,30.03,17.57,14.04,399.73,3.77,13.09,87.07,154.6,28.88,6.3,60.52,6.73,13.35,65.05,30.78,103.4,231.39,4.92,0,565.79,103.07,9.57,21.05,18.86,99.39,5.91,206.01,6.03,9.15,10.6,8.63,106.73,509.36,5.44,193.72,37.74,125.42,38.56,3.05,132.71,148.7,130.73,218.53,4.52,4.51,8.28,16.32,68.14,3.56,78.06,66.23,70.39,15.43,178.65,355.05,0.92,3.04,284.01,61.07,6.98,67.78,18.65,11.62,7.73,319.88,7.49,38.67,7.85,5.04,11.39,33.8,19.7,359.09,3.3,55.69,117.11,1990.5,4.51,44.96,0,95.54,14.3,481.06,31.47,2.36,62.23,205.52,54.4,10.95,28.78,107.84,222.02,124.96,54.53,117.04,21.08,27.02,142.16,4.15,51.24,6.18,3,210.3,1.24,138.76,39.75,21.51,71.23,46.3,49.46,9.4,85.76,7.74,8.16,101.31,5.35,4.13,47.08,4.48,8.36,74.07,235.32,12.63,7.74,86.46,15.97,16.58,3.99,7.56,48.35,14.65,33.43,35.39,44.55,37.49,7.78,25.12,82.9,6.31,1.67,4.15,10.38,14.3,137.15,1.68,120.91,130.69,79.58,9.73,34.2,38.78,62.98,8.47,25.2,12.05,77.34,0.31,53.12,127.14,243.13,6.09,567.6,4.43,52.67,139.72,80.75,1185.15,63.33,30.95,4.65,303.81,6.25,8.85,23.95,5.17,41.22,3.81,1.86,72.5,51.28,41.3,37.56,22.1,36.09,49.17,6.71,3.5,67.06,4.41,394.6,38.41,271.68,138.03,1129.67,2.34,12.26,20.86,87.14,238.61,43.97,107.83,21.66,5.88,17.45,16.63,103.57,23.8,48.16,45.58,10.97,10.29,28.09,26.56,40.05,9.63,5.02,5.82,1596.08,293.55],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle KNeighborsRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predkNN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle kNN\n","# réglage des paramètre pour la gridsearch\n","n_neighbors = np.linspace(1, 100, dtype=int)\n","param_gridkNN = {'kneighborsregressor__n_neighbors': n_neighbors}\n","\n","\n","GridkNN, \\\n","BestParametreskNN, \\\n","ScoreskNN, \\\n","TotalGHGEmissions_predkNN, \\\n","figkNN = reg_modelGrid(model=KNeighborsRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBM_train,\n","                         X_test=BEBM_test,\n","                         y_train=TotalGHGEmissions_train,\n","                         y_test=TotalGHGEmissions_test,\n","                         y_test_name='TotalGHGEmissions_test',\n","                         y_pred_name='TotalGHGEmissions_predkNN',\n","                         score=score,\n","                         param_grid=param_gridkNN)\n","\n","print(BestParametreskNN)\n","print(ScoreskNN)\n","figkNN.show()\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[383.85221924419704,356.56938402019773,352.13840127006324,364.0378345678553,362.9567378743617,360.87515123861266,364.86008238597884,365.0744396341462,360.0689407122069,363.71912368301565,371.1434800539335,367.99594237105265,369.7433546705576,372.92644626668107,369.0056993942216,371.43452665686743,373.4891674028322,375.8849208909543,377.54303504704154,379.3483674547251,380.99560803785727,382.3983724749079,383.3554281954906,385.09331472572813,386.67573192564686,387.6691359140329,388.58316241477115,389.73217716516956,390.69419197420456,391.53591502542474,392.35442897046426,393.4223068104203,394.1611047727706,394.3131654617599,395.2792283415041,396.2505711355478,396.2608762299782,396.89800271878534,397.72648801009177,398.5094965483508,399.3849848709143,400.0713714971302,400.590586937442,401.351583487594,401.8697584577628,402.49890758615595,403.1707208280216,403.8610017054645,404.3881653601688,404.8705442505796]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[507.8832269458885,459.90500925516875,459.7439863995978,471.7952343128885,471.6273140289443,471.08634769370894,473.63700131414674,478.945742234247,473.61868090419887,481.66934148589706,489.029976556668,486.67074898678385,489.98788291586555,492.5124025649139,488.90911137038484,492.0308370542472,494.6412603870266,497.44678786053316,499.01709077878064,501.0799114000621,503.38316112045925,505.38951824773335,505.3529866179075,507.2409775128205,508.951780022158,510.34110604867755,511.0516058683156,512.1883144990634,513.3804224886412,514.5359544375317,515.7923163409725,517.320691250297,518.0462795446737,519.1748009659308,520.2845411810238,521.3726968362737,521.5888667925043,522.5057241285598,523.3548101880905,524.2649879942096,525.2706608366689,525.9533970130055,526.567865271786,527.4636592531176,527.9346423417733,528.6498287525499,529.2221093888788,529.9657772131413,530.4549270275154,531.2849950548584]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[259.8212115425056,253.2337587852267,244.53281614052867,256.28043482282214,254.28616171977907,250.66395478351637,256.08316345781094,251.20313703404537,246.51920052021492,245.76890588013424,253.25698355119906,249.32113575532145,249.49882642524966,253.34048996844825,249.10228741805835,250.83821625948767,252.33707441863785,254.3230539213754,256.06897931530244,257.61682350938815,258.6080549552553,259.4072267020824,261.35786977307373,262.94565193863576,264.3996838291357,264.99716577938824,266.1147189612267,267.2760398312758,268.0079614597679,268.53587561331784,268.9165415999559,269.5239223705436,270.2759300008676,269.45152995758906,270.27391550198445,271.1284454348218,270.93288566745207,271.2902813090109,272.09816583209306,272.754005102492,273.4993089051597,274.1893459812548,274.61330860309806,275.2395077220704,275.80487457375244,276.34798641976204,277.1193322671644,277.75622619778767,278.32140369282223,278.4560934463008]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[367.0590019749551,374.72229793509064,335.0027508873355,350.0270540794758,373.26372254384603,369.6299190770408,377.13202726135864,377.2412624137012,370.14722327438596,373.3168880473112,381.37602466274836,385.46588829644094,389.0356814796208,388.1782367385744,374.4143161844997,377.6697008036837,379.86971815003596,381.5584016701901,382.8871515742132,385.4669511380749,386.61975954686966,387.5174494352259,389.78266433819005,392.05128033249633,393.8378852182956,395.65336918498684,397.98549404835154,398.046950987242,399.27130907429427,400.1379827832549,400.3704955580106,401.7053295395338,402.83971235798987,403.46704186425967,404.708139830307,405.66069842304415,406.1675954354825,407.4397570536989,408.49586906099245,409.4830676806664,410.64073794764005,411.08238602881727,410.6517828890903,411.65566177556167,411.98947849968926,412.6523360646213,413.5523269659997,414.277403309466,415.22781112417266,416.05591396926883],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[263.87275544202254,300.61503623591705,310.6507686544995,340.68042927348426,357.99781228006833,366.3293758297897,347.70747927103736,353.106703196348,354.4005367814225,362.9053209824582,373.4283739391586,358.6610545572114,361.0496286315419,367.37723755270247,366.87984362923817,368.2025172441325,372.95705207216236,376.64668326912596,379.53621076624273,382.9621120467077,386.94594591082364,388.67442662830507,391.33218925960523,393.98187901527115,396.7190580205337,399.4198009187833,400.9831343468683,403.77045633842715,406.03406120856874,407.7347344072446,410.1770016740447,412.0608081341438,412.8395157719013,414.7109024065085,416.4483737440601,418.0409560354064,419.71601996393895,420.8052215479388,422.0753666902721,423.35392351282525,424.62078100388396,425.6837625187645,426.95640903786,427.994801211956,428.8263139848754,429.7628741225282,430.8508776402295,431.78732413713266,432.6326915694909,433.66483125844553],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[577.8167091043423,461.98366391742337,446.08159821647564,450.4394961653033,434.48564533581117,422.365789176269,430.228918896827,427.19157721322784,410.1346670718597,419.1741697855801,423.9237568411749,418.6741561802448,421.9188842846962,425.06143760541244,419.39050127693946,424.2633383105385,426.81931581478284,431.39243981582484,435.0681375671951,437.4236672228618,440.31532774103636,443.5680772366276,446.49787836637506,448.5293140447434,450.40710637424957,451.08583536652793,451.79171131456457,453.8031885411236,454.4090194074214,455.8065529330283,457.21290135670154,458.95112901643574,460.3363806197639,460.9686672621368,462.16816557476744,463.44475510892715,462.9501298951222,463.88871129466713,464.9325272300131,465.79533090709555,466.86770310524116,467.7955670487724,468.6596341696227,469.73803475497306,470.4817812506235,471.3192962417901,471.5986176652417,472.42723569834783,472.5363555943884,473.0427862019886],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[462.45635135637076,457.91643120535196,486.8445874645243,495.48427764409666,483.9613904542985,487.3114112739019,496.8115286247897,505.29589883716864,506.53151556274526,510.9455186175831,518.0197615317356,519.8742032628084,521.5957182394069,524.5050485267095,526.3886879481419,528.3408030733465,529.3088780694767,530.4588595094459,529.9556855480212,530.337319768929,531.0137354933081,532.1185827504091,528.0466267519548,528.8993886545509,529.6029423713272,529.8886896880952,529.30122252199,529.3748210994797,529.978572687605,530.3182748651532,530.694542212889,531.2765816586723,531.1885499236894,531.1169070346392,531.4257217676859,531.9295923585079,531.2146001701416,531.34167761383,531.5666985459657,532.014287427481,532.4832885520545,532.8616311478919,533.3586076833878,533.726512306583,533.8770256181368,534.2372300274601,534.6126434059896,535.0786646413645,535.3733456014239,535.7986114006947],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[248.05627834329414,187.6094908072058,182.1123011274814,183.55791567691662,165.07511875778442,158.7392608360616,172.4204578758816,162.53675651028533,159.13076087062097,152.25372098214584,158.96948329484994,157.30440955855778,155.11686071752226,159.51027091000643,157.95514793228887,158.69627385263587,158.49087290770342,159.36822019018445,160.26798977953567,160.55178709705257,160.0832714972485,160.11332632397196,161.1177822613278,162.00471158157887,162.81166764382846,162.2979844117712,162.85424984208137,163.66546885957547,163.77799749313343,163.68203013844285,163.31720405067549,163.11768570331577,163.60136519050877,161.30230874125544,161.64574079069976,162.1768537518533,161.25603568520577,161.01464608379158,161.56197852321534,161.90087321368554,162.3124137457518,162.93351074140523,163.3265009072491,163.6429073888965,164.17419293548923,164.5228014743798,165.23913846264756,165.73438074101162,166.17062291136813,165.7905784225005],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle kNN en fonction de n neighbors"},"xaxis":{"title":{"text":"n neighbors"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE kNN pour tout les paramètres de GridSearchCV\n","FigRMSEGRidkNN = visuRMSEGrid(KNeighborsRegressor(), 'kNN', n_neighbors,\n","                              'n neighbors', GridkNN)\n","FigRMSEGRidkNN.show()\n","if write_data is True:\n","    FigRMSEGRidkNN.write_image('./Figures/EmissionsGraphRMSEkNN.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.1.6 Modèle RandomForestRegressor"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                             paramètre RandomForestRegressor()\n","0  randomforestregressor__n_estimators                      46\n","1  randomforestregressor__max_features                    sqrt\n","      RandomForestRegressor()\n","R²                   0.719674\n","RMSE               270.036660\n","MAE                 72.363731\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predRF=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[36.876086956521746,52.21869565217392,47.35195652173912,34.1004347826087,24.68978260869566,29.107173913043486,124.00086956521736,37.867391304347834,27.947391304347814,74.41847826086955,148.0189130434782,76.96999999999998,62.42934782608695,33.461739130434765,187.3865217391305,1833.7067391304342,176.16499999999994,55.9878260869565,86.73130434782607,211.2147826086957,11.686739130434784,52.11978260869566,303.94434782608687,39.06956521739129,21.286739130434782,32.525652173913045,10.623913043478261,69.45630434782608,308.5200000000001,24.294347826086952,33.59934782608696,37.932391304347846,115.85500000000002,67.17195652173913,43.06826086956521,7.924347826086959,23.629782608695656,47.56560248447204,26.66130434782609,42.257608695652166,6.673695652173914,30.94956521739131,43.21391304347826,51.96847826086957,13.999347826086959,55.61239130434783,12.946086956521746,41.683478260869585,60.618695652173905,18.289565217391306,148.55913043478253,16.228043478260872,45.99760869565216,47.12326086956522,89.06956521739131,117.2352173913043,292.41021739130423,34.14391304347827,23.783043478260858,173.47521739130428,93.31869565217384,78.84869565217392,49.112826086956524,43.29978260869563,38.06804347826087,45.30500000000001,25.74173913043479,66.68913043478263,39.767173913043464,39.14804347826087,22.710434782608694,74.23586956521736,10.930652173913044,57.8504347826087,19.380652173913045,19.602499999999992,11.804565217391303,40.08804347826087,22.817826086956526,736.2763043478263,151.71021739130438,47.94217391304348,35.14434782608697,31.535434782608707,177.23282608695652,34.238913043478256,20.05173913043478,24.791304347826085,24.537608695652175,25.883695652173905,88.52782608695658,13.313913043478262,24.599782608695644,26.615869565217398,11.23478260869565,772.4589130434784,29.415652173913042,59.28760869565217,74.91369565217393,40.118913043478244,15.42826086956521,5.26304347826087,14.32956521739131,27.758695652173905,76.30260869565214,50.81173913043481,174.40456521739134,22.167608695652177,86.52913043478262,17.3741304347826,190.61282608695657,52.88086956521738,54.77,384.71956521739133,84.00369565217393,9.657826086956526,41.435434782608695,1750.7089130434777,31.772608695652174,13.044130434782607,31.733043478260864,39.8641304347826,115.12456521739124,41.948695652173924,208.2582608695652,102.81521739130433,20.049565217391308,36.72630434782607,29.87478260869566,41.31521739130434,45.207608695652176,56.092826086956514,8.621739130434781,112.54499999999999,15.092608695652178,42.26108695652175,24.96217391304348,26.324782608695653,349.9367391304348,43.26108695652175,48.13086956521743,32.00695652173914,42.676739130434804,248.88239130434792,5.414347826086958,44.78108695652173,217.6976086956522,79.82260869565218,5.785217391304346,27.79934782608696,37.89652173913044,48.05782608695654,48.671956521739105,29.074347826086953,11.179130434782607,75.45500000000001,65.28326086956523,24.178695652173925,57.36195652173913,20.11869565217391,104.45782608695652,75.02369565217387,57.22782608695651,20.805434782608696,39.458478260869555,20.296956521739126,44.36804347826087,46.806086956521746,47.0728260869565,96.5215217391305,42.72543478260868,43.050652173913036,85.32999999999997,33.33652173913042,1339.7589130434783,89.97130434782608,45.27913043478258,50.10652173913043,150.34065217391307,978.5291304347832,3309.1032608695664,252.41717391304354,30.845652173913034,50.299130434782604,34.13652173913043,56.53347826086957,95.27152173913043,41.257608695652195,26.162826086956517,18.85326086956521,122.8391304347826,18.402173913043473,70.06586956521737,88.23652173913045,11.808043478260876,69.42652173913045,35.538043478260875,41.18260869565214,732.8849999999996,15.355869565217393,18.09008695652173,18.199999999999996,25.16086956521738,6.8404347826087015,63.09739130434781,41.35456521739129,139.92586956521728,1467.3445652173914,123.87130434782611,3.8513043478260873,81.76586956521737,181.92586956521743,329.1639130434784,25.736086956521728,147.06869565217391,5.617391304347826,78.17804347826088,66.42065217391304,44.42391304347827,26.118043478260862,35.44000000000002,19.821086956521746,125.81608695652166,38.08434782608695,8.020652173913044,9.9595652173913,31.851956521739133,90.52826086956519,86.3415217391304,85.41826086956524,31.246521739130436,14.637608695652185,140.22369565217394,42.974565217391294,164.14760869565225,49.290000000000006,61.278478260869534,42.61369565217393,173.7526086956522,14.167391304347833,85.09065217391301,344.3465217391304,42.13021739130438,41.98782608695651,22.723913043478266,108.83434782608694,37.388043478260876,24.319782608695643,24.799565217391297,136.41869565217394,11.216956521739133,261.48217391304354,64.62413043478261,244.9941304347826,16.644565217391307,33.853043478260886,23.73717391304348,7879.459999999999,33.14826086956521,11.118478260869571,408.09934782608696,86.14434782608694,37.078043478260874,60.975434782608694,47.19630434782608,26.251521739130432,57.775217391304345,11.93508695652174,15.318478260869565,103.66586956521743,123.76543478260872,42.67413043478261,138.3613043478261,46.5791304347826,290.14369565217396,101.81152173913046,54.3517391304348,123.99782608695654,46.01673913043479,52.868695652173926,68.40065217391303,8.185869565217393,15.532391304347831,25.21873913043478,46.965869565217396,41.91326086956523,96.34565217391298,57.11108695652173,59.38717391304347,24.107608695652164,22.364782608695645,16.042608695652174,27.036956521739135,49.24565217391306,5.076521739130434,11.141086956521745,30.76152173913042,31.923043478260876,39.03347826086956,39.2121739130435,48.95108695652172,68.40695652173913,209.204347826087,273.64956521739134,37.09239130434784,24.014927536231884,29.571086956521732,29.576956521739135,57.18934782608699,73.84434782608695,151.83739130434782,18.881304347826088,23.22195652173914,48.495434782608726,13.658913043478263,240.00391304347818,143.77543478260873,27.758695652173905,19.05913043478261,31.010869565217387,150.27869565217395,4.684130434782611,51.23565217391307,203.4743478260869,48.299130434782626,63.81434782608696,22.50413043478261,12.776739130434782,136.99999999999997,104.17043478260868,40.67695652173913,195.33673913043486,11.585652173913042,39.28173913043478,44.2695652173913,34.71847826086956,123.37130434782607,69.3482608695652,20.682826086956528,45.575652173913035,53.3873913043478,25.624347826086947,65.23934782608698,163.45086956521737,38.238260869565195,89.22065217391301,17.555217391304346,78.0263043478261,31.765000000000015,215.95521739130436,4.86794927536232,50.858695652173935,87.23739130434785,83.14934782608697,145.90195652173915,57.62282608695652,60.14565217391306,58.732173913043454,70.41043478260868,106.04021739130435,19.25728260869565,24.333695652173915,3.7115217391304354,213.06586956521735,13.204782608695655,21.015217391304354,107.81500000000005,54.626304347826085,36.091521739130435,55.66239130434783,347.89782608695646,14.183913043478267,71.15586956521736,123.45217391304348,338.1197826086956,3.7491304347826087,137.03391304347815,263.95782608695646,5.616304347826087,37.14210144927537,14.253260869565223,237.6708695652173,434.95217391304357,5.7660869565217405,283.20869565217396,6.4171739130434755,115.84760869565217,43.02543478260872,81.80260869565215,8.979130434782599,30.425652173913022,385.92869565217404,9.164565217391303,19.61021739130436,28.694130434782615,43.57978260869565,45.43391304347827,97.85,99.96804347826088,512.2230434782609,146.82456521739138,106.78032608695653,119.5241304347826,92.21434782608699,32.11391304347826,7.635000000000001,70.38586956521739,13.318043478260877,83.4506884057971,24.30826086956521,157.26413043478257,44.0786956521739,51.66565217391303,29.79934782608695,104.06086956521743,37.82173913043478,53.71891304347825,59.81565217391305,101.77152173913039,92.75934782608697,864.8710869565216,114.22000000000001,223.27239130434785,22.749782608695647,115.86630434782607,204.0591304347826,159.60413043478266,34.30456521739129,33.78891304347827,54.10652173913045,42.408043478260865,59.3917391304348,6.810217391304346,43.49369565217394,17.964782608695646,78.05717391304346,35.411956521739114,40.044184782608674,153.83565217391302,26.4854347826087,161.70086956521732,71.43695652173913,4.453913043478264,27.946739130434775,115.6163043478261,11.094782608695658,86.8247826086957,31.564782608695644,7.2976086956521735,178.46630434782605,557.3650000000001,13.999347826086959,265.33326086956515,26.627826086956524,12.539999999999994,18.299565217391294,26.733695652173928,104.79804347826084,11.879782608695658,42.85673913043479,47.7686956521739,7.49978260869565,574.7734782608696,18.742826086956523,20.733913043478257,41.0586956521739,39.12999999999999,28.72891304347827,10.344565217391297,218.4652173913043,29.583695652173912,36.161739130434775,56.809130434782595,222.37760869565227,50.29434782608696,27.5641304347826,542.1573913043477,76.9513043478261,20.36239130434783,31.898260869565217,42.16065217391304,14.175869565217395,94.09021739130438,10.681739130434783,53.01978260869565,56.191521739130444,106.5058695652174,167.69130434782613,75.72826086956523,19.07717391304348,35.321739130434786,43.29391304347828,41.490652173913034,12.98304347826087,121.97304347826085,23.66260869565219,249.68391304347833,102.27891304347824,51.79282608695652,15.650434782608693,75.17652173913048,184.09717391304346,42.82543478260869,117.7308695652174,60.66956521739131,166.98456521739135,48.04663043478258,51.39434782608697,18.994184782608695,5.206956521739128,26.306521739130442,85.02130434782607,98.68630434782612,50.27130434782609,47.73978260869567,30.293913043478256,31.27478260869565,29.251739130434775,107.62500000000001,23.155869565217397,18.004782608695653,14.96782608695652,321.0465217391306,60.38065217391305,68.5245652173913,13.042826086956518,24.054782608695657,172.47304347826088,179.86978260869566,157.8615217391304,5.035869565217392,15.361304347826087,169.98771739130436,82.47717391304346,125.55260869565214,53.52717391304347,548.85,261.71739130434787,136.56391304347827,13.200434782608701,60.53565217391305,47.44195652173914,1358.2495652173914,1197.4515217391302,20.10173913043478,319.94108695652176,42.42144927536231,91.48739130434781,28.735,46.48173913043479,80.45717391304349,46.24173913043478,52.774347826086974,79.3321739130435,76.54336956521738,18.03456521739131,15.921086956521737,164.83717391304342,55.11782608695654,52.671521739130434,60.07760869565218,48.3936956521739,54.765434782608715,19.93369565217391,70.03391304347826,18.040000000000003,242.44695652173922,28.941086956521737,74.7254891304348,25.262478260869567,73.78608695652177,8.878478260869565,79.02804347826087,19.425869565217386,11.525695652173914,82.77673913043479,29.939347826086962,32.76,40.538260869565214,137.07413043478257,6.67673913043478,49.83369565217391,25.000434782608682,62.115434782608695,104.8904347826087,72.55847826086956,128.90978260869565,54.33717391304349,170.6076086956522,162.64891304347816,22.597826086956527,39.94608695652173,303.35282608695655,40.480652173913036,19.375,74.07065217391306,41.40608695652174,12.66891304347827,6.867391304347826,91.88934782608695,34.227173913043465,10.560434782608697,64.56282608695653,21.88847826086957,100.12086956521738,77.21021739130438,12.640217391304347,23.863043478260856,117.42478260869561,796.9221739130433,49.04717391304347,17.244347826086948,114.46152173913046,64.22913043478263,190.71021739130444,102.50369565217389,54.15695652173915,25.653260869565216,104.24869565217396,128.18239130434782,13.361956521739128,304.4589130434783,10.50652173913044,56.642608695652164,40.842608695652174,16.938478260869555,24.599782608695644,105.99913043478261,50.349130434782616,7.704565217391306,43.80934782608697,9.117173913043482,131.72630434782607,52.78413043478261,196.9189130434783,38.381739130434774,137.30152173913046,222.6891304347826,16.794347826086945,147.04847826086956,26.969347826086963,31.43586956521739,255.04652173913058,22.50847826086956,32.23195652173913,34.46521739130435,228.95500000000015,77.6852173913043,86.27369565217391,231.3106521739131,117.83717391304343,41.450434782608696,24.279130434782612,35.24717391304347,671.7852173913043,50.935869565217374,31.73500000000001,37.49760869565217,43.88869565217391,15.286739130434773,42.38608695652173,39.89304347826085,11.823695652173908,33.4941304347826,106.27369565217394,92.13456521739135,109.65909420289856,46.25130434782609,36.880652173913006,25.383043478260873,174.7530434782608,6.130217391304348,146.80347826086955,130.86282608695655,27.934130434782606,439.4265217391304,373.0708695652174,88.5608695652174,45.89869565217391,15.554347826086953,28.46336956521739,21.665000000000006,50.89326086956517,110.60326086956522,14.063913043478255,33.677608695652175,50.08456521739128,11.97891304347826,25.813260869565223,19.632826086956513,160.5267391304348,16.071304347826093,8.733043478260868,7.368478260869567,74.35065217391305,11.113260869565226,55.061956521739106,35.85934782608693,105.76456521739131,35.67347826086959,44.49478260869565,24.978260869565226,289.25260869565216,4599.239130434785,50.014347826086954,88.88326086956525,39.47456521739132,56.592173913043474,41.17456521739131,19.790434782608695,117.40782608695659,402.62152173913046,162.0358695652173,62.02391304347827,65.10478260869561,13.428478260869571,100.06304347826082,44.738057971014484,26.399347826086963,5.450869565217391,19.585869565217397,7.874347826086952,47.522608695652174,23.870652173913047,14.990652173913043,19.39043478260869,57.47195652173914,15.539347826086964,4.655652173913046,25.39000000000001,16.58521739130435,278.8847826086956,814.7671739130435,125.09173913043477,21.984999999999985,51.96021739130435,30.191521739130454,271.1684782608696,16.268043478260864,128.47434782608696,37.1845652173913,115.40565217391305,26.467608695652178,108.26913043478261,558.4267391304346,12.254438405797096,43.357608695652196,33.614130434782616,53.253043478260885,290.9695652173912,115.29195652173915,73.37608695652177,24.414130434782606,81.61891304347827,23.139999999999997,11.425869565217386,36.99630434782608,28.883043478260873,21.882826086956538,3.5367391304347846,35.606739130434796,243.3367391304348,132.07108695652178,419.60195652173917,45.36499999999999,109.68652173913047,26.48195652173913,32.49847826086957,61.98565217391306,48.27478260869564,24.174130434782594,66.75847826086957,886.3241304347827,4.504130434782609,51.32956521739129,106.5208695652174,22.839565217391286,83.6867391304348,198.31326086956534,69.29391304347824,23.39739130434783,31.35166666666666,34.07652173913042,76.99804347826088,29.24782608695652,949.1595652173919,54.80456521739129,76.58695652173917,44.243260869565226,108.02139130434783,27.3217391304348,269.13,674.8649999999999,42.571956521739146,73.02021739130434,436.0236956521739,115.2158695652174,9.480434782608693,15.76739130434782,412.7097826086956,31.439130434782616,32.360434782608706,14.117391304347818,569.2628260869565,49.442826086956536,32.72760869565218,224.80260869565214,6.2873913043478264,24.196739130434782,33.23956521739131,15.232608695652177,9.099565217391307,5.377173913043479,76.89956521739127,50.866956521739134,32.08891304347826,143.56282608695653,75.1723913043478,7.457391304347828,96.12499999999996,41.408695652173904,54.22108695652175,28.147826086956524,45.41239130434783,30.570000000000004,117.0923913043478,29.470869565217388,10.750869565217394,15.695434782608684,79.30108695652174,583.2293478260867,34.204347826086966,25.456304347826073,16.22260869565218,88.80152173913042,38.5382608695652,126.02978260869568,495.6291304347827,27.372608695652165,110.1695652173913,226.26565217391303,20.795652173913037,20.47369565217391,7.764565217391307,104.63934782608695,77.1580434782609,117.55760869565225,54.934347826086956,16.086086956521743,83.96086956521738,59.44152173913044,58.16304347826085,11.293695652173911,80.81347826086957,18.799782608695665,121.93630434782607,42.16826086956521,1132.4339130434785,15.169130434782604,24.01869565217391,60.824347826086886,20.542173913043488,37.760434782608705,19.582608695652176,24.321086956521732,44.738057971014484,13.749999999999998,47.611956521739124,30.28586956521739,31.346739130434777,45.7321739130435,229.63173913043482,34.79608695652173,38.54347826086955,73.1495652173913,273.6545652173913,96.02239130434782,18.925217391304358,26.443478260869558,104.48760869565218,43.287391304347814,42.177608695652175,296.26217391304357,127.73413043478261,39.61217391304348,318.86239130434785,20.99804347826085,15.038695652173915,68.56740942028986,83.74847826086955,35.75521739130434,314.8360869565218,10.267173913043484,40.546304347826094,61.83760869565215,24.760217391304344,137.85543478260868,3976.313913043479,111.4173913043478,32.112608695652185,577.3815217391306,78.63108695652178,31.064782608695648,59.18152173913041,8.406521739130437,412.0610869565217,86.12913043478261,446.28065217391304,121.94260869565217,30.925217391304336,203.61934782608697,17.049782608695647,137.9152173913044,28.33499999999999,11.077173913043481,229.9576086956522,400.12826086956517,8.047391304347824,20.647173913043492,62.06685144927535,167.235652173913,116.63304347826087,785.4595652173913,43.67499999999997,125.37195652173918,51.0345652173913,55.416086956521774,102.34043478260875,15.789782608695651,41.28347826086957,28.940000000000026,67.18695652173916,16.682826086956513,87.61173913043477,288.90760869565213,46.64434782608696,39.37108695652174,60.31439130434783,156.32978260869564,117.24869565217388,18.325217391304342,12.353260869565224,58.630434782608745,27.4713043478261,30.998043478260872,82.81608695652174,64.14239130434783,19.88291304347826,40.49963354037266,20.132826086956516,127.25000000000004,12.518260869565223,33.36304347826086,54.244565217391305,81.62195652173911,219.16847826086948,1216.2115217391306,85.02630434782608,76.4454347826087,44.19804347826087,327.60086956521735,17.340543478260862,16.220000000000006,166.929347826087,41.05913043478264,35.06956521739131,12.845000000000002,98.94326086956521,28.953260869565216,70.5708695652174,18.936739130434773,29.545217391304362,49.05891304347824,173.76478260869567,181.02021739130436,75.46630434782608,883.1252173913047,65.35478260869569,226.16826086956516,84.9776086956522,88.05717391304348,45.51478260869567,137.59239130434779,29.77782608695653,11.915000000000003,63.846739130434806,24.871739130434786,45.049347826086965,13.597608695652172,16.437391304347837,44.464999999999975,45.06521739130435,88.4076086956522,117.16391304347825,166.20826086956518,204.16826086956522,2104.3963043478257,20.336521739130415,19.757826086956523,159.10804347826092,20.29826086956522,88.5219565217391,52.90152173913043,42.50499999999996,53.28538509316769,70.73413043478259,59.857608695652154,20.82760869565217,130.46413043478267,60.27717391304349,100.11956521739133,107.9132608695652,29.36608695652175,12.200869565217394,42.3091304347826,150.98760869565217,68.11369565217392,77.91260869565221,20.971521739130438,65.26760869565217,32.59739130434784,32.23641304347827,24.59347826086957,162.27347826086958,48.77521739130435,45.90217391304349,55.475217391304334,24.41173913043478,58.93782608695655,136.8536956521739,28.87500000000001,72.63369565217391,56.52173913043478,115.6195652173913,28.276739130434798,11.376739130434778,39.802391304347836,52.09804347826088,25.58195652173914,29.843043478260853,131.64913043478262,27.62434782608696,36.603695652173904,26.875869565217386,245.53304347826077,75.01065217391302,31.906956521739136,58.87000000000001,15.198260869565212,7.876956521739134,436.5186956521738,39.9904347826087,108.0919565217391,37.836521739130426,139.43840579710147,125.5028260869565,8.702391304347826,3.895579710144928,324.14652173913043,59.237826086956545,64.43217391304346,26.02934782608695,21.05630434782609,44.97630434782609,22.47108695652173,25.19652173913044,46.495,44.71478260869566,31.022608695652178,24.52978260869564,40.685869565217374,42.954999999999984,25.207608695652173,31.443043478260858,98.22717391304343,125.56391304347824,64.75586956521737,357.2758695652174,93.2528260869565,13.863913043478256,4.320521739130436,48.815217391304316,19.090434782608703,18.379275362318833,87.71304347826086,5.768260869565217,117.75630434782606,76.69710144927535,43.745,51.75608695652177,118.80608695652172,26.41304347826087,62.83478260869566,297.67239130434785,9.219130434782608,41.20391304347828,90.75173913043479,65.71260869565216,243.66478260869562,30.501739130434785,104.83739130434782,20.244565217391298,20.472391304347823,96.01380434782607,42.224565217391294,69.79826086956517,53.215869565217375,20.63586956521739,38.52630434782607,383.0595652173912,23.655434782608697,24.65260869565218,39.22717391304347,54.883405797101425,150.90608695652173,15.492391304347823,229.2678260869565,14.58847826086956,14.488695652173917,71.18434782608692,23.45586956521738,78.88847826086956,579.3810869565216,104.62130434782607,125.44521739130433,69.8836956521739,61.219782608695624,67.56717391304349,19.919565217391302,256.7500000000001,70.07608695652172,210.60934782608712,382.30543478260853,8.401521739130436,14.286956521739127,24.490434782608684,22.22630434782608,26.018260869565214,4.147173913043478,76.21804347826087,110.8936956521739,78.96304347826089,15.931304347826078,106.05586956521738,386.7954347826087,14.734565217391301,16.93380434782609,149.24130434782606,14.384347826086954,13.146521739130435,164.0595652173913,12.318695652173915,11.976304347826089,23.246956521739122,323.2265217391304,45.187391304347834,17.831521739130427,25.657391304347826,11.024347826086956,79.41565217391303,53.412826086956514,17.215217391304346,355.85521739130434,32.24282608695653,42.279130434782616,106.09760869565218,273.245652173913,3.9839130434782626,165.18043478260867,114.3676086956522,213.92521739130433,31.445217391304347,1341.0260869565213,22.39260869565217,59.064130434782626,22.51760869565218,190.31913043478258,65.37195652173914,48.63869565217392,81.22152173913048,95.56260869565217,193.87369565217384,181.0126086956521,62.27543478260872,140.81434782608693,97.98782608695653,50.29108695652174,379.76869565217385,19.1545652173913,80.15065217391306,24.09521739130434,88.2141304347826,105.36304347826082,37.93543478260868,120.07391304347833,134.279347826087,122.84934782608697,58.14282608695652,89.96652173913041,43.60391304347826,48.83217391304349,62.88913043478258,16.96369565217392,5.866086956521736,53.07217391304347,22.214565217391293,8.487608695652176,23.29891304347826,16.462608695652168,40.30369565217392,208.6797826086957,418.55413043478256,34.57,72.31217391304347,82.72847826086961,23.095869565217384,35.322971014492765,18.217608695652174,10.318043478260876,79.98646739130433,50.01086956521739,104.61543478260869,24.919130434782613,58.84543478260871,78.09434782608692,51.39565217391305,31.551086956521733,75.84326086956521,15.137391304347831,65.35913043478264,36.54543478260869,27.31130434782608,16.87391304347826,127.94934782608694,14.71804347826087,102.59391304347824,74.65913043478258,27.636304347826098,43.78065217391303,67.67152173913045,37.52021739130434,40.62369565217391,28.323913043478267,58.21913043478262,57.630217391304356,36.33847826086959,68.52434782608695,57.68173913043478,120.8319565217391,98.5489130434783,42.441521739130444,505.2130434782608,31.274130434782627,25.324347826086957,340.9467391304348,82.05586956521739,378.4156521739131,161.46282608695657,72.22326086956524,14.155652173913039,71.7617391304348,15.899347826086952,20.653695652173916,40.15782608695651,21.753695652173914,105.45978260869569,26.537826086956528,17.20630434782609,84.33956521739131,110.30586956521738,15.547173913043492,90.0923913043478,99.2832608695652,120.73304347826088,101.17456521739133,23.746304347826094,9.275652173913043,43.43999999999998,15.656521739130431,262.1071739130435,109.35521739130436,57.493260869565226,83.33369565217392,1518.2956521739136,13.871956521739133,20.578260869565213,40.28739130434781,87.58847826086956,245.19108695652167,39.74586956521737,92.90891304347825,22.926521739130433,12.944130434782608,21.55391304347826,67.34304347826088,144.86521739130427,43.42869565217391,16.38478260869565,41.77934782608696,102.65065217391307,173.55260869565228,154.29717391304345,42.761086956521744,63.77282608695651,29.550652173913047,5.817391304347828,38.12630434782608,1326.9552173913041,127.5691304347826],"xaxis":"x","y":[74.33,21.29,69.48,25.53,5.45,4.54,191.97,66.44,13.95,108.04,265.21,72.16,29.57,16.01,133.5,324.25,281.25,53.02,33.09,263.51,4.56,37.34,233.43,20.85,9.83,46.7,6.07,49.22,466.95,3.88,45.83,11.84,1936.34,193.94,9.76,3.29,10.7,111.95,21.24,107.56,14.24,17.04,5.81,26.94,2.82,27.77,7.63,48.1,52.43,33.04,50.29,6.25,3.34,43.19,64.23,77.76,84.75,4.41,16.8,282.72,82.72,60.57,89.29,51.1,8.14,35.4,25.32,64.59,1.6,79.74,159.98,42.8,5.62,13.29,4.53,6.79,3.15,75.03,6.05,500.93,96.43,58.79,39.48,4.99,26.46,65.02,5.94,6.18,102.27,15.2,95.77,6.17,3.95,61.98,33.52,1148.5,37.82,79.98,138.68,34.48,4,3.35,4.39,4.78,105.01,126.03,56.48,25.9,570.98,18.33,387.49,16.44,8.65,13.87,29.78,1.08,12.29,1638.46,89.57,54.25,12.72,24.39,192.12,7.44,193.34,45.44,17.43,38.39,60.5,122.73,10.65,10.37,25.22,23.58,4.86,116.21,25.21,6.3,331.61,11.8,15.26,7.9,23.85,138.79,4.6,47.26,304.62,23.84,4.89,30.79,12.25,13.23,59.73,17.22,14.35,29.41,10.66,25.14,46.46,27.47,69.02,52.87,76.68,4.75,22.46,5.07,8.65,5.99,45.33,52.49,54.24,10.84,20.22,3.4,2937.83,170.01,30.26,82.58,229.2,569.61,2061.48,145.67,15.1,8.49,53.31,250.87,20.29,119.98,2.93,22.37,0.17,17.02,120.1,1.55,4.13,51.93,8.42,5.43,602.12,4.6,61.33,16.01,4.43,42.92,72.78,47.12,97.39,321.72,39.27,3.63,12.99,350.87,96.2,13.36,69.18,4.82,60.98,17.23,1.73,5.53,7.06,9.57,380.17,10.65,18.88,9.45,70.31,41.8,157.77,1.63,56.07,64.13,73.9,45.79,106.37,22.97,61.15,53.88,227.18,1.97,127.74,1266.06,29.93,33.28,22.38,249.46,6.12,4.44,4.55,4.2,5.28,293.56,30.19,515.71,3.52,5.67,46.79,11824.89,51.83,48.95,188.77,64.56,38.97,13.39,13.56,34.61,53.87,9.04,16.99,280.16,117.64,3.75,30.57,8.86,467.9,56.07,17.24,272.54,40.65,22.78,32.62,6.66,6.7,103.42,114.33,185.33,239.17,67.17,19.95,15.9,5.49,4.68,23.62,38.88,2.99,5.53,61.88,6.38,27.92,23.15,89.29,14.83,73.14,118.2,42.8,3.7,12.68,66.43,16.89,8.25,29.33,21.3,7.48,65.46,10.21,190.61,175.3,49.58,5.64,3.14,229.84,4.2,62.41,147.14,5.82,30.35,6.41,134.8,175.46,67.57,17.12,309.6,4.5,73.61,28.19,6.89,9.76,60.12,4.92,132.1,72.5,30.69,73.31,105.97,23.12,35.6,16.14,103.99,3.48,367.3,3.54,5.8,24.56,100.02,93.21,17.49,30.86,41.9,54.04,40.78,32.89,4.38,4.32,193.47,35.11,58.59,84.93,0,33.45,6.71,458.44,12.28,168.36,75,398.19,5.46,39.41,166.84,3.59,7.07,3.06,262.98,144.57,5.22,358.88,4.73,105.32,23.5,16.83,4.64,1.06,266.05,10.83,11.81,21.44,78.36,11.17,27.27,127.27,295.42,457.51,198.59,61.18,110.74,14.97,4.11,197.3,5.96,25.11,9.95,505.01,71.87,22.66,17.92,35.43,44.15,93.63,109.9,278.47,80.09,452.16,33.17,300.14,7.36,126.21,178.21,119.2,77.75,35.48,46.26,8.42,6.22,5.28,84.95,6.41,31.27,29.66,48.95,72.5,3.21,141.15,31.13,4.25,3.49,52,3.96,125.59,14.7,19.57,143.33,804.2,9.25,375.56,14.37,16.18,78.29,29.97,3.93,22.93,32.05,126.92,5.74,105.41,7.69,9.79,34.66,4.08,5.59,26.16,143.82,6,11.87,20.37,422.39,4.55,11.29,181.67,9.43,5.71,21.78,2.15,7.57,229.12,4.08,41.16,24.25,28.3,155.54,110.62,40.56,18.3,79.21,10.9,4.85,113.97,8.36,91.53,9.28,47.5,3.63,20.06,168.62,49.28,51.4,11.27,15.5,53.65,35.2,8.06,4,27.93,286.43,91.09,85.28,49.89,16.72,18.74,19.11,119.46,28.94,8.75,6.29,94.82,35.72,33.37,18.76,19.69,402.04,328.23,152.28,5.23,86.9,158.55,49.63,23.42,10.01,607.41,235.69,12.05,7.21,62.23,39.5,1265.29,290.4,45.33,463.12,70.84,93.83,53.71,9.53,184.09,28.37,13.1,48.99,18.47,5.15,6.37,31.15,35.8,17.14,379.34,161.39,127.56,6.9,84.59,4.33,351.9,27.99,124.54,30.1,59.14,21.57,25.12,6.01,7.97,78.92,13.71,1.55,72.9,70.16,4.04,7.47,10.02,75.99,77.04,11.18,107.16,38.13,39.57,227.13,4.43,98.88,104.61,20.02,5.63,130.28,138.35,8.08,1.08,6.96,16.46,5.94,64.79,9.67,45.38,30.19,5.79,32.42,191.97,636.96,30.02,1.73,85.59,8.61,199.48,70.17,3.69,41.74,33.12,39.73,4.35,180.14,20.5,74.98,23.63,23.9,5.79,334.15,132.32,10.29,9.39,4.14,553.27,28.63,249.98,41.96,147.36,128.91,0.78,209.92,7.34,22.15,180.89,4.12,9.43,1.59,549.74,25.93,523.95,128.22,124.77,10.48,5.53,63.77,1623.34,39.1,232.12,9.56,6.22,4.2,36.42,13.27,16.47,4.24,48.6,109.16,47.14,11.1,19.61,6.16,95.26,6.15,24.12,40.46,28.6,192.51,159.16,28.55,44.26,4.02,47.25,60.88,75.95,115.34,7.13,38.38,216.18,5.69,60.08,4.86,122.92,11.59,13.18,2.99,51.47,117.19,13.34,63.41,178.86,4.09,100.71,13.12,1874.42,11140.56,8.69,1060.13,64.06,13.71,75.58,27.38,158.48,213.9,20.09,39.57,53.48,6.04,54.76,4.19,4.39,3.44,21.71,9.16,9.82,3.65,6.08,34.03,28.21,27.63,3.83,45.07,13.21,218.49,1789.69,44.74,14.67,94.23,5.77,31.76,18.42,134.38,69.44,40.35,14.73,25.15,752.57,17.39,56.86,36.97,33.35,177.51,7.21,113.3,6.35,150.23,8.37,3.16,21.51,38.59,6.51,5.42,5.6,248.15,477.85,691.57,23.79,174.79,68.05,64.86,191.2,50.52,5.46,6.72,1699.45,5.07,35.8,376.46,21.12,58.88,348.21,29.53,0.92,9.67,13.18,92.21,4.8,426.39,20.66,5.95,48.34,71.03,59.84,234.75,723.4,133.7,158.6,250.05,69.42,25.39,2.79,298.61,94.02,39.67,5.67,1597.56,11.38,35.14,1019.53,10.04,2.69,63.37,4.31,6.73,20.76,86.81,28.05,49.6,110.41,27.33,8.64,61.86,59.76,60.67,1.78,11.97,14.49,43.15,7.52,12.62,6.26,225.35,675.34,5.54,3.78,17.11,84.83,119.01,11.54,393.2,22.74,40.25,46.52,34.03,5.84,3.54,51.89,125.13,106.74,21.38,7.39,33.58,3.27,18.91,15.7,54.02,6.86,22.81,17.08,1894.2,20.33,44.93,22.96,13.29,14.24,23.22,11.27,4.46,26.63,2.72,5.68,0.09,9.56,181.93,40.69,3.47,101.34,259.49,9.86,6.87,10.24,171.64,5.87,49.53,469.57,83.68,9.95,285.62,3.43,12.36,36.08,6.43,14.09,377.09,27.14,23.03,4.99,32.77,238.77,3243.48,93.31,5.94,455.39,67.24,29.62,9.96,5.91,234.15,39.44,314.92,154.96,5.46,86.99,20.09,260.13,6.67,39.57,147.82,188.13,5.03,9.73,5.79,45.6,21.92,740.97,59.05,120.84,47.92,108.79,84.64,6.35,15.43,9.72,12.71,26.21,47.24,22.17,68.3,46.6,6.85,2.85,84.18,12.92,15.34,18.1,4.84,12.75,34.14,47.71,129.42,45.02,7.85,202.65,4.45,4.31,148.25,15.09,123.18,2452.86,38.86,15.9,21.77,301.02,5.45,17.75,47.99,16.3,6.74,11.94,176.79,4.74,51.98,4.16,5.9,64.57,46.78,313.83,179.33,1088.2,237.36,319.73,161.2,19.39,85.73,273.91,42.18,4.62,6.33,67.45,15.37,47.64,4.89,24.89,5.6,4.19,254.16,136.94,221.51,834.96,1.14,8.16,202.99,9.71,71.55,59.07,9.36,138.61,18.9,481.97,42.99,134.01,54.41,21.44,50.46,85.95,33.92,7.69,98.95,99.96,78.7,0,15.6,51.39,3.98,25.48,58.28,62.17,88.51,9.44,24.51,36.78,88.74,5.65,215,32.48,92.45,22.53,9.11,25.38,45.32,128.39,9.46,69.59,3.28,82.07,8.45,94.59,65.35,86.86,103.77,1.85,3.02,434.48,5,133.8,8.8,587.16,214.65,15.11,3.5,433.6,75.48,19.07,4.26,4.19,41.96,3.76,5.97,10.23,61.62,4.82,28.61,86.9,36.72,16.89,9.62,103.39,303.69,37.51,225.66,7.6,92.19,3.67,38.85,9.29,23.78,14.86,3.41,176.75,59.74,6.32,111.04,30.03,17.57,14.04,399.73,3.77,13.09,87.07,154.6,28.88,6.3,60.52,6.73,13.35,65.05,30.78,103.4,231.39,4.92,0,565.79,103.07,9.57,21.05,18.86,99.39,5.91,206.01,6.03,9.15,10.6,8.63,106.73,509.36,5.44,193.72,37.74,125.42,38.56,3.05,132.71,148.7,130.73,218.53,4.52,4.51,8.28,16.32,68.14,3.56,78.06,66.23,70.39,15.43,178.65,355.05,0.92,3.04,284.01,61.07,6.98,67.78,18.65,11.62,7.73,319.88,7.49,38.67,7.85,5.04,11.39,33.8,19.7,359.09,3.3,55.69,117.11,1990.5,4.51,44.96,0,95.54,14.3,481.06,31.47,2.36,62.23,205.52,54.4,10.95,28.78,107.84,222.02,124.96,54.53,117.04,21.08,27.02,142.16,4.15,51.24,6.18,3,210.3,1.24,138.76,39.75,21.51,71.23,46.3,49.46,9.4,85.76,7.74,8.16,101.31,5.35,4.13,47.08,4.48,8.36,74.07,235.32,12.63,7.74,86.46,15.97,16.58,3.99,7.56,48.35,14.65,33.43,35.39,44.55,37.49,7.78,25.12,82.9,6.31,1.67,4.15,10.38,14.3,137.15,1.68,120.91,130.69,79.58,9.73,34.2,38.78,62.98,8.47,25.2,12.05,77.34,0.31,53.12,127.14,243.13,6.09,567.6,4.43,52.67,139.72,80.75,1185.15,63.33,30.95,4.65,303.81,6.25,8.85,23.95,5.17,41.22,3.81,1.86,72.5,51.28,41.3,37.56,22.1,36.09,49.17,6.71,3.5,67.06,4.41,394.6,38.41,271.68,138.03,1129.67,2.34,12.26,20.86,87.14,238.61,43.97,107.83,21.66,5.88,17.45,16.63,103.57,23.8,48.16,45.58,10.97,10.29,28.09,26.56,40.05,9.63,5.02,5.82,1596.08,293.55],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle RandomForestRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predRF"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle RandomForestRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsRF = np.logspace(0, 3, 10, dtype=int)\n","param_gridRF = {\n","    'randomforestregressor__n_estimators': n_estimatorsRF,\n","    'randomforestregressor__max_features': ['auto', 'sqrt', 'log2'],\n","}\n","\n","GridRF, \\\n","BestParametresRF, \\\n","ScoresRF, \\\n","TotalGHGEmissions_predRF, \\\n","figRF = reg_modelGrid(model=RandomForestRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBM_train,\n","                         X_test=BEBM_test,\n","                         y_train=TotalGHGEmissions_train.ravel(),\n","                         y_test=TotalGHGEmissions_test,\n","                         y_test_name='TotalGHGEmissions_test',\n","                         y_pred_name='TotalGHGEmissions_predRF',\n","                         score=score,\n","                         param_grid=param_gridRF)\n","\n","print(BestParametresRF)\n","print(ScoresRF)\n","figRF.show()\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[516.8130224754193,441.90304702261244,415.642141490205,344.61598863421096,351.61400064655027,337.52777806736674,351.12342950320317,343.7612546912621,347.8060025689632,341.9136326945428]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[598.5405017335308,612.6046701219656,497.02525070575723,425.84243334033647,446.49440544002556,430.8270371542159,449.88470635865144,443.88257917433623,447.79877495780414,443.2755898835708]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[435.08554321730776,271.2014239232593,334.2590322746528,263.38954392808546,256.733595853075,244.22851898051763,252.36215264775487,243.63993020818793,247.81323018012228,240.55167550551482]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[397.3878187031421,714.0396383149728,382.06056965112737,315.6424197457467,360.39326964434036,316.7525962296101,353.44311685359446,343.2450066123929,349.50812934761194,341.520391256362],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[467.1340709021472,244.96304626014404,376.31926195956567,237.50795906162122,258.0341637481305,258.969999023552,259.85421442294796,250.33290879343429,263.068797845588,246.29246030089965],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[504.2459610452989,340.12050924497964,460.89965128292187,355.5696074310972,348.952117511811,363.6473028753247,394.68581928407906,381.3849646829579,377.6957272163982,375.8433884659751],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[609.4347255683623,560.4063840410572,547.8117626687141,486.9565147810638,522.5168991862133,503.6692264181711,510.7582641918746,510.4690012623376,516.687975509321,512.865196350694],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[605.862536158146,349.98565725190895,311.1194618886959,327.40344215152584,268.1735531422561,244.5997657901756,236.87573276351975,233.3743921051878,232.06938292589706,233.04672709878326],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle RF pour le paramètre<br>randomforestregressor__max_features=sqrt<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE RandomForestRegressor\n","# pour le meilleur paramètre max features\n","FigRMSEGRidRF = visuRMSEGrid(RandomForestRegressor(), 'RF', n_estimatorsRF,\n","                             'n estimators', GridRF, BestParametresRF,\n","                             'randomforestregressor__max_features')\n","FigRMSEGRidRF.show()\n","if write_data is True:\n","    FigRMSEGRidRF.write_image('./Figures/EmissionsGraphRMSERF.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.1.7 Modèle AdaboostRegressor"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                         paramètre AdaBoostRegressor()\n","0  adaboostregressor__n_estimators                  20\n","1          adaboostregressor__loss              square\n","      AdaBoostRegressor()\n","R²               0.650471\n","RMSE           301.531675\n","MAE            102.054896\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predAB=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[61.621744680851094,61.621744680851094,152.67451597520352,61.621744680851094,61.621744680851094,61.621744680851094,307.7837912087912,95.83699507389139,61.621744680851094,95.83699507389139,85.94284569138271,95.83699507389139,61.621744680851094,61.621744680851094,244.82013986013953,1875.0285573770448,95.83699507389139,61.621744680851094,175.9386973180075,152.67451597520352,61.621744680851094,61.621744680851094,152.67451597520352,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,175.9386973180075,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,152.67451597520352,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,152.67451597520352,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,175.9386973180075,95.83699507389139,61.621744680851094,61.621744680851094,175.9386973180075,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,175.9386973180075,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,834.0732411504781,111.12251785714281,95.83699507389139,61.621744680851094,61.621744680851094,188.26352697095433,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,145.75402061855667,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,834.0732411504781,61.621744680851094,61.621744680851094,152.67451597520352,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,307.7837912087912,61.621744680851094,61.621744680851094,61.621744680851094,175.9386973180075,61.621744680851094,61.621744680851094,152.67451597520352,95.83699507389139,95.83699507389139,95.83699507389139,1875.0285573770448,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,152.67451597520352,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,307.7837912087912,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,320.10313131313114,95.83699507389139,95.83699507389139,61.621744680851094,152.67451597520352,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,115.50376415094524,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,614.1207208589085,152.67451597520352,95.83699507389139,95.83699507389139,95.83699507389139,834.0732411504781,3424.3981034482626,307.7837912087912,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,312.1709638554216,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,312.1709638554216,827.1647396199811,152.67451597520352,95.83699507389139,61.621744680851094,85.94284569138271,307.7837912087912,61.621744680851094,152.67451597520352,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,152.67451597520352,95.83699507389139,95.83699507389139,95.83699507389139,152.67451597520352,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,175.9386973180075,61.621744680851094,61.621744680851094,95.83699507389139,152.67451597520352,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,152.67451597520352,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,185.23412186379932,95.83699507389139,175.9386973180075,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,9775.73239005729,61.621744680851094,61.621744680851094,307.7837912087912,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,175.9386973180075,61.621744680851094,95.83699507389139,61.621744680851094,320.10313131313114,152.67451597520352,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,152.67451597520352,312.1709638554216,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,185.23412186379932,188.26352697095433,61.621744680851094,95.83699507389139,61.621744680851094,175.9386973180075,95.83699507389139,61.621744680851094,320.10313131313114,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,312.1709638554216,61.621744680851094,95.83699507389139,152.67451597520352,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,152.67451597520352,95.83699507389139,95.83699507389139,152.67451597520352,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,79.47386904761905,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,307.7837912087912,61.621744680851094,61.621744680851094,185.23412186379932,61.621744680851094,95.83699507389139,95.83699507389139,320.10313131313114,61.621744680851094,61.621744680851094,95.83699507389139,452.5840894568689,95.83699507389139,307.7837912087912,312.1709638554216,95.83699507389139,61.621744680851094,61.621744680851094,320.10313131313114,307.7837912087912,95.83699507389139,175.9386973180075,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,185.23412186379932,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,152.67451597520352,320.10313131313114,61.621744680851094,61.621744680851094,95.83699507389139,152.67451597520352,61.621744680851094,61.621744680851094,152.67451597520352,61.621744680851094,79.47386904761905,61.621744680851094,175.9386973180075,95.83699507389139,61.621744680851094,95.83699507389139,175.9386973180075,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,807.6816835994114,152.67451597520352,175.9386973180075,95.83699507389139,152.67451597520352,307.7837912087912,175.9386973180075,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,152.67451597520352,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,175.9386973180075,61.621744680851094,95.83699507389139,61.621744680851094,175.9386973180075,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,185.23412186379932,320.10313131313114,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,85.94284569138271,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,312.1709638554216,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,312.1709638554216,95.83699507389139,95.83699507389139,95.83699507389139,185.23412186379932,61.621744680851094,95.83699507389139,320.10313131313114,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,175.9386973180075,61.621744680851094,244.82013986013953,152.67451597520352,152.67451597520352,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,175.9386973180075,95.83699507389139,175.9386973180075,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,188.26352697095433,61.621744680851094,95.83699507389139,61.621744680851094,185.23412186379932,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,85.94284569138271,95.83699507389139,61.621744680851094,61.621744680851094,307.7837912087912,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,175.9386973180075,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,85.94284569138271,61.621744680851094,152.67451597520352,95.83699507389139,274.41628352490454,307.7837912087912,152.67451597520352,61.621744680851094,95.83699507389139,61.621744680851094,1430.313203883495,834.0732411504781,95.83699507389139,312.1709638554216,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,79.47386904761905,95.83699507389139,95.83699507389139,307.7837912087912,152.67451597520352,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,85.94284569138271,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,145.75402061855667,95.83699507389139,95.83699507389139,61.621744680851094,307.7837912087912,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,185.23412186379932,614.1207208589085,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,312.1709638554216,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,337.50777976723623,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,85.94284569138271,95.83699507389139,95.83699507389139,95.83699507389139,175.9386973180075,175.9386973180075,61.621744680851094,152.67451597520352,61.621744680851094,95.83699507389139,307.7837912087912,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,320.10313131313114,85.94284569138271,95.83699507389139,95.83699507389139,95.83699507389139,175.9386973180075,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,152.67451597520352,152.67451597520352,95.83699507389139,61.621744680851094,61.621744680851094,307.7837912087912,95.83699507389139,185.23412186379932,175.9386973180075,61.621744680851094,312.1709638554216,312.1709638554216,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,85.94284569138271,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,307.7837912087912,95.83699507389139,95.83699507389139,95.83699507389139,85.94284569138271,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,312.1709638554216,3232.229999999999,95.83699507389139,152.67451597520352,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,79.47386904761905,354.7585338345863,185.23412186379932,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,307.7837912087912,320.10313131313114,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,307.7837912087912,61.621744680851094,95.83699507389139,152.67451597520352,95.83699507389139,95.83699507389139,61.621744680851094,312.1709638554216,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,312.1709638554216,152.67451597520352,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,152.67451597520352,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,312.1709638554216,152.67451597520352,452.5840894568689,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,312.1709638554216,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,175.9386973180075,185.23412186379932,152.67451597520352,61.621744680851094,61.621744680851094,95.83699507389139,175.9386973180075,95.83699507389139,337.50777976723623,61.621744680851094,95.83699507389139,95.83699507389139,79.47386904761905,61.621744680851094,307.7837912087912,312.1709638554216,61.621744680851094,61.621744680851094,175.9386973180075,79.47386904761905,61.621744680851094,61.621744680851094,307.7837912087912,95.83699507389139,61.621744680851094,95.83699507389139,307.7837912087912,61.621744680851094,95.83699507389139,307.7837912087912,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,115.50376415094524,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,312.1709638554216,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,307.7837912087912,61.621744680851094,95.83699507389139,111.12251785714281,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,175.9386973180075,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,152.67451597520352,61.621744680851094,61.621744680851094,61.621744680851094,175.9386973180075,61.621744680851094,312.1709638554216,61.621744680851094,61.621744680851094,79.47386904761905,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,312.1709638554216,61.621744680851094,61.621744680851094,95.83699507389139,175.9386973180075,95.83699507389139,95.83699507389139,95.83699507389139,152.67451597520352,61.621744680851094,61.621744680851094,152.67451597520352,175.9386973180075,95.83699507389139,312.1709638554216,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,244.82013986013953,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,79.47386904761905,2486.558470319625,152.67451597520352,95.83699507389139,274.41628352490454,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,312.1709638554216,95.83699507389139,307.7837912087912,85.94284569138271,61.621744680851094,185.23412186379932,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,320.10313131313114,312.1709638554216,95.83699507389139,95.83699507389139,95.83699507389139,238.8708219178085,61.621744680851094,312.1709638554216,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,152.67451597520352,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,85.94284569138271,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,79.47386904761905,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,312.1709638554216,307.7837912087912,61.621744680851094,95.83699507389139,95.83699507389139,175.9386973180075,95.83699507389139,95.83699507389139,312.1709638554216,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,152.67451597520352,61.621744680851094,61.621744680851094,61.621744680851094,188.26352697095433,307.7837912087912,152.67451597520352,312.1709638554216,85.94284569138271,175.9386973180075,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,152.67451597520352,881.9130455259043,61.621744680851094,61.621744680851094,152.67451597520352,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,152.67451597520352,95.83699507389139,95.83699507389139,175.9386973180075,61.621744680851094,61.621744680851094,175.9386973180075,61.621744680851094,61.621744680851094,95.83699507389139,111.12251785714281,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,307.7837912087912,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,307.7837912087912,61.621744680851094,61.621744680851094,95.83699507389139,312.1709638554216,152.67451597520352,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,188.26352697095433,61.621744680851094,152.67451597520352,61.621744680851094,152.67451597520352,61.621744680851094,61.621744680851094,61.621744680851094,312.1709638554216,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,152.67451597520352,61.621744680851094,95.83699507389139,307.7837912087912,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,185.23412186379932,95.83699507389139,61.621744680851094,152.67451597520352,61.621744680851094,307.7837912087912,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,175.9386973180075,95.83699507389139,85.94284569138271,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,152.67451597520352,485.01496302738354,85.94284569138271,175.9386973180075,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,244.82013986013953,95.83699507389139,320.10313131313114,307.7837912087912,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,152.67451597520352,152.67451597520352,95.83699507389139,61.621744680851094,152.67451597520352,307.7837912087912,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,307.7837912087912,61.621744680851094,95.83699507389139,95.83699507389139,152.67451597520352,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,244.82013986013953,61.621744680851094,61.621744680851094,61.621744680851094,320.10313131313114,95.83699507389139,307.7837912087912,61.621744680851094,61.621744680851094,95.83699507389139,827.1647396199811,95.83699507389139,61.621744680851094,61.621744680851094,307.7837912087912,61.621744680851094,95.83699507389139,61.621744680851094,85.94284569138271,61.621744680851094,79.47386904761905,61.621744680851094,61.621744680851094,79.47386904761905,61.621744680851094,437.5023148148152,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,79.47386904761905,152.67451597520352,175.9386973180075,61.621744680851094,152.67451597520352,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,307.7837912087912,307.7837912087912,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,175.9386973180075,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,529.1629032258063,95.83699507389139,95.83699507389139,887.9795718109087,61.621744680851094,312.1709638554216,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,95.83699507389139,61.621744680851094,61.621744680851094,61.621744680851094,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,320.10313131313114,79.47386904761905,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,312.1709638554216,61.621744680851094,61.621744680851094,95.83699507389139,834.0732411504781,61.621744680851094,61.621744680851094,95.83699507389139,61.621744680851094,312.1709638554216,61.621744680851094,85.94284569138271,95.83699507389139,95.83699507389139,61.621744680851094,61.621744680851094,175.9386973180075,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,175.9386973180075,175.9386973180075,61.621744680851094,95.83699507389139,95.83699507389139,95.83699507389139,61.621744680851094,1430.313203883495,95.83699507389139],"xaxis":"x","y":[74.33,21.29,69.48,25.53,5.45,4.54,191.97,66.44,13.95,108.04,265.21,72.16,29.57,16.01,133.5,324.25,281.25,53.02,33.09,263.51,4.56,37.34,233.43,20.85,9.83,46.7,6.07,49.22,466.95,3.88,45.83,11.84,1936.34,193.94,9.76,3.29,10.7,111.95,21.24,107.56,14.24,17.04,5.81,26.94,2.82,27.77,7.63,48.1,52.43,33.04,50.29,6.25,3.34,43.19,64.23,77.76,84.75,4.41,16.8,282.72,82.72,60.57,89.29,51.1,8.14,35.4,25.32,64.59,1.6,79.74,159.98,42.8,5.62,13.29,4.53,6.79,3.15,75.03,6.05,500.93,96.43,58.79,39.48,4.99,26.46,65.02,5.94,6.18,102.27,15.2,95.77,6.17,3.95,61.98,33.52,1148.5,37.82,79.98,138.68,34.48,4,3.35,4.39,4.78,105.01,126.03,56.48,25.9,570.98,18.33,387.49,16.44,8.65,13.87,29.78,1.08,12.29,1638.46,89.57,54.25,12.72,24.39,192.12,7.44,193.34,45.44,17.43,38.39,60.5,122.73,10.65,10.37,25.22,23.58,4.86,116.21,25.21,6.3,331.61,11.8,15.26,7.9,23.85,138.79,4.6,47.26,304.62,23.84,4.89,30.79,12.25,13.23,59.73,17.22,14.35,29.41,10.66,25.14,46.46,27.47,69.02,52.87,76.68,4.75,22.46,5.07,8.65,5.99,45.33,52.49,54.24,10.84,20.22,3.4,2937.83,170.01,30.26,82.58,229.2,569.61,2061.48,145.67,15.1,8.49,53.31,250.87,20.29,119.98,2.93,22.37,0.17,17.02,120.1,1.55,4.13,51.93,8.42,5.43,602.12,4.6,61.33,16.01,4.43,42.92,72.78,47.12,97.39,321.72,39.27,3.63,12.99,350.87,96.2,13.36,69.18,4.82,60.98,17.23,1.73,5.53,7.06,9.57,380.17,10.65,18.88,9.45,70.31,41.8,157.77,1.63,56.07,64.13,73.9,45.79,106.37,22.97,61.15,53.88,227.18,1.97,127.74,1266.06,29.93,33.28,22.38,249.46,6.12,4.44,4.55,4.2,5.28,293.56,30.19,515.71,3.52,5.67,46.79,11824.89,51.83,48.95,188.77,64.56,38.97,13.39,13.56,34.61,53.87,9.04,16.99,280.16,117.64,3.75,30.57,8.86,467.9,56.07,17.24,272.54,40.65,22.78,32.62,6.66,6.7,103.42,114.33,185.33,239.17,67.17,19.95,15.9,5.49,4.68,23.62,38.88,2.99,5.53,61.88,6.38,27.92,23.15,89.29,14.83,73.14,118.2,42.8,3.7,12.68,66.43,16.89,8.25,29.33,21.3,7.48,65.46,10.21,190.61,175.3,49.58,5.64,3.14,229.84,4.2,62.41,147.14,5.82,30.35,6.41,134.8,175.46,67.57,17.12,309.6,4.5,73.61,28.19,6.89,9.76,60.12,4.92,132.1,72.5,30.69,73.31,105.97,23.12,35.6,16.14,103.99,3.48,367.3,3.54,5.8,24.56,100.02,93.21,17.49,30.86,41.9,54.04,40.78,32.89,4.38,4.32,193.47,35.11,58.59,84.93,0,33.45,6.71,458.44,12.28,168.36,75,398.19,5.46,39.41,166.84,3.59,7.07,3.06,262.98,144.57,5.22,358.88,4.73,105.32,23.5,16.83,4.64,1.06,266.05,10.83,11.81,21.44,78.36,11.17,27.27,127.27,295.42,457.51,198.59,61.18,110.74,14.97,4.11,197.3,5.96,25.11,9.95,505.01,71.87,22.66,17.92,35.43,44.15,93.63,109.9,278.47,80.09,452.16,33.17,300.14,7.36,126.21,178.21,119.2,77.75,35.48,46.26,8.42,6.22,5.28,84.95,6.41,31.27,29.66,48.95,72.5,3.21,141.15,31.13,4.25,3.49,52,3.96,125.59,14.7,19.57,143.33,804.2,9.25,375.56,14.37,16.18,78.29,29.97,3.93,22.93,32.05,126.92,5.74,105.41,7.69,9.79,34.66,4.08,5.59,26.16,143.82,6,11.87,20.37,422.39,4.55,11.29,181.67,9.43,5.71,21.78,2.15,7.57,229.12,4.08,41.16,24.25,28.3,155.54,110.62,40.56,18.3,79.21,10.9,4.85,113.97,8.36,91.53,9.28,47.5,3.63,20.06,168.62,49.28,51.4,11.27,15.5,53.65,35.2,8.06,4,27.93,286.43,91.09,85.28,49.89,16.72,18.74,19.11,119.46,28.94,8.75,6.29,94.82,35.72,33.37,18.76,19.69,402.04,328.23,152.28,5.23,86.9,158.55,49.63,23.42,10.01,607.41,235.69,12.05,7.21,62.23,39.5,1265.29,290.4,45.33,463.12,70.84,93.83,53.71,9.53,184.09,28.37,13.1,48.99,18.47,5.15,6.37,31.15,35.8,17.14,379.34,161.39,127.56,6.9,84.59,4.33,351.9,27.99,124.54,30.1,59.14,21.57,25.12,6.01,7.97,78.92,13.71,1.55,72.9,70.16,4.04,7.47,10.02,75.99,77.04,11.18,107.16,38.13,39.57,227.13,4.43,98.88,104.61,20.02,5.63,130.28,138.35,8.08,1.08,6.96,16.46,5.94,64.79,9.67,45.38,30.19,5.79,32.42,191.97,636.96,30.02,1.73,85.59,8.61,199.48,70.17,3.69,41.74,33.12,39.73,4.35,180.14,20.5,74.98,23.63,23.9,5.79,334.15,132.32,10.29,9.39,4.14,553.27,28.63,249.98,41.96,147.36,128.91,0.78,209.92,7.34,22.15,180.89,4.12,9.43,1.59,549.74,25.93,523.95,128.22,124.77,10.48,5.53,63.77,1623.34,39.1,232.12,9.56,6.22,4.2,36.42,13.27,16.47,4.24,48.6,109.16,47.14,11.1,19.61,6.16,95.26,6.15,24.12,40.46,28.6,192.51,159.16,28.55,44.26,4.02,47.25,60.88,75.95,115.34,7.13,38.38,216.18,5.69,60.08,4.86,122.92,11.59,13.18,2.99,51.47,117.19,13.34,63.41,178.86,4.09,100.71,13.12,1874.42,11140.56,8.69,1060.13,64.06,13.71,75.58,27.38,158.48,213.9,20.09,39.57,53.48,6.04,54.76,4.19,4.39,3.44,21.71,9.16,9.82,3.65,6.08,34.03,28.21,27.63,3.83,45.07,13.21,218.49,1789.69,44.74,14.67,94.23,5.77,31.76,18.42,134.38,69.44,40.35,14.73,25.15,752.57,17.39,56.86,36.97,33.35,177.51,7.21,113.3,6.35,150.23,8.37,3.16,21.51,38.59,6.51,5.42,5.6,248.15,477.85,691.57,23.79,174.79,68.05,64.86,191.2,50.52,5.46,6.72,1699.45,5.07,35.8,376.46,21.12,58.88,348.21,29.53,0.92,9.67,13.18,92.21,4.8,426.39,20.66,5.95,48.34,71.03,59.84,234.75,723.4,133.7,158.6,250.05,69.42,25.39,2.79,298.61,94.02,39.67,5.67,1597.56,11.38,35.14,1019.53,10.04,2.69,63.37,4.31,6.73,20.76,86.81,28.05,49.6,110.41,27.33,8.64,61.86,59.76,60.67,1.78,11.97,14.49,43.15,7.52,12.62,6.26,225.35,675.34,5.54,3.78,17.11,84.83,119.01,11.54,393.2,22.74,40.25,46.52,34.03,5.84,3.54,51.89,125.13,106.74,21.38,7.39,33.58,3.27,18.91,15.7,54.02,6.86,22.81,17.08,1894.2,20.33,44.93,22.96,13.29,14.24,23.22,11.27,4.46,26.63,2.72,5.68,0.09,9.56,181.93,40.69,3.47,101.34,259.49,9.86,6.87,10.24,171.64,5.87,49.53,469.57,83.68,9.95,285.62,3.43,12.36,36.08,6.43,14.09,377.09,27.14,23.03,4.99,32.77,238.77,3243.48,93.31,5.94,455.39,67.24,29.62,9.96,5.91,234.15,39.44,314.92,154.96,5.46,86.99,20.09,260.13,6.67,39.57,147.82,188.13,5.03,9.73,5.79,45.6,21.92,740.97,59.05,120.84,47.92,108.79,84.64,6.35,15.43,9.72,12.71,26.21,47.24,22.17,68.3,46.6,6.85,2.85,84.18,12.92,15.34,18.1,4.84,12.75,34.14,47.71,129.42,45.02,7.85,202.65,4.45,4.31,148.25,15.09,123.18,2452.86,38.86,15.9,21.77,301.02,5.45,17.75,47.99,16.3,6.74,11.94,176.79,4.74,51.98,4.16,5.9,64.57,46.78,313.83,179.33,1088.2,237.36,319.73,161.2,19.39,85.73,273.91,42.18,4.62,6.33,67.45,15.37,47.64,4.89,24.89,5.6,4.19,254.16,136.94,221.51,834.96,1.14,8.16,202.99,9.71,71.55,59.07,9.36,138.61,18.9,481.97,42.99,134.01,54.41,21.44,50.46,85.95,33.92,7.69,98.95,99.96,78.7,0,15.6,51.39,3.98,25.48,58.28,62.17,88.51,9.44,24.51,36.78,88.74,5.65,215,32.48,92.45,22.53,9.11,25.38,45.32,128.39,9.46,69.59,3.28,82.07,8.45,94.59,65.35,86.86,103.77,1.85,3.02,434.48,5,133.8,8.8,587.16,214.65,15.11,3.5,433.6,75.48,19.07,4.26,4.19,41.96,3.76,5.97,10.23,61.62,4.82,28.61,86.9,36.72,16.89,9.62,103.39,303.69,37.51,225.66,7.6,92.19,3.67,38.85,9.29,23.78,14.86,3.41,176.75,59.74,6.32,111.04,30.03,17.57,14.04,399.73,3.77,13.09,87.07,154.6,28.88,6.3,60.52,6.73,13.35,65.05,30.78,103.4,231.39,4.92,0,565.79,103.07,9.57,21.05,18.86,99.39,5.91,206.01,6.03,9.15,10.6,8.63,106.73,509.36,5.44,193.72,37.74,125.42,38.56,3.05,132.71,148.7,130.73,218.53,4.52,4.51,8.28,16.32,68.14,3.56,78.06,66.23,70.39,15.43,178.65,355.05,0.92,3.04,284.01,61.07,6.98,67.78,18.65,11.62,7.73,319.88,7.49,38.67,7.85,5.04,11.39,33.8,19.7,359.09,3.3,55.69,117.11,1990.5,4.51,44.96,0,95.54,14.3,481.06,31.47,2.36,62.23,205.52,54.4,10.95,28.78,107.84,222.02,124.96,54.53,117.04,21.08,27.02,142.16,4.15,51.24,6.18,3,210.3,1.24,138.76,39.75,21.51,71.23,46.3,49.46,9.4,85.76,7.74,8.16,101.31,5.35,4.13,47.08,4.48,8.36,74.07,235.32,12.63,7.74,86.46,15.97,16.58,3.99,7.56,48.35,14.65,33.43,35.39,44.55,37.49,7.78,25.12,82.9,6.31,1.67,4.15,10.38,14.3,137.15,1.68,120.91,130.69,79.58,9.73,34.2,38.78,62.98,8.47,25.2,12.05,77.34,0.31,53.12,127.14,243.13,6.09,567.6,4.43,52.67,139.72,80.75,1185.15,63.33,30.95,4.65,303.81,6.25,8.85,23.95,5.17,41.22,3.81,1.86,72.5,51.28,41.3,37.56,22.1,36.09,49.17,6.71,3.5,67.06,4.41,394.6,38.41,271.68,138.03,1129.67,2.34,12.26,20.86,87.14,238.61,43.97,107.83,21.66,5.88,17.45,16.63,103.57,23.8,48.16,45.58,10.97,10.29,28.09,26.56,40.05,9.63,5.02,5.82,1596.08,293.55],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle AdaBoostRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predAB"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle AdaBoostRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsAB = np.logspace(0, 2, 30, dtype=int)\n","param_gridAB = {\n","    'adaboostregressor__n_estimators': n_estimatorsAB,\n","    'adaboostregressor__loss': ['linear', 'square', 'exponential']\n","}\n","\n","GridAB, \\\n","BestParametresAB, \\\n","ScoresAB, \\\n","TotalGHGEmissions_predAB, \\\n","figAB = reg_modelGrid(model=AdaBoostRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBM_train,\n","                         X_test=BEBM_test,\n","                         y_train=TotalGHGEmissions_train.ravel(),\n","                         y_test=TotalGHGEmissions_test,\n","                         y_test_name='TotalGHGEmissions_test',\n","                         y_pred_name='TotalGHGEmissions_predAB',\n","                         score=score,\n","                         param_grid=param_gridAB)\n","\n","print(BestParametresAB)\n","print(ScoresAB)\n","figAB.show()\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[447.35222611406436,445.4613747034924,451.90295628173146,438.1606557626704,520.9181468644871,527.873498782527,450.54913718324735,457.47654982935666,436.0023727158874,446.85539909692625,425.52267266490054,446.12504774625694,481.34420012930985,463.63691256705016,431.08633374576283,462.3452765616363,393.21600573799117,416.89251309797356,471.2544819385668,388.8807091915059,414.3200223350009,394.8751367384994,415.7387837819632,410.9607421805284,438.00422972437,490.79969256071064,499.3565422387502,460.1948708072461,486.06217431671877,454.9599270697187]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[537.6265535120018,560.9917354162194,536.3246551194803,513.2200585732128,634.9421338966787,697.7943752100136,510.8310527918634,593.8584089497447,528.6492110108233,582.2688783225431,525.7002744645351,575.0265410612848,624.239352721963,584.2002166499339,584.0594755489022,571.6000705241488,465.4552334548346,551.2427272563058,573.4490612327018,540.218979966492,546.7455912192102,530.7673781613761,549.5728056303681,530.8148591562729,566.0807674906525,666.5066047118529,660.5743556144871,599.6783404937717,636.5813517516301,609.8595179853049]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[357.0778987161269,329.9310139907655,367.4812574439826,363.10125295212805,406.89415983229554,357.9526223550404,390.2672215746313,321.09469070896864,343.3555344209515,311.44191987130944,325.345070865266,317.2235544312291,338.44904753665674,343.07360848416647,278.1131919426234,353.0904825991238,320.9767780211477,282.54229893964134,369.0599026444319,237.54243841651976,281.8944534507916,258.9828953156227,281.90476193355835,291.10662520478405,309.9276919580875,315.0927804095684,338.1387288630133,320.7114011207205,335.5429968818074,300.0603361541324]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[581.0840178030635,410.6466002741237,431.060184761928,500.98551044309414,659.3662457569982,855.1103211001148,425.8927629445066,487.8992762648651,316.0489942821526,671.9554417905521,460.29574751545965,609.8661119460153,672.2819728105763,625.8946143036123,473.1298953843439,403.110609381992,422.08614649313716,456.7696657370488,472.94829907943785,421.9985458531661,470.6472444227172,460.989078013501,452.25777418594373,473.0816267105272,547.3141905931614,693.1343419873854,688.9913353180145,545.8478751596645,652.5938821932493,566.5429374527496],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[329.8054192670937,591.3451069141576,541.4641633827931,347.6623201651061,604.7758982440467,456.49618747150686,423.637692671167,561.882341557367,551.853189400984,356.0898464937366,321.12977475384633,323.6566875852177,298.5044056188674,373.6311264170589,297.0990860230195,517.6164177996225,305.12579618380914,293.1533115763617,524.8176468780117,272.7973204883916,280.9233388200008,277.06798148615945,250.0630180648928,250.78397799982955,292.6822690910312,275.2552695103481,273.8669571509561,253.83870768837448,255.73771906180565,257.0451605688299],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[468.74337682615607,470.94251305301276,474.0671139022512,481.6449149855518,487.29186282011386,434.8873076995507,448.6012394464476,474.5795062410277,469.1487921801094,360.08097551561127,472.8813344725905,418.1838882216919,481.455897114872,489.9389857317806,548.3979535135426,461.5708180189356,469.53721121391163,467.5801019417254,436.7122253382305,464.42489100270245,455.9333889436246,481.1776478945288,448.3076401889417,444.8418795223173,430.00048000782965,466.0016520677973,540.2276593508145,495.78926284365036,514.8415511620193,475.95710269874223],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[491.3425890162705,508.0869312996515,512.6652671399694,513.9714164387044,526.2694696870042,518.1186770900487,565.0612655502522,567.7094272710732,503.90161998161847,532.8816873107874,570.0325866687485,581.583809108949,604.0734822113664,542.2284145879739,622.3630062143559,627.6676074238042,460.58647933124854,620.8681350522946,615.7180821102422,609.6279753181062,611.4289295652907,559.2802872487975,629.9397767923213,582.873569635344,614.7992239755819,690.950841445195,640.8379262628167,648.4718957480147,628.1817178122919,668.9738580820542],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[365.7857276577379,246.28572197651633,300.25805222171533,346.53911678089554,326.88725781427246,374.7550005514141,389.5527253038632,195.31219781245036,339.0592677345722,313.26904437394404,303.2739199138575,297.3347418694108,350.40524289086727,286.49142179482493,214.44172759355217,301.76093018382704,308.7443954678495,246.09135118243753,306.076156286912,175.55481329516334,252.66720992337105,195.86068904951026,298.1257096777163,303.2226570346243,305.224984954246,328.6563577928273,352.858833111149,357.0266125965263,378.95600135422774,306.2805765462174],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle AB pour le paramètre<br>adaboostregressor__loss=square<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE AdaBoostRegressor\n","# pour le meilleur paramètre loss\n","FigRMSEGRidAB = visuRMSEGrid(AdaBoostRegressor(), 'AB', n_estimatorsAB,\n","                             'n estimators', GridAB, BestParametresAB,\n","                             'adaboostregressor__loss')\n","FigRMSEGRidAB.show()\n","if write_data is True:\n","    FigRMSEGRidAB.write_image('./Figures/EmissionsGraphRMSEAB.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["## 1.2 Émissions au log"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["TotalGHGEmissions_train_log = np.log2(1 + TotalGHGEmissions_train)\n","TotalGHGEmissions_test_log = np.log2(1 + TotalGHGEmissions_test)\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.2.1 Modèle LinearRegression"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["r2 : -0.3095599261157267\n","rmse : 583.6520545460625\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_pred_logLR=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4.72064208984375,4.90777587890625,5.3863525390625,4.69744873046875,4.712646484375,4.73388671875,6.0726318359375,4.794677734375,4.91973876953125,5.1654052734375,5.429443359375,4.87615966796875,4.7730712890625,4.98040771484375,5.93975830078125,10.73052978515625,5.20928955078125,4.65570068359375,5.23016357421875,5.22393798828125,4.6319580078125,4.60089111328125,5.2969970703125,4.83526611328125,4.75067138671875,4.56488037109375,4.68212890625,4.92913818359375,5.48529052734375,4.61041259765625,4.902099609375,5.01312255859375,4.979736328125,5.0909423828125,4.93389892578125,4.6322021484375,4.8768310546875,4.744140625,4.7066650390625,4.7166748046875,4.61676025390625,4.574951171875,4.69964599609375,4.7952880859375,4.76690673828125,5.3646240234375,4.7303466796875,4.67144775390625,4.90557861328125,4.710205078125,5.866943359375,4.7396240234375,4.6563720703125,4.65545654296875,5.577392578125,4.937255859375,5.22381591796875,4.65667724609375,4.61669921875,5.7757568359375,4.83038330078125,4.789306640625,5.10626220703125,4.60711669921875,5.03515625,4.67242431640625,5.0587158203125,5.17584228515625,4.58038330078125,4.71728515625,4.75494384765625,4.9915771484375,4.6536865234375,4.62969970703125,4.68267822265625,4.7440185546875,4.58197021484375,4.69866943359375,4.64617919921875,14.525634765625,5.6854248046875,4.79498291015625,4.60498046875,4.61590576171875,5.74798583984375,4.696533203125,4.636962890625,4.68280029296875,4.689208984375,4.5985107421875,5.72418212890625,4.6448974609375,4.64569091796875,4.8621826171875,4.6007080078125,13.1993408203125,4.55078125,4.62567138671875,5.109619140625,4.75408935546875,4.619873046875,4.626953125,4.62677001953125,4.62579345703125,4.9361572265625,5.22882080078125,5.68017578125,4.640869140625,4.77886962890625,4.6202392578125,5.807373046875,4.68780517578125,4.76019287109375,5.3121337890625,4.79156494140625,4.63385009765625,4.81939697265625,10.37860107421875,4.65484619140625,4.71197509765625,5.04229736328125,4.66656494140625,4.68145751953125,4.6263427734375,5.47711181640625,4.842041015625,4.7012939453125,4.67474365234375,4.607177734375,4.61895751953125,4.8831787109375,4.8521728515625,4.6070556640625,4.66241455078125,4.60809326171875,4.74456787109375,4.76849365234375,4.6280517578125,5.87322998046875,4.99847412109375,5.17230224609375,4.79656982421875,4.65594482421875,7.03338623046875,4.6080322265625,4.857666015625,5.13232421875,5.3306884765625,4.65118408203125,4.80975341796875,4.92138671875,5.0283203125,4.57928466796875,4.63922119140625,4.7371826171875,4.8446044921875,4.87762451171875,4.69171142578125,4.86614990234375,4.6385498046875,5.10418701171875,5.1971435546875,4.91876220703125,4.63360595703125,4.7821044921875,4.62371826171875,4.90057373046875,4.64093017578125,4.6690673828125,5.71832275390625,4.615478515625,4.905517578125,4.97454833984375,4.5997314453125,8.873291015625,5.24609375,4.89141845703125,4.81585693359375,5.3131103515625,14.3067626953125,11.86737060546875,6.15985107421875,4.9342041015625,4.88543701171875,4.76202392578125,4.68408203125,5.07733154296875,4.8131103515625,4.639892578125,4.60211181640625,4.7349853515625,4.7169189453125,4.923828125,4.57049560546875,4.64044189453125,4.8660888671875,4.93505859375,4.61370849609375,7.39056396484375,4.69232177734375,4.6090087890625,4.66619873046875,4.79974365234375,4.61016845703125,4.85888671875,4.73687744140625,6.8173828125,14.509521484375,5.4346923828125,4.62274169921875,4.7684326171875,5.3031005859375,6.11236572265625,4.6544189453125,5.318603515625,4.68603515625,4.89501953125,5.0633544921875,4.7119140625,4.74615478515625,4.65093994140625,4.577392578125,4.7481689453125,5.1510009765625,4.60223388671875,4.79302978515625,4.695556640625,5.3802490234375,4.97747802734375,4.55584716796875,4.60906982421875,4.6544189453125,4.8458251953125,4.64434814453125,5.84423828125,4.67608642578125,4.60223388671875,5.05224609375,5.3509521484375,4.6171875,5.02459716796875,4.8258056640625,4.7584228515625,5.22662353515625,4.63568115234375,5.07110595703125,4.65460205078125,4.63623046875,4.634033203125,5.8455810546875,4.73956298828125,5.67724609375,4.88983154296875,4.648681640625,4.5543212890625,4.72088623046875,4.86279296875,18.883056640625,4.658447265625,4.63543701171875,6.42138671875,5.092529296875,4.8424072265625,5.13580322265625,4.94293212890625,4.6058349609375,5.14227294921875,4.70465087890625,4.7332763671875,5.06109619140625,5.82269287109375,4.68963623046875,5.05401611328125,4.8551025390625,7.2105712890625,5.33123779296875,5.029296875,4.86529541015625,4.8431396484375,4.7034912109375,5.07080078125,4.6923828125,4.7476806640625,4.64892578125,4.89019775390625,4.79132080078125,4.8555908203125,5.218994140625,4.595947265625,4.76287841796875,4.7398681640625,4.65887451171875,4.60614013671875,4.60174560546875,4.58880615234375,4.76068115234375,4.618408203125,4.5560302734375,4.70306396484375,5.0179443359375,4.8629150390625,5.08929443359375,5.2955322265625,7.24554443359375,5.00732421875,4.60540771484375,4.576171875,4.58221435546875,5.0037841796875,4.89312744140625,5.1307373046875,4.7392578125,4.7186279296875,5.0657958984375,4.6104736328125,5.63128662109375,5.89935302734375,4.62579345703125,4.72137451171875,4.5589599609375,5.99737548828125,4.6502685546875,5.24835205078125,7.61260986328125,4.5706787109375,4.66717529296875,4.6865234375,4.64892578125,5.9169921875,4.79656982421875,4.85711669921875,5.43621826171875,4.669921875,4.64007568359375,4.83782958984375,4.808837890625,4.94537353515625,4.67584228515625,4.696533203125,4.84765625,4.8475341796875,4.63128662109375,5.08673095703125,5.26080322265625,5.04766845703125,5.6551513671875,4.68084716796875,4.9376220703125,4.7530517578125,5.2733154296875,4.60845947265625,4.7144775390625,4.78863525390625,5.1929931640625,4.69091796875,5.212646484375,5.02392578125,4.9595947265625,5.2127685546875,4.93853759765625,4.62164306640625,4.69451904296875,4.606201171875,5.79559326171875,4.617431640625,4.61273193359375,5.75103759765625,4.7935791015625,4.73944091796875,4.96435546875,7.08050537109375,4.58929443359375,4.64111328125,5.20660400390625,8.9154052734375,4.6279296875,6.02569580078125,7.61956787109375,4.63775634765625,4.6807861328125,4.61962890625,7.751708984375,5.9256591796875,4.6112060546875,5.5010986328125,4.701171875,4.81671142578125,5.0042724609375,4.96722412109375,4.6717529296875,4.69677734375,5.96368408203125,4.6728515625,4.62017822265625,4.671875,4.5711669921875,5.02459716796875,4.80426025390625,5.62249755859375,7.72955322265625,4.80657958984375,5.24456787109375,5.21197509765625,5.4429931640625,4.69964599609375,4.597412109375,5.0753173828125,4.59320068359375,5.2449951171875,4.74847412109375,5.65289306640625,4.90106201171875,4.85382080078125,4.656494140625,5.71514892578125,4.61199951171875,4.64263916015625,4.8035888671875,4.6805419921875,4.68060302734375,14.47052001953125,5.3876953125,5.267333984375,4.80816650390625,5.5986328125,6.7457275390625,5.64190673828125,4.75396728515625,4.77471923828125,4.9686279296875,4.77178955078125,4.7789306640625,4.74267578125,4.6171875,4.62457275390625,5.50555419921875,4.72955322265625,4.64404296875,5.25201416015625,4.64849853515625,5.5677490234375,4.81951904296875,4.61419677734375,4.5693359375,5.796875,4.64593505859375,4.7049560546875,4.65826416015625,4.64007568359375,5.76910400390625,7.28839111328125,4.76690673828125,5.65081787109375,4.608154296875,4.63970947265625,4.6282958984375,4.688720703125,5.0009765625,4.7467041015625,4.551025390625,4.62445068359375,4.6966552734375,6.98565673828125,4.80682373046875,4.63214111328125,4.59246826171875,4.62109375,4.6158447265625,4.63470458984375,7.0513916015625,4.76666259765625,4.83502197265625,4.93804931640625,6.0545654296875,4.666259765625,4.9217529296875,7.46246337890625,4.84466552734375,4.7767333984375,4.64105224609375,4.77679443359375,4.727783203125,4.77655029296875,4.6435546875,5.570556640625,4.611572265625,5.94970703125,5.56719970703125,5.11566162109375,4.67449951171875,4.84765625,4.6795654296875,4.8634033203125,4.626708984375,5.7833251953125,4.7818603515625,5.5323486328125,4.8370361328125,4.7581787109375,4.62725830078125,4.8824462890625,5.78997802734375,4.72149658203125,5.228271484375,4.66455078125,5.90869140625,4.64678955078125,4.90325927734375,4.711669921875,4.61700439453125,4.72930908203125,4.995361328125,5.08636474609375,5.09661865234375,5.0458984375,4.672607421875,4.96600341796875,4.68914794921875,5.4754638671875,4.8509521484375,4.7398681640625,4.621337890625,6.09674072265625,4.81439208984375,4.702880859375,4.6995849609375,4.6751708984375,5.67315673828125,5.022216796875,5.257080078125,4.63427734375,4.6715087890625,5.28436279296875,4.7635498046875,5.5709228515625,4.92218017578125,7.633056640625,6.61322021484375,5.23828125,4.7369384765625,5.21661376953125,5.266357421875,9.0450439453125,11.92498779296875,4.69012451171875,6.74639892578125,4.77081298828125,4.94891357421875,4.581787109375,5.05499267578125,4.7557373046875,5.0205078125,5.040283203125,4.69073486328125,5.35174560546875,4.695556640625,4.66253662109375,6.489501953125,5.24542236328125,4.9075927734375,4.8504638671875,4.7633056640625,4.7144775390625,4.670654296875,5.1287841796875,4.6826171875,4.83050537109375,4.670654296875,4.59783935546875,4.61480712890625,4.83709716796875,4.67498779296875,5.0338134765625,4.67779541015625,4.6318359375,5.47808837890625,4.7535400390625,4.60772705078125,4.939208984375,4.69842529296875,4.67193603515625,4.82366943359375,4.88458251953125,4.6171875,4.71380615234375,4.68017578125,4.83209228515625,5.05096435546875,6.2891845703125,5.00714111328125,4.6534423828125,4.826904296875,6.11865234375,4.74578857421875,4.6812744140625,4.84967041015625,4.90985107421875,4.7498779296875,4.61822509765625,4.7254638671875,5.04779052734375,4.6683349609375,4.8668212890625,4.8218994140625,4.6937255859375,4.6376953125,4.70013427734375,4.58056640625,5.663330078125,9.42413330078125,4.69451904296875,4.6766357421875,5.0458984375,4.77557373046875,6.1768798828125,5.00732421875,4.61944580078125,5.00946044921875,4.84710693359375,4.6781005859375,4.61126708984375,8.40118408203125,4.66143798828125,4.89697265625,4.76788330078125,4.7305908203125,4.64471435546875,4.60418701171875,4.66986083984375,4.61895751953125,4.91656494140625,4.6527099609375,5.15087890625,5.064453125,5.238037109375,4.920166015625,5.69244384765625,5.8927001953125,4.60809326171875,5.3353271484375,4.60357666015625,4.6619873046875,5.988525390625,4.67529296875,4.77459716796875,4.59307861328125,4.786376953125,4.9437255859375,4.79400634765625,6.9737548828125,5.60638427734375,4.92901611328125,4.73565673828125,4.680908203125,5.61358642578125,4.74267578125,4.5509033203125,4.87017822265625,4.7564697265625,4.66326904296875,4.98956298828125,4.82720947265625,4.6265869140625,4.7491455078125,4.7386474609375,5.34332275390625,5.3277587890625,5.01220703125,4.72711181640625,4.62786865234375,5.76214599609375,4.7120361328125,5.9427490234375,5.48162841796875,4.61309814453125,7.71392822265625,6.72607421875,4.89410400390625,4.68707275390625,4.6116943359375,4.64117431640625,4.60015869140625,5.06787109375,5.36273193359375,4.812744140625,4.60479736328125,4.5316162109375,4.61785888671875,4.6964111328125,4.661865234375,5.903564453125,4.81414794921875,4.6572265625,4.61578369140625,5.47540283203125,4.8199462890625,5.03753662109375,4.8028564453125,4.84674072265625,4.59490966796875,4.78900146484375,4.631591796875,6.93316650390625,81.8016357421875,4.81988525390625,5.47998046875,4.6807861328125,4.71307373046875,4.64306640625,4.6229248046875,5.45751953125,8.2264404296875,5.56402587890625,5.06658935546875,5.1392822265625,4.71221923828125,5.02410888671875,4.63116455078125,4.69659423828125,4.641845703125,4.6776123046875,4.6474609375,4.98931884765625,4.61383056640625,4.7010498046875,4.689697265625,5.118896484375,4.7247314453125,4.60809326171875,4.82757568359375,4.63134765625,5.9974365234375,6.87646484375,5.162109375,5.0057373046875,4.80902099609375,4.753662109375,6.0849609375,4.6607666015625,4.934326171875,5.4136962890625,5.15692138671875,4.67889404296875,4.7894287109375,6.142578125,4.64068603515625,4.60980224609375,4.603515625,4.66693115234375,6.74114990234375,5.5572509765625,4.7410888671875,4.6094970703125,4.814697265625,4.89080810546875,4.564697265625,5.21270751953125,4.65301513671875,4.721923828125,4.611328125,4.7427978515625,5.987060546875,5.2860107421875,9.00213623046875,4.7568359375,5.20147705078125,4.88592529296875,4.57843017578125,4.8572998046875,4.92596435546875,4.67828369140625,4.83514404296875,7.78680419921875,4.6531982421875,5.072265625,5.0418701171875,4.61090087890625,5.3516845703125,5.822998046875,5.5313720703125,4.6068115234375,4.7645263671875,4.97723388671875,5.36358642578125,4.75494384765625,8.210693359375,4.8212890625,4.781982421875,4.8892822265625,5.251708984375,4.634033203125,6.2071533203125,7.245849609375,4.60333251953125,4.5704345703125,6.59454345703125,5.58984375,4.6788330078125,4.71197509765625,5.920166015625,4.80975341796875,4.64898681640625,4.63983154296875,6.26416015625,4.72357177734375,4.9898681640625,6.66522216796875,4.67529296875,4.61700439453125,4.5567626953125,4.70062255859375,4.68670654296875,4.6363525390625,4.8284912109375,4.7314453125,4.59136962890625,5.1942138671875,5.64593505859375,4.6982421875,5.21417236328125,4.982177734375,4.59381103515625,4.6922607421875,4.86712646484375,4.8492431640625,4.86041259765625,4.72088623046875,4.70404052734375,4.6881103515625,4.70965576171875,7.03466796875,4.69647216796875,4.68267822265625,4.65533447265625,4.95257568359375,4.69342041015625,4.95208740234375,5.900146484375,4.65234375,5.17364501953125,5.67462158203125,4.62884521484375,4.72222900390625,4.605224609375,4.5516357421875,4.9927978515625,5.56414794921875,5.26629638671875,4.7626953125,5.08642578125,4.6407470703125,5.40460205078125,4.738037109375,4.65716552734375,4.62652587890625,5.29266357421875,4.727783203125,6.6480712890625,4.58526611328125,4.6641845703125,5.14068603515625,4.65618896484375,4.72943115234375,4.66046142578125,4.6097412109375,4.63116455078125,4.65264892578125,4.77996826171875,4.8001708984375,4.751708984375,4.81524658203125,6.88372802734375,4.57232666015625,4.64215087890625,4.876953125,6.61016845703125,4.89227294921875,4.76373291015625,4.82318115234375,5.19842529296875,4.71197509765625,4.70819091796875,5.41241455078125,5.41485595703125,4.8056640625,6.38604736328125,4.6141357421875,4.7310791015625,4.65155029296875,4.72283935546875,4.89886474609375,6.49591064453125,4.70904541015625,4.7945556640625,4.81201171875,4.6190185546875,5.1160888671875,21.04693603515625,5.14593505859375,4.78021240234375,7.37701416015625,5.12811279296875,4.58795166015625,4.974609375,4.72100830078125,7.526611328125,5.171875,6.09039306640625,5.31365966796875,4.75347900390625,5.6580810546875,4.610595703125,5.15069580078125,4.76617431640625,4.61865234375,6.785888671875,7.16156005859375,4.6392822265625,4.74957275390625,4.7581787109375,5.757080078125,5.00439453125,7.261474609375,4.69171142578125,4.82330322265625,4.8140869140625,4.7637939453125,5.5421142578125,4.7840576171875,4.66607666015625,4.56463623046875,4.98858642578125,4.66168212890625,5.220947265625,5.01104736328125,5.06378173828125,4.9173583984375,4.78851318359375,5.010986328125,5.282470703125,4.63580322265625,4.75335693359375,4.6480712890625,4.7310791015625,4.60296630859375,5.367431640625,5.2568359375,4.73150634765625,4.73980712890625,4.8590087890625,5.418212890625,4.6544189453125,4.6749267578125,4.636962890625,5.07562255859375,6.35711669921875,5.9830322265625,4.62890625,5.146484375,5.0455322265625,6.248779296875,4.63836669921875,4.6612548828125,6.36285400390625,5.00042724609375,4.74420166015625,4.693359375,4.817626953125,4.68121337890625,5.23004150390625,4.64947509765625,4.62408447265625,4.7078857421875,5.741455078125,6.22998046875,5.2852783203125,6.99542236328125,5.4853515625,5.64947509765625,4.815185546875,5.03656005859375,4.74786376953125,4.7803955078125,4.7425537109375,4.5933837890625,4.66046142578125,4.59881591796875,4.61480712890625,4.62347412109375,4.68218994140625,4.93914794921875,4.6845703125,4.94171142578125,4.8919677734375,5.07861328125,5.33587646484375,19.50262451171875,4.65081787109375,4.6986083984375,5.6029052734375,4.724365234375,5.05462646484375,5.022216796875,4.86175537109375,4.744384765625,5.20855712890625,5.02294921875,4.9818115234375,5.79327392578125,4.68939208984375,4.886962890625,5.72882080078125,4.6392822265625,4.66278076171875,4.763916015625,5.6697998046875,4.954833984375,4.76971435546875,4.710693359375,5.068359375,4.76739501953125,4.66571044921875,4.96240234375,5.0919189453125,4.65863037109375,4.80780029296875,4.94439697265625,4.60443115234375,4.738525390625,6.20294189453125,4.65228271484375,5.13897705078125,4.8006591796875,4.7296142578125,4.72344970703125,4.66839599609375,4.70611572265625,4.9957275390625,4.68572998046875,4.59686279296875,6.24200439453125,4.591064453125,4.76953125,4.79583740234375,7.50982666015625,5.25750732421875,4.6988525390625,4.8165283203125,4.6212158203125,4.6185302734375,5.77642822265625,4.695068359375,5.62396240234375,4.7633056640625,5.3160400390625,4.77105712890625,4.62908935546875,4.6473388671875,6.3408203125,4.72637939453125,5.276123046875,4.6602783203125,4.724365234375,4.64678955078125,4.59918212890625,4.6251220703125,4.91778564453125,4.996337890625,4.693115234375,4.58502197265625,4.81005859375,4.61248779296875,4.71868896484375,4.73101806640625,5.3897705078125,4.868408203125,4.97003173828125,6.07928466796875,4.66888427734375,4.7806396484375,4.61181640625,4.81817626953125,4.80926513671875,4.6602783203125,5.1312255859375,4.62744140625,4.75238037109375,4.78125,4.98944091796875,4.8780517578125,5.16107177734375,4.78619384765625,4.97991943359375,5.71136474609375,4.6204833984375,4.61761474609375,5.0126953125,4.84039306640625,5.91888427734375,4.60467529296875,4.74957275390625,4.7320556640625,4.6104736328125,5.26947021484375,4.8897705078125,4.62896728515625,4.92193603515625,4.65869140625,4.65020751953125,5.32666015625,4.702392578125,4.7786865234375,4.9091796875,4.7655029296875,5.60015869140625,4.80169677734375,5.63531494140625,4.6671142578125,4.76080322265625,4.89501953125,4.81463623046875,5.21234130859375,9.1156005859375,5.44091796875,5.667236328125,5.03643798828125,4.89117431640625,4.61376953125,4.6226806640625,5.9290771484375,4.85205078125,7.54315185546875,6.0052490234375,4.62762451171875,4.63604736328125,4.810791015625,4.71148681640625,4.62835693359375,4.62371826171875,5.316162109375,5.257568359375,4.97100830078125,4.7149658203125,5.06805419921875,6.5189208984375,4.67779541015625,4.64276123046875,4.97222900390625,4.629150390625,4.75152587890625,6.01513671875,4.6014404296875,4.692138671875,4.78662109375,5.3568115234375,4.70709228515625,4.62103271484375,4.84271240234375,4.60028076171875,4.7742919921875,4.75555419921875,4.70562744140625,6.6053466796875,4.615966796875,4.85308837890625,4.84954833984375,6.90972900390625,4.6561279296875,6.29766845703125,4.83837890625,4.95672607421875,4.74505615234375,15.088134765625,4.72454833984375,4.6739501953125,4.61578369140625,5.90777587890625,5.0811767578125,4.92474365234375,4.8870849609375,5.36688232421875,5.23541259765625,5.08172607421875,4.92120361328125,4.78656005859375,5.41729736328125,4.84783935546875,7.97857666015625,4.62298583984375,4.8323974609375,4.7640380859375,4.827392578125,4.78668212890625,4.72491455078125,5.47259521484375,5.4503173828125,5.67388916015625,4.61419677734375,5.2554931640625,4.58642578125,4.87420654296875,5.09063720703125,4.6607666015625,4.6512451171875,4.8787841796875,4.611328125,4.63031005859375,4.60943603515625,4.61798095703125,4.86572265625,6.1131591796875,6.43701171875,4.656982421875,4.71685791015625,4.83795166015625,4.63232421875,4.69140625,4.66009521484375,4.69561767578125,4.75494384765625,5.1077880859375,4.75146484375,4.6932373046875,4.60333251953125,4.9134521484375,4.84698486328125,4.7645263671875,4.977783203125,4.60443115234375,4.85430908203125,4.67071533203125,4.7799072265625,4.65399169921875,5.613525390625,4.80657958984375,4.7891845703125,4.86761474609375,4.69921875,4.9007568359375,4.728271484375,4.77069091796875,4.826171875,4.65093994140625,5.05908203125,4.85552978515625,4.6015625,4.771484375,4.76556396484375,5.06298828125,4.622802734375,4.714599609375,9.5433349609375,4.67962646484375,4.81707763671875,9.028564453125,5.0684814453125,6.35302734375,4.87451171875,5.03790283203125,4.60760498046875,4.80615234375,4.68695068359375,4.67120361328125,4.8746337890625,4.54888916015625,4.8468017578125,4.604736328125,4.6767578125,4.72479248046875,5.04559326171875,4.65447998046875,4.96722412109375,4.7930908203125,6.40533447265625,5.13726806640625,4.6124267578125,4.65496826171875,5.03363037109375,4.639404296875,6.47515869140625,5.297607421875,4.7906494140625,4.786865234375,11.2109375,4.64300537109375,4.65008544921875,4.997802734375,4.74609375,6.986572265625,4.607177734375,5.37054443359375,4.65704345703125,4.6490478515625,4.57366943359375,4.8790283203125,5.68890380859375,4.832763671875,4.6392822265625,4.8154296875,4.821533203125,5.7738037109375,5.69049072265625,4.62457275390625,5.08453369140625,4.8538818359375,4.63824462890625,4.7059326171875,9.0294189453125,5.1502685546875],"xaxis":"x","y":[6.235152624217932,4.478324711485028,6.139142019145689,4.729552770453971,2.689299160535892,2.4698859762744636,7.592232766722112,6.075532631167357,3.9020735793107426,6.768713657030485,8.05642095591347,6.192983169973992,4.934044647112246,4.088311235888661,7.071462362556624,8.345405246717796,8.140829770773001,5.7554217347342425,5.091276694365222,8.047178455318921,2.475084882948783,5.260778431893426,7.873013392989595,4.4495613746132365,3.4369613378336026,5.575917361118149,2.821710215034674,5.650190123496775,8.870210577263595,2.2868811477881614,5.54936113292434,3.682573297347578,10.919861451442168,7.606886339275915,3.4276061727818994,2.1009776477248208,3.548436624696042,6.819540460505722,4.475084882948782,6.762348815644129,3.929790997718597,4.173127433480656,2.7676547982373463,4.804260115634738,1.9335726382610239,4.846493315964563,3.10936055940423,5.617651119427331,5.739578112048051,5.089159131911238,5.6806056662025926,2.8579809951275723,2.1176950426697547,5.465648025982281,6.027463723408577,6.299391206126829,6.422064766172812,2.4356285940520905,4.153805336079036,8.148324042480084,6.387500406480984,5.944155663440363,6.496494306853713,5.703211467391146,3.192194165283345,5.185866545311334,4.718087583960517,6.035403969942067,1.3785116232537298,6.335211681705785,7.330737650206952,5.452858964713811,2.726831217032493,3.8369340113210915,2.4672794804599825,2.9616233282869446,2.0531113364595623,6.248496885959384,2.817623257511431,8.971342367308225,6.606294160697713,5.901832305785489,5.339137384919585,2.582556003014061,4.779259720372376,6.044831233447812,2.7949356628035362,2.8439838440483265,6.690277401023435,4.017921907997263,6.596487956856693,2.84197311892718,2.3074285251922473,5.976821852360685,5.10936055940423,10.16679075071818,5.2787282129389395,6.339493737901733,7.125981653854716,5.148934104526339,2.321928094887362,2.1210154009613658,2.430285272977781,2.5310694927259543,6.72805655144938,6.989025440631378,5.844988156682612,4.749534267669262,9.159820892013935,4.272769732436511,8.601733652922286,4.124328135002202,3.270528942380718,3.894332742277694,4.943921326553485,1.0565835283663676,3.7322691995014496,10.679004987548526,6.500961352460057,5.787902559391432,3.7782085763980877,4.6661884898727815,7.593353770980297,3.077242998932461,7.602439063741618,5.537296067090842,4.203984165855989,5.299757511839318,5.94251450533924,6.951051533288609,3.542258049766918,3.507160349117523,4.71259578044703,4.6194130105979365,2.550900664647523,6.872951851130105,4.712045448553693,2.867896463992655,8.377687734026882,3.678071905112638,4.0232553523003025,3.1538053360790355,4.635173946674924,7.127117349726852,2.4854268271702415,5.592756010441026,8.25559514727367,4.634593268445757,2.5582676340557358,4.990501111363251,3.727920454563199,3.8308637567517576,5.924337464075348,4.187451054027326,3.940166750482817,4.926473911256073,3.5434958834257717,4.708187236020708,5.568640195419223,4.831370587967541,6.12969515665378,5.751410160064371,6.279471295644468,2.523561956057013,4.552131108253784,2.6016965164809576,3.270528942380718,2.805292455600712,5.533874777258546,5.741197298571427,5.787641414483327,3.565597175854225,4.40735275114004,2.137503523749935,11.52102619171175,7.417936880550136,4.96624587322494,6.385085853547684,7.846744023244868,9.156361220297567,11.010164414182617,7.196430000954568,4.008988783227255,3.2464080872463845,5.763145958149476,7.976535484023556,4.4121040446775694,6.918624755574591,1.9745293124838819,4.546585829008101,0.22650852980867975,4.1715271060388135,6.920055054806967,1.3504972470841332,2.3589588258323295,5.726013748860151,3.2357270598380583,2.6848187375532224,9.236301266827088,2.4854268271702415,5.9618548076361595,4.088311235888661,2.440952198029637,5.456806149230474,6.205157883920094,5.588564737401351,6.620439787651722,8.334139178838237,5.331633567171029,2.211012193485512,3.8063240573900288,8.45889870671479,6.602884408718419,3.8439838440483265,6.132988042627442,2.541019153133559,5.953730849811491,4.18824265626441,1.4489009511451278,2.7070829917717063,3.0107798387532423,3.4019034716079584,8.574290766120185,3.542258049766918,4.313245851787562,3.3854310371935203,6.156032498888866,5.419538891513785,7.3107945270667845,1.3950627995175775,5.8346606579017255,6.025250321561575,6.2268938135713885,5.548128323585586,6.746447139362815,4.583158003874408,5.9576824861651225,5.778208576398088,7.834028534577023,1.570462931026041,7.0083165637349865,10.307269127730322,4.95093492831454,5.099295204337775,4.547203024756931,7.9684364043347875,2.8318772411916733,2.4436066514756147,2.4724877714627436,2.37851162325373,2.6507645591169022,8.202417721575182,4.963011647599427,9.01321099459576,2.176322772640463,2.73768676140986,5.5786368624717415,13.52966114241807,5.72328550477764,5.642412772905056,7.568108130120591,6.034743949324734,5.320845667645722,3.8469946869655733,3.8639384504239715,5.154210530588339,5.777945670695148,3.327687364176047,4.169123281476757,8.135247549809566,6.890446692679906,2.2479275134435857,4.9804823555382605,3.3015876466031866,8.873136468833458,5.8346606579017255,4.189033824390017,8.095608004811197,5.380244590478185,4.571676809970931,5.071247819461442,2.9373443921502322,2.944858445807539,6.706254253573203,6.849624029924543,7.541716163393138,7.907912142840534,6.091065077930535,4.388878338811989,4.078951341394822,2.698218478224414,2.5058909297299574,4.621758856709732,5.3175935046234715,1.996388746447621,2.7070829917717063,5.974529312483882,2.883620816285671,4.853995647176393,4.593951283948411,6.496494306853713,3.9845893503624565,6.212180210044431,6.897240425574799,5.452858964713811,2.232660756790275,3.773996325111173,6.075318692553354,4.161081482277184,3.2094533656289497,4.922673592849446,4.478971805032942,3.0840642647884744,6.054414387917307,3.486714373030702,7.582029046119882,7.461888664432819,5.6604951318885135,2.7311832415722,2.049630767724601,7.850749425896496,2.37851162325373,5.986638471730853,7.210817432089213,2.769771739249448,4.970393537914677,2.889473542531109,7.08533966935737,7.463197379833495,6.09950561709114,4.17951105027151,8.27891401949994,2.4594316186372973,6.221297103163338,4.867402305727543,2.980025300238734,3.4276061727818994,5.933572638261024,2.565597175854225,7.056366761024529,6.199672344836364,4.985955754607509,6.215484464192794,6.7410624357490905,4.59215800212536,5.19377174339668,4.099295204337775,6.714108111595169,2.1634987322828794,8.524737587012376,2.18269229751619,2.765534746362977,4.67581593117227,6.658497136656478,6.557808298936688,4.208673319629471,4.993674361750585,5.422905742612183,5.7824085649273735,5.3847405872923835,5.08278773170927,2.4276061727818994,2.411426245726465,7.603403804166527,5.174326515173915,5.896998342338911,6.425089989874059,0,5.106432077816929,2.9467308601403097,8.843732656611305,3.7311832415722,7.403949364232865,6.247927513443585,8.640931770341906,2.6915341649192004,5.336640446408735,7.390942772802543,2.1984941536390834,3.0125686735030555,2.0214797274104517,8.044284820139424,7.1855692560893685,2.636914580355878,8.491372117815171,2.5185351389821804,6.73226919950145,4.614709844115208,4.15623479785027,2.495695162624069,1.0426443374084937,8.060966074004773,3.5643781685650637,3.679198570566922,4.488000770834068,6.3103401206121505,3.605257262939004,4.821199978056451,7.003039979749236,8.211498981977794,8.84080938879262,7.640895629328568,5.95837871156634,6.804001915179304,3.997292407636508,2.3533232911628965,7.631540867341486,2.7990873060740036,4.706530552538117,3.452858964713811,8.983022086172278,6.187253085582297,4.564378168565064,4.241840183564671,5.187055089968057,5.496654082593496,6.564225720206398,6.793115555274348,8.126549613543878,6.3414521074811265,8.823876711241038,5.094658342654544,8.23429054170594,3.063502942306158,6.9910682753603695,7.485507332459785,6.9092930858238235,6.2992080183872785,5.189033824390017,5.562547724199122,3.2357270598380583,2.851998837112446,2.6507645591169022,6.425425734590699,2.889473542531109,5.012121672712217,4.938285791884053,5.642412772905056,6.199672344836364,2.0738202332916713,7.151270288790165,5.005849075696688,2.3923174227787602,2.1667154449664223,5.727920454563199,2.3103401206121505,6.984019633085816,3.9726926540042644,4.362469888750209,7.173227395028653,9.653203362033977,3.357552004618084,8.556735947914529,3.9420452599160467,4.1026581313637385,6.309067020588001,4.952799477899938,2.3015876466031866,4.580748491763774,5.046578366620329,6.999098033705607,2.752748591407134,6.73348992588162,3.1193561770396756,3.431622959713292,5.15623479785027,2.344828496997441,2.720278465233327,4.763411574470007,7.178117045706399,2.807354922057604,3.685940148445977,4.417515002885087,8.72584338467341,2.4724877714627436,3.619413010597937,7.5130959085510085,3.382667252745041,2.7463127664254587,4.509695841933387,1.6553518286125541,3.0992952043377753,7.846242565109514,2.344828496997441,5.39780296186249,4.658211482751795,4.872828759534886,7.290387539763883,6.802451741275622,5.377123749129487,4.270528942380718,6.325710207515785,3.572889668420581,2.548436624696042,6.845113646617966,3.2265085298086795,6.531849285751203,3.361768359419153,5.599912842187128,2.211012193485512,4.396433531250992,7.406162478777607,5.6519127446457835,5.7114949066500875,3.61706334388818,4.044394119358453,5.772149590784542,5.177917792195843,3.1795110502715103,2.321928094887362,4.854494418154875,8.167066838182594,6.5249725982866265,6.43095427138573,5.669310286202474,4.147306698780294,4.303050084681673,4.3298411765306275,6.912410353808268,4.904002316283692,3.2854022188622483,2.8659188145522125,6.582254908357664,5.198494153639083,5.103077946597573,4.3045110418099535,4.370861740085285,8.654779216959517,8.362951992360674,7.2600256559614555,2.6392321632492775,6.457791260256042,7.31786479839195,5.661920579135087,4.609991295212678,3.460742563789644,9.248900055578497,7.886854944198882,3.7059779016825223,3.0373822220030804,5.982537314399271,5.339850002884624,10.306392126750833,8.18685706717715,5.533874777258546,8.85835405767524,6.166715444966422,6.56727163085093,5.7737326506078555,3.396433531250992,7.532083141492505,4.876271360550127,3.817623257511431,5.643567621908798,4.28318097894557,2.6205864104518777,2.881664619320345,5.006746832440585,5.20163386116965,4.181102550753798,8.571145863602528,7.3433189836610735,7.006298023900369,2.9818526532897405,6.419370342528151,2.414135532984451,8.463115619892362,4.857483428286672,6.972003303808356,4.958842675243241,5.910252962799278,4.4963345134171115,4.707082991771706,2.8094144442358986,3.1651079851445365,6.320484678017694,3.8787253414801053,1.3504972470841332,6.207502459258789,6.152994605492435,2.333423733725192,3.0823619695574735,3.4620523187964327,6.266599165535096,6.2861418728343015,3.6064422281316078,6.75702324650746,5.29020320512607,5.342341397431548,7.833712369021444,2.440952198029637,6.642123916153487,6.722602636851373,4.393690764187454,2.729008870337863,7.036503333738988,7.122569192216447,3.18269229751619,1.0565835283663676,2.992768430768924,4.125981653854716,2.7949356628035362,6.039796407620025,3.4154882710497003,5.535430914871279,4.963011647599427,2.763411574470007,5.062639828286463,7.592232766722112,9.317322159829551,4.955126781261366,1.4489009511451278,6.436128517259249,3.2645364309990255,7.647314509547136,6.15319733085508,2.229587922740652,5.417515002885087,5.0925457415435655,5.34801990922869,2.4195388915137843,7.500961352460057,4.426264754702098,6.247547806890298,4.622344722724236,4.638073837180719,2.763411574470007,8.388663123619393,7.058749412335524,3.496973580998276,3.377123749129487,2.361768359419153,9.114445113289024,4.888986721186558,7.971428593590467,5.424922088210688,7.212958362728285,7.02136867825973,0.8318772411916731,7.720552091605108,3.060047383669939,4.532940288372874,7.506922418047323,2.356143810225275,3.382667252745041,1.372952097911829,9.10522758436473,4.751142324726947,9.036036206482532,7.013685569928653,6.974644026040249,3.521050736900963,2.7070829917717063,6.017253838968937,10.665637927637095,5.325530331567558,7.864928972289788,3.400537929583729,2.851998837112446,2.37851162325373,5.22573765346606,3.8349134297010385,4.126807703142035,2.389566811762726,5.632268215499513,6.783456654360239,5.589164236699772,3.596935142387232,4.365272599764531,2.8399595874895316,6.588864518190148,2.8379432418910273,4.650764559116903,5.37364821133469,4.8875252707415875,7.596264312101167,7.323370069061268,4.88508622529017,5.500164679492167,2.327687364176047,5.592457037268081,5.951401291674311,6.265849421440848,6.862203399053224,3.023255352300303,5.299391206126829,7.7627474419531515,2.7420062108667365,5.932628157022127,2.550900664647523,6.9532652390148435,3.6542063779442917,3.8257856274647914,1.996388746447621,5.713420884868084,6.884964164708779,3.84197311892718,6.009212786805205,7.490730563589641,2.3476656563009706,6.668317719915721,3.8196681834964554,10.872998007770835,13.44366362719111,3.2764966656403565,10.051385697621988,6.023698917266407,3.8787253414801053,6.258895755075436,4.826802684285827,7.317231700222606,7.7475216725404215,4.398487190018967,5.342341397431548,5.767654798237347,2.8155754288625725,5.8011586560936985,2.3757345385831563,2.430285272977781,2.150559676575381,4.505255800932163,3.344828496997441,3.4356285940520905,2.2172307162206693,2.823749360308273,5.130519083027338,4.868390453054454,4.839455765224629,2.2720231890610485,5.525755692829486,3.828834649468056,7.7780114016125745,10.806299887662895,5.515384460636948,3.9699332746978557,6.57334422759282,2.759155833800272,5.033863451866284,4.279471295644468,7.080870811759721,6.138323004056484,5.369815424283912,3.975446765640962,4.708739041359579,9.557597721454812,4.200849574637213,5.854494418154875,5.246788093844365,5.102238193930738,7.479861085016818,3.0373822220030804,6.836681593327116,2.877744249949002,7.240600549955795,3.228049047884462,2.0565835283663674,4.4924941516794314,5.307064162255372,2.9088129077395473,2.682573297347578,2.722466024471091,7.96087076379617,8.903429991567151,9.435816085556171,4.631686366311998,7.457709193357593,6.109569509370688,6.0413306068206225,7.586464525886388,5.687060688339892,2.6915341649192004,2.948600847493356,10.7317008703535,2.6016965164809576,5.20163386116965,8.56017995776418,4.4672794804599825,5.904002316283692,8.44795106244151,4.932155684432056,0.9411063109464314,3.4154882710497003,3.8257856274647914,6.542412837068272,2.5360529002402097,8.739409342229193,4.436961337833602,2.797012977836145,5.624685810525398,6.1705259991768475,5.926948247949772,7.881113960675097,9.500642736062806,7.073606040574941,7.318316841334983,7.971830914780502,6.137913322088407,4.721919445551544,1.922197848396367,8.226941966807852,6.570159301636119,5.3458930856874085,2.73768676140986,10.642557179595622,3.62993940943954,5.175524601089875,9.995102877826515,3.4646682670034443,1.8836208162856714,6.0083165637349865,2.408711861029429,2.950468414150123,4.443606651475615,6.456313341355182,4.8604662585171665,5.661065479806948,6.7997349224788675,4.824258696603374,3.269033146455237,5.9740703670240265,5.925049964727359,5.946496941203293,1.4750848829487828,3.6971065744769747,3.953265239014844,5.464341532761104,3.0908534304511135,3.7676547982373463,2.859969548221026,7.822411496360477,9.401604869714957,2.7092906357233577,2.257010618206024,4.178714641175443,6.423410093269615,6.9070108151861795,3.6484654430273142,8.622783966156124,4.569248029867182,5.366322214245816,5.570462931026041,5.130519083027338,2.773996325111173,2.18269229751619,5.724923070266613,6.9787676510367875,6.751410160064371,4.484138131201669,3.0686708106650986,5.111865963867557,2.0942360698457656,4.315421315945778,4.06177619758669,5.781884234541486,2.974529312483882,4.57349571549561,4.176322772640463,10.888134388352869,4.414812060565847,5.521364878446219,4.582556003014061,3.8369340113210915,3.929790997718597,4.598126959919604,3.61706334388818,2.4489009511451276,4.788163657037767,1.8953026213333068,2.7398481026993275,0.12432813500220177,3.400537929583729,7.515147882149842,5.381629467033664,2.1602748314085933,6.677226328235316,8.025084179448086,3.440952198029637,2.9763636357327616,3.4905701304462013,7.431622959713292,2.7803100990433753,5.659068274843228,8.878265537895738,6.403949364232865,3.452858964713811,8.162995472417697,2.1473066987802936,3.7398481026993275,5.212569338850806,2.8933622107638715,3.9155209007519587,8.562585882154105,4.814550423461808,4.5867647433551415,2.582556003014061,5.077670274232752,7.905507349949873,11.663771557482411,6.559338847606539,2.7949356628035362,8.834123370734224,6.0925457415435655,4.936402377725063,3.454175893185802,2.788685710613534,7.877437521970619,5.3377110921282895,8.303415462685624,7.285032249622911,2.6915341649192004,6.459267666703387,4.398487190018967,8.028624401408491,2.9392265777282085,5.342341397431548,7.217424613813984,7.563234413022304,2.5921580021253603,3.423578170981797,2.763411574470007,5.542258049766918,4.51853513898218,9.535217045453418,5.908092340818271,6.928844036712568,5.612352498752662,6.7786028451432125,6.4202128906107845,2.877744249949002,4.038260575175349,3.4222330006830477,3.777156666004501,4.766065051474954,5.59215800212536,4.534186139090097,6.1147834472498515,5.572889668420581,2.9726926540042644,1.944858445807539,6.412442824775613,3.7990873060740036,4.030336078370959,4.255500733148386,2.5459683691052923,3.7813597135246595,5.135042286233651,5.6061460780657235,7.027021314622255,5.524189078449365,3.145677455195635,7.669948004116052,2.4462562298895643,2.408711861029429,7.221587121264805,4.008092420948722,6.956289026583943,11.260837225974512,5.316869805064333,4.078951341394822,4.509062386361898,8.238500278879492,2.689299160535892,4.22881869049588,5.614415386508126,4.112700132749362,2.9523335663696857,3.693765712217783,7.474030370207135,2.521050736900963,5.727375938594897,2.3673710656485296,2.786596361890807,6.034963989749303,5.578334948681608,8.298429210890312,7.4944956156680504,10.089053172156433,7.896998342338911,8.325215494683038,7.3416300093299105,4.349789870241872,6.438459204423481,8.102815576392821,5.43229133824778,2.4905701304462013,2.873813198359087,6.096978636370538,4.032982416699428,5.604071323668861,2.5582676340557358,4.694323060125386,2.722466024471091,2.3757345385831563,7.995258373404689,7.1078970616646115,7.797726364623276,9.707290101958575,1.0976107966264221,3.1953475983222193,7.672354619892956,3.4208865749755315,6.180903709179382,5.908572758751745,3.372952097911829,7.12525847253797,4.314696525656286,8.915789767665157,5.459103696135399,7.07692245939234,5.79207446192887,4.488000770834068,5.685379551959075,6.442114122419225,5.125981653854716,3.1193561770396756,6.643134661857265,6.657640005207824,6.316507819103109,0,4.053111336459563,5.711219556894335,2.3161457422933562,4.726831217032493,5.8894735425311096,5.981167667105682,6.483976963236257,3.3840498067951597,4.672990993827748,5.239550796613645,6.48767927890162,2.733354340613827,7.754887502163468,5.065227622775619,6.546122758857796,4.556429415449573,3.33771109212829,4.72137265947682,5.533563348214512,7.015582311680932,3.3868109464722167,6.141391916170729,2.097610796626422,6.376255649313362,3.24031432933371,6.5787877956769725,6.052024560482831,6.4571345943180365,6.711081862306696,1.5109619192773793,2.0071955014042038,8.766462652425766,2.584962500721156,7.074676686294496,3.292781749227846,9.20006486151431,7.7525479058471385,4.009884588931844,2.169925001442312,8.76354436429392,6.257010618206024,4.3269687115878925,2.3950627995175777,2.3757345385831563,5.424922088210688,2.250961573533219,2.8011586560936985,3.4892860226258766,5.968551603363945,2.541019153133559,4.888012585402829,6.457791260256042,5.237257770900372,4.161081482277184,3.408711861029429,6.705839705883682,8.251198340973543,5.2671612180222604,7.824386002585445,3.1043366598147357,6.542103245856593,2.2234225499349374,5.316507819103109,3.363171077119244,4.631104282365877,3.9873208659292536,2.140778655782796,7.473705749619415,5.924575003392735,2.871843648509318,6.807870078299782,4.955591791753994,4.214901910160696,3.910732661902913,8.646486708605355,2.2539892662307865,3.816599706534912,6.460578760776026,7.281698250098829,4.901108243014512,2.867896463992655,5.9429835981871015,2.950468414150123,3.8429788317883253,6.045486656359465,4.990047219573794,6.705977901682522,7.860404179081675,2.565597175854225,0,9.146670494470888,6.7014104362328135,3.4019034716079584,4.462706750670158,4.311793717753649,6.649471757181635,2.788685710613534,7.693556651228518,2.8135246892978105,3.343407822297814,3.5360529002402097,3.2675357980687347,6.75127624861109,8.995371450572195,2.6870606883398924,7.605257262939004,5.275752048828529,6.982080909763738,5.305970520984387,2.017921907997262,7.062963556559086,7.2259304111710545,7.0414401300729725,7.778274295338579,2.4646682670034443,2.4620523187964327,3.2141248053528475,4.114367024952,6.111448698487674,2.189033824390017,6.304876050044889,6.0710332444568245,6.157650096861711,4.038260575175349,7.489045125114802,8.475936042446387,0.9411063109464314,2.01435529297707,8.154868729042667,5.955824240806441,2.996388746447621,6.1039172107520665,4.296457407371244,3.657640005207824,3.1259816538547156,8.325890061039784,3.085764553778314,5.309976492370556,3.145677455195635,2.594548549550354,3.631104282365877,5.121015400961366,4.371558862611963,8.492213725013189,2.1043366598147357,5.825022364044702,6.883987308105472,10.959639764364185,2.4620523187964327,5.522306892871389,0,6.593054921669878,3.9354597478052895,8.913068913647477,5.021035479511363,1.7484612330040354,5.982537314399271,7.690137692986635,5.791814071161826,3.578938713093386,4.896271848807375,6.766065051474954,7.801029283808189,6.976821852360685,5.795195490817005,6.883132015657282,4.464668267003444,4.808385050656093,7.161484638641659,2.3645724322958563,5.707082991771706,2.8439838440483265,2,7.723148957047578,1.1634987322828796,7.126807703142035,5.348728154231077,4.4924941516794314,6.174526265266431,5.563768278452033,5.6570683012015754,3.37851162325373,6.43895814789755,3.127633279725874,3.1953475983222193,6.676803353873854,2.6667565918848033,2.3589588258323295,5.587364990936461,2.454175893185802,3.2265085298086795,6.230164577884252,7.884597920990064,3.7687136570304847,3.127633279725874,6.450551443306483,4.084914659767938,4.135863165368679,2.319039815562536,3.097610796626422,5.624978179569036,3.968090752045256,5.105594275795191,5.185470146066942,5.509379148914689,5.266411766036114,3.1342209397606338,4.707082991771706,6.390598905552461,2.8698714061777126,1.4168397419128291,2.3645724322958563,3.5084286525318573,3.9354597478052895,7.11009175192513,1.4222330006830475,6.929672661579345,7.041001987167506,6.332349900373874,3.423578170981797,5.137503523749935,5.3139713710590195,5.999549087340949,3.2433644256936605,4.7114949066500875,3.7059779016825223,6.291677223176022,0.3895668117627256,5.758089934201813,7.001577085389535,7.931505781869729,2.8257856274647914,9.151270288790165,2.440952198029637,5.74604398299834,7.136683577697236,6.353146825498083,10.212070748654236,6.007419783571914,4.997744026059632,2.4982508675278257,8.25176642434399,2.8579809951275723,3.300123724569014,4.640967910449898,2.625270489374693,5.399854673572431,2.2660368939953175,1.516015147003665,6.199672344836364,5.708187236020708,5.402585758232587,5.269033146455237,4.529820946528695,5.212958362728285,5.648753032989572,2.9467308601403097,2.169925001442312,6.088735246190284,2.4356285940520905,8.627898615871748,5.300489844363109,8.091065077930535,7.119252412157343,10.142962207064079,1.7398481026993275,3.729008870337863,4.45022149589718,6.461724991502711,7.904544309292717,5.490890978894291,6.76593249347046,4.502075956045791,2.782408564927373,4.205548911173033,4.139960569545456,6.708325207143167,4.632268215499513,5.6194130105979365,5.541638734436141,3.581351247168777,3.496973580998276,4.862451391325582,4.784503982929566,5.359310316890443,3.41006969175638,2.5897634869849773,2.769771739249448,10.641220865859923,8.202368742775262],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données d'émissions prédites par le modèle de régression linéaire<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_pred_logLR"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle régression linéaire\n","pipeLR = make_pipeline(scaler, LinearRegression())\n","\n","pipeLR.fit(BEBM_train, TotalGHGEmissions_train_log)\n","\n","TotalGHGEmissions_pred_logLR = pipeLR.predict(BEBM_test)\n","\n","LRr2_log = metrics.r2_score(TotalGHGEmissions_test_log,\n","                            TotalGHGEmissions_pred_logLR)\n","print(\"r2 :\", LRr2)\n","LRrmse_log = metrics.mean_squared_error(TotalGHGEmissions_test_log,\n","                                        TotalGHGEmissions_pred_logLR,\n","                                        squared=False)\n","print(\"rmse :\", LRrmse)\n","\n","fig = px.scatter(\n","    x=TotalGHGEmissions_pred_logLR.squeeze(),\n","    y=TotalGHGEmissions_test_log.squeeze(),\n","    labels={\n","        'x': f'{TotalGHGEmissions_pred_logLR=}'.partition('=')[0],\n","        'y': f'{TotalGHGEmissions_test_log=}'.partition('=')[0]\n","    },\n","    title=\n","    \"Visualisation des données d'émissions prédites par le modèle de régression linéaire<br>vs les données test\"\n",")\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.2.2 Modèle Ridge"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre     Ridge()\n","0  ridge__alpha  572.476624\n","       Ridge()\n","R²   -0.425062\n","RMSE  2.512048\n","MAE   1.550505\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_pred_logRidge=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4.741963969685656,4.9130695966381355,5.339630548540099,4.720583318742041,4.739735327986697,4.748126172504533,5.94609714635984,4.815531406170852,4.918825779434528,5.211828876379649,5.347934095013472,4.887657643127656,4.808540050749218,4.978906408495461,6.0186012855381525,10.274844614724296,5.233192970816317,4.6828397385405145,5.284273592337782,5.249245139161847,4.669143573509371,4.637653711032849,5.295626956700868,4.893138138797183,4.787692148405324,4.604829386814611,4.724163745679924,5.006284560016168,5.4642342769671295,4.6441479608762535,4.940488750012413,5.011277345873923,5.006501194582304,5.112572027299563,4.949161972608301,4.66767768962517,4.903361074462593,4.778359136469071,4.764666087722576,4.7551961065814075,4.656088804746938,4.608635640784091,4.732377722074288,4.811165921338432,4.770047887384003,5.342735488709284,4.760916550936716,4.6881175193563,4.903853293506804,4.744711419645898,5.798100669104166,4.763635213693011,4.678447894920209,4.692691481531149,5.545878079868045,4.977806044767984,5.176909205474808,4.689487878379119,4.651208113813242,5.751010805439623,4.8241997102467025,4.798608284281678,5.083390482222088,4.630770843732353,5.03198669569577,4.693280539260652,5.072398830028805,5.162222410498062,4.620169524840539,4.745673077670794,4.786261042481926,4.973521817865272,4.690448182225665,4.650798770026443,4.727636676983455,4.772428803143101,4.6316649062205935,4.726427404843143,4.677117021422978,14.175278406962896,5.61966600821042,4.81052480002007,4.6348246974599006,4.648553765605847,5.730676680332577,4.726225615088506,4.672964745768039,4.709298176081446,4.723194027692616,4.628005807216411,5.600520608453474,4.704959279634534,4.674188622448985,4.868866061344276,4.624875048861756,12.709520811515125,4.593924743447179,4.671976953370323,5.159775190347003,4.793844117330134,4.657873651862127,4.665051005492444,4.668203195403199,4.657004898705535,4.940353142061202,5.192946849735176,5.624856405441192,4.6601450207038,4.776495866714605,4.643871109980269,5.683160220139994,4.702552646119755,4.7782204332683245,5.296924374793782,4.799985651139901,4.668862774021269,4.8522985768506866,10.00861031141853,4.687167275807427,4.743162098287954,5.049435695470286,4.682647913171629,4.696914872568133,4.654437844215355,5.46305150678354,4.8665885543305,4.740954910217166,4.694661330814352,4.6693149307188575,4.652624943860408,4.902102865428967,4.88110143598584,4.641520048875125,4.680176771365892,4.640747616949007,4.7756229659410865,4.801482089434301,4.654788495370884,5.815534214353623,4.999332935676862,5.143514016862856,4.817249778545197,4.683298958744003,6.823070621522289,4.652777703325992,4.878532964527561,5.123996446247853,5.279391349653193,4.686241795850207,4.818183783352606,4.921708780623672,5.030223507233831,4.624070988715393,4.662135014672599,4.771233666076818,4.838108765607519,4.925338969380472,4.722440726535173,4.843612177544609,4.673492009943535,5.10562625751947,5.180912033251126,4.902925475296711,4.671467328693623,4.811470812575041,4.668133758390968,4.9047567952409405,4.666733094824893,4.682162023373991,5.619144854966967,4.643588848339887,4.909296899528648,5.005223006633953,4.635698321779284,8.448986479345772,5.253686339902229,4.900124211719962,4.849956981609333,5.277498773257565,13.83036312420754,11.335749322295122,6.096368678722016,4.928317879508794,4.904383202841559,4.793713386176042,4.704278049848835,5.042691308476354,4.82788160151658,4.6693668103123525,4.630731100459069,4.77193189943199,4.752899320414227,4.939290500165585,4.60760071442641,4.699908299181632,4.869157524770389,4.933232882458746,4.641784346415867,7.272851122551026,4.721776127917645,4.644764217409448,4.703533523961928,4.836366652625077,4.645060488508474,4.860819491155158,4.754167460757959,6.611794517357438,13.731706268465242,5.400876686402607,4.659900573275763,4.795222014334584,5.2432474018403665,6.086839326505002,4.704939964845193,5.304542790046433,4.732427784673773,4.883823871829101,5.057044079418822,4.727386362401435,4.778458452130063,4.694487049247707,4.6243052178056425,4.7684058645134995,5.204850759448863,4.644676108157017,4.810386793119155,4.748002823446492,5.35836316828531,4.996992670655351,4.595656304229348,4.6430900978974154,4.689174879730554,4.848675791556897,4.669923607795183,5.855802043511915,4.710496316338111,4.638290693895752,5.061678266960312,5.336478087615168,4.640869003302751,5.04626400543132,4.843466083819688,4.7784238264407115,5.269101386610049,4.65598351425938,5.040787449027776,4.682647162326148,4.6879032485067045,4.669506599269342,5.722268443461266,4.762202983769093,5.5694685753904505,4.898423651322139,4.6891895925078915,4.617731697240937,4.740316331751363,4.885746905794293,17.273494342751974,4.687960543647292,4.665063472599015,6.469683606403375,5.077726643901616,4.866866596824083,5.113372742020677,4.953606509794351,4.648484945485308,5.146434719558163,4.737992061942719,4.765117101311966,5.065238611880785,5.836649535626925,4.716035380528987,5.046652030890029,4.827868241797123,6.942290519343853,5.3495550193563846,5.027184756969413,4.865748255887163,4.838222589140774,4.721402519372479,5.081686426786413,4.727645306516209,4.7689431512719125,4.676388435359245,4.896973355590259,4.819613126060693,4.854472683499554,5.177503582296167,4.643923969459476,4.789974651152291,4.777593964671479,4.688874796128207,4.639879927363182,4.637461134818483,4.633436601070895,4.780638452905924,4.652408011764293,4.599215894737987,4.73299700256341,5.014765367706955,4.86463168490225,5.067168043438622,5.311323430031018,7.058713148782865,5.031371391515979,4.648614385223651,4.612957207394061,4.620265718907987,5.03848217787661,4.894647378031694,5.17573674895909,4.774137273307965,4.754072600835076,5.049210926043201,4.645281817935543,5.750930023451463,5.822526026001912,4.657004898705535,4.755049841980555,4.595548564509458,6.003168844517059,4.6834393120932996,5.185768462181137,7.512577230712228,4.611160304935049,4.70139771063948,4.71023740842285,4.6715595918678785,5.7493164037954525,4.794084883332889,4.8779369336252625,5.397564711285189,4.695401633139174,4.685141993885114,4.86077399682224,4.832627501137998,4.90235295994041,4.690054690647254,4.725477620799683,4.8725702486651565,4.852687274705188,4.652810664261688,5.131196522680307,5.267149252587673,5.0655999521941135,5.594744739825305,4.717253597717907,4.9607719674591015,4.764308016830664,5.243257240987752,4.643875404112373,4.736563817442343,4.8066823460340835,5.2954438742385666,4.733752908489355,5.181858528431425,5.029910769204937,4.965986333647977,5.1928564434481865,4.926718390996799,4.643365087312037,4.734436946038616,4.648089485073263,5.782170800408148,4.6407239517993055,4.644798529425705,5.77950967507755,4.792802013340058,4.781899707133073,5.028543965781712,6.971770551439961,4.629673401503199,4.682358369924802,5.242236062507231,8.656326156231945,4.665455656382482,6.0734930262663225,7.397444286731906,4.675020527974635,4.698018402858377,4.642058772605205,7.408702703761423,5.879869377146143,4.653717997403518,5.50956675101867,4.744372194870834,4.8195800627145315,5.002820957509893,4.952562526309026,4.7024006624308985,4.715441952204373,5.809757912620858,4.703452424832437,4.653822706262138,4.7005253646122584,4.610807751618082,5.0220347086396515,4.818626756231145,5.535317731768457,7.3086753368024375,4.81718527352433,5.206400068714025,5.207602311475025,5.433576950475575,4.712640677041556,4.633439254649218,5.100627617102502,4.624847768200234,5.209744736288136,4.7659613172430975,5.704504540322068,4.908501277044894,4.879588193273893,4.698661392137876,5.660716669595753,4.640164984019826,4.67839801850352,4.835576117045532,4.699813933327682,4.694336681925365,13.707143331024435,5.388933135188163,5.304979227345766,4.84261996944952,5.513236856977459,6.798586912735246,5.583393446525189,4.776418868567312,4.776632281418244,4.982165403970489,4.790385772063846,4.79811579600531,4.7645879661162445,4.649064833251665,4.674755759968049,5.458492026998805,4.7590244401742225,4.667338493236023,5.283592003443811,4.7086954387427316,5.608899101792614,4.828207875148248,4.653422364855713,4.608677003685248,5.841700750242662,4.685310523907286,4.715060018302132,4.699559798401956,4.675731578612295,5.723633346374764,7.0799195967078985,4.770047887384003,5.549321976949225,4.636337111229962,4.6703027861454025,4.659575050489576,4.7135122216171945,5.039521134960868,4.768528371733277,4.598331357314915,4.657726077991792,4.72605322478169,6.8707480201489375,4.826130504738471,4.690377232986425,4.6188264335352915,4.652427510225599,4.648509324940993,4.6668216007318595,6.94624999581463,4.799557458589013,4.864242987047748,4.970096053316841,6.072118341712639,4.702260313491559,4.921463513049979,7.283636762078258,4.879178255145276,4.794555787782623,4.666207213624123,4.779406858022922,4.751759427528462,4.826490342387017,4.6771661627730445,5.581978544678131,4.655868379656538,5.865779107312417,5.515598028714595,5.153639112594095,4.721852423265499,4.872969787532496,4.716325721984282,4.900930391694437,4.6765977032580395,5.775780542687898,4.800580219741427,5.546870088246749,4.847033924800698,4.783493062357788,4.672040901846174,4.922776376311834,5.758470228363283,4.742223050560897,5.202084837952804,4.684351940413991,5.604150149260902,4.680832126421879,4.926187535388464,4.739015085364033,4.650097097191899,4.758284423061747,5.018445604779365,5.104881498490078,5.072611757590099,5.03552751075186,4.716659875961046,4.995220356042726,4.7147724173805035,5.424869013091568,4.859298429697059,4.758887844753875,4.64928272709232,6.074894916307941,4.863488862438571,4.733156824179941,4.731631620114648,4.754884461476078,5.616374935857975,5.036044483484191,5.1941795025078505,4.666066109429345,4.711567749286873,5.226417583096584,4.791320625241617,5.502412978082389,4.960563108347155,7.291997476624957,6.727136981303641,5.236398528946945,4.754285514070391,5.1835362637171425,5.2061589451487915,8.556503007703489,11.504768856527019,4.73379683346256,6.573305695890582,4.7855966843309785,4.950675467991102,4.620706711786331,5.046515798268651,4.744238968635646,5.020042285498709,5.072399523536819,4.725056769893138,5.3080489018132315,4.7267299579446345,4.714704119685293,6.377139341287567,5.299164400262164,4.943826291097796,4.847551031444411,4.7915427210329575,4.736519376777489,4.7339357352462095,5.14124491499613,4.727220117723864,4.875444372478726,4.697863405132202,4.645592895901021,4.651780411199313,4.8317217508067,4.708381958613201,5.013781962928907,4.714660923386894,4.666243938260241,5.4005708561560875,4.786547457412095,4.631362100276603,4.9432534390272425,4.7169393492256715,4.702696933529924,4.838149346371168,4.8918616698824735,4.661267707370438,4.728883759422732,4.696296350611052,4.831524472911593,5.05344274250183,6.219435368289351,5.0563421997498335,4.688426795205516,4.83960229199368,6.041716132551341,4.795241542059999,4.7054748505060235,4.922600233397357,4.926473247259015,4.77104667607499,4.656918363824208,4.743930839159468,5.033619659457011,4.706833059161078,4.857521929371131,4.847951709426373,4.70709569586574,4.65822308292814,4.728630783343207,4.624757024989048,5.690943216014593,8.889786208930609,4.7404232444144245,4.712177481437358,5.020910380154319,4.790164843465386,6.029161147239139,5.001863565472254,4.651212798719596,4.992398596356778,4.878116403670334,4.695151285668678,4.651769659714644,8.228219016361342,4.688454075867038,4.888709167091083,4.797414053042733,4.738117269686584,4.674240469891314,4.633884294212876,4.686998413420943,4.6568489809558065,4.929264820757451,4.688865489945071,5.144556912455408,5.07403105388997,5.238722416058966,4.954584663711124,5.6957623399520205,5.84374580196566,4.634246513752681,5.324533677418106,4.6331144071407495,4.701166040786513,5.900398416676326,4.720272065082417,4.798354908661156,4.619417690079541,4.78867865460632,4.941276642797274,4.7884402769116665,6.790903330217226,5.513821292508554,4.965914167397676,4.789317773649815,4.715079508090509,5.561418203382814,4.7478915532518,4.589733713448157,4.886689894644454,4.7812911054303795,4.695497545823616,5.014937108300169,4.854631033619193,4.658338118651147,4.76505496851995,4.786892435964075,5.367840538153037,5.286009535999799,5.040161486294414,4.74585660130313,4.660095453464861,5.664281068457434,4.738330939965155,5.801445598062374,5.509801381311076,4.642001864833781,7.296730926605376,6.575528783419835,4.907315817226054,4.700061929553361,4.65126248848824,4.684283702070228,4.636773239896546,5.073622677157373,5.289959225508011,4.826093846273833,4.628459929159958,4.56548225990701,4.6527055865928775,4.718688541522538,4.6947124274112,5.811885387972349,4.834154766615693,4.691508014635376,4.6556618660096385,5.4134998607128395,4.827064509545881,5.070954053226832,4.822664132879878,4.839953082091648,4.63245415324496,4.818708552874083,4.652324566186421,6.671533675822508,75.92641838391378,4.833881091271836,5.411829215823719,4.706744131325477,4.719166030692635,4.666634084846588,4.6615475997464735,5.385167914205332,8.147950669711303,5.540174938832903,5.065387152698891,5.120211605485633,4.7502948566139285,5.025317917607172,4.664140347285683,4.718984812621564,4.675788110960979,4.703899928774837,4.683264271305005,4.974888974404401,4.6390777524712075,4.722406743815302,4.713830219196514,5.116841157203676,4.750836302215131,4.643260641581897,4.856528907730583,4.675155179144387,5.932814019960839,6.71099704556766,5.142279047766011,4.989391444701674,4.817494953047373,4.75988110378109,5.9682062192758565,4.68812817765811,4.9684220612169865,5.352486114689013,5.134352519402508,4.718030893768924,4.8092491662673575,6.024429436612374,4.66422688216701,4.6515751956412155,4.6255619495987865,4.688329723072665,6.586554251907162,5.477796697892755,4.772884843629474,4.66126750174577,4.801610112193778,4.904653204098598,4.612197642841654,5.2691911787031955,4.6751963196084105,4.748525387642736,4.651822500920978,4.771656378276371,5.919901293854775,5.298469597105602,8.715533390456606,4.783847934001249,5.156689832252218,4.896556866847833,4.615267563269715,4.89844284108183,4.925263189894974,4.73007088138887,4.845721806584239,7.5054233057427195,4.686750141624904,5.060427099684453,5.021499426937681,4.635150971091568,5.385804037780242,5.695104630337056,5.46222455434513,4.631514243351037,4.774805384971734,5.014263533709553,5.445263734954234,4.781627397039104,8.002399871797058,4.817702895042691,4.799081328820229,4.946728029580948,5.214085294532602,4.664352421961355,6.186024283812279,7.103598990471286,4.640680533663347,4.604885019891755,6.493583422334663,5.495496549611118,4.703477537667928,4.724032300799067,5.8630917401865394,4.835119299430841,4.6970003715926865,4.672573471898942,6.1800403465091005,4.742375415889587,4.996361698669397,6.744219766892616,4.70442321107668,4.639214570054565,4.5995722397665,4.739621059420385,4.717446549355014,4.671410705472884,4.83453054124131,4.751430737821631,4.6317064480969945,5.155381755518509,5.718679270868927,4.725769045366495,5.167198920204264,4.9734307820264,4.641552407789003,4.724121891634419,4.927466508754582,4.870055041617307,4.880128258984156,4.773022217275097,4.750463944022712,4.71342504460849,4.726883534704442,6.9829808488722644,4.727633765857606,4.711809487366977,4.682247196342464,4.992355857134497,4.723501292253399,4.963102156126688,5.825665802297175,4.693751326960278,5.172460217752913,5.56018649835759,4.661881280155617,4.743481747604576,4.655277905392348,4.5965960923413975,4.988642609331155,5.499180502424895,5.223351671272616,4.8098587414554865,5.093630836983474,4.6665034847231475,5.373179529267164,4.755248395142223,4.686834443881381,4.659518518140893,5.323700380342313,4.7828576933818345,6.541078150647704,4.611994662831644,4.69061685488992,5.190883970904833,4.677277624079061,4.765419496219154,4.690491636798303,4.638303161002323,4.664140347285683,4.691140011759668,4.783017117576091,4.819179881527556,4.802395821828818,4.829851804325096,6.638883134971701,4.611885019488212,4.672747022712359,4.880199922271502,6.505527832531725,4.906249241269563,4.7833419266845265,4.8632686149358,5.221725944989223,4.732687350951469,4.735139670135708,5.414098591435848,5.438348660031438,4.826757123134038,6.306665787280814,4.647213138882758,4.760971850874193,4.689453300442418,4.737198453856865,4.9282489140126495,6.28380334905847,4.734931229103843,4.806423646990067,4.82205333071529,4.640992196648716,5.1379874112428965,19.272173932611565,5.164212930185426,4.826396723677221,7.137121308149791,5.142786775925989,4.623829429685274,4.962855682877632,4.746115463092036,7.275499655640374,5.167609468300441,6.060417271086197,5.260918434966969,4.784124730867361,5.589729642969812,4.644487444771729,5.107982203440663,4.790277966639977,4.658981909057307,6.567226121533164,6.890594309857853,4.675953781936564,4.78676465903477,4.783600788473433,5.744795417155537,4.98886942669668,7.115543400668347,4.714896271455019,4.836873429371988,4.837840244504693,4.788606607325588,5.503370611980884,4.8061766462191375,4.688359592212387,4.6008325267740044,5.029494033584703,4.688054109883354,5.210987190306482,4.983585361425542,5.041041625910327,4.932554879379763,4.804925276248498,4.9901992769819605,5.213582058575754,4.665767116459199,4.773979759955337,4.667214677100052,4.776318099611301,4.6266331769946,5.29676347202615,5.204376258225547,4.769709095259632,4.76915288748961,4.861276414705792,5.351696686792984,4.690745496948671,4.716457975234153,4.657794786305294,5.055223633241561,6.345763067250603,5.863621941285911,4.665899420481047,5.151900604069343,5.031449473656653,6.265562491135466,4.678623750223855,4.711812845250385,6.1997345475964245,5.003097143485829,4.764566102083059,4.7216730974224745,4.8197692091551305,4.69710866467517,5.2283280459568395,4.690495485835785,4.646822694578897,4.739720889748063,5.732851006958475,6.2126530129755,5.30496482546109,6.8323144397050015,5.3985219555535,5.644490072546194,4.819223829493056,5.016855775581291,4.786704054259528,4.855992105366823,4.760969149224526,4.625212386747421,4.679078077538439,4.629570003464107,4.649645283765885,4.654745831575469,4.713120991309249,4.975293053052916,4.713714089687632,4.920011227794817,4.892093266543743,5.058878468901396,5.305940423341224,18.41903827154358,4.66842690041843,4.738324175600217,5.580134390202456,4.769513636469212,5.089086537139146,5.039277527242332,4.869097596297311,4.778729475342852,5.20569273190549,5.065954594461305,4.986768852840141,5.754989143202449,4.709166522982747,5.066696976521656,5.711386584269677,4.671250853662283,4.689091058729941,4.810685146024964,5.6077215980133595,4.959203304509607,4.793198102203298,4.746715970242835,5.104228623768619,4.765248671002538,4.69907991460152,4.978720159861624,5.273178983371578,4.680603267165616,4.818045776451691,4.948507403673171,4.631247155297076,4.752273967890884,6.183630643545318,4.684186851318001,5.112313948264402,4.810670164443947,4.754147266686428,4.744333527014273,4.694483192732196,4.735915906784001,4.997955275066396,4.732825497488452,4.63601242201957,6.247701532649564,4.629719471818221,4.775256827469815,4.828275429966886,7.377577295375689,5.271655719234773,4.730286963145256,4.830977634501391,4.6427104209301575,4.657271747667798,5.73412594769629,4.721414178062042,5.565871492345965,4.777815853013047,5.265973213095556,4.764054372702395,4.659271372613076,4.675373706845084,6.235368149345945,4.746799983913656,5.239720274071006,4.704513565037091,4.750169692242324,4.664916087894984,4.627133342274146,4.6547258431601355,4.9631804319211135,4.991456520239823,4.723886007630712,4.618196482201232,4.835355865120749,4.641875700573456,4.7494703578657536,4.767008971314108,5.36756966907474,4.8588334092719645,4.967154630880268,6.13460532971842,4.688818147915335,4.79426261397746,4.650992941843708,4.846750222385128,4.823042253953876,4.687283805025889,5.115268501631696,4.670021148669039,4.757189721686317,4.793289604257032,4.975111177728669,4.888614079522841,5.128459238539766,4.831953098488134,4.978413849637317,5.63423154491722,4.657797048472686,4.638577587191662,5.1012056917700335,4.8356921724975654,5.7874805563692,4.628282166500544,4.761806490502012,4.756998741074851,4.649056024375962,5.220979156032035,4.912313630137803,4.66617697670846,4.955224096822858,4.700249422723742,4.68692440879834,5.382874055975322,4.737603718585011,4.7910971981242465,4.9157154783061525,4.781011889073567,5.5854562181765575,4.822829129768065,5.533542377492329,4.705318917507604,4.808554020519565,4.894231889565968,4.836597213310814,5.21327057309441,8.647072973915225,5.360126279261482,5.598159516731876,5.060958434746346,4.900050143945206,4.653955010283985,4.657921510811335,5.876998848281731,4.863934544195773,7.483239264629679,5.944758430157901,4.669684550898324,4.6738650706884375,4.841911604782593,4.733912191106069,4.648502669008999,4.6552050517789585,5.329494081550297,5.246084278942578,4.991689424830349,4.726172859489522,5.086518302633843,6.442321973032645,4.721120640087051,4.683883970714963,4.959365589559422,4.662347907136581,4.758103477186941,5.899044294053553,4.639396522306099,4.727297187974854,4.815304142517488,5.341260144965141,4.7229478139937635,4.644039104062318,4.875636975689526,4.636609355408785,4.775045667171794,4.760085771679287,4.742811289492426,6.3813841695832245,4.648642646935555,4.848496537080221,4.855282995895851,6.7496712335975095,4.6904562522338376,6.286412082163187,4.844694395339304,4.93553852762376,4.777363906416216,14.543393331950664,4.764314114753319,4.6904765833907565,4.648717650093607,5.872340610031853,5.058079382072876,4.956528813272757,4.881788878829659,5.30396042479566,5.187169567798997,5.117707003708506,4.915797703703866,4.7748644866452725,5.3476465999016725,4.847587522278688,7.837605404302539,4.653975526718004,4.859446952247148,4.793639585801462,4.833997740912352,4.775998782899457,4.734727687473868,5.433627680925157,5.412821096599669,5.632629606562648,4.6473686812097466,5.240932288737973,4.621192153461981,4.900657605339073,5.071446072025027,4.696192331297733,4.686293643292536,4.910107046066142,4.639377143736288,4.673366113710225,4.631755804231238,4.659522918496561,4.892575084604429,6.100282294064538,6.481628016600437,4.683615415956216,4.729340440769439,4.832272224956727,4.663152524010304,4.7076245923758915,4.698200644179479,4.723828469667882,4.781097297686672,5.097845131422143,4.763384134104321,4.743973132033499,4.639925313243589,4.8980666292727015,4.8573664297714245,4.785285224254261,4.991142594148153,4.633972918050204,4.846708034256719,4.692404985555408,4.786749795168795,4.692031521689219,5.642819329437577,4.820701712271578,4.84240834909345,4.900507862457732,4.72660601122124,4.908849395586248,4.748340593628586,4.77156573136335,4.870688745795227,4.683194874021782,5.036754990451713,4.853107114572918,4.637120423054603,4.808963006351253,4.779601879482077,5.046934058704334,4.645350103675504,4.736741580101757,9.174744023255887,4.718479119200039,4.841312816095662,8.864534958827285,5.033763677606389,6.340526098191885,4.871876573845888,5.0655869778986515,4.651630056022973,4.820053755109981,4.713047298957234,4.692589943013971,4.899765392369936,4.602734646235109,4.870737946137153,4.634476578918546,4.705111359051012,4.752633629866448,5.039778625600127,4.689323015280067,4.952205112290424,4.804889715912046,6.215234221220172,5.144651680309763,4.642844085173823,4.6982490532733046,5.0583346891156475,4.677429315009373,6.293763212226741,5.239089102382678,4.8081093449129195,4.8218096815454,10.50185878238807,4.661775614245319,4.687454257845921,5.025296845292223,4.757818650410275,6.922906534364665,4.630881945394488,5.296588291348695,4.689898022052044,4.692200074965412,4.612069796403054,4.8890347924765925,5.663522162520626,4.872877856684872,4.674751162114391,4.838034633660493,4.816778234786921,5.6498794852521685,5.655125489433431,4.645483231696207,5.0989729783602336,4.870377203286887,4.689799285262706,4.732124146062703,8.544558597506427,5.129820948850104],"xaxis":"x","y":[6.235152624217932,4.478324711485028,6.139142019145689,4.729552770453971,2.689299160535892,2.4698859762744636,7.592232766722112,6.075532631167357,3.9020735793107426,6.768713657030485,8.05642095591347,6.192983169973992,4.934044647112246,4.088311235888661,7.071462362556624,8.345405246717796,8.140829770773001,5.7554217347342425,5.091276694365222,8.047178455318921,2.475084882948783,5.260778431893426,7.873013392989595,4.4495613746132365,3.4369613378336026,5.575917361118149,2.821710215034674,5.650190123496775,8.870210577263595,2.2868811477881614,5.54936113292434,3.682573297347578,10.919861451442168,7.606886339275915,3.4276061727818994,2.1009776477248208,3.548436624696042,6.819540460505722,4.475084882948782,6.762348815644129,3.929790997718597,4.173127433480656,2.7676547982373463,4.804260115634738,1.9335726382610239,4.846493315964563,3.10936055940423,5.617651119427331,5.739578112048051,5.089159131911238,5.6806056662025926,2.8579809951275723,2.1176950426697547,5.465648025982281,6.027463723408577,6.299391206126829,6.422064766172812,2.4356285940520905,4.153805336079036,8.148324042480084,6.387500406480984,5.944155663440363,6.496494306853713,5.703211467391146,3.192194165283345,5.185866545311334,4.718087583960517,6.035403969942067,1.3785116232537298,6.335211681705785,7.330737650206952,5.452858964713811,2.726831217032493,3.8369340113210915,2.4672794804599825,2.9616233282869446,2.0531113364595623,6.248496885959384,2.817623257511431,8.971342367308225,6.606294160697713,5.901832305785489,5.339137384919585,2.582556003014061,4.779259720372376,6.044831233447812,2.7949356628035362,2.8439838440483265,6.690277401023435,4.017921907997263,6.596487956856693,2.84197311892718,2.3074285251922473,5.976821852360685,5.10936055940423,10.16679075071818,5.2787282129389395,6.339493737901733,7.125981653854716,5.148934104526339,2.321928094887362,2.1210154009613658,2.430285272977781,2.5310694927259543,6.72805655144938,6.989025440631378,5.844988156682612,4.749534267669262,9.159820892013935,4.272769732436511,8.601733652922286,4.124328135002202,3.270528942380718,3.894332742277694,4.943921326553485,1.0565835283663676,3.7322691995014496,10.679004987548526,6.500961352460057,5.787902559391432,3.7782085763980877,4.6661884898727815,7.593353770980297,3.077242998932461,7.602439063741618,5.537296067090842,4.203984165855989,5.299757511839318,5.94251450533924,6.951051533288609,3.542258049766918,3.507160349117523,4.71259578044703,4.6194130105979365,2.550900664647523,6.872951851130105,4.712045448553693,2.867896463992655,8.377687734026882,3.678071905112638,4.0232553523003025,3.1538053360790355,4.635173946674924,7.127117349726852,2.4854268271702415,5.592756010441026,8.25559514727367,4.634593268445757,2.5582676340557358,4.990501111363251,3.727920454563199,3.8308637567517576,5.924337464075348,4.187451054027326,3.940166750482817,4.926473911256073,3.5434958834257717,4.708187236020708,5.568640195419223,4.831370587967541,6.12969515665378,5.751410160064371,6.279471295644468,2.523561956057013,4.552131108253784,2.6016965164809576,3.270528942380718,2.805292455600712,5.533874777258546,5.741197298571427,5.787641414483327,3.565597175854225,4.40735275114004,2.137503523749935,11.52102619171175,7.417936880550136,4.96624587322494,6.385085853547684,7.846744023244868,9.156361220297567,11.010164414182617,7.196430000954568,4.008988783227255,3.2464080872463845,5.763145958149476,7.976535484023556,4.4121040446775694,6.918624755574591,1.9745293124838819,4.546585829008101,0.22650852980867975,4.1715271060388135,6.920055054806967,1.3504972470841332,2.3589588258323295,5.726013748860151,3.2357270598380583,2.6848187375532224,9.236301266827088,2.4854268271702415,5.9618548076361595,4.088311235888661,2.440952198029637,5.456806149230474,6.205157883920094,5.588564737401351,6.620439787651722,8.334139178838237,5.331633567171029,2.211012193485512,3.8063240573900288,8.45889870671479,6.602884408718419,3.8439838440483265,6.132988042627442,2.541019153133559,5.953730849811491,4.18824265626441,1.4489009511451278,2.7070829917717063,3.0107798387532423,3.4019034716079584,8.574290766120185,3.542258049766918,4.313245851787562,3.3854310371935203,6.156032498888866,5.419538891513785,7.3107945270667845,1.3950627995175775,5.8346606579017255,6.025250321561575,6.2268938135713885,5.548128323585586,6.746447139362815,4.583158003874408,5.9576824861651225,5.778208576398088,7.834028534577023,1.570462931026041,7.0083165637349865,10.307269127730322,4.95093492831454,5.099295204337775,4.547203024756931,7.9684364043347875,2.8318772411916733,2.4436066514756147,2.4724877714627436,2.37851162325373,2.6507645591169022,8.202417721575182,4.963011647599427,9.01321099459576,2.176322772640463,2.73768676140986,5.5786368624717415,13.52966114241807,5.72328550477764,5.642412772905056,7.568108130120591,6.034743949324734,5.320845667645722,3.8469946869655733,3.8639384504239715,5.154210530588339,5.777945670695148,3.327687364176047,4.169123281476757,8.135247549809566,6.890446692679906,2.2479275134435857,4.9804823555382605,3.3015876466031866,8.873136468833458,5.8346606579017255,4.189033824390017,8.095608004811197,5.380244590478185,4.571676809970931,5.071247819461442,2.9373443921502322,2.944858445807539,6.706254253573203,6.849624029924543,7.541716163393138,7.907912142840534,6.091065077930535,4.388878338811989,4.078951341394822,2.698218478224414,2.5058909297299574,4.621758856709732,5.3175935046234715,1.996388746447621,2.7070829917717063,5.974529312483882,2.883620816285671,4.853995647176393,4.593951283948411,6.496494306853713,3.9845893503624565,6.212180210044431,6.897240425574799,5.452858964713811,2.232660756790275,3.773996325111173,6.075318692553354,4.161081482277184,3.2094533656289497,4.922673592849446,4.478971805032942,3.0840642647884744,6.054414387917307,3.486714373030702,7.582029046119882,7.461888664432819,5.6604951318885135,2.7311832415722,2.049630767724601,7.850749425896496,2.37851162325373,5.986638471730853,7.210817432089213,2.769771739249448,4.970393537914677,2.889473542531109,7.08533966935737,7.463197379833495,6.09950561709114,4.17951105027151,8.27891401949994,2.4594316186372973,6.221297103163338,4.867402305727543,2.980025300238734,3.4276061727818994,5.933572638261024,2.565597175854225,7.056366761024529,6.199672344836364,4.985955754607509,6.215484464192794,6.7410624357490905,4.59215800212536,5.19377174339668,4.099295204337775,6.714108111595169,2.1634987322828794,8.524737587012376,2.18269229751619,2.765534746362977,4.67581593117227,6.658497136656478,6.557808298936688,4.208673319629471,4.993674361750585,5.422905742612183,5.7824085649273735,5.3847405872923835,5.08278773170927,2.4276061727818994,2.411426245726465,7.603403804166527,5.174326515173915,5.896998342338911,6.425089989874059,0,5.106432077816929,2.9467308601403097,8.843732656611305,3.7311832415722,7.403949364232865,6.247927513443585,8.640931770341906,2.6915341649192004,5.336640446408735,7.390942772802543,2.1984941536390834,3.0125686735030555,2.0214797274104517,8.044284820139424,7.1855692560893685,2.636914580355878,8.491372117815171,2.5185351389821804,6.73226919950145,4.614709844115208,4.15623479785027,2.495695162624069,1.0426443374084937,8.060966074004773,3.5643781685650637,3.679198570566922,4.488000770834068,6.3103401206121505,3.605257262939004,4.821199978056451,7.003039979749236,8.211498981977794,8.84080938879262,7.640895629328568,5.95837871156634,6.804001915179304,3.997292407636508,2.3533232911628965,7.631540867341486,2.7990873060740036,4.706530552538117,3.452858964713811,8.983022086172278,6.187253085582297,4.564378168565064,4.241840183564671,5.187055089968057,5.496654082593496,6.564225720206398,6.793115555274348,8.126549613543878,6.3414521074811265,8.823876711241038,5.094658342654544,8.23429054170594,3.063502942306158,6.9910682753603695,7.485507332459785,6.9092930858238235,6.2992080183872785,5.189033824390017,5.562547724199122,3.2357270598380583,2.851998837112446,2.6507645591169022,6.425425734590699,2.889473542531109,5.012121672712217,4.938285791884053,5.642412772905056,6.199672344836364,2.0738202332916713,7.151270288790165,5.005849075696688,2.3923174227787602,2.1667154449664223,5.727920454563199,2.3103401206121505,6.984019633085816,3.9726926540042644,4.362469888750209,7.173227395028653,9.653203362033977,3.357552004618084,8.556735947914529,3.9420452599160467,4.1026581313637385,6.309067020588001,4.952799477899938,2.3015876466031866,4.580748491763774,5.046578366620329,6.999098033705607,2.752748591407134,6.73348992588162,3.1193561770396756,3.431622959713292,5.15623479785027,2.344828496997441,2.720278465233327,4.763411574470007,7.178117045706399,2.807354922057604,3.685940148445977,4.417515002885087,8.72584338467341,2.4724877714627436,3.619413010597937,7.5130959085510085,3.382667252745041,2.7463127664254587,4.509695841933387,1.6553518286125541,3.0992952043377753,7.846242565109514,2.344828496997441,5.39780296186249,4.658211482751795,4.872828759534886,7.290387539763883,6.802451741275622,5.377123749129487,4.270528942380718,6.325710207515785,3.572889668420581,2.548436624696042,6.845113646617966,3.2265085298086795,6.531849285751203,3.361768359419153,5.599912842187128,2.211012193485512,4.396433531250992,7.406162478777607,5.6519127446457835,5.7114949066500875,3.61706334388818,4.044394119358453,5.772149590784542,5.177917792195843,3.1795110502715103,2.321928094887362,4.854494418154875,8.167066838182594,6.5249725982866265,6.43095427138573,5.669310286202474,4.147306698780294,4.303050084681673,4.3298411765306275,6.912410353808268,4.904002316283692,3.2854022188622483,2.8659188145522125,6.582254908357664,5.198494153639083,5.103077946597573,4.3045110418099535,4.370861740085285,8.654779216959517,8.362951992360674,7.2600256559614555,2.6392321632492775,6.457791260256042,7.31786479839195,5.661920579135087,4.609991295212678,3.460742563789644,9.248900055578497,7.886854944198882,3.7059779016825223,3.0373822220030804,5.982537314399271,5.339850002884624,10.306392126750833,8.18685706717715,5.533874777258546,8.85835405767524,6.166715444966422,6.56727163085093,5.7737326506078555,3.396433531250992,7.532083141492505,4.876271360550127,3.817623257511431,5.643567621908798,4.28318097894557,2.6205864104518777,2.881664619320345,5.006746832440585,5.20163386116965,4.181102550753798,8.571145863602528,7.3433189836610735,7.006298023900369,2.9818526532897405,6.419370342528151,2.414135532984451,8.463115619892362,4.857483428286672,6.972003303808356,4.958842675243241,5.910252962799278,4.4963345134171115,4.707082991771706,2.8094144442358986,3.1651079851445365,6.320484678017694,3.8787253414801053,1.3504972470841332,6.207502459258789,6.152994605492435,2.333423733725192,3.0823619695574735,3.4620523187964327,6.266599165535096,6.2861418728343015,3.6064422281316078,6.75702324650746,5.29020320512607,5.342341397431548,7.833712369021444,2.440952198029637,6.642123916153487,6.722602636851373,4.393690764187454,2.729008870337863,7.036503333738988,7.122569192216447,3.18269229751619,1.0565835283663676,2.992768430768924,4.125981653854716,2.7949356628035362,6.039796407620025,3.4154882710497003,5.535430914871279,4.963011647599427,2.763411574470007,5.062639828286463,7.592232766722112,9.317322159829551,4.955126781261366,1.4489009511451278,6.436128517259249,3.2645364309990255,7.647314509547136,6.15319733085508,2.229587922740652,5.417515002885087,5.0925457415435655,5.34801990922869,2.4195388915137843,7.500961352460057,4.426264754702098,6.247547806890298,4.622344722724236,4.638073837180719,2.763411574470007,8.388663123619393,7.058749412335524,3.496973580998276,3.377123749129487,2.361768359419153,9.114445113289024,4.888986721186558,7.971428593590467,5.424922088210688,7.212958362728285,7.02136867825973,0.8318772411916731,7.720552091605108,3.060047383669939,4.532940288372874,7.506922418047323,2.356143810225275,3.382667252745041,1.372952097911829,9.10522758436473,4.751142324726947,9.036036206482532,7.013685569928653,6.974644026040249,3.521050736900963,2.7070829917717063,6.017253838968937,10.665637927637095,5.325530331567558,7.864928972289788,3.400537929583729,2.851998837112446,2.37851162325373,5.22573765346606,3.8349134297010385,4.126807703142035,2.389566811762726,5.632268215499513,6.783456654360239,5.589164236699772,3.596935142387232,4.365272599764531,2.8399595874895316,6.588864518190148,2.8379432418910273,4.650764559116903,5.37364821133469,4.8875252707415875,7.596264312101167,7.323370069061268,4.88508622529017,5.500164679492167,2.327687364176047,5.592457037268081,5.951401291674311,6.265849421440848,6.862203399053224,3.023255352300303,5.299391206126829,7.7627474419531515,2.7420062108667365,5.932628157022127,2.550900664647523,6.9532652390148435,3.6542063779442917,3.8257856274647914,1.996388746447621,5.713420884868084,6.884964164708779,3.84197311892718,6.009212786805205,7.490730563589641,2.3476656563009706,6.668317719915721,3.8196681834964554,10.872998007770835,13.44366362719111,3.2764966656403565,10.051385697621988,6.023698917266407,3.8787253414801053,6.258895755075436,4.826802684285827,7.317231700222606,7.7475216725404215,4.398487190018967,5.342341397431548,5.767654798237347,2.8155754288625725,5.8011586560936985,2.3757345385831563,2.430285272977781,2.150559676575381,4.505255800932163,3.344828496997441,3.4356285940520905,2.2172307162206693,2.823749360308273,5.130519083027338,4.868390453054454,4.839455765224629,2.2720231890610485,5.525755692829486,3.828834649468056,7.7780114016125745,10.806299887662895,5.515384460636948,3.9699332746978557,6.57334422759282,2.759155833800272,5.033863451866284,4.279471295644468,7.080870811759721,6.138323004056484,5.369815424283912,3.975446765640962,4.708739041359579,9.557597721454812,4.200849574637213,5.854494418154875,5.246788093844365,5.102238193930738,7.479861085016818,3.0373822220030804,6.836681593327116,2.877744249949002,7.240600549955795,3.228049047884462,2.0565835283663674,4.4924941516794314,5.307064162255372,2.9088129077395473,2.682573297347578,2.722466024471091,7.96087076379617,8.903429991567151,9.435816085556171,4.631686366311998,7.457709193357593,6.109569509370688,6.0413306068206225,7.586464525886388,5.687060688339892,2.6915341649192004,2.948600847493356,10.7317008703535,2.6016965164809576,5.20163386116965,8.56017995776418,4.4672794804599825,5.904002316283692,8.44795106244151,4.932155684432056,0.9411063109464314,3.4154882710497003,3.8257856274647914,6.542412837068272,2.5360529002402097,8.739409342229193,4.436961337833602,2.797012977836145,5.624685810525398,6.1705259991768475,5.926948247949772,7.881113960675097,9.500642736062806,7.073606040574941,7.318316841334983,7.971830914780502,6.137913322088407,4.721919445551544,1.922197848396367,8.226941966807852,6.570159301636119,5.3458930856874085,2.73768676140986,10.642557179595622,3.62993940943954,5.175524601089875,9.995102877826515,3.4646682670034443,1.8836208162856714,6.0083165637349865,2.408711861029429,2.950468414150123,4.443606651475615,6.456313341355182,4.8604662585171665,5.661065479806948,6.7997349224788675,4.824258696603374,3.269033146455237,5.9740703670240265,5.925049964727359,5.946496941203293,1.4750848829487828,3.6971065744769747,3.953265239014844,5.464341532761104,3.0908534304511135,3.7676547982373463,2.859969548221026,7.822411496360477,9.401604869714957,2.7092906357233577,2.257010618206024,4.178714641175443,6.423410093269615,6.9070108151861795,3.6484654430273142,8.622783966156124,4.569248029867182,5.366322214245816,5.570462931026041,5.130519083027338,2.773996325111173,2.18269229751619,5.724923070266613,6.9787676510367875,6.751410160064371,4.484138131201669,3.0686708106650986,5.111865963867557,2.0942360698457656,4.315421315945778,4.06177619758669,5.781884234541486,2.974529312483882,4.57349571549561,4.176322772640463,10.888134388352869,4.414812060565847,5.521364878446219,4.582556003014061,3.8369340113210915,3.929790997718597,4.598126959919604,3.61706334388818,2.4489009511451276,4.788163657037767,1.8953026213333068,2.7398481026993275,0.12432813500220177,3.400537929583729,7.515147882149842,5.381629467033664,2.1602748314085933,6.677226328235316,8.025084179448086,3.440952198029637,2.9763636357327616,3.4905701304462013,7.431622959713292,2.7803100990433753,5.659068274843228,8.878265537895738,6.403949364232865,3.452858964713811,8.162995472417697,2.1473066987802936,3.7398481026993275,5.212569338850806,2.8933622107638715,3.9155209007519587,8.562585882154105,4.814550423461808,4.5867647433551415,2.582556003014061,5.077670274232752,7.905507349949873,11.663771557482411,6.559338847606539,2.7949356628035362,8.834123370734224,6.0925457415435655,4.936402377725063,3.454175893185802,2.788685710613534,7.877437521970619,5.3377110921282895,8.303415462685624,7.285032249622911,2.6915341649192004,6.459267666703387,4.398487190018967,8.028624401408491,2.9392265777282085,5.342341397431548,7.217424613813984,7.563234413022304,2.5921580021253603,3.423578170981797,2.763411574470007,5.542258049766918,4.51853513898218,9.535217045453418,5.908092340818271,6.928844036712568,5.612352498752662,6.7786028451432125,6.4202128906107845,2.877744249949002,4.038260575175349,3.4222330006830477,3.777156666004501,4.766065051474954,5.59215800212536,4.534186139090097,6.1147834472498515,5.572889668420581,2.9726926540042644,1.944858445807539,6.412442824775613,3.7990873060740036,4.030336078370959,4.255500733148386,2.5459683691052923,3.7813597135246595,5.135042286233651,5.6061460780657235,7.027021314622255,5.524189078449365,3.145677455195635,7.669948004116052,2.4462562298895643,2.408711861029429,7.221587121264805,4.008092420948722,6.956289026583943,11.260837225974512,5.316869805064333,4.078951341394822,4.509062386361898,8.238500278879492,2.689299160535892,4.22881869049588,5.614415386508126,4.112700132749362,2.9523335663696857,3.693765712217783,7.474030370207135,2.521050736900963,5.727375938594897,2.3673710656485296,2.786596361890807,6.034963989749303,5.578334948681608,8.298429210890312,7.4944956156680504,10.089053172156433,7.896998342338911,8.325215494683038,7.3416300093299105,4.349789870241872,6.438459204423481,8.102815576392821,5.43229133824778,2.4905701304462013,2.873813198359087,6.096978636370538,4.032982416699428,5.604071323668861,2.5582676340557358,4.694323060125386,2.722466024471091,2.3757345385831563,7.995258373404689,7.1078970616646115,7.797726364623276,9.707290101958575,1.0976107966264221,3.1953475983222193,7.672354619892956,3.4208865749755315,6.180903709179382,5.908572758751745,3.372952097911829,7.12525847253797,4.314696525656286,8.915789767665157,5.459103696135399,7.07692245939234,5.79207446192887,4.488000770834068,5.685379551959075,6.442114122419225,5.125981653854716,3.1193561770396756,6.643134661857265,6.657640005207824,6.316507819103109,0,4.053111336459563,5.711219556894335,2.3161457422933562,4.726831217032493,5.8894735425311096,5.981167667105682,6.483976963236257,3.3840498067951597,4.672990993827748,5.239550796613645,6.48767927890162,2.733354340613827,7.754887502163468,5.065227622775619,6.546122758857796,4.556429415449573,3.33771109212829,4.72137265947682,5.533563348214512,7.015582311680932,3.3868109464722167,6.141391916170729,2.097610796626422,6.376255649313362,3.24031432933371,6.5787877956769725,6.052024560482831,6.4571345943180365,6.711081862306696,1.5109619192773793,2.0071955014042038,8.766462652425766,2.584962500721156,7.074676686294496,3.292781749227846,9.20006486151431,7.7525479058471385,4.009884588931844,2.169925001442312,8.76354436429392,6.257010618206024,4.3269687115878925,2.3950627995175777,2.3757345385831563,5.424922088210688,2.250961573533219,2.8011586560936985,3.4892860226258766,5.968551603363945,2.541019153133559,4.888012585402829,6.457791260256042,5.237257770900372,4.161081482277184,3.408711861029429,6.705839705883682,8.251198340973543,5.2671612180222604,7.824386002585445,3.1043366598147357,6.542103245856593,2.2234225499349374,5.316507819103109,3.363171077119244,4.631104282365877,3.9873208659292536,2.140778655782796,7.473705749619415,5.924575003392735,2.871843648509318,6.807870078299782,4.955591791753994,4.214901910160696,3.910732661902913,8.646486708605355,2.2539892662307865,3.816599706534912,6.460578760776026,7.281698250098829,4.901108243014512,2.867896463992655,5.9429835981871015,2.950468414150123,3.8429788317883253,6.045486656359465,4.990047219573794,6.705977901682522,7.860404179081675,2.565597175854225,0,9.146670494470888,6.7014104362328135,3.4019034716079584,4.462706750670158,4.311793717753649,6.649471757181635,2.788685710613534,7.693556651228518,2.8135246892978105,3.343407822297814,3.5360529002402097,3.2675357980687347,6.75127624861109,8.995371450572195,2.6870606883398924,7.605257262939004,5.275752048828529,6.982080909763738,5.305970520984387,2.017921907997262,7.062963556559086,7.2259304111710545,7.0414401300729725,7.778274295338579,2.4646682670034443,2.4620523187964327,3.2141248053528475,4.114367024952,6.111448698487674,2.189033824390017,6.304876050044889,6.0710332444568245,6.157650096861711,4.038260575175349,7.489045125114802,8.475936042446387,0.9411063109464314,2.01435529297707,8.154868729042667,5.955824240806441,2.996388746447621,6.1039172107520665,4.296457407371244,3.657640005207824,3.1259816538547156,8.325890061039784,3.085764553778314,5.309976492370556,3.145677455195635,2.594548549550354,3.631104282365877,5.121015400961366,4.371558862611963,8.492213725013189,2.1043366598147357,5.825022364044702,6.883987308105472,10.959639764364185,2.4620523187964327,5.522306892871389,0,6.593054921669878,3.9354597478052895,8.913068913647477,5.021035479511363,1.7484612330040354,5.982537314399271,7.690137692986635,5.791814071161826,3.578938713093386,4.896271848807375,6.766065051474954,7.801029283808189,6.976821852360685,5.795195490817005,6.883132015657282,4.464668267003444,4.808385050656093,7.161484638641659,2.3645724322958563,5.707082991771706,2.8439838440483265,2,7.723148957047578,1.1634987322828796,7.126807703142035,5.348728154231077,4.4924941516794314,6.174526265266431,5.563768278452033,5.6570683012015754,3.37851162325373,6.43895814789755,3.127633279725874,3.1953475983222193,6.676803353873854,2.6667565918848033,2.3589588258323295,5.587364990936461,2.454175893185802,3.2265085298086795,6.230164577884252,7.884597920990064,3.7687136570304847,3.127633279725874,6.450551443306483,4.084914659767938,4.135863165368679,2.319039815562536,3.097610796626422,5.624978179569036,3.968090752045256,5.105594275795191,5.185470146066942,5.509379148914689,5.266411766036114,3.1342209397606338,4.707082991771706,6.390598905552461,2.8698714061777126,1.4168397419128291,2.3645724322958563,3.5084286525318573,3.9354597478052895,7.11009175192513,1.4222330006830475,6.929672661579345,7.041001987167506,6.332349900373874,3.423578170981797,5.137503523749935,5.3139713710590195,5.999549087340949,3.2433644256936605,4.7114949066500875,3.7059779016825223,6.291677223176022,0.3895668117627256,5.758089934201813,7.001577085389535,7.931505781869729,2.8257856274647914,9.151270288790165,2.440952198029637,5.74604398299834,7.136683577697236,6.353146825498083,10.212070748654236,6.007419783571914,4.997744026059632,2.4982508675278257,8.25176642434399,2.8579809951275723,3.300123724569014,4.640967910449898,2.625270489374693,5.399854673572431,2.2660368939953175,1.516015147003665,6.199672344836364,5.708187236020708,5.402585758232587,5.269033146455237,4.529820946528695,5.212958362728285,5.648753032989572,2.9467308601403097,2.169925001442312,6.088735246190284,2.4356285940520905,8.627898615871748,5.300489844363109,8.091065077930535,7.119252412157343,10.142962207064079,1.7398481026993275,3.729008870337863,4.45022149589718,6.461724991502711,7.904544309292717,5.490890978894291,6.76593249347046,4.502075956045791,2.782408564927373,4.205548911173033,4.139960569545456,6.708325207143167,4.632268215499513,5.6194130105979365,5.541638734436141,3.581351247168777,3.496973580998276,4.862451391325582,4.784503982929566,5.359310316890443,3.41006969175638,2.5897634869849773,2.769771739249448,10.641220865859923,8.202368742775262],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Ridge()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_pred_logRidge"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression ridge\n","# réglage des paramètre pour la gridsearch\n","alphasridge_log = np.logspace(-3, 5, 1000)\n","param_gridRidge_log = {'ridge__alpha': alphasridge_log}\n","\n","GridRidge_log, \\\n","BestParametresRidge_log, \\\n","ScoresRidge_log, \\\n","TotalGHGEmissions_pred_logRidge_log, \\\n","figRidge_log = reg_modelGrid(model=Ridge(),\n","                            scaler=scaler,\n","                            X_train=BEBM_train,\n","                            X_test=BEBM_test,\n","                            y_train=TotalGHGEmissions_train_log,\n","                            y_test=TotalGHGEmissions_test_log,\n","                            y_test_name='TotalGHGEmissions_test_log',\n","                            y_pred_name='TotalGHGEmissions_pred_logRidge',\n","                            score=score,\n","                            param_grid=param_gridRidge_log)\n","\n","print(BestParametresRidge_log)\n","print(ScoresRidge_log)\n","figRidge_log.show()\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[1.7706763809449992,1.7706763806926318,1.770676380435568,1.77067638017372,1.7706763799069993,1.7706763796353147,1.770676379358574,1.7706763790766833,1.7706763787895465,1.770676378497066,1.7706763781991424,1.770676377895675,1.7706763775865593,1.7706763772716916,1.770676376950964,1.7706763766242672,1.7706763762914914,1.770676375952522,1.7706763756072443,1.7706763752555417,1.7706763748972931,1.7706763745323784,1.7706763741606717,1.7706763737820481,1.7706763733963782,1.7706763730035309,1.7706763726033732,1.7706763721957681,1.7706763717805778,1.7706763713576605,1.7706763709268731,1.7706763704880686,1.7706763700410977,1.770676369585809,1.7706763691220477,1.7706763686496554,1.7706763681684723,1.770676367678334,1.7706763671790742,1.7706763666705236,1.7706763661525087,1.770676365624854,1.7706763650873796,1.7706763645399026,1.770676363982237,1.7706763634141935,1.7706763628355788,1.7706763622461963,1.7706763616458452,1.770676361034322,1.7706763604114186,1.7706763597769224,1.7706763591306185,1.7706763584722875,1.770676357801705,1.7706763571186428,1.7706763564228687,1.7706763557141472,1.770676354992236,1.7706763542568904,1.7706763535078598,1.7706763527448903,1.770676351967722,1.7706763511760908,1.7706763503697274,1.7706763495483582,1.7706763487117034,1.7706763478594785,1.7706763469913938,1.7706763461071549,1.7706763452064602,1.770676344289004,1.7706763433544743,1.7706763424025531,1.7706763414329174,1.7706763404452364,1.7706763394391758,1.7706763384143922,1.770676337370538,1.7706763363072582,1.770676335224191,1.7706763341209686,1.7706763329972155,1.7706763318525502,1.7706763306865831,1.7706763294989176,1.7706763282891504,1.77067632705687,1.7706763258016576,1.7706763245230863,1.7706763232207217,1.7706763218941206,1.770676320542832,1.7706763191663968,1.7706763177643468,1.7706763163362056,1.7706763148814875,1.7706763133996986,1.770676311890334,1.770676310352881,1.7706763087868176,1.7706763071916103,1.7706763055667174,1.770676303911587,1.7706763022256553,1.7706763005083495,1.7706762987590867,1.7706762969772711,1.7706762951622974,1.7706762933135483,1.7706762914303957,1.7706762895121997,1.7706762875583073,1.7706762855680545,1.7706762835407652,1.77067628147575,1.770676279372307,1.7706762772297207,1.770676275047263,1.7706762728241927,1.7706762705597527,1.7706762682531738,1.7706762659036723,1.7706762635104496,1.7706762610726912,1.7706762585895697,1.7706762560602396,1.7706762534838418,1.7706762508595006,1.7706762481863234,1.7706762454634024,1.7706762426898113,1.770676239864607,1.77067623698683,1.7706762340555016,1.7706762310696251,1.770676228028186,1.7706762249301504,1.7706762217744654,1.7706762185600575,1.7706762152858349,1.770676211950684,1.770676208553472,1.7706762050930436,1.7706762015682223,1.7706761979778107,1.770676194320588,1.7706761905953106,1.770676186800713,1.7706761829355055,1.770676178998374,1.77067617498798,1.770676170902961,1.7706761667419282,1.7706761625034673,1.7706761581861383,1.7706761537884728,1.7706761493089769,1.7706761447461274,1.7706761400983748,1.770676135364138,1.7706761305418088,1.7706761256297476,1.7706761206262858,1.7706761155297226,1.7706761103383255,1.7706761050503306,1.7706760996639404,1.7706760941773247,1.770676088588619,1.7706760828959234,1.7706760770973033,1.770676071190789,1.770676065174372,1.7706760590460093,1.7706760528036163,1.7706760464450735,1.7706760399682189,1.7706760333708527,1.7706760266507318,1.7706760198055729,1.7706760128330508,1.7706760057307953,1.7706759984963933,1.770675991127387,1.7706759836212718,1.7706759759754978,1.7706759681874664,1.770675960254532,1.770675952173999,1.7706759439431223,1.7706759355591042,1.7706759270190968,1.7706759183201988,1.7706759094594546,1.7706759004338533,1.7706758912403289,1.7706758818757578,1.770675872336959,1.7706758626206913,1.7706758527236546,1.7706758426424858,1.770675832373761,1.770675821913991,1.7706758112596237,1.770675800407039,1.7706757893525507,1.7706757780924036,1.770675766622773,1.770675754939763,1.7706757430394056,1.7706757309176584,1.7706757185704038,1.7706757059934493,1.7706756931825218,1.7706756801332717,1.770675666841266,1.7706756533019914,1.7706756395108492,1.7706756254631564,1.770675611154142,1.7706755965789474,1.7706755817326223,1.7706755666101262,1.7706755512063235,1.7706755355159842,1.7706755195337802,1.7706755032542856,1.770675486671972,1.7706754697812108,1.7706754525762662,1.7706754350512977,1.7706754172003554,1.7706753990173791,1.7706753804961965,1.7706753616305206,1.7706753424139474,1.7706753228399532,1.7706753029018945,1.7706752825930039,1.7706752619063884,1.7706752408350268,1.7706752193717676,1.7706751975093276,1.7706751752402863,1.7706751525570876,1.7706751294520333,1.7706751059172845,1.770675081944854,1.7706750575266093,1.7706750326542646,1.770675007319382,1.7706749815133658,1.7706749552274617,1.770674928452753,1.7706749011801566,1.7706748734004214,1.7706748451041256,1.7706748162816717,1.770674786923284,1.7706747570190058,1.770674726558696,1.770674695532025,1.7706746639284723,1.7706746317373212,1.7706745989476578,1.770674565548365,1.7706745315281194,1.7706744968753878,1.7706744615784238,1.7706744256252627,1.7706743890037184,1.7706743517013792,1.7706743137056022,1.7706742750035123,1.7706742355819938,1.770674195427689,1.770674154526993,1.7706741128660486,1.7706740704307422,1.770674027206698,1.7706739831792753,1.770673938333561,1.7706738926543673,1.7706738461262237,1.7706737987333745,1.7706737504597725,1.770673701289073,1.7706736512046284,1.7706736001894843,1.7706735482263714,1.7706734952977023,1.7706734413855627,1.7706733864717084,1.7706733305375573,1.7706732735641832,1.7706732155323113,1.7706731564223095,1.770673096214183,1.7706730348875677,1.7706729724217225,1.7706729087955253,1.7706728439874606,1.770672777975618,1.7706727107376814,1.7706726422509234,1.770672572492196,1.770672501437924,1.7706724290640985,1.7706723553462655,1.7706722802595212,1.7706722037785028,1.7706721258773777,1.770672046529839,1.7706719657090937,1.7706718833878543,1.7706717995383319,1.7706717141322241,1.7706716271407075,1.770671538534429,1.7706714482834918,1.7706713563574517,1.7706712627253018,1.7706711673554665,1.770671070215787,1.770670971273514,1.770670870495295,1.770670767847165,1.7706706632945335,1.7706705568021754,1.7706704483342162,1.7706703378541242,1.7706702253246955,1.7706701107080431,1.770669993965584,1.7706698750580276,1.7706697539453617,1.7706696305868401,1.7706695049409693,1.7706693769654946,1.770669246617387,1.7706691138528285,1.7706689786271987,1.7706688408950595,1.7706687006101407,1.7706685577253254,1.770668412192634,1.7706682639632099,1.7706681129873019,1.77066795921425,1.7706678025924678,1.7706676430694266,1.7706674805916383,1.7706673151046384,1.7706671465529678,1.7706669748801562,1.7706668000287025,1.7706666219400589,1.7706664405546082,1.7706662558116495,1.7706660676493755,1.7706658760048533,1.770665680814006,1.7706654820115912,1.7706652795311801,1.7706650733051361,1.7706648632645958,1.7706646493394445,1.7706644314582956,1.770664209548467,1.7706639835359599,1.7706637533454341,1.7706635189001851,1.77066328012212,1.770663036931733,1.7706627892480804,1.7706625369887554,1.770662280069864,1.770662018405997,1.7706617519102026,1.770661480493964,1.7706612040671672,1.770660922538076,1.770660635813303,1.7706603437977813,1.7706600463947335,1.7706597435056461,1.7706594350302356,1.7706591208664197,1.770658800910286,1.7706584750560594,1.770658143196074,1.7706578052207356,1.7706574610184909,1.7706571104757955,1.7706567534770774,1.7706563899047036,1.7706560196389454,1.7706556425579418,1.7706552585376645,1.7706548674518792,1.7706544691721124,1.7706540635676085,1.7706536505052952,1.7706532298497428,1.770652801463126,1.7706523652051822,1.7706519209331724,1.7706514685018386,1.7706510077633641,1.7706505385673281,1.770650060760665,1.7706495741876196,1.7706490786897038,1.770648574105651,1.7706480602713708,1.7706475370199026,1.770647004181369,1.7706464615829305,1.7706459090487328,1.7706453463998628,1.7706447734542976,1.7706441900268537,1.770643595929137,1.770642990969493,1.7706423749529527,1.7706417476811815,1.7706411089524259,1.7706404585614588,1.7706397962995262,1.7706391219542907,1.7706384353097775,1.7706377361463166,1.7706370242404852,1.7706362993650515,1.7706355612889155,1.7706348097770495,1.7706340445904392,1.770633265486023,1.77063247221663,1.7706316645309212,1.7706308421733226,1.770630004883968,1.7706291523986295,1.7706282844486587,1.7706274007609175,1.7706265010577167,1.7706255850567463,1.7706246524710125,1.7706237030087675,1.770622736373443,1.7706217522635836,1.7706207503727753,1.7706197303895785,1.7706186919974556,1.7706176348747036,1.7706165586943825,1.7706154631242412,1.7706143478266507,1.7706132124585288,1.770612056671267,1.7706108801106601,1.7706096824168316,1.77060846322416,1.770607222161205,1.7706059588506335,1.7706046729091454,1.7706033639473986,1.7706020315699345,1.7706006753751011,1.7705992949549825,1.7705978898953174,1.7705964597754296,1.7705950041681482,1.7705935226397362,1.7705920147498115,1.770590480051275,1.7705889180902343,1.770587328405928,1.770585710530654,1.7705840639896917,1.7705823883012308,1.770580682976298,1.7705789475186822,1.7705771814248643,1.7705753841839424,1.7705735552775643,1.7705716941798546,1.7705698003573462,1.7705678732689107,1.7705659123656918,1.7705639170910374,1.7705618868804336,1.7705598211614422,1.7705577193536353,1.770555580868535,1.7705534051095508,1.7705511914719234,1.7705489393426652,1.7705466481005065,1.7705443171158386,1.7705419457506657,1.7705395333585539,1.770537079284581,1.7705345828652945,1.7705320434286662,1.7705294602940533,1.7705268327721584,1.7705241601649955,1.7705214417658577,1.7705186768592867,1.7705158647210477,1.7705130046181043,1.770510095808601,1.7705071375418455,1.7705041290582961,1.7705010695895544,1.7704979583583587,1.7704947945785832,1.7704915774552432,1.7704883061845027,1.7704849799536848,1.770481597941292,1.7704781593170265,1.7704746632418171,1.7704711088678544,1.770467495338626,1.7704638217889606,1.7704600873450793,1.7704562911246473,1.77045243223684,1.7704485097824059,1.7704445228537444,1.7704404705349863,1.7704363519020798,1.7704321660228886,1.7704279119572903,1.77042358875729,1.7704191954671362,1.7704147311234455,1.7704101947553377,1.7704055853845753,1.7704009020257172,1.7703961436862734,1.7703913093668777,1.7703863980614614,1.7703814087574397,1.7703763404359105,1.7703711920718561,1.7703659626343637,1.7703606510868468,1.770355256387284,1.770349777488466,1.770344213338252,1.7703385628798405,1.7703328250520471,1.770326998789598,1.7703210830234326,1.7703150766810196,1.770308978686683,1.7703027879619437,1.7702965034258704,1.7702901239954454,1.7702836485859443,1.770277076111326,1.7702704054846392,1.77026363561844,1.7702567654252264,1.7702497938178845,1.7702427197101513,1.7702355420170917,1.7702282596555876,1.7702208715448464,1.770213376606924,1.770205773767262,1.7701980619552429,1.7701902401047591,1.7701823071548017,1.770174262050065,1.7701661037415668,1.770157831187289,1.7701494433528342,1.7701409392121004,1.770132317747978,1.77012357795306,1.7701147188303765,1.7701057393941475,1.7700966386705559,1.7700874156985407,1.7700780695306133,1.770068599233694,1.7700590038899704,1.770049282597783,1.7700394344725268,1.7700294586475842,1.7700193542752785,1.7700091205278554,1.7699987565984887,1.7699882617023128,1.769977635077486,1.7699668759862797,1.7699559837161967,1.7699449575811232,1.7699337969225097,1.7699225011105841,1.769911069545601,1.769899501659123,1.7698877969153393,1.7698759548124225,1.769863974883922,1.7698518567001984,1.7698395998698992,1.7698272040414782,1.7698146689047582,1.7698019941925403,1.769789179682261,1.7697762251976996,1.769763130610737,1.7697498958431663,1.7697365208685614,1.769723005714201,1.7697093504630559,1.7696955552558307,1.7696816202930805,1.7696675458373832,1.7696533322155887,1.7696389798211374,1.7696244891164512,1.7696098606354025,1.7695950949858634,1.7695801928523374,1.7695651549986742,1.7695499822708751,1.7695346755999881,1.7695192360050984,1.7695036645964142,1.7694879625784552,1.769472131253344,1.7694561720242021,1.7694400863986615,1.7694238759924836,1.7694075425332982,1.7693910878644648,1.7693745139490513,1.7693578228739455,1.7693410168540957,1.7693240982368823,1.7693070695066317,1.7692899332892646,1.7692726923570945,1.7692553496337695,1.769237908199368,1.7692203712956438,1.769202742331434,1.7691850248882226,1.7691672227258728,1.7691493397885196,1.7691313802106383,1.7691133483232797,1.769095248660486,1.7690770859658813,1.7690588651994428,1.7690405915444571,1.7690222704146588,1.7690039074615598,1.768985508581966,1.768967079925686,1.7689486279034328,1.768930159194922,1.7689116807571637,1.7688931998329518,1.768874723959555,1.7688562609776028,1.7688378190401735,1.7688194066220828,1.7688010325293704,1.7687827059089902,1.7687644362586972,1.7687462334371364,1.7687281076741292,1.7687100695811577,1.768692130162045,1.768674300823833,1.7686565933878478,1.7686390201009616,1.7686215936470426,1.768604327158585,1.7685872342285323,1.7685703289222698,1.7685536257898018,1.7685371398780982,1.7685208867436077,1.7685048824649399,1.768489143655703,1.7684736874774978,1.768458531653059,1.7684436944795459,1.768429194841962,1.7684150522267146,1.7684012867352936,1.76838791909807,1.7683749706882075,1.7683624635356747,1.7683504203413534,1.7683388644912381,1.768327820070705,1.768317311878866,1.768307365442967,1.7682980070328522,1.7682892636754606,1.7682811631693596,1.7682737340992971,1.7682670058507657,1.768261008624565,1.7682557734513527,1.7682513322061726,1.7682477176229452,1.7682449633089097,1.768243103759008,1.768242174370192,1.7682422114556409,1.768243252258884,1.768245334967801,1.7682484987284972,1.7682527836590318,1.7682582308629864,1.768264882442861,1.7682727815132744,1.7682819722139649,1.7682924997225602,1.7683044102671144,1.7683177511383819,1.7683325707018256,1.7683489184093237,1.7683668448105792,1.7683864015641901,1.7684076414483851,1.7684306183713883,1.7684553873814033,1.7684820046762,1.7685105276122741,1.7685410147135703,1.7685735256797481,1.7686081213939648,1.7686448639301624,1.7686838165598346,1.7687250437582591,1.7687686112101673,1.7688145858148379,1.7688630356905943,1.7689140301786836,1.7689676398465146,1.7690239364902447,1.7690829931366854,1.7691448840445154,1.769209684704776,1.7692774718406334,1.7693483234063894,1.7694223185857214,1.7694995377891325,1.7695800626505949,1.7696639760233732,1.7697513619750052,1.7698423057814252,1.7699368939202202,1.7700352140629938,1.7701373550668342,1.7702434069648614,1.7703534609558496,1.7704676093929084,1.770585945771209,1.7707085647147474,1.7708355619621305,1.7709670343513833,1.7711030798037541,1.7712437973065231,1.7713892868947987,1.7715396496322966,1.7716949875910963,1.7718554038303655,1.7720210023740535,1.7721918881875438,1.772368167153261,1.7725499460452352,1.7727373325026132,1.7729304350021202,1.7731293628294615,1.773334226049672,1.7735451354764038,1.7737622026401538,1.7739855397554272,1.7742152596868372,1.7744514759141374,1.7746943024961837,1.774943854033826,1.7752002456317257,1.7754635928590894,1.7757340117093245,1.7760116185586057,1.7762965301233458,1.7765888634165663,1.7768887357031615,1.7771962644540433,1.777511567299157,1.7778347619793642,1.7781659662971692,1.7785052980662819,1.7788528750600023,1.7792088149584089,1.7795732352943283,1.779946253398076,1.7803279863409398,1.780718550877387,1.7811180633859727,1.7815266398089218,1.781944395590364,1.7823714456131878,1.7828079041344935,1.7832538847196155,1.783709500174676,1.7841748624776597,1.7846500827079588,1.7851352709743782,1.7856305363415594,1.786135986754801,1.7866517289632515,1.7871778684414419,1.7877145093091418,1.7882617542495123,1.7888197044255427,1.7893884593947469,1.7899681170221153,1.790558773391309,1.7911605227140917,1.7917734572380026,1.7923976671522734,1.7930332404919984,1.7936802630405821,1.7943388182304816,1.7950089870422772,1.7956908479021074,1.7963844765775216,1.797089946071797,1.7978073265167862,1.7985366850643725,1.7992780857766075,1.800031589514631,1.8007972538264698,1.8015751328338268,1.802365277117995,1.8031677336050138,1.8039825454502245,1.8048097519223802,1.805649388287467,1.8065014856924286,1.8073660710489705,1.8082431669176526,1.8091327913924715,1.8100349579861614,1.8109496755164343,1.811876947993408,1.8128167745084613,1.813769149124782,1.814734060769864,1.815711493130231,1.8167014245486612,1.8177038279242026,1.818718670615257,1.8197459143460395,1.8207855151166927,1.821837423117369,1.8229015826465662,1.8239779320340215,1.8250664035684614,1.8261669234304958,1.82727941163095,1.8284037819549273,1.829539941911864,1.8306877926918728,1.8318472291286159,1.8330181396689782,1.8342004063497748,1.8353939047817285,1.8365985041409316,1.8378140671680008,1.8390404501751043,1.8402775030610425,1.8415250693345318,1.842782986145825,1.8440510843267872,1.8453291884395262,1.8466171168336452,1.8479146817121819,1.8492216892062558,1.850537939458442,1.8518632267148551,1.8531973394259023,1.854540060355656,1.8558911666997553,1.8572504302117303,1.8586176173376256,1.8599924893587638,1.8613748025424801,1.8627643083006198,1.8641607533555948,1.8655638799137422,1.8669734258457371,1.8683891248737683,1.8698107067651837,1.871237897532282,1.8726704196379245,1.8741079922066006,1.8755503312406017,1.8769971498409028,1.878448158432379,1.87990306499294,1.8813615752861803,1.8828233930971148,1.884288220470583,1.88575575795188,1.8872257048291778,1.8886977593773018,1.8901716191024165,1.891646980987178,1.8931235417359218,1.8946009980194312,1.8960790467188775,1.8975573851684817,1.8990357113964904,1.900513724364052,1.901991124201577,1.9034676124422127,1.9049428922520242,1.906416668656528,1.9078886487632154,1.9093585419797272,1.9108260602273444,1.9122909181494894,1.913752833314938,1.915211526415464,1.9166667214576543,1.9181181459486472,1.919565531075572,1.9210086118784795,1.9224471274165722,1.9238808209275633,1.9253094399800161,1.9267327366185267,1.928150467501632,1.9295623940323545,1.9309682824812957,1.9323679041022257,1.9337610352401218,1.935147457431632,1.9365269574979531,1.9378993276301344,1.9392643654668285,1.940621874164527,1.9419716624603425,1.9433135447273913,1.9446473410228744,1.945972877128943,1.9472899845864522,1.948598500721733,1.9498982686665016,1.9511891373710513,1.9524709616108766,1.953743601986885,1.9550069249193627,1.9562608026358734,1.9575051131532626,1.9587397402539601,1.9599645734567672,1.9611795079823318,1.9623844447134993,1.9635792901507596,1.9647639563629808,1.9659383609336434,1.9671024269027915,1.9682560827048952,1.969399262102856,1.9705319041183447,1.9716539529586978,1.9727653579405728,1.9738660734105733,1.9749560586630452,1.9760352778552552,1.977103699920138,1.9781612984768244,1.9792080517391326,1.9802439424222176,1.9812689576475637,1.9822830888465013,1.9832863316624223,1.984278685851876,1.9852601551847002,1.9862307473433565,1.987190473821633,1.988139349822855,1.9890773941577589]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[1.8016331396732712,1.8016331395456004,1.801633139415554,1.801633139283087,1.8016331391481555,1.801633139010712,1.8016331388707112,1.8016331387281046,1.8016331385828444,1.8016331384348807,1.8016331382841633,1.8016331381306412,1.8016331379742618,1.8016331378149726,1.8016331376527188,1.8016331374874448,1.8016331373190961,1.8016331371476135,1.80163313697294,1.8016331367950158,1.80163313661378,1.801633136429172,1.8016331362411278,1.8016331360495843,1.8016331358544762,1.801633135655737,1.8016331354532997,1.8016331352470945,1.801633135037052,1.8016331348231005,1.8016331346051675,1.8016331343831784,1.801633134157058,1.8016331339267295,1.801633133692115,1.801633133453134,1.8016331332097058,1.8016331329617465,1.801633132709173,1.8016331324518995,1.8016331321898376,1.8016331319228993,1.801633131650993,1.8016331313740261,1.801633131091905,1.8016331308045335,1.8016331305118138,1.801633130213647,1.8016331299099309,1.8016331296005628,1.8016331292854375,1.801633128964447,1.8016331286374831,1.801633128304435,1.8016331279651883,1.801633127619628,1.8016331272676362,1.801633126909095,1.8016331265438805,1.8016331261718697,1.8016331257929352,1.801633125406949,1.8016331250137796,1.8016331246132928,1.8016331242053532,1.8016331237898218,1.8016331233665575,1.8016331229354157,1.80163312249625,1.8016331220489121,1.801633121593249,1.8016331211291057,1.8016331206563247,1.801633120174745,1.8016331196842035,1.801633119184532,1.8016331186755625,1.8016331181571204,1.8016331176290303,1.801633117091112,1.801633116543183,1.8016331159850572,1.8016331154165441,1.8016331148374514,1.8016331142475817,1.8016331136467336,1.8016331130347043,1.8016331124112845,1.8016331117762634,1.801633111129424,1.8016331104705472,1.801633109799408,1.801633109115779,1.8016331084194275,1.8016331077101164,1.8016331069876053,1.8016331062516477,1.8016331055019945,1.8016331047383898,1.801633103960574,1.801633103168283,1.8016331023612473,1.8016331015391922,1.8016331007018391,1.8016330998489023,1.801633098980092,1.8016330980951136,1.8016330971936652,1.8016330962754405,1.8016330953401272,1.801633094387408,1.8016330934169587,1.8016330924284485,1.801633091421542,1.8016330903958966,1.801633089351164,1.8016330882869886,1.8016330872030086,1.8016330860988554,1.8016330849741542,1.8016330838285215,1.801633082661568,1.8016330814728978,1.8016330802621063,1.8016330790287811,1.8016330777725045,1.8016330764928472,1.8016330751893754,1.8016330738616457,1.8016330725092065,1.8016330711315984,1.8016330697283525,1.8016330682989916,1.8016330668430303,1.8016330653599728,1.8016330638493152,1.801633062310544,1.8016330607431361,1.8016330591465586,1.8016330575202677,1.8016330558637113,1.801633054176326,1.8016330524575381,1.801633050706763,1.8016330489234054,1.8016330471068593,1.8016330452565068,1.8016330433717183,1.8016330414518533,1.8016330394962594,1.8016330375042715,1.8016330354752117,1.8016330334083905,1.8016330313031055,1.8016330291586402,1.8016330269742662,1.80163302474924,1.8016330224828054,1.8016330201741915,1.8016330178226145,1.801633015427273,1.801633012987354,1.8016330105020268,1.801633007970447,1.8016330053917542,1.8016330027650709,1.8016330000895038,1.8016329973641436,1.8016329945880638,1.8016329917603204,1.8016329888799512,1.8016329859459774,1.8016329829574018,1.8016329799132074,1.80163297681236,1.8016329736538037,1.8016329704364662,1.8016329671592526,1.8016329638210495,1.8016329604207206,1.8016329569571101,1.8016329534290412,1.8016329498353132,1.8016329461747045,1.8016329424459707,1.8016329386478434,1.8016329347790319,1.8016329308382195,1.8016329268240676,1.8016329227352106,1.8016329185702586,1.8016329143277945,1.8016329100063766,1.801632905604535,1.801632901120774,1.8016328965535673,1.8016328919013636,1.8016328871625795,1.8016328823356051,1.8016328774187982,1.8016328724104875,1.8016328673089697,1.8016328621125104,1.801632856819342,1.8016328514276658,1.8016328459356477,1.8016328403414201,1.8016328346430812,1.8016328288386934,1.8016328229262826,1.8016328169038394,1.8016328107693151,1.8016328045206238,1.8016327981556413,1.8016327916722026,1.801632785068104,1.8016327783410988,1.8016327714889004,1.8016327645091788,1.8016327573995605,1.8016327501576277,1.8016327427809187,1.801632735266924,1.8016327276130897,1.8016327198168123,1.8016327118754416,1.8016327037862767,1.8016326955465676,1.8016326871535113,1.8016326786042556,1.8016326698958918,1.8016326610254592,1.801632651989942,1.8016326427862672,1.8016326334113053,1.801632623861869,1.8016326141347105,1.8016326042265223,1.801632594133935,1.801632583853517,1.8016325733817724,1.8016325627151404,1.8016325518499934,1.8016325407826377,1.801632529509309,1.8016325180261743,1.8016325063293284,1.8016324944147946,1.8016324822785204,1.8016324699163797,1.801632457324168,1.8016324444976042,1.8016324314323253,1.8016324181238896,1.8016324045677716,1.8016323907593608,1.801632376693961,1.8016323623667907,1.8016323477729774,1.8016323329075576,1.801632317765477,1.801632302341586,1.8016322866306393,1.801632270627295,1.80163225432611,1.8016322377215417,1.801632220807944,1.8016322035795636,1.8016321860305426,1.801632168154913,1.801632149946596,1.8016321313993997,1.8016321125070165,1.8016320932630212,1.8016320736608706,1.801632053693898,1.8016320333553122,1.801632012638198,1.801631991535509,1.8016319700400694,1.8016319481445684,1.8016319258415603,1.8016319031234598,1.801631879982541,1.8016318564109342,1.8016318324006226,1.8016318079434412,1.8016317830310724,1.8016317576550434,1.801631731806724,1.8016317054773237,1.8016316786578879,1.8016316513392951,1.801631623512255,1.801631595167303,1.8016315662947988,1.8016315368849236,1.801631506927675,1.8016314764128636,1.8016314453301117,1.801631413668847,1.8016313814183025,1.8016313485675082,1.8016313151052914,1.8016312810202713,1.801631246300855,1.8016312109352337,1.801631174911378,1.8016311382170367,1.8016311008397277,1.8016310627667382,1.8016310239851197,1.8016309844816798,1.8016309442429828,1.801630903255342,1.801630861504815,1.8016308189772021,1.8016307756580368,1.801630731532584,1.8016306865858358,1.801630640802502,1.80163059416701,1.8016305466634963,1.8016304982758027,1.8016304489874686,1.8016303987817286,1.8016303476415034,1.8016302955493975,1.80163024248769,1.8016301884383314,1.801630133382934,1.80163007730277,1.8016300201787614,1.8016299619914753,1.8016299027211167,1.801629842347522,1.801629780850153,1.8016297182080871,1.8016296544000143,1.8016295894042256,1.8016295231986093,1.801629455760641,1.801629387067377,1.8016293170954463,1.8016292458210428,1.8016291732199161,1.8016290992673654,1.8016290239382295,1.8016289472068776,1.8016288690472024,1.80162878943261,1.8016287083360107,1.8016286257298113,1.8016285415859037,1.8016284558756563,1.8016283685699044,1.8016282796389393,1.8016281890525008,1.8016280967797618,1.8016280027893237,1.8016279070492018,1.801627809526815,1.8016277101889762,1.8016276090018803,1.8016275059310907,1.8016274009415296,1.8016272939974673,1.8016271850625067,1.8016270740995735,1.801626961070901,1.8016268459380207,1.801626728661746,1.8016266092021604,1.8016264875186043,1.8016263635696594,1.8016262373131358,1.8016261087060568,1.8016259777046464,1.801625844264312,1.801625708339628,1.8016255698843253,1.8016254288512705,1.801625285192452,1.8016251388589632,1.801624989800987,1.8016248379677755,1.801624683307637,1.8016245257679155,1.8016243652949733,1.8016242018341726,1.8016240353298565,1.8016238657253323,1.801623692962849,1.8016235169835786,1.8016233377275976,1.801623155133864,1.8016229691401981,1.801622779683261,1.8016225866985323,1.8016223901202895,1.8016221898815823,1.8016219859142155,1.8016217781487187,1.8016215665143276,1.8016213509389578,1.801621131349181,1.8016209076701986,1.8016206798258174,1.8016204477384232,1.8016202113289541,1.801619970516873,1.801619725220141,1.801619475355188,1.801619220836886,1.8016189615785192,1.8016186974917525,1.8016184284866044,1.8016181544714125,1.8016178753528076,1.801617591035675,1.8016173014231274,1.8016170064164694,1.8016167059151629,1.8016163998167947,1.8016160880170407,1.8016157704096294,1.8016154468863064,1.801615117336797,1.8016147816487682,1.8016144397077911,1.8016140913973004,1.801613736598556,1.8016133751906012,1.8016130070502214,1.8016126320519021,1.8016122500677867,1.8016118609676308,1.80161146461876,1.8016110608860225,1.801610649631743,1.801610230715678,1.8016098039949633,1.8016093693240702,1.8016089265547508,1.8016084755359916,1.8016080161139574,1.8016075481319438,1.8016070714303183,1.8016065858464696,1.8016060912147496,1.8016055873664172,1.8016050741295822,1.8016045513291437,1.801604018786733,1.8016034763206497,1.801602923745802,1.801602360873643,1.801601787512103,1.8016012034655302,1.801600608534618,1.8016000025163386,1.8015993852038747,1.8015987563865485,1.8015981158497494,1.8015974633748595,1.8015967987391814,1.8015961217158611,1.80159543207381,1.801594729577627,1.8015940139875164,1.8015932850592111,1.8015925425438826,1.8015917861880615,1.8015910157335484,1.8015902309173286,1.8015894314714795,1.8015886171230824,1.8015877875941286,1.8015869426014248,1.8015860818564988,1.8015852050654981,1.8015843119290944,1.8015834021423809,1.8015824753947676,1.8015815313698798,1.8015805697454466,1.801579590193197,1.8015785923787466,1.8015775759614858,1.8015765405944653,1.8015754859242794,1.8015744115909484,1.8015733172277955,1.801572202461328,1.80157106691111,1.8015699101896356,1.8015687319022005,1.8015675316467705,1.8015663090138483,1.801565063586339,1.8015637949394072,1.8015625026403443,1.8015611862484209,1.801559845314742,1.801558479382102,1.8015570879848333,1.8015556706486557,1.8015542268905205,1.8015527562184546,1.8015512581314013,1.8015497321190568,1.8015481776617093,1.801546594230068,1.801544981285097,1.8015433382778423,1.8015416646492566,1.8015399598300252,1.8015382232403834,1.8015364542899353,1.8015346523774705,1.8015328168907767,1.8015309472064487,1.8015290426896984,1.8015271026941586,1.8015251265616858,1.801523113622163,1.801521063193295,1.801518974580405,1.8015168470762284,1.801514679960702,1.8015124725007563,1.8015102239500966,1.8015079335489914,1.8015056005240533,1.801503224088018,1.8015008034395241,1.8014983377628873,1.801495826227876,1.8014932679894848,1.8014906621877034,1.801488007947288,1.801485304377528,1.8014825505720158,1.8014797456084086,1.8014768885481978,1.8014739784364713,1.8014710143016757,1.8014679951553845,1.8014649199920547,1.8014617877887984,1.8014585975051387,1.801455348082779,1.8014520384453678,1.801448667498262,1.801445234128298,1.8014417372035603,1.801438175573152,1.8014345480669702,1.8014308534954842,1.8014270906495131,1.8014232583000127,1.8014193551978614,1.8014153800736552,1.8014113316375049,1.80140720857884,1.80140300956622,1.801398733247149,1.8013943782479032,1.8013899431733615,1.8013854266068483,1.8013808271099854,1.801376143222551,1.8013713734623538,1.8013665163251191,1.8013615702843842,1.801356533791411,1.8013514052751116,1.8013461831419888,1.8013408657760974,1.8013354515390183,1.801329938769854,1.8013243257852454,1.8013186108794061,1.8013127923241856,1.801306868369149,1.8013008372416888,1.8012946971471582,1.8012884462690375,1.8012820827691263,1.801275604787769,1.8012690104441131,1.8012622978364023,1.8012554650423085,1.8012485101192954,1.8012414311050315,1.8012342260178384,1.8012268928571873,1.8012194296042405,1.801211834222439,1.8012041046581477,1.801196238841346,1.801188234686378,1.80118009009276,1.8011718029460477,1.8011633711187667,1.8011547924714073,1.8011460648534887,1.8011371861046932,1.8011281540560746,1.801118966531342,1.801109621348225,1.8011001163199176,1.8010904492566113,1.801080617967116,1.8010706202605684,1.8010604539482415,1.8010501168454476,1.8010396067735435,1.8010289215620419,1.8010180590508296,1.8010070170924968,1.800995793554785,1.8009843863231454,1.8009727933034314,1.800961012424703,1.8009490416421723,1.8009368789402755,1.8009245223358836,1.8009119698816514,1.8008992196695133,1.8008862698343249,1.8008731185576565,1.8008597640717408,1.8008462046635807,1.8008324386792194,1.8008184645281735,1.8008042806880415,1.8007898857092814,1.8007752782201643,1.8007604569319138,1.8007454206440243,1.8007301682497678,1.8007146987418954,1.8006990112185275,1.8006831048892438,1.8006669790813772,1.8006506332465044,1.8006340669671532,1.8006172799637086,1.8006002721015406,1.8005830433983432,1.8005655940316938,1.8005479243468288,1.8005300348646482,1.8005119262899416,1.8004935995198434,1.8004750556525164,1.800456295996067,1.800437322077693,1.800418135653068,1.800398738715958,1.8003791335080779,1.8003593225291852,1.800339308547409,1.800319094609826,1.800298684053266,1.8002780805153655,1.800257287945855,1.8002363106180903,1.8002151531408181,1.8001938204701804,1.8001723179219584,1.800150651184047,1.800128826329166,1.8001068498278023,1.800084728561379,1.800062469835656,1.8000400813943476,1.8000175714329683,1.7999949486128894,1.7999722220756116,1.7999494014572461,1.799926496903201,1.799903519083063,1.799880479205677,1.7998573890344152,1.7998342609026183,1.7998111077292271,1.7997879430345676,1.7997647809563062,1.7997416362655558,1.7997185243831229,1.7996954613958942,1.7996724640733497,1.799649549884186,1.799626737013055,1.7996040443773922,1.7995814916443287,1.7995590992476793,1.799536888404987,1.7995148811346136,1.7994931002728696,1.799471569491156,1.7994503133131166,1.799429357131779,1.7994087272266635,1.7993884507808633,1.7993685558980503,1.799349071619416,1.7993300279405122,1.7993114558279806,1.7992933872361512,1.7992758551234875,1.7992588934688616,1.7992425372876346,1.7992268226475272,1.7992117866842503,1.7991974676168794,1.7991839047629508,1.7991711385532512,1.7991592105462781,1.7991481634423552,1.7991380410973625,1.799128888536074,1.7991207519650618,1.7991136787851534,1.7991077176034078,1.7991029182445888,1.7990993317621098,1.7990970104484176,1.7990960078447984,1.7990963787505674,1.7990981792316298,1.7991014666283702,1.7991062995628608,1.7991127379453469,1.799120842979993,1.7991306771698636,1.7991423043211054,1.7991557895463182,1.799171199267077,1.7991886012155889,1.7992080644354649,1.7992296592815713,1.799253457418954,1.7992795318208015,1.799307956765439,1.7993388078323236,1.799372161897028,1.7994080971251984,1.7994466929654678,1.7994880301413054,1.799532190641805,1.799579257711382,1.7996293158383854,1.7996824507426046,1.7997387493616714,1.799798299836353,1.799861191494726,1.7999275148352374,1.7999973615086478,1.8000708242988641,1.8001479971026608,1.800228974908298,1.800313853773045,1.8004027307996167,1.8004957041115404,1.8005928728274534,1.8006943370343629,1.8008001977598742,1.8009105569434083,1.8010255174064305,1.8011451828217098,1.8012696576816387,1.801399047265627,1.8015334576066049,1.8016729954566604,1.8018177682518386,1.8019678840761317,1.8021234516246896,1.8022845801662857,1.8024513795050652,1.8026239599416063,1.8028024322333334,1.8029869075543046,1.8031774974544166,1.8033743138180436,1.8035774688221564,1.8037870748939424,1.8040032446679592,1.804226090942848,1.8044557266376358,1.8046922647476515,1.804935818300078,1.8051865003091656,1.8054444237311247,1.8057097014187125,1.8059824460755367,1.8062627702100849,1.8065507860894883,1.806846605693031,1.8071503406654104,1.8074621022697475,1.807782001340347,1.8081101482352067,1.8084466527882648,1.8087916242613746,1.8091451712959985,1.8095074018645978,1.8098784232217022,1.810258341854636,1.810647263433873,1.8110452927629928,1.8114525337282068,1.8118690892474167,1.8122950612187732,1.8127305504686886,1.8131756566992685,1.8136304784351174,1.814095112969463,1.814569656309573,1.8150542031213934,1.8155488466733773,1.8160536787794446,1.8165687897410252,1.8170942682881446,1.817630201519488,1.8181766748414112,1.81873377190584,1.8193015745470233,1.8198801627170906,1.8204696144203796,1.8210700056464966,1.8216814103020778,1.8223039001412242,1.8229375446945855,1.8235824111970758,1.82423856451421,1.8249060670670556,1.8255849787557954,1.8262753568819114,1.8269772560690105,1.8276907281823032,1.8284158222467763,1.8291525843640968,1.8299010576282897,1.8306612820402608,1.831433294421218,1.832217128325076,1.8330128139499384,1.8338203780487377,1.8346398438391602,1.835471230912967,1.8363145551448343,1.8371698286008684,1.8380370594469284,1.8389162518569353,1.8398074059213219,1.8407105175558196,1.8416255784107607,1.8425525757811076,1.843491492517406,1.8444423069378943,1.8454049927419824,1.8463795189253394,1.847365849696829,1.8483639443975408,1.8493737574221645,1.8503952381429698,1.851428330836643,1.852472974614254,1.853529103354609,1.8545966456412606,1.8556755247034395,1.856765658361172,1.8578669589748433,1.858979333399479,1.8601026829439768,1.8612369033355654,1.8623818846897116,1.8635375114857264,1.8647036625482907,1.865880211035118,1.867067024430965,1.8682639645481842,1.8694708875339932,1.870687643884641,1.8719140784666186,1.8731500305450488,1.8743953338193828,1.8756498164665045,1.8769133011913282,1.8781856052849615,1.8794665406904747,1.880755914076311,1.8820535269173482,1.883359175583589,1.8846726514364622,1.8859937409326695,1.8873222257355096,1.8886578828335832,1.8900004846667608,1.8913497992592838,1.8927055903598262,1.894067617588361,1.8954356365896143,1.8968093991929094,1.89818865357815,1.899573144447705,1.900962613203916,1.9023567981319514,1.9037554345876968,1.9051582551903807,1.906564990019593,1.9079753668163701,1.9093891111879897,1.9108059468161134,1.9122255956679117,1.9136477782097965,1.9150722136233722,1.9164986200232217,1.9179267146761387,1.9193562142214073,1.9207868348917374,1.9222182927344627,1.9236503038326016,1.9250825845254012,1.9265148516279658,1.9279468226495977,1.9293782160104709,1.9308087512562617,1.9322381492703966,1.9336661324835418,1.9350924250800094,1.9365167532007446,1.9379388451425807,1.9393584315534487,1.9407752456232579,1.942189023270163,1.9435995033219604,1.9450064276923604,1.946409541551906,1.9478085934933178,1.9492033356910714,1.9505935240550143,1.9519789183778602,1.9533592824764099,1.9547343843263643,1.9561039961906086,1.957467894740874,1.958825861172686,1.9601776813135383,1.9615231457242335,1.9628620497933633,1.9641941938249008,1.965519383118904,1.9668374280453411,1.9681481441110569,1.969451352019927,1.970746877726239,1.9720345524813765,1.9733142128738788,1.9745857008629555,1.9758488638055696,1.9771035544771878,1.9783496310863193,1.9795869572829772,1.9808154021611903,1.9820348402557142,1.9832451515330982,1.9844462213772587,1.985637940569728,1.9868202052647466,1.9879929169593777,1.9891559824588128,1.9903093138370658,1.991452828393224,1.992586448603454,1.9937101020689538,1.9948237214600266,1.9959272444564906,1.9970206136845938,1.9981037766506413,1.9991766856715167,2.000239297802295,2.001291574761125,2.002333482851583,2.0033649928826613,2.0043860800865976,2.005396724034703,2.006396908551377,2.007386621626479,2.008365855326224,2.0093346057027652,2.010292872702636,2.0112406600741917,2.0121779752742097,2.013104829373811,2.0140212369638206,2.0149272160597205]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[1.7397196222167273,1.739719621839663,1.7397196214555821,1.7397196210643529,1.7397196206658432,1.7397196202599172,1.7397196198464369,1.739719619425262,1.7397196189962487,1.7397196185592514,1.7397196181141215,1.7397196176607086,1.7397196171988567,1.7397196167284106,1.739719616249209,1.7397196157610897,1.7397196152638867,1.7397196147574303,1.7397196142415488,1.7397196137160675,1.7397196131808061,1.7397196126355847,1.7397196120802156,1.739719611514512,1.7397196109382802,1.7397196103513248,1.7397196097534466,1.7397196091444418,1.7397196085241036,1.7397196078922206,1.7397196072485788,1.7397196065929588,1.7397196059251374,1.7397196052448884,1.7397196045519805,1.7397196038461769,1.7397196031272388,1.7397196023949213,1.7397196016489753,1.7397196008891478,1.7397196001151798,1.7397195993268089,1.7397195985237661,1.739719597705779,1.739719596872569,1.7397195960238536,1.7397195951593438,1.7397195942787456,1.7397195933817595,1.739719592468081,1.7397195915373997,1.7397195905893978,1.7397195896237538,1.7397195886401402,1.7397195876382219,1.7397195866176576,1.7397195855781011,1.7397195845191993,1.7397195834405914,1.7397195823419112,1.7397195812227844,1.7397195800828316,1.7397195789216644,1.7397195777388887,1.7397195765341016,1.7397195753068946,1.7397195740568494,1.7397195727835413,1.7397195714865374,1.7397195701653976,1.7397195688196714,1.739719567448902,1.739719566052624,1.7397195646303611,1.7397195631816313,1.7397195617059407,1.739719560202789,1.739719558671664,1.7397195571120458,1.7397195555234044,1.739719553905199,1.73971955225688,1.7397195505778869,1.7397195488676491,1.7397195471255846,1.7397195453511016,1.7397195435435966,1.7397195417024554,1.7397195398270517,1.7397195379167487,1.7397195359708961,1.7397195339888332,1.739719531969885,1.739719529913366,1.739719527818577,1.739719525684806,1.7397195235113272,1.7397195212974026,1.739719519042278,1.7397195167451882,1.739719514405352,1.7397195120219733,1.7397195095942426,1.7397195071213347,1.7397195046024083,1.739719502036607,1.7397194994230598,1.7397194967608771,1.7397194940491543,1.7397194912869693,1.7397194884733835,1.7397194856074407,1.7397194826881661,1.739719479714567,1.7397194766856339,1.7397194736003359,1.7397194704576253,1.7397194672564327,1.7397194639956708,1.7397194606742312,1.739719457290984,1.7397194538447796,1.7397194503344469,1.7397194467587929,1.7397194431166012,1.7397194394066349,1.739719435627632,1.7397194317783082,1.7397194278573556,1.7397194238634404,1.7397194197952064,1.73971941565127,1.7397194114302224,1.73971940713063,1.7397194027510303,1.739719398289935,1.739719393745828,1.7397193891171647,1.7397193844023722,1.7397193795998473,1.7397193747079585,1.7397193697250422,1.7397193646494058,1.739719359479324,1.7397193542130391,1.7397193488487621,1.7397193433846692,1.7397193378189029,1.7397193321495728,1.7397193263747517,1.7397193204924766,1.7397193145007483,1.7397193083975315,1.7397193021807509,1.7397192958482943,1.7397192893980105,1.7397192828277055,1.7397192761351483,1.7397192693180634,1.739719262374135,1.739719255301003,1.7397192480962638,1.7397192407574684,1.7397192332821245,1.739719225667691,1.7397192179115801,1.7397192100111574,1.739719201963737,1.7397191937665857,1.7397191854169176,1.7397191769118956,1.7397191682486293,1.739719159424176,1.7397191504355367,1.7397191412796587,1.7397191319534289,1.7397191224536808,1.7397191127771852,1.7397191029206558,1.739719092880743,1.7397190826540356,1.7397190722370603,1.7397190616262774,1.7397190508180822,1.7397190398088034,1.7397190285947002,1.7397190171719636,1.7397190055367133,1.7397189936849966,1.7397189816127874,1.739718969315986,1.7397189567904139,1.739718944031817,1.7397189310358625,1.7397189177981354,1.7397189043141392,1.7397188905792942,1.739718876588936,1.7397188623383126,1.7397188478225845,1.7397188330368216,1.739718817976002,1.7397188026350117,1.7397187870086401,1.7397187710915816,1.7397187548784305,1.7397187383636812,1.739718721541726,1.7397187044068527,1.7397186869532433,1.7397186691749718,1.7397186510660017,1.739718632620184,1.7397186138312573,1.739718594692841,1.7397185751984394,1.7397185553414332,1.7397185351150823,1.7397185145125196,1.7397184935267522,1.7397184721506562,1.739718450376976,1.7397184281983207,1.7397184056071626,1.7397183825958347,1.7397183591565268,1.7397183352812837,1.7397183109620036,1.7397182861904328,1.739718260958166,1.7397182352566407,1.7397182090771361,1.7397181824107688,1.739718155248491,1.7397181275810878,1.7397180993991723,1.7397180706931843,1.7397180414533842,1.7397180116698538,1.7397179813324908,1.7397179504310043,1.7397179189549132,1.739717886893542,1.7397178542360174,1.7397178209712636,1.739717787088001,1.7397177525747383,1.7397177174197744,1.7397176816111877,1.7397176451368388,1.7397176079843613,1.7397175701411598,1.7397175315944062,1.7397174923310337,1.7397174523377343,1.7397174116009524,1.7397173701068818,1.7397173278414606,1.739717284790366,1.7397172409390103,1.7397171962725346,1.739717150775806,1.7397171044334105,1.7397170572296496,1.7397170091485323,1.7397169601737739,1.7397169102887862,1.7397168594766752,1.739716807720233,1.7397167550019346,1.7397167013039294,1.7397166466080372,1.739716590895742,1.7397165341481833,1.739716476346154,1.7397164174700896,1.7397163575000658,1.7397162964157882,1.7397162341965882,1.739716170821415,1.7397161062688278,1.7397160405169902,1.7397159735436623,1.7397159053261935,1.7397158358415132,1.7397157650661264,1.7397156929761037,1.7397156195470735,1.7397155447542134,1.7397154685722445,1.739715390975419,1.7397153119375168,1.7397152314318303,1.7397151494311618,1.7397150659078116,1.7397149808335677,1.739714894179699,1.739714805916944,1.7397147160155024,1.7397146244450237,1.739714531174598,1.7397144361727481,1.739714339407413,1.7397142408459445,1.7397141404550915,1.7397140382009917,1.739713934049158,1.7397138279644702,1.7397137199111603,1.7397136098528034,1.739713497752304,1.7397133835718859,1.7397132672730755,1.739713148816695,1.7397130281628455,1.7397129052708935,1.7397127800994616,1.7397126526064115,1.739712522748831,1.739712390483022,1.7397122557644815,1.7397121185478932,1.7397119787871074,1.7397118364351303,1.7397116914441055,1.7397115437652995,1.7397113933490864,1.7397112401449324,1.739711084101377,1.7397109251660194,1.7397107632854982,1.7397105984054784,1.7397104304706297,1.739710259424611,1.7397100852100515,1.7397099077685332,1.7397097270405706,1.739709542965593,1.7397093554819243,1.7397091645267637,1.7397089700361648,1.7397087719450162,1.7397085701870205,1.7397083646946727,1.7397081553992386,1.7397079422307347,1.7397077251179025,1.7397075039881902,1.7397072787677261,1.7397070493812976,1.7397068157523257,1.7397065778028424,1.7397063354534652,1.739706088623373,1.7397058372302792,1.739705581190408,1.7397053204184656,1.739705054827617,1.7397047843294546,1.7397045088339753,1.7397042282495492,1.7397039424828915,1.7397036514390358,1.7397033550213021,1.7397030531312696,1.7397027456687426,1.7397024325317243,1.7397021136163822,1.7397017888170176,1.739701458026033,1.739701121133899,1.7397007780291223,1.7397004285982098,1.7397000727256358,1.7396997102938068,1.739699341183025,1.739698965271454,1.7396985824350815,1.7396981925476818,1.739697795480777,1.7396973911036027,1.7396969792830639,1.7396965598837002,1.7396961327676428,1.7396956977945754,1.7396952548216915,1.7396948037036553,1.7396943442925557,1.7396938764378662,1.7396933999863993,1.7396929147822624,1.7396924206668156,1.739691917478622,1.7396914050534031,1.7396908832239935,1.7396903518202909,1.7396898106692091,1.7396892595946298,1.7396886984173514,1.7396881269550395,1.739687545022176,1.7396869524300094,1.7396863489864982,1.7396857344962628,1.7396851087605278,1.739684471577071,1.7396838227401659,1.7396831620405273,1.739682489265254,1.7396818041977742,1.7396811066177833,1.739680396301189,1.7396796730200512,1.7396789365425216,1.7396781866327828,1.739677423050989,1.739676645553201,1.7396758538913257,1.7396750478130534,1.7396742270617904,1.7396733913765983,1.7396725404921258,1.7396716741385445,1.7396707920414791,1.7396698939219453,1.739668979496276,1.7396680484760567,1.7396671005680548,1.7396661354741494,1.7396651528912612,1.739664152511281,1.739663134020999,1.739662097102032,1.739661041430749,1.7396599666782009,1.7396588725100444,1.739657758586468,1.7396566245621183,1.7396554700860236,1.739654294801517,1.7396530983461644,1.7396518803516818,1.7396506404438659,1.7396493782425082,1.7396480933613259,1.7396467854078776,1.7396454539834896,1.7396440986831743,1.7396427190955555,1.7396413148027854,1.7396398853804689,1.7396384303975851,1.739636949416407,1.739635441992424,1.7396339076742615,1.7396323460036052,1.7396307565151221,1.7396291387363794,1.7396274921877712,1.7396258163824396,1.7396241108261956,1.7396223750174455,1.7396206084471146,1.7396188105985708,1.7396169809475506,1.7396151189620856,1.7396132241024296,1.7396112958209873,1.739609333562242,1.7396073367626859,1.7396053048507538,1.7396032372467523,1.7396011333627976,1.739598992602748,1.7395968143621439,1.7395945980281435,1.7395923429794677,1.73959004858634,1.739587714210431,1.739585339204809,1.7395829229138853,1.7395804646733672,1.7395779638102151,1.7395754196425968,1.7395728314798489,1.7395701986224383,1.7395675203619316,1.7395647959809626,1.7395620247532066,1.7395592059433562,1.7395563388071043,1.7395534225911264,1.7395504565330717,1.7395474398615562,1.7395443717961605,1.7395412515474342,1.7395380783169012,1.7395348512970763,1.7395315696714821,1.7395282326146742,1.73952483929227,1.739521388860987,1.7395178804686868,1.73951431325442,1.739510686348487,1.7395069988724992,1.739503249939451,1.7394994386537963,1.7394955641115364,1.739491625400314,1.7394876215995165,1.7394835517803862,1.7394794150061408,1.7394752103321052,1.7394709368058487,1.7394665934673357,1.7394621793490836,1.739457693476334,1.739453134867231,1.739448502533016,1.7394437954782287,1.739439012700921,1.7394341531928856,1.7394292159398943,1.7394241999219484,1.7394191041135458,1.7394139274839568,1.7394086689975161,1.7394033276139302,1.7393979022885926,1.7393923919729237,1.739386795614715,1.7393811121584974,1.7393753405459194,1.7393694797161416,1.739363528606253,1.7393574861516934,1.7393513512867038,1.7393451229447876,1.7393388000591876,1.7393323815633874,1.7393258663916225,1.7393192534794186,1.7393125417641382,1.7393057301855577,1.7392988176864514,1.7392918032132036,1.7392846857164366,1.7392774641516575,1.739270137479929,1.739262704668555,1.7392551646917889,1.7392475165315642,1.7392397591782423,1.739231891631383,1.739223912900534,1.739215822006044,1.739207617979895,1.739199299866555,1.739190866723853,1.7391823176238748,1.7391736516538794,1.7391648679172356,1.7391559655343838,1.739146943643812,1.7391378014030585,1.739128537989731,1.7391191526025496,1.7391096444624075,1.7391000128134544,1.739090256924198,1.7390803760886242,1.739070369627339,1.7390602368887287,1.73904997725014,1.7390395901190747,1.7390290749344066,1.7390184311676147,1.7390076583240324,1.7389967559441153,1.738985723604724,1.738974560920423,1.7389632675447946,1.7389518431717703,1.7389402875369708,1.7389286004190643,1.7389167816411368,1.7389048310720743,1.738892748627955,1.7388805342734577,1.7388681880232748,1.7388557099435384,1.7388431001532574,1.738830358825758,1.7388174861901369,1.7388044825327185,1.7387913481985235,1.738778083592737,1.7387646891821866,1.7387511654968244,1.7387375131312133,1.7387237327460154,1.7387098250694866,1.7386957908989717,1.7386816311024016,1.7386673466197948,1.7386529384647573,1.7386384077259853,1.7386237555687705,1.738608983236502,1.7385940920521719,1.7385790834198809,1.738563958826345,1.7385487198424006,1.738533368124512,1.7385179054162803,1.7385023335499517,1.7384866544479307,1.7384708701242908,1.7384549826862932,1.7384389943359053,1.7384229073713269,1.738406724188516,1.7383904472827296,1.7383740792500633,1.7383576227890052,1.7383410807019992,1.7383244558970188,1.7383077513891536,1.7382909703022136,1.7382741158703499,1.7382571914396918,1.7382402004700095,1.7382231465363955,1.7382060333309775,1.7381888646646548,1.7381716444688688,1.7381543767974064,1.73813706582824,1.7381197158654091,1.738102331340943,1.7380849168168286,1.7380674769870341,1.7380500166795751,1.7380325408586472,1.7380150546268143,1.7379975632272602,1.7379800720461103,1.7379625866148207,1.7379451126126484,1.7379276558691958,1.7379102223670424,1.7378928182444588,1.7378754497982198,1.7378581234865036,1.7378408459319021,1.7378236239245228,1.7378064644252096,1.7377893745688664,1.7377723616679042,1.7377554332158047,1.7377385968908077,1.737721860559729,1.7377052322819084,1.7376887203132936,1.7376723331106658,1.7376560793360065,1.7376399678610106,1.7376240077717537,1.7376082083735092,1.7375925791957232,1.7375771299971516,1.7375618707711586,1.737546811751181,1.7375319634163633,1.7375173364973617,1.7375029419823245,1.7374887911230468,1.7374748954413046,1.737461266735369,1.7374479170867039,1.737434858866844,1.7374221047444651,1.7374096676926325,1.7373975609962462,1.73738579825967,1.7373743934145516,1.7373633607278374,1.737352714809972,1.7373424706232974,1.7373326434906406,1.7373232491040926,1.7373143035339855,1.7373058232380565,1.7372978250708095,1.737290326293063,1.7372833445816995,1.7372768980395952,1.7372710052057498,1.7372656850656003,1.7372609570615265,1.7372568411035454,1.7372533575801934,1.7372505273695902,1.7372483718506972,1.7372469129147468,1.7372461729768687,1.7372461749878838,1.7372469424462884,1.737248499410409,1.7372508705107386,1.737254080962443,1.7372581565780438,1.7372631237802683,1.7372690096150707,1.737275841764818,1.73728364856164,1.73729245900094,1.7373023027550654,1.7373132101871327,1.7373252123650036,1.7373383410754129,1.7373526288382393,1.7373681089209205,1.7373848153530018,1.7374027829408194,1.7374220472823143,1.73744264478196,1.73746461266582,1.7374879889967028,1.7375128126894304,1.7375391235261963,1.7375669621720213,1.7375963701902772,1.7376273900582975,1.7376600651830334,1.7376944399167773,1.737730559572913,1.7377684704417011,1.7378082198060818,1.7378498559574713,1.7378934282115517,1.7379389869240314,1.7379865835063584,1.7380362704413708,1.7380881012988678,1.7381421307510792,1.738198414588011,1.7382570097326477,1.7383179742559902,1.7383813673918995,1.7384472495517238,1.7385156823386845,1.7385867285619887,1.7386604522506455,1.7387369186669475,1.7388161943195954,1.7388983469764256,1.7389834456767168,1.7390715607430276,1.739162763792542,1.7392571277478823,1.7393547268473495,1.7394556366545524,1.7395599340673955,1.739667697326371,1.739779006022128,1.7398939411022694,1.7400125848773362,1.7401350210259425,1.7402613345990097,1.7403916120230642,1.7405259411025513,1.7406644110211278,1.7408071123418813,1.7409541370064414,1.741105578332937,1.7412615310127546,1.741422091106061,1.7415873560360415,1.7417574245818213,1.7419323968700224,1.7421123743649156,1.742297459857137,1.7424877574509219,1.7426833725498239,1.7428844118408795,1.7430909832771877,1.7433031960588652,1.7435211606123484,1.7437449885680063,1.7439747927360387,1.7442106870806233,1.7444527866922892,1.7447012077584865,1.7449560675323268,1.7452174842994663,1.7454855773431124,1.7457604669071265,1.7460422741572033,1.7463311211401016,1.7466271307409127,1.7469304266383392,1.7472411332579671,1.7475593757235217,1.7478852798060736,1.7482189718711891,1.7485605788240062,1.74891022805222,1.7492680473669544,1.7496341649415161,1.7500087092480066,1.7503918089917814,1.7507835930437385,1.7511841903704268,1.7515937299619548,1.752012340757687,1.7524401515697186,1.7528772910041137,1.7533238873798893,1.7537800686457463,1.7542459622945241,1.754721695275379,1.7552073939036743,1.7557031837685768,1.7562091896383585,1.7567255353633957,1.7572523437768723,1.7577897365931845,1.758337834304062,1.7588967560724031,1.759466619623851,1.7600475411361216,1.7606396351261056,1.761243014334781,1.7618577896099614,1.762484069786921,1.7631219615669542,1.7637715693939076,1.764432995328759,1.7651063389223034,1.7657916970860328,1.766489163961291,1.767198830786796,1.7679207857646482,1.7686551139249254,1.7694018969890013,1.7701612132317217,1.7709331373425774,1.7717177402860516,1.77251508916129,1.7733252470612888,1.7741482729317934,1.7749842214300995,1.7758331427839888,1.7766950826510126,1.77757008197837,1.7784581768636212,1.7793593984165033,1.7802737726221078,1.7812013202057084,1.7821420564995167,1.7830959913116697,1.7840631287977458,1.7850434673351225,1.7860369994004934,1.7870437114508644,1.7880635838083494,1.7890965905491092,1.7901426993967424,1.791201871620484,1.7922740619385233,1.7933592184267824,1.7944572824334832,1.7955681884998196,1.7966918642870569,1.7978282305103754,1.7989772008797513,1.8001386820481802,1.8013125735675202,1.80249876785223,1.8036971501512589,1.804907598528339,1.8061299838508982,1.8073641697878173,1.8086100128162155,1.8098673622374442,1.8111360602024449,1.812415941746601,1.8137068348341916,1.8150085604125479,1.8163209324759622,1.8176437581394023,1.818976837722037,1.820319964840573,1.8216729265123621,1.8230355032682155,1.82440746927485,1.825788592466841,1.827178634687951,1.8285773518416681,1.8299844940507668,1.8313998058256764,1.8328230262414134,1.8342538891228286,1.83569212323787,1.8371374524985649,1.8385895961693866,1.8400482690826623,1.8415131818606483,1.8429840411438976,1.8444605498255044,1.8459424072908226,1.8474293096622127,1.8489209500483876,1.8504170187978903,1.8519172037562472,1.8534211905263178,1.8549286627313697,1.8564393022803878,1.857952789635134,1.8594688040784648,1.8609870239834256,1.8625071270826188,1.864028790737381,1.8655516922062607,1.8670755089123539,1.8685999187089977,1.870124600143383,1.871649232717633,1.8731734971468923,1.8746970756140289,1.8762196520205066,1.8777409122330466,1.8792605443256862,1.8807782388168737,1.88229368890124,1.8838065906757209,1.885316643359713,1.8868235495089678,1.888327015222948,1.8898267503453883,1.891322468657826,1.8928138880658876,1.89430073077813,1.8957827234772664,1.8972595974836224,1.8987310889106892,1.9001969388126556,1.9016568933238351,1.9031107037899053,1.904558126890913,1.90599892475601,1.9074328650699008,1.9088597211710054,1.9102792721413648,1.9116913028883158,1.913095604217997,1.914491972900758,1.9158802117285436,1.9172601295643723,1.9186315413840074,1.919994268309949,1.9213481376378965,1.9226929828558155,1.9240286436557832,1.925354965938776,1.9266718018125797,1.9279790095830112,1.9292764537386486,1.9305640049292665,1.9318415399381923,1.9331089416487879,1.9343660990052858,1.9356129069681858,1.9368492664644534,1.9380750843327377,1.9392902732638329,1.940494751736629,1.9416884439497637,1.9428712797492214,1.9440431945520955,1.9452041292667543,1.946354030209629,1.9474928490188514,1.9486205425649654,1.9497370728589276,1.9508424069576145,1.951936516867051,1.9530193794435622,1.9540909762930583,1.9551512936686486,1.9562003223667788,1.9572380576220791,1.9582644990011155,1.959279650295209,1.9602835194125032,1.9612761182694547,1.9622574626818894,1.9632275722557975]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.7358155016537276,1.735815500535595,1.7358154993966544,1.7358154982365177,1.735815497054791,1.735815495851072,1.7358154946249522,1.7358154933760142,1.7358154921038333,1.7358154908079773,1.7358154894880051,1.7358154881434684,1.7358154867739102,1.7358154853788639,1.7358154839578561,1.7358154825104035,1.7358154810360136,1.7358154795341854,1.7358154780044082,1.7358154764461617,1.7358154748589163,1.7358154732421327,1.7358154715952607,1.7358154699177402,1.7358154682090012,1.7358154664684626,1.7358154646955328,1.7358154628896088,1.7358154610500767,1.7358154591763109,1.7358154572676745,1.7358154553235186,1.735815453343182,1.7358154513259916,1.7358154492712614,1.7358154471782925,1.735815445046374,1.735815442874781,1.7358154406627742,1.7358154384096025,1.7358154361144997,1.7358154337766851,1.7358154313953642,1.7358154289697272,1.7358154264989492,1.7358154239821904,1.7358154214185955,1.7358154188072918,1.7358154161473927,1.735815413437993,1.735815410678172,1.7358154078669914,1.7358154050034946,1.7358154020867091,1.7358153991156429,1.7358153960892855,1.7358153930066087,1.7358153898665634,1.7358153866680828,1.7358153834100791,1.7358153800914449,1.7358153767110516,1.7358153732677497,1.7358153697603693,1.7358153661877174,1.7358153625485795,1.7358153588417182,1.7358153550658735,1.735815351219761,1.7358153473020739,1.7358153433114798,1.735815339246622,1.7358153351061187,1.735815330888562,1.7358153265925182,1.7358153222165262,1.735815317759099,1.7358153132187208,1.7358153085938477,1.7358153038829078,1.7358152990842992,1.7358152941963907,1.7358152892175205,1.7358152841459955,1.7358152789800918,1.7358152737180532,1.7358152683580905,1.7358152628983818,1.7358152573370704,1.7358152516722662,1.7358152459020428,1.7358152400244389,1.735815234037456,1.7358152279390588,1.735815221727174,1.7358152153996897,1.7358152089544554,1.735815202389279,1.735815195701929,1.7358151888901319,1.735815181951572,1.73581517488389,1.7358151676846836,1.735815160351505,1.735815152881862,1.7358151452732142,1.7358151375229756,1.7358151296285111,1.7358151215871374,1.73581511339612,1.7358151050526751,1.7358150965539656,1.735815087897103,1.735815079079144,1.735815070097091,1.7358150609478904,1.7358150516284325,1.7358150421355487,1.7358150324660118,1.7358150226165348,1.7358150125837697,1.7358150023643055,1.7358149919546682,1.7358149813513197,1.7358149705506551,1.735814959549003,1.735814948342623,1.7358149369277063,1.7358149253003725,1.7358149134566685,1.735814901392569,1.7358148891039729,1.7358148765867023,1.7358148638365032,1.735814850849041,1.7358148376199007,1.7358148241445859,1.735814810418516,1.7358147964370252,1.7358147821953607,1.7358147676886821,1.7358147529120578,1.7358147378604651,1.7358147225287883,1.735814706911816,1.7358146910042394,1.7358146748006518,1.7358146582955452,1.73581464148331,1.7358146243582318,1.7358146069144895,1.7358145891461543,1.7358145710471866,1.735814552611435,1.7358145338326336,1.7358145147043995,1.7358144952202315,1.7358144753735074,1.7358144551574812,1.7358144345652826,1.735814413589913,1.735814392224243,1.735814370461011,1.7358143482928212,1.735814325712139,1.73581430271129,1.7358142792824576,1.7358142554176792,1.7358142311088445,1.735814206347692,1.735814181125807,1.7358141554346178,1.735814129265394,1.7358141026092415,1.7358140754571028,1.7358140477997501,1.7358140196277854,1.7358139909316346,1.7358139617015464,1.735813931927588,1.7358139015996423,1.7358138707074033,1.7358138392403737,1.7358138071878613,1.7358137745389743,1.7358137412826196,1.7358137074074964,1.735813672902095,1.735813637754691,1.7358136019533423,1.7358135654858848,1.7358135283399279,1.7358134905028508,1.735813451961798,1.735813412703676,1.735813372715146,1.735813331982622,1.735813290492267,1.7358132482299844,1.7358132051814168,1.7358131613319399,1.735813116666657,1.7358130711703947,1.7358130248276984,1.7358129776228248,1.7358129295397393,1.7358128805621085,1.7358128306732954,1.735812779856354,1.7358127280940239,1.7358126753687226,1.735812621662542,1.735812566957241,1.7358125112342395,1.7358124544746112,1.7358123966590795,1.7358123377680086,1.7358122777813987,1.7358122166788774,1.7358121544396947,1.7358120910427146,1.735812026466408,1.7358119606888471,1.7358118936876952,1.735811825440202,1.7358117559231934,1.7358116851130656,1.7358116129857761,1.7358115395168356,1.735811464681301,1.7358113884537643,1.7358113108083466,1.735811231718688,1.7358111511579395,1.7358110690987532,1.7358109855132733,1.7358109003731268,1.735810813649414,1.7358107253126982,1.735810635332997,1.7358105436797704,1.7358104503219112,1.7358103552277364,1.7358102583649735,1.735810159700751,1.7358100592015877,1.73580995683338,1.7358098525613934,1.7358097463502462,1.7358096381639019,1.7358095279656545,1.7358094157181172,1.7358093013832092,1.7358091849221435,1.7358090662954133,1.7358089454627788,1.7358088223832535,1.7358086970150914,1.7358085693157712,1.7358084392419837,1.7358083067496164,1.735808171793738,1.7358080343285847,1.735807894307544,1.7358077516831392,1.7358076064070127,1.7358074584299115,1.735807307701669,1.7358071541711884,1.7358069977864266,1.735806838494375,1.7358066762410436,1.7358065109714416,1.735806342629559,1.7358061711583481,1.7358059964997046,1.735805818594448,1.7358056373823014,1.7358054528018714,1.7358052647906272,1.7358050732848815,1.7358048782197661,1.735804679529213,1.7358044771459307,1.7358042710013815,1.7358040610257603,1.735803847147969,1.735803629295595,1.7358034073948845,1.7358031813707204,1.7358029511465949,1.7358027166445862,1.73580247778533,1.7358022344879955,1.7358019866702572,1.7358017342482668,1.735801477136626,1.7358012152483586,1.735800948494881,1.7358006767859726,1.7358004000297447,1.7358001181326128,1.7357998309992626,1.7357995385326195,1.7357992406338165,1.7357989372021616,1.7357986281351032,1.7357983133281971,1.7357979926750713,1.7357976660673913,1.7357973333948238,1.735796994545,1.735796649403479,1.7357962978537091,1.73579593977699,1.7357955750524334,1.7357952035569222,1.7357948251650712,1.7357944397491856,1.735794047179217,1.735793647322723,1.7357932400448217,1.735792825208149,1.7357924026728115,1.735791972296342,1.7357915339336516,1.7357910874369833,1.735790632655862,1.7357901694370468,1.7357896976244784,1.7357892170592313,1.7357887275794583,1.7357882290203401,1.7357877212140305,1.735787203989601,1.7357866771729857,1.7357861405869255,1.7357855940509077,1.7357850373811097,1.7357844703903382,1.7357838928879683,1.7357833046798816,1.7357827055684036,1.735782095352239,1.735781473826407,1.735780840782175,1.7357801960069905,1.7357795392844129,1.7357788703940438,1.7357781891114556,1.735777495208119,1.7357767884513307,1.7357760686041368,1.7357753354252579,1.7357745886690115,1.7357738280852337,1.7357730534191977,1.7357722644115348,1.7357714607981485,1.7357706423101336,1.7357698086736877,1.735768959610026,1.735768094835292,1.735767214060467,1.7357663169912798,1.7357654033281122,1.7357644727659056,1.7357635249940637,1.735762559696355,1.7357615765508134,1.735760575229637,1.7357595553990852,1.7357585167193743,1.7357574588445712,1.7357563814224855,1.735755284094559,1.7357541664957554,1.7357530282544456,1.7357518689922937,1.7357506883241391,1.735749485857877,1.7357482611943384,1.7357470139271676,1.735745743642696,1.7357444499198154,1.7357431323298507,1.735741790436427,1.735740423795338,1.7357390319544097,1.7357376144533647,1.7357361708236811,1.735734700588452,1.735733203262242,1.73573167835094,1.7357301253516115,1.7357285437523493,1.7357269330321183,1.7357252926606026,1.735723622098046,1.735721920795094,1.73572018819263,1.735718423721611,1.7357166268029014,1.7357147968471012,1.7357129332543757,1.7357110354142806,1.735709102705583,1.7357071344960848,1.7357051301424362,1.735703088989955,1.735701010372435,1.7356988936119584,1.7356967380187016,1.73569454289074,1.7356923075138488,1.7356900311613026,1.7356877130936719,1.7356853525586144,1.7356829487906684,1.7356805010110383,1.7356780084273802,1.7356754702335848,1.735672885609555,1.7356702537209836,1.7356675737191272,1.7356648447405754,1.735662065907019,1.7356592363250165,1.7356563550857536,1.735653421264805,1.7356504339218874,1.735647392100617,1.735644294828256,1.7356411411154622,1.7356379299560336,1.7356346603266495,1.7356313311866105,1.7356279414775733,1.7356244901232853,1.7356209760293133,1.7356173980827732,1.7356137551520527,1.735610046086536,1.7356062697163197,1.7356024248519322,1.7355985102840472,1.7355945247831934,1.7355904670994655,1.735586335962228,1.7355821300798207,1.735577848139259,1.7355734888059329,1.7355690507233033,1.7355645325125955,1.735559932772492,1.7355552500788218,1.7355504829842487,1.7355456300179544,1.7355406896853247,1.7355356604676315,1.7355305408217099,1.735525329179639,1.7355200239484185,1.7355146235096435,1.7355091262191773,1.7355035304068276,1.7354978343760137,1.7354920364034412,1.7354861347387687,1.735480127604279,1.7354740131945472,1.735467789676108,1.7354614551871252,1.7354550078370583,1.7354484457063308,1.735441766846,1.7354349692774238,1.7354280509919306,1.7354210099504919,1.7354138440833908,1.735406551289897,1.735399129437941,1.7353915763637893,1.7353838898717235,1.7353760677337224,1.7353681076891423,1.7353600074444058,1.7353517646726895,1.7353433770136184,1.73533484207296,1.7353261574223275,1.7353173205988834,1.7353083291050493,1.735299180408221,1.7352898719404883,1.7352804010983618,1.735270765242505,1.7352609616974721,1.7352509877514557,1.7352408406560373,1.7352305176259504,1.7352200158388489,1.7352093324350846,1.7351984645174947,1.7351874091511974,1.7351761633633993,1.735164724143212,1.7351530884414794,1.7351412531706194,1.7351292152044722,1.7351169713781678,1.735104518488,1.7350918532913209,1.735078972506441,1.7350658728125543,1.7350525508496688,1.7350390032185596,1.7350252264807358,1.7350112171584247,1.7349969717345737,1.7349824866528711,1.734967758317784,1.7349527830946185,1.7349375573095966,1.7349220772499565,1.7349063391640736,1.7348903392616026,1.7348740737136432,1.7348575386529288,1.7348407301740376,1.7348236443336318,1.7348062771507178,1.7347886246069357,1.7347706826468723,1.734752447178404,1.7347339140730638,1.7347150791664412,1.7346959382586067,1.7346764871145686,1.7346567214647575,1.7346366370055448,1.7346162293997889,1.734595494277416,1.7345744272360297,1.734553023841559,1.7345312796289336,1.7345091901027976,1.7344867507382533,1.7344639569816451,1.734440804251373,1.7344172879387452,1.734393403408865,1.734369146001555,1.7343445110323168,1.7343194937933275,1.7342940895544743,1.734268293564425,1.7342421010517357,1.7342155072259997,1.734188507279027,1.7341610963860685,1.734133269707073,1.7341050223879844,1.734076349562075,1.7340472463513175,1.734017707867792,1.7339877292151331,1.7339573054900095,1.7339264317836443,1.733895103183368,1.7338633147742093,1.7338310616405188,1.7337983388676288,1.7337651415435453,1.7337314647606774,1.7336973036175929,1.7336626532208106,1.7336275086866233,1.7335918651429476,1.733555717731207,1.7335190616082417,1.7334818919482444,1.7334442039447262,1.733405992812503,1.7333672537897107,1.7333279821398404,1.7332881731537972,1.733247822151979,1.733206924486374,1.7331654755426793,1.733123470742434,1.7330809055451668,1.7330377754505635,1.732994076000641,1.7329498027819397,1.732904951427719,1.7328595176201718,1.7328134970926377,1.73276688563183,1.7327196790800654,1.7326718733374988,1.732623464364363,1.7325744481832095,1.7325248208811523,1.732474578612113,1.7324237175990629,1.7323722341362702,1.7323201245915405,1.732267385408458,1.7322140131086277,1.7321600042939087,1.732105355648653,1.7320500639419363,1.7319941260297897,1.7319375388574307,1.7318802994614892,1.7318224049722386,1.731763852615824,1.7317046397164944,1.7316447636988344,1.731584222090007,1.7315230125219978,1.731461132733868,1.7313985805740215,1.7313353540024823,1.7312714510931886,1.7312068700363057,1.7311416091405618,1.7310756668356078,1.7310090416744068,1.7309417323356588,1.730873737626259,1.7308050564838007,1.7307356879791234,1.7306656313189124,1.730594885848358,1.7305234510538696,1.7304513265658656,1.7303785121616326,1.7303050077682631,1.7302308134656843,1.730155929489774,1.7300803562355787,1.7300040942606376,1.729927144288419,1.7298495072118778,1.7297711840971406,1.7296921761873252,1.7296124849065038,1.729532111863816,1.7294510588577405,1.729369327880532,1.7292869211228308,1.7292038409784567,1.7291200900493882,1.7290356711509423,1.7289505873171553,1.7288648418063792,1.7287784381070945,1.7286913799439514,1.7286036712840467,1.7285153163434384,1.7284263195939107,1.7283366857699938,1.728246419876243,1.7281555271947866,1.7280640132931435,1.7279718840323244,1.7278791455752116,1.7277858043952283,1.7276918672853023,1.7275973413671246,1.727502234100709,1.727406553294254,1.7273103071143128,1.7272135040962713,1.7271161531551347,1.7270182635966271,1.726919845128604,1.7268209078727759,1.726721462376745,1.7266215196263541,1.7265210910583453,1.7264201885733272,1.7263188245490488,1.7262170118539748,1.7261147638611651,1.7260120944624435,1.7259090180828645,1.7258055496954603,1.7257017048362724,1.7255974996196555,1.72549295075385,1.7253880755568143,1.7252828919723104,1.725177418586233,1.7250716746431767,1.7249656800632274,1.7248594554589733,1.7247530221527216,1.724646402193912,1.7245396183767177,1.724432694257818,1.7243256541743388,1.7242185232619383,1.7241113274730362,1.7240040935951677,1.7238968492694489,1.7237896230091454,1.723682444218325,1.7235753432105836,1.7234683512278306,1.7233615004591185,1.7232548240595023,1.7231483561689143,1.7230421319310405,1.7229361875121811,1.7228305601200844,1.722725288022731,1.722620410567064,1.722515968197638,1.7224120024751808,1.722308556095044,1.7222056729055346,1.7221033979261033,1.7220017773653804,1.7219008586390387,1.7218006903874674,1.721701322493244,1.7216028060983828,1.7215051936213446,1.721408538773795,1.7213128965770863,1.7212183233784548,1.7211248768669072,1.7210326160887852,1.72094160146299,1.7208518947958429,1.7207635592955717,1.7206766595864007,1.7205912617222248,1.7205074331998509,1.7204252429717881,1.7203447614585656,1.7202660605605589,1.720189213669308,1.7201142956783018,1.7200413829932135,1.7199705535415635,1.7199018867817908,1.7198354637117088,1.7197713668763293,1.7197096803750254,1.719650489868019,1.7195938825821655,1.719539947316014,1.7194887744441227,1.7194404559206018,1.7193950852818658,1.7193527576485663,1.7193135697266857,1.7192776198077655,1.7192450077682457,1.7192158350678906,1.719190204747276,1.719168221424316,1.7191499912897992,1.719135622101916,1.7191252231797458,1.7191189053956863,1.7191167811667913,1.7191189644450016,1.719125570706237,1.7191367169383276,1.7191525216277606,1.7191731047452163,1.7191985877298712,1.71922909347244,1.7192647462969375,1.7193056719411342,1.7193519975356808,1.7194038515818806,1.7194613639280831,1.7195246657446819,1.7195938894976868,1.7196691689208505,1.7197506389863308,1.7198384358738594,1.719932696938399,1.7200335606762676,1.7201411666897024,1.7202556556498494,1.7203771692581475,1.7205058502060937,1.7206418421333607,1.7207852895842466,1.7209363379624376,1.721095133484057,1.7212618231289778,1.72143655459038,1.7216194762225254,1.7218107369867273,1.7220104863954937,1.722218874454815,1.7224360516045794,1.7226621686570813,1.7228973767336075,1.723141827199068,1.7233956715946526,1.7236590615684806,1.7239321488042254,1.724215084947679,1.7245080215312403,1.724811109896293,1.7251245011134526,1.7254483459006562,1.7257827945390687,1.7261279967867846,1.7264841017903014,1.7268512579937416,1.7272296130458047,1.727619313704432,1.7280205057391638,1.7284333338311804,1.7288579414710095,1.7292944708538966,1.7297430627728305,1.7302038565092241,1.7306769897212508,1.73116259832985,1.7316608164024043,1.7321717760341169,1.7326956072271056,1.7332324377672457,1.7337823930988012,1.734345596196885,1.7349221674377995,1.7355122244673242,1.7361158820670097,1.7367332520185634,1.7373644429664095,1.738009560278523,1.7386687059056452,1.7393419782389925,1.7400294719665985,1.7407312779284179,1.7414474829703486,1.7421781697973375,1.7429234168257368,1.7436832980351045,1.7444578828196444,1.7452472358394937,1.7460514168720833,1.7468704806638033,1.7477044767822167,1.7485534494690815,1.7494174374944431,1.75029647401208,1.7511905864165866,1.7520997962023916,1.7530241188250157,1.7539635635648902,1.7549181333940467,1.755887824846015,1.7568726278892621,1.7578725258045047,1.7588874950662485,1.759917505228888,1.760962518817724,1.7620224912252382,1.763097370612978,1.7641870978193903,1.7652916062739503,1.7664108219179189,1.7675446631320606,1.7686930406716446,1.769855857609043,1.771033009284235,1.7722243832634998,1.773429859306591,1.7746493093426485,1.775882597455104,1.7771295798758109,1.778390104988616,1.779664013342568,1.7809511376749412,1.782251302944228,1.7835643263732328,1.7848900175023779,1.7862281782533045,1.7875786030028324,1.7889410786673121,1.7903153847973778,1.7917012936830847,1.793098570469388,1.7945069732818903,1.7959262533627602,1.7973561552166997,1.798796416766802,1.8002467695201287,1.8017069387427942,1.80317664364433,1.8046555975710732,1.806143508208292,1.8076400777907504,1.8091450033213774,1.8106579767976956,1.8121786854456319,1.8137068119603241,1.81524203475351,1.816784028207072,1.8183324629322968,1.8198870060343864,1.8214473213817537,1.8230130698796214,1.8245839097474261,1.8261594967995312,1.8277394847287407,1.8293235253920985,1.8309112690984604,1.8325023648973173,1.8340964608683559,1.8356932044112355,1.83729224253507,1.8388932221471066,1.8404957903400945,1.8420995946778538,1.8437042834785526,1.8453095060952198,1.8469149131930294,1.8485201570228997,1.8501248916909794,1.8517287734235883,1.853331460827215,1.8549326151431742,1.8565319004965628,1.8581289841391546,1.8597235366859015,1.8613152323447346,1.862903749139364,1.8644887691248173,1.8660699785954518,1.867647068285228,1.8692197335600254,1.8707876746018217,1.8723505965845704,1.8739082098416362,1.8754602300246654,1.8770063782537978,1.8785463812591356,1.8800799715134218,1.8816068873558789,1.8831268731072057,1.8846396791757223,1.8861450621546898,1.8876427849108486,1.8891326166642224,1.890614333059269,1.892087716227467,1.8935525548414374,1.8950086441607301,1.896455786069399,1.8978937891055163,1.8993224684827854,1.9007416461044195,1.9021511505694686,1.903550817171783,1.904940487891812,1.9063200113814476,1.9076892429421215,1.9090480444963802,1.9103962845531648,1.9117338381670186,1.9130605868914705,1.9143764187268157,1.9156812280625493,1.9169749156146871,1.9182573883582192,1.919528559454941,1.9207883481769057,1.9220366798257456,1.9232734856480962,1.924498702747376,1.9257122739921508,1.9269141479213217,1.9281042786463771,1.9292826257509248,1.9304491541877415,1.9316038341735589,1.9327466410817964,1.933877555333465,1.9349965622864382,1.9361036521232986,1.937198819737953,1.9382820646212118,1.9393533907455087,1.940412806448949,1.9414603243188533,1.9424959610749637,1.9435197374524753],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.7669774524069393,1.7669774522627326,1.7669774521158423,1.7669774519662182,1.76697745181381,1.7669774516585652,1.7669774515004317,1.7669774513393552,1.7669774511752814,1.766977451008154,1.7669774508379164,1.7669774506645106,1.7669774504878781,1.7669774503079585,1.7669774501246907,1.7669774499380122,1.7669774497478599,1.7669774495541688,1.7669774493568733,1.7669774491559063,1.7669774489511993,1.7669774487426828,1.7669774485302863,1.7669774483139367,1.7669774480935614,1.7669774478690847,1.7669774476404307,1.7669774474075217,1.7669774471702784,1.7669774469286201,1.766977446682465,1.766977446431729,1.7669774461763268,1.7669774459161716,1.7669774456511755,1.766977445381248,1.7669774451062972,1.7669774448262296,1.7669774445409505,1.7669774442503625,1.766977443954367,1.7669774436528631,1.7669774433457488,1.766977443032919,1.766977442714268,1.7669774423896871,1.766977442059066,1.7669774417222928,1.7669774413792523,1.766977441029828,1.7669774406739016,1.7669774403113516,1.766977439942055,1.7669774395658864,1.7669774391827175,1.7669774387924184,1.7669774383948562,1.7669774379898961,1.7669774375774001,1.7669774371572282,1.7669774367292372,1.766977436293282,1.7669774358492143,1.7669774353968832,1.766977434936135,1.7669774344668125,1.766977433988757,1.7669774335018056,1.7669774330057928,1.7669774325005498,1.7669774319859053,1.766977431461684,1.7669774309277078,1.7669774303837953,1.7669774298297616,1.7669774292654183,1.7669774286905737,1.766977428105032,1.766977427508595,1.7669774269010592,1.7669774262822184,1.7669774256518622,1.7669774250097765,1.766977424355743,1.7669774236895393,1.766977423010939,1.7669774223197114,1.766977421615622,1.7669774208984308,1.7669774201678945,1.7669774194237646,1.7669774186657887,1.7669774178937085,1.766977417107262,1.7669774163061818,1.7669774154901956,1.7669774146590262,1.7669774138123913,1.7669774129500027,1.7669774120715678,1.7669774111767875,1.7669774102653582,1.7669774093369703,1.7669774083913077,1.7669774074280495,1.7669774064468684,1.7669774054474305,1.7669774044293967,1.7669774033924208,1.7669774023361506,1.7669774012602268,1.7669774001642842,1.7669773990479503,1.7669773979108456,1.766977396752584,1.7669773955727714,1.7669773943710076,1.7669773931468842,1.7669773918999847,1.7669773906298858,1.766977389336156,1.7669773880183557,1.766977386676037,1.7669773853087443,1.7669773839160126,1.7669773824973691,1.7669773810523313,1.766977379580409,1.7669773780811016,1.7669773765538999,1.766977374998285,1.7669773734137288,1.7669773717996926,1.7669773701556284,1.7669773684809775,1.7669773667751714,1.7669773650376301,1.766977363267764,1.7669773614649715,1.7669773596286404,1.7669773577581471,1.766977355852856,1.7669773539121205,1.766977351935281,1.7669773499216663,1.7669773478705926,1.766977345781363,1.766977343653268,1.7669773414855854,1.7669773392775787,1.7669773370284982,1.76697733473758,1.7669773324040465,1.7669773300271052,1.7669773276059488,1.7669773251397554,1.7669773226276877,1.7669773200688927,1.7669773174625014,1.7669773148076295,1.7669773121033752,1.7669773093488208,1.7669773065430308,1.766977303685053,1.766977300773917,1.7669772978086349,1.7669772947882,1.7669772917115874,1.7669772885777522,1.7669772853856314,1.7669772821341414,1.7669772788221785,1.766977275448619,1.7669772720123182,1.7669772685121095,1.7669772649468056,1.7669772613151966,1.76697725761605,1.7669772538481112,1.7669772500101015,1.7669772461007187,1.7669772421186367,1.7669772380625046,1.7669772339309464,1.7669772297225608,1.7669772254359202,1.7669772210695707,1.7669772166220314,1.766977212091794,1.7669772074773218,1.7669772027770503,1.7669771979893854,1.7669771931127038,1.7669771881453524,1.7669771830856462,1.7669771779318706,1.7669771726822778,1.7669771673350885,1.7669771618884904,1.7669771563406365,1.7669771506896477,1.7669771449336078,1.7669771390705662,1.7669771330985367,1.766977127015495,1.76697712081938,1.7669771145080932,1.7669771080794954,1.766977101531409,1.7669770948616166,1.7669770880678586,1.7669770811478338,1.7669770740991988,1.7669770669195668,1.766977059606506,1.7669770521575412,1.7669770445701496,1.7669770368417632,1.7669770289697653,1.766977020951492,1.7669770127842286,1.7669770044652118,1.7669769959916262,1.7669769873606045,1.766976978569227,1.766976969614519,1.7669769604934518,1.7669769512029398,1.7669769417398413,1.7669769321009559,1.7669769222830243,1.766976912282727,1.7669769020966835,1.7669768917214503,1.7669768811535205,1.7669768703893227,1.7669768594252195,1.7669768482575063,1.7669768368824101,1.7669768252960885,1.766976813494628,1.766976801474043,1.7669767892302743,1.7669767767591884,1.7669767640565748,1.766976751118146,1.766976737939535,1.7669767245162953,1.7669767108438972,1.766976696917729,1.7669766827330924,1.766976668285204,1.7669766535691922,1.766976638580095,1.7669766233128599,1.766976607762341,1.766976591923298,1.7669765757903941,1.766976559358195,1.7669765426211657,1.7669765255736698,1.7669765082099673,1.7669764905242136,1.7669764725104558,1.7669764541626318,1.7669764354745694,1.7669764164399817,1.7669763970524675,1.7669763773055076,1.7669763571924642,1.766976336706577,1.7669763158409624,1.7669762945886103,1.7669762729423828,1.7669762508950113,1.7669762284390935,1.766976205567093,1.7669761822713344,1.766976158544003,1.7669761343771402,1.7669761097626433,1.7669760846922609,1.7669760591575912,1.7669760331500792,1.766976006661014,1.7669759796815254,1.766975952202582,1.766975924214988,1.7669758957093797,1.7669758666762236,1.7669758371058122,1.7669758069882622,1.7669757763135094,1.7669757450713082,1.766975713251226,1.7669756808426413,1.7669756478347394,1.7669756142165105,1.7669755799767441,1.7669755451040274,1.7669755095867408,1.7669754734130538,1.7669754365709232,1.7669753990480868,1.7669753608320615,1.7669753219101394,1.766975282269382,1.7669752418966185,1.76697520077844,1.7669751589011968,1.766975116250993,1.7669750728136828,1.766975028574866,1.7669749835198842,1.766974937633815,1.7669748909014693,1.7669748433073846,1.7669747948358223,1.7669747454707618,1.7669746951958956,1.7669746439946246,1.7669745918500543,1.7669745387449878,1.7669744846619215,1.7669744295830403,1.7669743734902121,1.7669743163649818,1.7669742581885668,1.7669741989418508,1.7669741386053788,1.7669740771593505,1.7669740145836155,1.7669739508576667,1.7669738859606352,1.766973819871283,1.7669737525679985,1.766973684028789,1.7669736142312746,1.7669735431526827,1.7669734707698406,1.7669733970591692,1.766973321996677,1.7669732455579525,1.7669731677181577,1.7669730884520212,1.766973007733832,1.76697292553743,1.7669728418362025,1.7669727566030735,1.7669726698104986,1.7669725814304564,1.7669724914344414,1.7669723997934559,1.7669723064780034,1.7669722114580797,1.766972114703165,1.7669720161822167,1.7669719158636612,1.7669718137153845,1.766971709704726,1.7669716037984682,1.7669714959628293,1.7669713861634546,1.7669712743654078,1.7669711605331622,1.7669710446305922,1.7669709266209643,1.7669708064669276,1.766970684130506,1.7669705595730885,1.7669704327554192,1.76697030363759,1.7669701721790287,1.7669700383384928,1.766969902074057,1.7669697633431058,1.7669696221023223,1.7669694783076808,1.7669693319144346,1.7669691828771075,1.7669690311494848,1.766968876684602,1.7669687194347357,1.7669685593513937,1.7669683963853051,1.7669682304864098,1.7669680616038497,1.766967889685957,1.7669677146802463,1.766967536533402,1.7669673551912721,1.7669671705988534,1.7669669827002863,1.7669667914388414,1.7669665967569108,1.766966398595999,1.766966196896712,1.7669659915987483,1.7669657826408882,1.7669655699609859,1.7669653534959577,1.7669651331817757,1.766964908953455,1.7669646807450472,1.76696444848963,1.7669642121192992,1.7669639715651595,1.766963726757316,1.7669634776248653,1.7669632240958892,1.766962966097445,1.7669627035555584,1.7669624363952168,1.766962164540361,1.7669618879138793,1.7669616064376013,1.7669613200322902,1.7669610286176398,1.7669607321122662,1.7669604304337045,1.7669601234984043,1.766959811221725,1.7669594935179331,1.7669591703001981,1.7669588414805912,1.7669585069700826,1.766958166678541,1.766957820514732,1.7669574683863192,1.7669571101998642,1.7669567458608286,1.7669563752735764,1.7669559983413772,1.7669556149664098,1.7669552250497678,1.7669548284914651,1.7669544251904432,1.7669540150445795,1.7669535979506954,1.7669531738045683,1.7669527425009413,1.766952303933537,1.7669518579950727,1.7669514045772732,1.7669509435708906,1.7669504748657214,1.766949998350627,1.7669495139135545,1.7669490214415626,1.7669485208208446,1.7669480119367562,1.7669474946738453,1.7669469689158823,1.7669464345458947,1.766945891446201,1.7669453394984498,1.7669447785836594,1.7669442085822606,1.7669436293741423,1.7669430408386995,1.7669424428548834,1.766941835301256,1.766941218056047,1.7669405909972125,1.7669399540024988,1.7669393069495098,1.7669386497157753,1.7669379821788262,1.7669373042162713,1.7669366157058788,1.7669359165256608,1.7669352065539652,1.7669344856695672,1.76693375375177,1.7669330106805057,1.7669322563364456,1.7669314906011115,1.7669307133569956,1.7669299244876817,1.7669291238779774,1.7669283114140462,1.7669274869835498,1.766926650475795,1.7669258017818865,1.7669249407948873,1.7669240674099844,1.766923181524663,1.7669222830388878,1.7669213718552899,1.7669204478793634,1.7669195110196698,1.7669185611880487,1.7669175982998389,1.7669166222741077,1.7669156330338884,1.766914630506427,1.7669136146234397,1.7669125853213796,1.7669115425417123,1.766910486231203,1.766909416342214,1.7669083328330133,1.766907235668094,1.7669061248185052,1.766905000262196,1.7669038619843698,1.7669027099778527,1.7669015442434743,1.7669003647904615,1.766899171636845,1.766897964809883,1.7668967443464938,1.7668955102937092,1.7668942627091366,1.7668930016614426,1.7668917272308469,1.766890439509636,1.7668891386026921,1.7668878246280373,1.7668864977173984,1.766885158016786,1.7668838056870941,1.7668824409047155,1.7668810638621792,1.7668796747688036,1.7668782738513718,1.7668768613548256,1.7668754375429792,1.766874002699256,1.7668725571274437,1.7668711011524727,1.7668696351212145,1.766868159403305,1.766866674391989,1.7668651805049853,1.76686367818538,1.7668621679025385,1.7668606501530464,1.7668591254616683,1.76685759438234,1.7668560574991752,1.766854515427507,1.7668529688149504,1.766851418342489,1.766849864725593,1.7668483087153584,1.7668467510996761,1.7668451927044266,1.7668436343947025,1.7668420770760564,1.7668405216957792,1.7668389692442033,1.7668374207560338,1.7668358773117112,1.7668343400387947,1.7668328101133823,1.766831288761551,1.7668297772608295,1.7668282769416985,1.766826789189118,1.7668253154440814,1.7668238572052006,1.7668224160303154,1.7668209935381336,1.7668195914098934,1.7668182113910602,1.7668168552930432,1.7668155249949429,1.7668142224453218,1.7668129496640044,1.7668117087438993,1.7668105018528486,1.7668093312355,1.7668081992152043,1.7668071081959367,1.7668060606642388,1.766805059191186,1.7668041064343731,1.7668032051399247,1.766802358144523,1.7668015683774554,1.7668008388626832,1.7668001727209248,1.766799573171759,1.7667990435357428,1.7667985872365444,1.766798207803093,1.766797908871739,1.7667976941884274,1.7667975676108851,1.7667975331108152,1.7667975947761039,1.7667977568130335,1.7667980235485037,1.7667983994322596,1.7667988890391242,1.7667994970712364,1.7668002283602904,1.7668010878697775,1.766802080697231,1.766803212076469,1.7668044873798356,1.7668059121204442,1.7668074919544117,1.7668092326830942,1.766811140255315,1.7668132207695884,1.766815480476335,1.766817925780093,1.7668205632417175,1.7668233995805747,1.7668264416767252,1.766829696573097,1.7668331714776484,1.7668368737655231,1.7668408109811886,1.7668449908405688,1.7668494212331627,1.766854110224151,1.7668590660564933,1.7668642971530122,1.7668698121184676,1.7668756197416187,1.7668817289972778,1.7668881490483526,1.7668948892478804,1.7669019591410526,1.7669093684672346,1.7669171271619755,1.7669252453590147,1.7669337333922859,1.7669426017979137,1.7669518613162143,1.7669615228936917,1.7669715976850402,1.7669820970551464,1.7669930325811012,1.7670044160542158,1.7670162594820493,1.7670285750904482,1.7670413753256005,1.7670546728561054,1.767068480575063,1.7670828116021853,1.7670976792859325,1.7671130972056748,1.767129079173885,1.7671456392383635,1.767162791684498,1.7671805510375633,1.7671989320650587,1.7672179497790939,1.7672376194388173,1.7672579565528972,1.7672789768820545,1.767300696441651,1.7673231315043363,1.7673462986027544,1.7673702145323167,1.7673948963540385,1.767420361397447,1.7674466272635596,1.7674737118279353,1.7675016332438045,1.7675304099452753,1.7675600606506205,1.7675906043656477,1.767622060387152,1.7676544483064565,1.7676877880130375,1.76772209969824,1.7677574038590813,1.767793721302147,1.7678310731475753,1.7678694808331346,1.7679089661183947,1.767949551088986,1.767991258160956,1.7680341100852126,1.7680781299520616,1.768123341195835,1.7681697675996062,1.7682174332999971,1.7682663627920692,1.768316580934302,1.7683681129536546,1.768420984450708,1.7684752214048876,1.7685308501797616,1.7685878975284142,1.7686463905988903,1.7687063569397052,1.768767824505424,1.7688308216622959,1.7688953771939526,1.7689615203071543,1.769029280637588,1.7690986882557103,1.7691697736726308,1.7692425678460317,1.7693171021861167,1.7693934085615897,1.7694715193056503,1.7695514672220094,1.7696332855909123,1.7697170081751665,1.769802669226173,1.7698903034899456,1.7699799462131218,1.770071633148954,1.7701654005632759,1.7702612852404396,1.770359324489215,1.7704595561486451,1.770562018593854,1.7706667507417948,1.770773792056938,1.7708831825568878,1.7709949628179236,1.771109173980455,1.7712258577543887,1.7713450564243967,1.7714668128550788,1.7715911704960141,1.7717181733866907,1.7718478661613104,1.771980294053456,1.772115502900618,1.7722535391485694,1.7723944498555824,1.7725382826964786,1.772685085966504,1.772834908585021,1.7729878000990085,1.7731438106863626,1.7733029911589866,1.7734653929656643,1.773631068194705,1.7738000695763536,1.7739724504849537,1.7741482649408558,1.7743275676120596,1.7745104138155823,1.7746968595185413,1.7748869613389406,1.7750807765461525,1.7752783630610822,1.7754797794560049,1.7756850849540626,1.7758943394284175,1.7761076034010361,1.776324938041108,1.7765464051630768,1.776772067224278,1.7770019873221672,1.7772362291911306,1.777474857198864,1.777717936342306,1.7779655322431198,1.7782177111427035,1.7784745398967197,1.7787360859691368,1.77900241742576,1.779273602927251,1.779549711721613,1.7798308136361385,1.7801169790688023,1.7804082789790876,1.7807047848782385,1.7810065688189203,1.7813137033842832,1.7816262616764105,1.781944317304148,1.7822679443702993,1.782597217458175,1.7829322116174908,1.7832730023496004,1.7836196655920538,1.783972277702473,1.7843309154417342,1.7846956559564504,1.7850665767607423,1.7854437557172897,1.7858272710176608,1.7862172011619009,1.7866136249373854,1.78701662139692,1.7874262698360843,1.7878426497698157,1.788265840908219,1.7886959231316026,1.7891329764647292,1.789577081050278,1.7900283171215094,1.7904867649741285,1.790952504937336,1.7914256173440661,1.7919061825003961,1.7923942806541304,1.7928899919625427,1.7933933964592725,1.793904574020367,1.7944236043294608,1.794950566842082,1.7954855407490757,1.7960286049391376,1.7965798379604416,1.7971393179813542,1.7977071227502222,1.7982833295542215,1.7988680151772527,1.7994612558568701,1.8000631272402312,1.8006737043390484,1.8012930614835303,1.801921272275294,1.8025584095392353,1.8032045452743344,1.8038597506033875,1.8045240957216384,1.8051976498443025,1.8058804811529552,1.8065726567407787,1.8072742425566435,1.8079853033480109,1.808705902602643,1.8094361024891055,1.8101759637960488,1.810925545870261,1.8116849065534815,1.8124541021179623,1.813233187200786,1.8140222147369256,1.8148212358910514,1.8156302999880949,1.8164494544425678,1.817278744686657,1.8181182140971048,1.818967903920904,1.8198278531998195,1.8206980986937882,1.8215786748032101,1.822469613490195,1.823370944198798,1.8242826937743086,1.8252048863816515,1.8261375434229687,1.8270806834544604,1.8280343221025688,1.8289984719795953,1.8299731425988512,1.8309583402894511,1.8319540681108604,1.8329603257673261,1.8339771095223159,1.835004412113113,1.8360422226657065,1.8370905266101405,1.8381493055964784,1.83921853741156,1.8402981958967237,1.8413882508666823,1.8424886680297452,1.8435994089095804,1.844720430768725,1.8458516865340509,1.8469931247243987,1.8481446893806035,1.8493063199981215,1.850477951462498,1.8516595139878906,1.8528509330588805,1.8540521293758006,1.855263018803807,1.8564835123259225,1.857713516000274,1.8589529309217525,1.8602016531883054,1.8614595738720834,1.8627265789956446,1.8640025495134174,1.8652873612986225,1.86658088513583,1.8678829867193372,1.8691935266575253,1.8705123604833573,1.8718393386711543,1.8731743066597832,1.8745171048823719,1.8758675688026551,1.877225528958037,1.8785908110094447,1.8799632357980243,1.8813426194087233,1.8827287732407791,1.8841215040851156,1.8855206142086383,1.8869259014453945,1.888337159294548,1.8897541770250985,1.8911767397872645,1.8926046287304172,1.8940376211274474,1.8954754905054203,1.8969180067823617,1.8983649364099957,1.8998160425222455,1.901271085089278,1.9027298210768777,1.9041920046108982,1.9056573871465405,1.907125717642191,1.9085967427375279,1.9100702069356101,1.9115458527886389,1.913023421087074,1.9145026510517857,1.9159832805289012,1.91746504618701,1.9189476837163828,1.9204309280298475,1.9219145134649716,1.9233981739871924,1.924881643393535,1.9263646555165597,1.927846944428178,1.929328244642986,1.930808291320751,1.9322868204677146,1.9337635691363577,1.9352382756232909,1.9367106796649431,1.938180522630719,1.9396475477133137,1.9411115001158794,1.942572127235748,1.9440291788444275,1.9454824072636003,1.9469315675368646,1.9483764175969693,1.949816718428319,1.9512522342245175,1.9526827325407592,1.9541079844408673,1.9555277646388123,1.956941851634549,1.958350027844026,1.9597520797232408,1.96114779788623,1.9625369772168877,1.9639194169745444,1.965294920893224,1.966663297274534,1.9680243590741522,1.9693779239818845,1.970723814495284,1.9720618579868394,1.9733918867647475,1.9747137381273046,1.9760272544109587,1.9773322830320792,1.9786286765225118,1.9799162925589977,1.9811949939865434,1.9824646488358488,1.9837251303348882,1.984976316914779,1.986218092210047,1.9874503450534333,1.9886729694653802,1.9898858646383382,1.9910889349160548,1.9922820897679978,1.9934652437590752,1.9946383165148236,1.9958012326822308,1.9969539218863657,1.9980963186829983,1.999228362507378,2.0003499976193546,2.0014611730450254,2.0025618425150817,2.0036519644000412,2.0047315016425444,2.0058004216868963,2.0068586964060278,2.007906302026061,2.0089432190486427,2.0099694321712325,2.010984930205501,2.0119897059940177,2.012983756325385,2.013967081847983,2.0149396869824807,2.01590157983327,2.0168527720989684,2.0177932789821376,2.018723119098361,2.019642314384815,2.020550890008466],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.7388445147658906,1.7388445150667602,1.738844515373229,1.738844515685401,1.7388445160033827,1.7388445163272823,1.7388445166572093,1.7388445169932767,1.7388445173355982,1.7388445176842902,1.7388445180394712,1.7388445184012626,1.7388445187697867,1.7388445191451694,1.7388445195275375,1.738844519917022,1.7388445203137548,1.7388445207178704,1.738844521129507,1.7388445215488042,1.7388445219759046,1.7388445224109532,1.738844522854098,1.73884452330549,1.7388445237652823,1.7388445242336315,1.7388445247106965,1.7388445251966402,1.738844525691627,1.7388445261958254,1.7388445267094073,1.738844527232547,1.738844527765422,1.7388445283082141,1.7388445288611079,1.7388445294242905,1.7388445299979542,1.7388445305822937,1.738844531177508,1.7388445317837993,1.7388445324013735,1.738844533030441,1.7388445336712155,1.7388445343239147,1.738844534988761,1.7388445356659796,1.7388445363558016,1.738844537058461,1.7388445377741972,1.7388445385032532,1.7388445392458771,1.738844540002321,1.7388445407728423,1.7388445415577034,1.7388445423571703,1.7388445431715156,1.7388445440010158,1.7388445448459533,1.7388445457066148,1.7388445465832933,1.7388445474762868,1.7388445483858987,1.738844549312439,1.7388445502562218,1.7388445512175685,1.738844552196806,1.7388445531942667,1.7388445542102906,1.7388445552452225,1.7388445562994146,1.738844557373225,1.7388445584670194,1.7388445595811686,1.7388445607160525,1.7388445618720567,1.738844563049574,1.7388445642490047,1.7388445654707567,1.7388445667152455,1.7388445679828943,1.7388445692741339,1.7388445705894033,1.73884457192915,1.7388445732938294,1.7388445746839052,1.73884457609985,1.7388445775421457,1.7388445790112823,1.7388445805077595,1.738844582032086,1.7388445835847797,1.7388445851663694,1.738844586777392,1.7388445884183956,1.738844590089938,1.7388445917925877,1.7388445935269237,1.7388445952935352,1.7388445970930229,1.738844598925999,1.7388446007930867,1.7388446026949202,1.7388446046321469,1.7388446066054248,1.7388446086154254,1.7388446106628312,1.7388446127483392,1.7388446148726582,1.7388446170365102,1.7388446192406308,1.7388446214857698,1.7388446237726902,1.73884462610217,1.7388446284750003,1.738844630891989,1.738844633353957,1.7388446358617415,1.738844638416195,1.7388446410181864,1.7388446436686,1.7388446463683371,1.7388446491183154,1.7388446519194702,1.7388446547727534,1.7388446576791352,1.7388446606396037,1.7388446636551658,1.7388446667268462,1.7388446698556892,1.7388446730427591,1.7388446762891392,1.738844679595933,1.7388446829642645,1.7388446863952796,1.7388446898901444,1.738844693450047,1.7388446970761977,1.7388447007698293,1.738844704532198,1.7388447083645824,1.738844712268285,1.738844716244634,1.7388447202949806,1.7388447244207021,1.7388447286232007,1.7388447329039056,1.738844737264272,1.738844741705782,1.7388447462299457,1.7388447508383016,1.7388447555324158,1.7388447603138841,1.7388447651843328,1.738844770145417,1.7388447751988234,1.7388447803462703,1.738844785589507,1.7388447909303166,1.738844796370515,1.7388448019119505,1.7388448075565077,1.7388448133061056,1.7388448191626986,1.7388448251282782,1.7388448312048714,1.7388448373945447,1.7388448436994022,1.738844850121587,1.7388448566632828,1.7388448633267128,1.7388448701141428,1.7388448770278795,1.7388448840702737,1.7388448912437195,1.7388448985506553,1.738844905993565,1.7388449135749786,1.7388449212974735,1.7388449291636747,1.7388449371762567,1.7388449453379422,1.738844953651507,1.7388449621197761,1.7388449707456284,1.7388449795319965,1.7388449884818664,1.7388449975982812,1.7388450068843393,1.738845016343197,1.7388450259780701,1.7388450357922334,1.738845045789023,1.7388450559718367,1.7388450663441366,1.738845076909447,1.7388450876713606,1.7388450986335342,1.7388451097996949,1.7388451211736378,1.738845132759229,1.738845144560406,1.7388451565811804,1.738845168825638,1.7388451812979409,1.7388451940023277,1.7388452069431168,1.7388452201247069,1.7388452335515778,1.7388452472282934,1.7388452611595018,1.7388452753499384,1.7388452898044258,1.7388453045278767,1.7388453195252955,1.738845334801779,1.7388453503625192,1.738845366212805,1.7388453823580228,1.7388453988036598,1.7388454155553053,1.7388454326186524,1.7388454499994999,1.7388454677037544,1.738845485737433,1.738845504106664,1.7388455228176902,1.7388455418768698,1.7388455612906801,1.738845581065718,1.7388456012087037,1.7388456217264825,1.7388456426260264,1.7388456639144374,1.738845685598949,1.7388457076869304,1.7388457301858868,1.7388457531034633,1.7388457764474474,1.7388458002257707,1.738845824446513,1.738845849117904,1.7388458742483264,1.7388458998463185,1.7388459259205786,1.7388459524799649,1.7388459795335016,1.73884600709038,1.738846035159963,1.7388460637517864,1.7388460928755645,1.7388461225411915,1.7388461527587455,1.7388461835384925,1.7388462148908888,1.738846246826585,1.73884627935643,1.7388463124914748,1.7388463462429744,1.7388463806223942,1.738846415641413,1.7388464513119248,1.7388464876460468,1.7388465246561202,1.7388465623547158,1.7388466007546381,1.7388466398689295,1.7388466797108746,1.7388467202940054,1.7388467616321046,1.7388468037392117,1.7388468466296267,1.7388468903179148,1.738846934818913,1.7388469801477335,1.7388470263197686,1.738847073350697,1.7388471212564893,1.7388471700534118,1.7388472197580338,1.7388472703872313,1.738847321958195,1.7388473744884343,1.7388474279957835,1.7388474824984093,1.7388475380148143,1.7388475945638455,1.7388476521647003,1.7388477108369318,1.7388477706004566,1.7388478314755615,1.7388478934829088,1.7388479566435457,1.7388480209789097,1.738848086510836,1.7388481532615656,1.7388482212537524,1.7388482905104705,1.7388483610552223,1.738848432911947,1.7388485061050272,1.738848580659299,1.738848656600059,1.7388487339530725,1.7388488127445847,1.738848893001326,1.7388489747505238,1.7388490580199103,1.7388491428377322,1.7388492292327604,1.7388493172342991,1.7388494068721974,1.7388494981768559,1.7388495911792405,1.7388496859108917,1.7388497824039333,1.7388498806910861,1.7388499808056777,1.7388500827816529,1.7388501866535857,1.7388502924566924,1.7388504002268403,1.7388505100005631,1.7388506218150703,1.738850735708262,1.7388508517187404,1.7388509698858228,1.7388510902495546,1.7388512128507243,1.7388513377308747,1.7388514649323192,1.7388515944981542,1.7388517264722747,1.7388518608993886,1.7388519978250312,1.7388521372955814,1.7388522793582766,1.7388524240612289,1.7388525714534402,1.73885272158482,1.738852874506202,1.7388530302693592,1.7388531889270231,1.7388533505329014,1.7388535151416942,1.7388536828091146,1.7388538535919051,1.7388540275478581,1.7388542047358337,1.7388543852157816,1.7388545690487587,1.7388547562969503,1.738854947023692,1.7388551412934883,1.7388553391720372,1.7388555407262485,1.7388557460242695,1.7388559551355054,1.738856168130643,1.7388563850816738,1.73885660606192,1.7388568311460548,1.7388570604101312,1.738857293931605,1.7388575317893609,1.7388577740637392,1.7388580208365612,1.7388582721911576,1.738858528212395,1.738858788986705,1.7388590546021119,1.7388593251482614,1.7388596007164523,1.7388598813996636,1.7388601672925879,1.7388604584916605,1.7388607550950927,1.7388610572029033,1.7388613649169518,1.7388616783409707,1.7388619975806028,1.7388623227434317,1.7388626539390197,1.7388629912789437,1.7388633348768305,1.7388636848483952,1.7388640413114782,1.7388644043860841,1.7388647741944212,1.7388651508609405,1.7388655345123776,1.7388659252777923,1.7388663232886126,1.738866728678676,1.7388671415842738,1.7388675621441945,1.7388679904997701,1.7388684267949213,1.7388688711762033,1.738869323792855,1.7388697847968457,1.7388702543429253,1.7388707325886739,1.7388712196945522,1.7388717158239537,1.7388722211432572,1.7388727358218807,1.7388732600323347,1.7388737939502787,1.738874337754577,1.7388748916273562,1.7388754557540633,1.7388760303235247,1.7388766155280073,1.7388772115632791,1.738877818628671,1.7388784369271426,1.7388790666653422,1.738879708053678,1.73888036130638,1.73888102664157,1.7388817042813307,1.7388823944517746,1.7388830973831169,1.7388838133097453,1.7388845424702963,1.7388852851077292,1.7388860414694018,1.7388868118071479,1.7388875963773562,1.73888839544105,1.7388892092639687,1.738890038116651,1.7388908822745168,1.7388917420179548,1.7388926176324082,1.738893509408463,1.738894417641937,1.7388953426339713,1.7388962846911238,1.7388972441254602,1.738898221254653,1.738899216402076,1.7389002298969038,1.7389012620742115,1.7389023132750765,1.7389033838466823,1.7389044741424227,1.738905584522008,1.7389067153515751,1.738907867003795,1.7389090398579852,1.7389102343002243,1.7389114507234653,1.7389126895276537,1.738913951119845,1.7389152359143258,1.7389165443327368,1.7389178768041949,1.7389192337654207,1.7389206156608665,1.7389220229428446,1.7389234560716607,1.738924915515746,1.738926401751794,1.7389279152648975,1.7389294565486886,1.738931026105479,1.7389326244464052,1.7389342520915743,1.7389359095702102,1.738937597420805,1.7389393161912705,1.7389410664390919,1.738942848731485,1.7389446636455541,1.738946511768453,1.7389483936975474,1.7389503100405808,1.7389522614158408,1.7389542484523302,1.7389562717899372,1.7389583320796107,1.7389604299835366,1.7389625661753167,1.738964741340149,1.7389669561750123,1.7389692113888513,1.7389715077027659,1.7389738458501998,1.7389762265771362,1.7389786506422908,1.7389811188173108,1.7389836318869756,1.7389861906493982,1.738988795916232,1.7389914485128777,1.7389941492786924,1.7389968990672047,1.738999698746327,1.739002549198575,1.739005451321288,1.7390084060268498,1.7390114142429154,1.7390144769126377,1.7390175949948987,1.7390207694645416,1.7390240013126057,1.7390272915465652,1.7390306411905683,1.7390340512856817,1.7390375228901347,1.7390410570795676,1.7390446549472829,1.7390483176044973,1.7390520461805985,1.7390558418234023,1.7390597056994153,1.7390636389940957,1.7390676429121217,1.7390717186776576,1.7390758675346287,1.73908009074699,1.7390843895990071,1.7390887653955314,1.7390932194622846,1.739097753146141,1.7391023678154143,1.7391070648601477,1.7391118456924053,1.7391167117465667,1.7391216644796246,1.7391267053714845,1.7391318359252672,1.739137057667615,1.7391423721489983,1.7391477809440283,1.7391532856517686,1.7391588878960544,1.7391645893258092,1.7391703916153687,1.739176296464806,1.7391823056002598,1.7391884207742645,1.739194643766087,1.739200976382063,1.7392074204559382,1.7392139778492133,1.7392206504514902,1.7392274401808256,1.739234348984085,1.7392413788373011,1.739248531746037,1.7392558097457536,1.7392632149021792,1.739270749311686,1.7392784151016696,1.739286214430933,1.7392941494900762,1.739302222501892,1.7393104357217628,1.739318791438069,1.7393272919725986,1.739335939680964,1.7393447369530273,1.739353686213328,1.739362789921523,1.7393720505728283,1.7393814706984736,1.7393910528661607,1.7394007996805336,1.7394107137836552,1.7394207978554947,1.7394310546144245,1.7394414868177264,1.739452097262111,1.7394628887842465,1.7394738642612992,1.7394850266114887,1.7394963787946538,1.739507923812835,1.7395196647108684,1.7395316045769982,1.7395437465435022,1.7395560937873369,1.7395686495307983,1.739581417042201,1.7395943996365792,1.7396076006764054,1.7396210235723313,1.7396346717839508,1.7396485488205868,1.739662658242102,1.7396770036597344,1.7396915887369602,1.7397064171903842,1.7397214927906588,1.7397368193634328,1.7397524007903333,1.7397682410099768,1.7397843440190184,1.7398007138732319,1.7398173546886306,1.739834270642624,1.7398514659752133,1.7398689449902307,1.7398867120566168,1.7399047716097473,1.7399231281528005,1.739941786258174,1.7399607505689503,1.7399800258004114,1.739999616741606,1.7400195282569701,1.740039765288002,1.7400603328549942,1.7400812360588251,1.740102480082808,1.7401240701946037,1.7401460117481948,1.7401683101859275,1.740190971040616,1.7402139999377186,1.7402374025975809,1.7402611848377523,1.7402853525753732,1.7403099118296381,1.7403348687243347,1.7403602294904583,1.7403860004689065,1.7404121881132546,1.7404387989926111,1.7404658397945547,1.7404933173281598,1.7405212385271016,1.7405496104528515,1.7405784402979587,1.740607735389419,1.740637503192134,1.7406677513124604,1.7406984875018512,1.7407297196605858,1.7407614558415954,1.7407937042543795,1.7408264732690164,1.7408597714202674,1.740893607411775,1.7409279901203545,1.740962928600383,1.740998432088277,1.7410345100070714,1.741071171971087,1.7411084277906943,1.7411462874771704,1.7411847612476503,1.741223859530167,1.7412635929687885,1.741303972428841,1.7413450090022262,1.7413867140128256,1.7414290990219936,1.741472175834138,1.7415159565023868,1.7415604533343383,1.7416056788978957,1.7416516460271818,1.7416983678285367,1.7417458576865896,1.7417941292704127,1.7418431965397438,1.7418930737512888,1.7419437754650884,1.7419953165509594,1.7420477121949995,1.7421009779061591,1.7421551295228734,1.742210183219757,1.742266155514355,1.7423230632739515,1.742380923722429,1.7424397544471804,1.7424995734060689,1.7425603989344334,1.7426222497521366,1.7426851449706564,1.7427491041002119,1.7428141470569263,1.742880294170024,1.7429475661890563,1.7430159842911548,1.7430855700883081,1.7431563456346657,1.7432283334338554,1.74330155644632,1.7433760380966683,1.7434518022810372,1.743528873374462,1.7436072762382515,1.7436870362273682,1.7437681791978077,1.743850731513976,1.7439347200560606,1.7440201722273965,1.744107115961817,1.7441955797309956,1.7442855925517704,1.7443771839934472,1.744470384185083,1.7445652238227447,1.74466173417674,1.7447599470988175,1.7448598950293333,1.7449616110043824,1.7450651286628902,1.7451704822536602,1.7452777066423788,1.7453868373185693,1.7454979104024944,1.7456109626520038,1.745726031469323,1.7458431549077789,1.745962371678461,1.74608372115681,1.7462072433891387,1.7463329790990672,1.7464609696938835,1.7465912572708155,1.746723884623211,1.7468588952466249,1.746996333344807,1.7471362438355822,1.747278672356624,1.7474236652711101,1.7475712696732595,1.7477215333937395,1.7478745050049442,1.74803023382613,1.7481887699284078,1.7483501641395813,1.7485144680488276,1.7486817340112089,1.7488520151520102,1.749025365370895,1.7492018393458713,1.749381492537056,1.7495643811902326,1.7497505623401928,1.7499400938138494,1.7501330342331156,1.7503294430175356,1.7505293803866597,1.750732907362153,1.7509400857696245,1.751150978240168,1.7513656482116002,1.7515841599293893,1.7518065784472558,1.7520329696274368,1.7522634001406006,1.7524979374653988,1.7527366498876422,1.7529796064990848,1.7532268771958082,1.7534785326761853,1.7537346444384156,1.753995284777611,1.7542605267824234,1.7545304443311958,1.754805112087623,1.7550846054959055,1.7553690007753828,1.7556583749146284,1.7559528056649916,1.7562523715335696,1.7565571517755945,1.7568672263862157,1.757182676091665,1.7575035823397822,1.7578300272898895,1.7581620938019924,1.7584998654252901,1.7588434263859793,1.7591928615743317,1.7595482565310225,1.7599096974326984,1.7602772710767565,1.7606510648653229,1.7610311667883987,1.761417665406166,1.7618106498304207,1.762210209705115,1.7626164351859863,1.7630294169192473,1.7634492460193143,1.7638760140455483,1.7643098129779813,1.7647507351920042,1.7651988734319866,1.765654320783798,1.7661171706462087,1.7665875167011287,1.7670654528826641,1.7675510733449535,1.76804447242875,1.7685457446267197,1.769054984547417,1.7695722868779042,1.7700977463449765,1.7706314576749556,1.7711735155520152,1.7717240145749966,1.7722830492126769,1.772850713757449,1.7734271022773755,1.7740123085665704,1.7746064260938745,1.7752095479497798,1.7758217667915626,1.77644317478659,1.777073863553755,1.7777139241030058,1.7783634467729315,1.7790225211663682,1.7796912360839952,1.7803696794558839,1.781057938270977,1.781756098504468,1.7824642450430574,1.78318246160807,1.7839108306764142,1.7846494333993725,1.785398349519219,1.7861576572836586,1.786927433358096,1.7877077527357428,1.7884986886455758,1.7893003124581783,1.7901126935894836,1.7909358994024696,1.7917699951068424,1.7926150436567665,1.7934711056467065,1.7943382392054488,1.7952164998883877,1.7961059405681663,1.7970066113237733,1.797918559328209,1.7988418287348387,1.7997764605625715,1.8007224925800023,1.8016799591886754,1.8026488913056324,1.8036293162454187,1.80462125760174,1.8056247351289594,1.8066397646236478,1.8076663578064016,1.8087045222041565,1.8097542610332358,1.810815573083376,1.8118884526029864,1.8129728891859076,1.814068867659933,1.815176367977377,1.8162953651079683,1.8174258289343626,1.818567724150557,1.8197210101635182,1.8208856409983079,1.8220615652070198,1.8232487257818204,1.8244470600724019,1.8256564997081424,1.8268769705252743,1.8281083924993538,1.8293506796833194,1.8306037401514226,1.8318674759493108,1.833141783050522,1.8344265513196576,1.8357216644824748,1.8370270001031332,1.8383424295688275,1.8396678180819994,1.8410030246603353,1.8423479021447196,1.8437022972153045,1.8450660504158416,1.8464389961863903,1.8478209629045168,1.8492117729350575,1.8506112426885115,1.8520191826881065,1.8534353976455489,1.854859686545458,1.8562918427384578,1.8577316540428732,1.8591789028549601,1.8606333662675723,1.8620948161971422,1.863563019518842,1.8650377382097476,1.8665187294998309,1.8680057460305632,1.8694985360209029,1.8709968434404154,1.8725004081892556,1.8740089662847241,1.875522250054087,1.8770399883333349,1.8785619066715447,1.8800877275404773,1.8816171705490534,1.883149952662316,1.8846857884244892,1.8862243901857247,1.8877654683321308,1.8893087315186516,1.8908538869043805,1.8924006403898694,1.893948696856,1.8954977604039804,1.8970475345960331,1.8985977226963306,1.9001480279117502,1.9016981536320126,1.9032478036687817,1.9047966824933045,1.9063444954721753,1.9078909491008234,1.9094357512343278,1.910978611315175,1.912519240597587,1.9140573523680602,1.9155926621617687,1.9171248879744982,1.9186537504697978,1.920178973181041,1.921700282708119,1.9232174089084921,1.9247300850823503,1.9262380481516517,1.9277410388328193,1.929238801802903,1.9307310858590265,1.9322176440709573,1.9336982339266622,1.9351726174707213,1.9366405614354965,1.9381018373649683,1.9395562217311733,1.9410034960431874,1.9424434469486263,1.9438758663276414,1.945300551379414,1.9467173047011614,1.9481259343596913,1.949526253955541,1.9509180826797743,1.9523012453634987,1.9536755725202013,1.9550409003809954,1.9563970709228968,1.9577439318902456,1.9590813368094153,1.9604091449969467,1.9617272215612633,1.963035437398127,1.9643336691800033,1.9656217993395129,1.9668997160471509,1.9681673131834607,1.969424490305856,1.9706711526102874,1.9719072108879547,1.9731325814772658,1.9743471862112516,1.9755509523606407,1.9767438125728043,1.9779257048067826,1.9790965722645963,1.9802563633190586,1.9814050314382907,1.9825425351071517,1.9836688377457843,1.9847839076254796,1.985887717782064,1.9869802459270032,1.9880614743564156,1.9891313898581882,1.9901899836173835,1.9912372511201122,1.9922731920560588,1.9932978102198293,1.9943111134112874,1.9953131133350486,1.99630382549929,1.997283269114023,1.99825146698899,1.9992084454313142,2.000154234143052],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.8118672153725714,1.8118672152171718,1.8118672150588802,1.8118672148976425,1.8118672147334045,1.8118672145661097,1.8118672143957015,1.811867214222122,1.8118672140453123,1.8118672138652119,1.81186721368176,1.8118672134948937,1.81186721330455,1.8118672131106637,1.8118672129131694,1.8118672127119995,1.8118672125070856,1.8118672122983586,1.8118672120857469,1.811867211869178,1.8118672116485794,1.811867211423875,1.811867211194989,1.811867210961843,1.8118672107243583,1.811867210482454,1.8118672102360476,1.8118672099850557,1.8118672097293924,1.8118672094689714,1.8118672092037038,1.8118672089334993,1.8118672086582666,1.8118672083779113,1.8118672080923386,1.8118672078014513,1.8118672075051507,1.8118672072033355,1.8118672068959032,1.81186720658275,1.8118672062637684,1.8118672059388508,1.8118672056078864,1.8118672052707623,1.8118672049273643,1.8118672045775754,1.811867204221277,1.8118672038583477,1.8118672034886638,1.8118672031121001,1.8118672027285283,1.8118672023378182,1.811867201939837,1.811867201534449,1.8118672011215162,1.811867200700899,1.8118672002724534,1.8118671998360345,1.8118671993914937,1.8118671989386796,1.8118671984774384,1.8118671980076133,1.8118671975290446,1.8118671970415692,1.811867196545022,1.8118671960392334,1.811867195524032,1.8118671949992422,1.811867194464686,1.8118671939201814,1.8118671933655432,1.8118671928005827,1.811867192225108,1.8118671916389233,1.8118671910418296,1.8118671904336232,1.811867189814098,1.8118671891830427,1.8118671885402435,1.811867187885481,1.811867187218533,1.8118671865391727,1.8118671858471689,1.8118671851422865,1.8118671844242857,1.8118671836929223,1.8118671829479478,1.8118671821891088,1.8118671814161473,1.8118671806288003,1.8118671798267998,1.8118671790098737,1.811867178177744,1.8118671773301276,1.8118671764667362,1.8118671755872766,1.8118671746914494,1.81186717377895,1.8118671728494684,1.811867171902688,1.8118671709382872,1.811867169955938,1.8118671689553065,1.8118671679360523,1.8118671668978286,1.8118671658402825,1.8118671647630546,1.8118671636657782,1.8118671625480804,1.8118671614095812,1.811867160249893,1.8118671590686222,1.8118671578653662,1.8118671566397166,1.811867155391256,1.8118671541195603,1.8118671528241965,1.8118671515047247,1.811867150160696,1.811867148791653,1.8118671473971306,1.8118671459766542,1.811867144529741,1.8118671430558986,1.8118671415546264,1.8118671400254134,1.8118671384677394,1.8118671368810748,1.8118671352648803,1.811867133618606,1.8118671319416924,1.8118671302335687,1.8118671284936543,1.8118671267213573,1.811867124916075,1.8118671230771934,1.8118671212040873,1.81186711929612,1.8118671173526417,1.8118671153729922,1.8118671133564976,1.811867111302473,1.811867109210219,1.8118671070790242,1.8118671049081643,1.8118671026969,1.8118671004444804,1.8118670981501386,1.8118670958130945,1.8118670934325534,1.8118670910077053,1.811867088537726,1.8118670860217747,1.8118670834589965,1.811867080848519,1.811867078189455,1.8118670754808996,1.8118670727219315,1.8118670699116128,1.8118670670489871,1.811867064133081,1.8118670611629026,1.8118670581374419,1.8118670550556693,1.811867051916537,1.8118670487189772,1.8118670454619017,1.8118670421442036,1.8118670387647535,1.8118670353224022,1.8118670318159786,1.8118670282442906,1.8118670246061224,1.8118670209002368,1.811867017125373,1.811867013280247,1.8118670093635512,1.8118670053739525,1.8118670013100941,1.8118669971705934,1.8118669929540425,1.8118669886590069,1.811866984284025,1.8118669798276088,1.811866975288242,1.8118669706643806,1.8118669659544515,1.811866961156852,1.8118669562699503,1.8118669512920835,1.811866946221558,1.8118669410566486,1.8118669357955985,1.8118669304366173,1.8118669249778816,1.8118669194175343,1.8118669137536836,1.8118669079844019,1.8118669021077267,1.811866896121658,1.8118668900241592,1.8118668838131553,1.8118668774865332,1.8118668710421395,1.8118668644777818,1.8118668577912258,1.8118668509801967,1.8118668440423762,1.8118668369754034,1.8118668297768734,1.8118668224443368,1.8118668149752972,1.8118668073672137,1.8118667996174962,1.8118667917235076,1.8118667836825613,1.81186677549192,1.8118667671487962,1.8118667586503505,1.8118667499936898,1.8118667411758675,1.8118667321938828,1.8118667230446777,1.8118667137251376,1.8118667042320906,1.811866694562305,1.811866684712489,1.8118666746792889,1.8118666644592896,1.8118666540490114,1.8118666434449102,1.8118666326433757,1.8118666216407304,1.811866610433228,1.8118665990170522,1.8118665873883164,1.8118665755430605,1.811866563477251,1.8118665511867793,1.8118665386674604,1.8118665259150302,1.8118665129251463,1.8118664996933849,1.8118664862152394,1.8118664724861189,1.811866458501348,1.8118664442561632,1.811866429745712,1.8118664149650523,1.8118663999091487,1.8118663845728726,1.8118663689509995,1.811866353038207,1.8118663368290746,1.8118663203180791,1.8118663034995952,1.8118662863678925,1.8118662689171332,1.8118662511413715,1.8118662330345492,1.8118662145904965,1.8118661958029274,1.8118661766654391,1.8118661571715085,1.8118661373144922,1.811866117087621,1.8118660964840008,1.8118660754966074,1.8118660541182865,1.8118660323417495,1.8118660101595723,1.8118659875641923,1.8118659645479038,1.8118659411028595,1.8118659172210638,1.8118658928943732,1.8118658681144904,1.8118658428729641,1.8118658171611852,1.8118657909703828,1.811865764291623,1.811865737115804,1.8118657094336548,1.8118656812357306,1.8118656525124097,1.811865623253891,1.81186559345019,1.8118655630911353,1.8118655321663655,1.811865500665325,1.8118654685772613,1.81186543589122,1.8118654025960428,1.8118653686803612,1.8118653341325952,1.8118652989409474,1.8118652630933998,1.8118652265777093,1.8118651893814037,1.8118651514917772,1.811865112895886,1.811865073580544,1.811865033532318,1.8118649927375234,1.8118649511822191,1.8118649088522023,1.8118648657330052,1.8118648218098874,1.8118647770678336,1.8118647314915455,1.8118646850654394,1.8118646377736387,1.8118645895999692,1.8118645405279528,1.811864490540803,1.811864439621418,1.8118643877523755,1.8118643349159251,1.8118642810939847,1.8118642262681313,1.811864170419597,1.8118641135292612,1.811864055577644,1.811863996544899,1.8118639364108085,1.8118638751547733,1.8118638127558075,1.811863749192531,1.811863684443161,1.8118636184855061,1.8118635512969563,1.8118634828544773,1.8118634131346005,1.8118633421134165,1.8118632697665646,1.8118631960692264,1.8118631209961162,1.811863044521471,1.8118629666190436,1.8118628872620912,1.811862806423368,1.811862724075113,1.8118626401890434,1.8118625547363418,1.8118624676876476,1.8118623790130457,1.8118622886820568,1.8118621966636264,1.8118621029261122,1.8118620074372755,1.8118619101642686,1.8118618110736215,1.8118617101312327,1.8118616073023555,1.811861502551586,1.8118613958428498,1.8118612871393909,1.8118611764037562,1.8118610635977839,1.8118609486825894,1.811860831618551,1.8118607123652957,1.8118605908816852,1.8118604671258016,1.81186034105493,1.811860212625546,1.8118600817932988,1.811859948512994,1.8118598127385799,1.8118596744231283,1.81185953351882,1.8118593899769246,1.8118592437477852,1.8118590947808002,1.8118589430244034,1.811858788426047,1.8118586309321805,1.8118584704882335,1.8118583070385945,1.81185814052659,1.811857970894466,1.8118577980833641,1.8118576220333014,1.811857442683149,1.811857259970609,1.8118570738321906,1.8118568842031884,1.811856691017657,1.8118564942083883,1.8118562937068858,1.811856089443338,1.8118558813465948,1.8118556693441403,1.8118554533620645,1.811855233325038,1.8118550091562817,1.8118547807775405,1.811854548109051,1.8118543110695158,1.8118540695760688,1.8118538235442463,1.8118535728879557,1.8118533175194413,1.8118530573492524,1.8118527922862095,1.8118525222373694,1.8118522471079892,1.8118519668014932,1.8118516812194327,1.8118513902614497,1.8118510938252406,1.8118507918065132,1.8118504840989502,1.8118501705941674,1.8118498511816712,1.8118495257488165,1.8118491941807642,1.811848856360435,1.8118485121684655,1.811848161483162,1.8118478041804524,1.8118474401338382,1.8118470692143445,1.8118466912904714,1.8118463062281407,1.8118459138906442,1.8118455141385896,1.8118451068298458,1.8118446918194864,1.8118442689597332,1.8118438380998965,1.8118433990863163,1.8118429517623005,1.8118424959680626,1.8118420315406578,1.811841558313918,1.8118410761183847,1.811840584781241,1.8118400841262416,1.8118395739736426,1.8118390541401277,1.8118385244387338,1.8118379846787762,1.811837434665769,1.8118368742013478,1.8118363030831857,1.8118357211049136,1.8118351280560328,1.8118345237218287,1.8118339078832837,1.8118332803169837,1.8118326407950283,1.8118319890849341,1.8118313249495377,1.8118306481468982,1.8118299584301942,1.8118292555476214,1.8118285392422853,1.8118278092520936,1.811827065309647,1.8118263071421221,1.8118255344711598,1.8118247470127427,1.811823944477078,1.8118231265684706,1.8118222929851975,1.811821443419378,1.8118205775568417,1.8118196950769923,1.8118187956526688,1.8118178789500046,1.8118169446282821,1.811815992339784,1.8118150217296423,1.8118140324356822,1.8118130240882648,1.8118119963101242,1.8118109487162009,1.8118098809134728,1.8118087925007829,1.811807683068658,1.811806552199132,1.8118053994655563,1.8118042244324122,1.8118030266551157,1.8118018056798195,1.8118005610432095,1.8117992922722974,1.8117979988842086,1.811796680385964,1.8117953362742594,1.811793966035238,1.8117925691442573,1.8117911450656539,1.8117896932524986,1.81178821314635,1.8117867041770002,1.8117851657622157,1.8117835973074725,1.8117819982056858,1.8117803678369317,1.811778705568167,1.811777010752937,1.8117752827310822,1.811773520828436,1.8117717243565163,1.8117698926122097,1.811768024877451,1.811766120418893,1.811764178487572,1.8117621983185637,1.8117601791306333,1.8117581201258792,1.8117560204893661,1.8117538793887527,1.811751695973912,1.811749469376543,1.8117471987097729,1.8117448830677554,1.8117425215252552,1.81174011313723,1.8117376569383998,1.81173515194281,1.8117325971433844,1.8117299915114715,1.8117273339963798,1.8117246235249058,1.811721859000853,1.811719039304541,1.8117161632923064,1.8117132297959944,1.8117102376224403,1.8117071855529436,1.8117040723427307,1.8117008967204107,1.8116976573874195,1.8116943530174556,1.8116909822559075,1.8116875437192705,1.8116840359945534,1.8116804576386791,1.8116768071778724,1.8116730831070402,1.8116692838891437,1.811665407954559,1.8116614537004312,1.8116574194900175,1.8116533036520248,1.8116491044799345,1.8116448202313233,1.8116404491271718,1.8116359893511689,1.8116314390490067,1.8116267963276667,1.8116220592547019,1.8116172258575096,1.8116122941225994,1.811607261994853,1.8116021273767808,1.8115968881277718,1.8115915420633388,1.8115860869543592,1.811580520526313,1.8115748404585157,1.8115690443833512,1.8115631298854997,1.8115570945011668,1.8115509357173103,1.8115446509708695,1.8115382376479912,1.8115316930832626,1.8115250145589434,1.8115181993042027,1.811511244494361,1.8115041472501363,1.8114969046368996,1.8114895136639355,1.811481971283712,1.8114742743911656,1.8114664198229904,1.8114584043569448,1.8114502247111737,1.811441877543542,1.8114333594509897,1.8114246669689047,1.811415796570515,1.811406744666305,1.8113975076034545,1.8113880816653065,1.8113784630708578,1.8113686479742845,1.8113586324644975,1.811348412564731,1.8113379842321677,1.811327343357605,1.8113164857651565,1.8113054072120012,1.8112941033881782,1.8112825699164232,1.8112708023520654,1.8112587961829676,1.8112465468295287,1.8112340496447419,1.8112212999143151,1.8112082928568547,1.8111950236241168,1.8111814873013279,1.8111676789075803,1.8111535933963014,1.8111392256558028,1.8111245705099126,1.8111096227186931,1.8110943769792451,1.8110788279266077,1.8110629701347516,1.8110467981176699,1.8110303063305742,1.8110134891711935,1.8109963409811831,1.8109788560476472,1.8109610286047733,1.8109428528355924,1.8109243228738559,1.8109054328060423,1.8108861766734936,1.8108665484746826,1.810846542167622,1.8108261516724073,1.8108053708739094,1.8107841936246099,1.8107626137475916,1.8107406250396774,1.8107182212747304,1.8106953962071153,1.8106721435753195,1.8106484571057442,1.8106243305166658,1.8105997575223693,1.8105747318374572,1.8105492471813396,1.8105232972829046,1.8104968758853743,1.8104699767513472,1.810442593668031,1.8104147204526695,1.8103863509581604,1.8103574790788732,1.8103280987566672,1.8102982039871074,1.8102677888258858,1.8102368473954473,1.8102053738918238,1.8101733625916716,1.8101408078595254,1.810107704155258,1.8100740460417528,1.8100398281927923,1.8100050454011563,1.809969692586937,1.809933764806069,1.8098972572590732,1.8098601653000188,1.8098224844456976,1.8097842103850157,1.8097453389886011,1.809705866318623,1.8096657886388277,1.8096251024247858,1.8095838043743537,1.8095418914183428,1.8094993607314003,1.8094562097430984,1.8094124361492268,1.8093680379232935,1.809323013328223,1.8092773609282586,1.809231079601059,1.8091841685499885,1.8091366273166054,1.8090884557933289,1.809039654236299,1.808990223278415,1.8089401639425526,1.8088894776549551,1.8088381662587938,1.8087862320278965,1.8087336776806338,1.8086805063939657,1.8086267218176377,1.8085723280885246,1.8085173298451138,1.8084617322421284,1.8084055409652757,1.8083487622461263,1.8082914028771027,1.8082334702265912,1.8081749722541491,1.8081159175258186,1.808056315229527,1.8079961751905782,1.8079355078872155,1.8078743244662612,1.807812636758814,1.8077504572960068,1.8076877993248064,1.807624676823856,1.8075611045193478,1.8074970979009173,1.8074326732375505,1.8073678475934984,1.8073026388441846,1.8072370656921009,1.8071711476826795,1.807104905220134,1.8070383595832569,1.8069715329411653,1.8069044483689873,1.806837129863473,1.8067696023585247,1.8067018917406363,1.806634024864225,1.8065660295668537,1.8064979346843253,1.8064297700656424,1.8063615665878205,1.8062933561705417,1.8062251717906386,1.8061570474963995,1.8060890184216798,1.8060211207998051,1.8059533919772668,1.80588587042718,1.8058185957625084,1.8057516087490324,1.8056849513180577,1.8056186665788427,1.8055527988307385,1.8054873935750293,1.8054224975264572,1.8053581586244216,1.8052944260438404,1.8052313502056574,1.8051689827869897,1.8051073767308954,1.8050465862557534,1.8049866668642414,1.8049276753518988,1.8048696698152633,1.8048127096595667,1.8047568556059799,1.8047021696983903,1.8046487153097055,1.8045965571476632,1.8045457612601417,1.804496395039954,1.8044485272291158,1.8044022279225729,1.8043575685713809,1.8043146219853172,1.8042734623349197,1.80423416515294,1.8041968073351966,1.8041614671408153,1.8041282241918544,1.804097159472289,1.8040683553263563,1.8040418954562432,1.8040178649191068,1.8039963501234175,1.8039774388246144,1.803961220120058,1.8039477844432743,1.8039372235574773,1.8039296305483576,1.8039250998161267,1.8039237270668091,1.8039256093027656,1.8039308448124391,1.803939533159313,1.8039517751700656,1.803967672921914,1.8039873297291291,1.8040108501287124,1.8040383398652218,1.804069905874727,1.8041056562678857,1.8041457003121237,1.8041901484128984,1.8042391120940349,1.8042927039771115,1.8043510377598817,1.8044142281937041,1.8044823910599663,1.8045556431454794,1.8046341022168144,1.8047178869935616,1.8048071171204825,1.804901913138528,1.805002396454693,1.8051086893106756,1.8052209147503089,1.8053391965857297,1.8054636593622513,1.8055944283218974,1.8057316293655603,1.8058753890137453,1.8060258343658488,1.806183093057938,1.806347293218975,1.806518563425444,1.8066970326543292,1.8068828302343958,1.8070760857957184,1.8072769292174087,1.8074854905734834,1.8077019000768235,1.8079262880211675,1.8081587847210816,1.8083995204498549,1.8086486253752654,1.8089062294931568,1.8091724625587833,1.8094474540158565,1.8097313329232603,1.8100242278793686,1.8103262669439362,1.8106375775575054,1.810958286458301,1.8112885195965678,1.811628402046327,1.811978057914518,1.812337610247509,1.8127071809349589,1.813086890611018,1.81347685855287,1.8138772025766154,1.8142880389305107,1.8147094821855827,1.815141645123648,1.8155846386227776,1.8160385715402525,1.8165035505930678,1.8169796802360623,1.817467062537743,1.8179657970539036,1.8184759806991393,1.8189977076163697,1.8195310690445017,1.8200761531843694,1.8206330450631076,1.8212018263971175,1.8217825754538115,1.822375366912322,1.822980271723378,1.8235973569685722,1.824226685719237,1.8248683168951818,1.8255223051235343,1.8261887005979558,1.8268675489385053,1.8275588910524332,1.8282627629962054,1.8289791958390607,1.8297082155284103,1.830449842757407,1.831204092835002,1.8319709755588327,1.8327504950912685,1.8335426498389653,1.8343474323362632,1.8351648291327827,1.8359948206855559,1.8368373812560441,1.8376924788123805,1.8385600749371758,1.8394401247412264,1.8403325767834469,1.8412373729973488,1.8421544486243793,1.8430837321544191,1.8440251452737273,1.8449786028206157,1.845944012749109,1.8469212761008418,1.8479102869854225,1.8489109325694784,1.8499230930745714,1.8509466417841682,1.8519814450598082,1.8530273623666074,1.8540842463082063,1.8551519426712493,1.8562302904794543,1.8573191220573155,1.8584182631034456,1.859527532773555,1.8606467437730205,1.8617757024589858,1.8629142089519044,1.864062057256407,1.8652190353913591,1.8663849255289398,1.8675595041425523,1.868742542163353,1.8699338051451635,1.8711330534374995,1.8723400423664427,1.8735545224230414,1.8747762394589267,1.8760049348887895,1.8772403458993678,1.878482205664559,1.8797302435662653,1.8809841854205638,1.8822437537087822,1.8835086678130424,1.8847786442558365,1.8860533969431768,1.8873326374108645,1.888616075073418,1.8899034174751785,1.8911943705431427,1.892488638841032,1.8937859258241385,1.8950859340944761,1.8963883656557674,1.8976929221678118,1.8989993051997742,1.90030721648195,1.9016163581555687,1.9029264330202051,1.9042371447783832,1.9055481982769658,1.9068592997449394,1.908170157027218,1.9094804798140996,1.9107899798660337,1.9120983712333657,1.9134053704707477,1.9147106968459218,1.9160140725425985,1.9173152228571706,1.9186138763890288,1.9199097652242556,1.9212026251124974,1.9224921956368413,1.9237782203765261,1.9250604470623554,1.9263386277246854,1.92761251883389,1.9288818814332158,1.9301464812639662,1.9314060888829652,1.9326604797722777,1.933909434441172,1.93515273852033,1.9363901828483365,1.9376215635504743,1.938846682109891,1.9400653454311996,1.9412773658965976,1.9424825614146044,1.9436807554615199,1.9448717771157302,1.9460554610849907,1.9472316477268303,1.948400183062225,1.9495609187827092,1.950713712251086,1.9518584264959187,1.9529949301999863,1.9541230976828905,1.9552428088780103,1.9563539493040056,1.957456410031066,1.9585500876421245,1.9596348841892293,1.9607107071452987,1.961777469351463,1.9628350889602109,1.9638834893745534,1.9649225991834174,1.965952352093482,1.9669726868576725,1.9679835472005138,1.9689848817405615,1.9699766439101036,1.9709587918723477,1.9719312884362836,1.9728941009694172,1.9738472013085748,1.974790565668956,1.975724174551625,1.9766480126496142,1.9775620687528188,1.9784663356518457,1.9793608100409872,1.9802454924204687,1.9811203869981355,1.9819855015907177],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.7998772205258673,1.7998772203808997,1.799877220233234,1.7998772200828206,1.7998772199296076,1.7998772197735433,1.7998772196145747,1.7998772194526476,1.799877219287707,1.7998772191196968,1.79987721894856,1.799877218774238,1.7998772185966723,1.7998772184158016,1.799877218231565,1.7998772180438996,1.7998772178527418,1.7998772176580262,1.7998772174596873,1.799877217257657,1.7998772170518667,1.7998772168422468,1.7998772166287258,1.799877216411231,1.7998772161896885,1.7998772159640228,1.7998772157341578,1.7998772155000147,1.7998772152615143,1.7998772150185753,1.799877214771115,1.7998772145190494,1.7998772142622925,1.7998772140007575,1.7998772137343553,1.799877213462995,1.7998772131865846,1.79987721290503,1.7998772126182359,1.7998772123261042,1.7998772120285358,1.7998772117254296,1.799877211416682,1.799877211102189,1.799877210781843,1.7998772104555354,1.7998772101231546,1.7998772097845883,1.7998772094397213,1.7998772090884358,1.7998772087306127,1.7998772083661305,1.799877207994865,1.7998772076166902,1.7998772072314773,1.7998772068390954,1.7998772064394108,1.7998772060322883,1.7998772056175887,1.799877205195171,1.7998772047648923,1.7998772043266056,1.7998772038801625,1.7998772034254105,1.7998772029621954,1.7998772024903595,1.7998772020097427,1.7998772015201812,1.7998772010215083,1.799877200513555,1.7998771999961483,1.7998771994691125,1.799877198932268,1.7998771983854325,1.79987719782842,1.7998771972610412,1.7998771966831029,1.7998771960944089,1.7998771954947588,1.7998771948839487,1.799877194261771,1.799877193628014,1.7998771929824622,1.7998771923248964,1.7998771916550929,1.7998771909728235,1.7998771902778565,1.7998771895699555,1.79987718884888,1.7998771881143847,1.7998771873662198,1.7998771866041308,1.7998771858278586,1.7998771850371393,1.7998771842317038,1.7998771834112786,1.7998771825755842,1.799877181724337,1.7998771808572471,1.7998771799740199,1.7998771790743546,1.7998771781579457,1.7998771772244817,1.7998771762736445,1.7998771753051115,1.7998771743185529,1.7998771733136334,1.799877172290011,1.799877171247338,1.7998771701852592,1.7998771691034143,1.7998771680014345,1.7998771668789457,1.799877165735566,1.7998771645709064,1.7998771633845712,1.7998771621761565,1.7998771609452515,1.7998771596914376,1.7998771584142887,1.79987715711337,1.7998771557882394,1.7998771544384464,1.7998771530635316,1.7998771516630272,1.7998771502364577,1.7998771487833374,1.799877147303172,1.7998771457954583,1.7998771442596835,1.7998771426953257,1.7998771411018522,1.7998771394787214,1.7998771378253815,1.7998771361412698,1.7998771344258135,1.7998771326784297,1.7998771308985235,1.79987712908549,1.7998771272387124,1.7998771253575625,1.7998771234414006,1.799877121489575,1.7998771195014218,1.7998771174762649,1.7998771154134154,1.7998771133121718,1.7998771111718193,1.79987710899163,1.799877106770862,1.7998771045087605,1.799877102204556,1.7998770998574645,1.7998770974666878,1.7998770950314125,1.7998770925508107,1.7998770900240384,1.799877087450236,1.7998770848285282,1.7998770821580232,1.7998770794378125,1.7998770766669712,1.7998770738445564,1.799877070969608,1.7998770680411482,1.7998770650581812,1.7998770620196913,1.7998770589246458,1.7998770557719912,1.7998770525606558,1.7998770492895464,1.7998770459575508,1.799877042563535,1.7998770391063448,1.799877035584804,1.7998770319977142,1.7998770283438557,1.7998770246219848,1.7998770208308357,1.7998770169691185,1.7998770130355193,1.7998770090287,1.7998770049472974,1.7998770007899223,1.799876996555161,1.7998769922415723,1.7998769878476888,1.7998769833720152,1.7998769788130289,1.7998769741691785,1.799876969438884,1.799876964620536,1.7998769597124946,1.79987695471309,1.7998769496206213,1.7998769444333549,1.7998769391495262,1.799876933767337,1.7998769282849558,1.799876922700517,1.7998769170121198,1.7998769112178292,1.7998769053156722,1.7998768993036411,1.7998768931796894,1.7998768869417328,1.7998768805876484,1.7998768741152738,1.7998768675224055,1.7998768608068003,1.799876853966172,1.7998768469981923,1.7998768399004896,1.7998768326706478,1.7998768253062056,1.7998768178046567,1.7998768101634475,1.799876802379976,1.7998767944515932,1.7998767863756,1.799876778149247,1.799876769769733,1.7998767612342064,1.79987675253976,1.7998767436834342,1.7998767346622133,1.7998767254730257,1.7998767161127427,1.7998767065781767,1.7998766968660815,1.7998766869731495,1.799876676896012,1.7998766666312374,1.7998766561753297,1.799876645524728,1.7998766346758053,1.799876623624866,1.799876612368147,1.799876600901814,1.7998765892219606,1.799876577324609,1.7998765652057065,1.7998765528611245,1.7998765402866577,1.7998765274780222,1.7998765144308544,1.7998765011407092,1.7998764876030582,1.7998764738132889,1.7998764597667025,1.7998764454585126,1.7998764308838435,1.799876416037728,1.7998764009151074,1.7998763855108273,1.7998763698196376,1.7998763538361895,1.7998763375550357,1.7998763209706263,1.7998763040773076,1.7998762868693212,1.7998762693407997,1.7998762514857674,1.7998762332981366,1.7998762147717056,1.7998761959001575,1.7998761766770568,1.7998761570958475,1.799876137149852,1.799876116832267,1.7998760961361626,1.79987607505448,1.799876053580027,1.7998760317054774,1.7998760094233692,1.7998759867260996,1.7998759636059238,1.7998759400549533,1.7998759160651505,1.7998758916283286,1.7998758667361472,1.7998758413801101,1.7998758155515617,1.7998757892416855,1.799875762441499,1.7998757351418515,1.7998757073334222,1.799875679006715,1.7998756501520563,1.7998756207595912,1.7998755908192803,1.7998755603208967,1.7998755292540214,1.7998754976080404,1.7998754653721416,1.7998754325353088,1.7998753990863212,1.7998753650137467,1.7998753303059387,1.7998752949510333,1.7998752589369436,1.7998752222513563,1.799875184881727,1.7998751468152763,1.799875108038985,1.79987506853959,1.7998750283035785,1.7998749873171853,1.7998749455663865,1.7998749030368946,1.7998748597141538,1.7998748155833364,1.7998747706293345,1.7998747248367577,1.7998746781899255,1.7998746306728641,1.7998745822692983,1.799874532962648,1.799874482736021,1.7998744315722075,1.799874379453674,1.7998743263625576,1.7998742722806598,1.7998742171894386,1.7998741610700035,1.7998741039031092,1.799874045669147,1.7998739863481399,1.7998739259197345,1.7998738643631933,1.799873801657389,1.7998737377807956,1.7998736727114817,1.799873606427102,1.7998735389048903,1.7998734701216508,1.7998734000537495,1.7998733286771076,1.7998732559671906,1.7998731818990015,1.799873106447071,1.7998730295854486,1.7998729512876932,1.7998728715268648,1.7998727902755134,1.7998727075056695,1.7998726231888358,1.7998725372959743,1.7998724497974983,1.7998723606632607,1.7998722698625438,1.7998721773640471,1.7998720831358779,1.799871987145538,1.7998718893599137,1.7998717897452627,1.7998716882672026,1.7998715848906988,1.7998714795800503,1.799871372298879,1.7998712630101152,1.7998711516759844,1.7998710382579939,1.7998709227169192,1.7998708050127883,1.7998706851048691,1.7998705629516538,1.799870438510843,1.7998703117393322,1.7998701825931935,1.7998700510276624,1.7998699169971193,1.799869780455074,1.7998696413541484,1.7998694996460582,1.799869355281597,1.7998692082106167,1.7998690583820096,1.79986890574369,1.7998687502425739,1.7998685918245605,1.7998684304345118,1.7998682660162317,1.7998680985124456,1.79986792786478,1.7998677540137384,1.799867576898682,1.7998673964578051,1.799867212628113,1.7998670253453977,1.7998668345442141,1.7998666401578567,1.799866442118332,1.7998662403563352,1.799866034801223,1.799865825380987,1.799865612022227,1.7998653946501224,1.7998651731884046,1.7998649475593276,1.7998647176836378,1.7998644834805457,1.799864244867693,1.7998640017611214,1.7998637540752418,1.7998635017228,1.799863244614844,1.7998629826606891,1.7998627157678828,1.7998624438421706,1.7998621667874568,1.79986188450577,1.7998615968972222,1.7998613038599722,1.7998610052901853,1.7998607010819911,1.7998603911274438,1.7998600753164795,1.799859753536872,1.7998594256741893,1.7998590916117474,1.7998587512305662,1.7998584044093187,1.7998580510242868,1.7998576909493091,1.7998573240557318,1.7998569502123563,1.7998565692853872,1.7998561811383786,1.7998557856321784,1.7998553826248724,1.7998549719717278,1.7998545535251322,1.799854127134536,1.79985369264639,1.7998532499040811,1.7998527987478716,1.7998523390148304,1.799851870538768,1.7998513931501663,1.79985090667611,1.7998504109402138,1.7998499057625506,1.799849390959574,1.7998488663440453,1.799848331724951,1.7998477869074265,1.7998472316926717,1.7998466658778678,1.7998460892560926,1.799845501616231,1.7998449027428873,1.7998442924162923,1.7998436704122098,1.7998430365018412,1.7998423904517264,1.799841732023645,1.7998410609745117,1.7998403770562732,1.7998396800157996,1.7998389695947752,1.7998382455295858,1.799837507551204,1.799836755385072,1.7998359887509798,1.799835207362944,1.7998344109290814,1.7998335991514793,1.7998327717260652,1.7998319283424704,1.7998310686838945,1.799830192426962,1.7998292992415792,1.7998283887907864,1.7998274607306073,1.799826514709894,1.7998255503701686,1.7998245673454634,1.799823565262153,1.799822543738788,1.7998215023859185,1.7998204408059206,1.799819358592813,1.7998182553320727,1.799817130600446,1.7998159839657526,1.7998148149866908,1.7998136232126318,1.7998124081834141,1.7998111694291303,1.7998099064699118,1.799808618815705,1.7998073059660467,1.7998059674098301,1.79980460262507,1.7998032110786584,1.7998017922261171,1.7998003455113458,1.7997988703663603,1.7997973662110307,1.7997958324528085,1.799794268486451,1.7997926736937386,1.7997910474431855,1.799789389089744,1.799787697974505,1.7997859734243866,1.7997842147518217,1.7997824212544358,1.799780592214718,1.7997787268996859,1.7997768245605432,1.7997748844323294,1.7997729057335636,1.7997708876658793,1.7997688294136516,1.7997667301436193,1.7997645890044969,1.7997624051265777,1.7997601776213326,1.799757905580998,1.7997555880781566,1.7997532241653091,1.7997508128744395,1.7997483532165695,1.7997458441813063,1.7997432847363815,1.7997406738271808,1.799738010376265,1.7997352932828827,1.799732521422476,1.7997296936461713,1.7997268087802705,1.7997238656257237,1.7997208629576014,1.7997177995245501,1.7997146740482464,1.799711485222835,1.7997082317143644,1.7997049121602087,1.7997015251684823,1.7996980693174487,1.799694543154914,1.7996909451976186,1.799687273930616,1.7996835278066456,1.799679705245495,1.7996758046333563,1.7996718243221734,1.7996677626289808,1.7996636178352365,1.7996593881861462,1.799655071889981,1.7996506671173873,1.7996461720006918,1.7996415846331983,1.7996369030684811,1.7996321253196683,1.7996272493587269,1.7996222731157359,1.7996171944781605,1.7996120112901195,1.7996067213516498,1.7996013224179703,1.7995958121987405,1.7995901883573209,1.7995844485100279,1.7995785902253942,1.799572611023426,1.7995665083748615,1.799560279700433,1.7995539223701316,1.7995474337024753,1.799540810963782,1.7995340513674487,1.799527152073236,1.7995201101865639,1.799512922757812,1.7995055867816339,1.7994980991962801,1.7994904568829355,1.7994826566650708,1.7994746953078076,1.799466569517301,1.7994582759401427,1.799449811162781,1.799441171710963,1.7994323540492017,1.799423354580265,1.799414169644694,1.7994047955203487,1.7993952284219825,1.7993854645008525,1.79937549984436,1.7993653304757313,1.799354952353734,1.7993443613724354,1.7993335533610042,1.7993225240835566,1.799311269239051,1.7992997844612308,1.799288065318625,1.7992761073145964,1.7992639058874556,1.7992514564106288,1.799238754192894,1.7992257944786811,1.7992125724484411,1.7991990832190872,1.7991853218445122,1.7991712833161837,1.7991569625638182,1.799142354456143,1.7991274538017432,1.799112255350002,1.7990967537921325,1.7990809437623094,1.7990648198389017,1.7990483765458078,1.7990316083539037,1.7990145096825978,1.798997074901503,1.7989792983322315,1.7989611742503084,1.798942696887213,1.7989238604325533,1.7989046590363718,1.7988850868115933,1.7988651378366125,1.79884480615803,1.7988240857935391,1.7988029707349658,1.7987814549514702,1.7987595323929093,1.7987371969933685,1.7987144426748638,1.7986912633512206,1.7986676529321342,1.798643605327411,1.7986191144514052,1.7985941742276417,1.7985687785936433,1.798542921505956,1.7985165969453853,1.798489798922439,1.7984625214829906,1.7984347587141591,1.7984065047504196,1.7983777537799361,1.7983485000511354,1.7983187378795147,1.7982884616546968,1.798257665847729,1.7982263450186393,1.7981944938242458,1.7981621070262297,1.7981291794994743,1.7980957062406753,1.798061682377225,1.7980271031763784,1.7979919640547022,1.7979562605878117,1.797919988520404,1.7978831437765845,1.7978457224704978,1.7978077209172632,1.7977691356442203,1.7977299634024884,1.7976902011788445,1.7976498462079202,1.7976088959847278,1.797567348277511,1.7975252011409313,1.7974824529295865,1.7974391023118703,1.7973951482841715,1.797350590185417,1.7973054277119627,1.7972596609328308,1.7972132903052986,1.7971663166908396,1.7971187413714171,1.7970705660661324,1.7970217929482295,1.796972424662452,1.7969224643427595,1.7968719156303943,1.796820782692307,1.7967690702399295,1.7967167835483022,1.796663928475549,1.7966105114826953,1.7965565396538317,1.7965020207166116,1.796446963063084,1.7963913757708536,1.7963352686245628,1.7962786521376857,1.7962215375746347,1.7961639369731606,1.7961058631670448,1.796047329809073,1.7959883513942734,1.7959289432834176,1.7958691217267617,1.7958089038880205,1.7957483078685579,1.7956873527317785,1.7956260585277026,1.7955644463177083,1.795502538199421,1.795440357331733,1.7953779279599258,1.7953152754408828,1.7952524262683596,1.7951894080982935,1.795126249774124,1.7950629813520993,1.7949996341265393,1.794936240655026,1.7948728347834928,1.794809451671179,1.7947461278154189,1.7946829010762289,1.7946198107006612,1.7945568973468853,1.7944942031079605,1.7944317715352625,1.7943696476615243,1.794307878023449,1.7942465106838568,1.7941855952533208,1.7941251829112517,1.7940653264263842,1.794006080176624,1.7939475001682068,1.7938896440541257,1.7938325711517815,1.793776342459806,1.7937210206740146,1.7936666702024384,1.7936133571793886,1.7935611494785098,1.7935101167247667,1.7934603303053258,1.7934118633792788,1.7933647908861639,1.7933191895532383,1.7932751379014598,1.7932327162501245,1.7931920067201292,1.793153093235805,1.793116061525289,1.7930809991193881,1.7930479953489018,1.7930171413403642,1.7929885300101707,1.7929622560570586,1.7929384159529065,1.7929171079318307,1.7928984319775438,1.7928824898089601,1.7928693848640187,1.7928592222817141,1.7928521088823113,1.7928481531457356,1.7928474651881314,1.7928501567365736,1.792856341101941,1.7928661331499385,1.7928796492702845,1.7928970073440569,1.7929183267092197,1.7929437281243372,1.7929733337304925,1.793007267011434,1.793045652751974,1.793088616994661,1.793136286994765,1.7931887911736024,1.7932462590702432,1.7933088212916353,1.7933766094611967,1.7934497561659126,1.793528394901996,1.7936126600191569,1.793702686663534,1.7937986107193507,1.793900568749348,1.7940086979340553,1.794123136009963,1.7942440212066568,1.7943714921829803,1.7945056879622863,1.7946467478668464,1.7947948114514807,1.7949500184364742,1.7951125086398438,1.7952824219090189,1.7954598980520011,1.795645076768061,1.7958380975780313,1.7960390997542603,1.7962482222502714,1.7964656036301894,1.7966913819979766,1.7969256949265333,1.7971686793866954,1.797420471676179,1.7976812073484987,1.7979510211418968,1.7982300469083057,1.79851841754237,1.7988162649105433,1.7991237197802696,1.7994409117492671,1.7997679691749042,1.8001050191036776,1.8004521872007768,1.8008095976797283,1.8011773732321017,1.8015556349572555,1.8019445022920972,1.8023440929408283,1.8027545228046389,1.8031759059113133,1.8036083543447106,1.8040519781740643,1.8045068853830677,1.804973181798683,1.8054509710196316,1.8059403543445065,1.8064414306994592,1.806954296565401,1.8074790459046726,1.8080157700871207,1.8085645578155403,1.8091254950504285,1.8096986649340048,1.8102841477134577,1.81088202066338,1.8114923580073585,1.8121152308386947,1.8127507070402313,1.8133988512032773,1.81405972454562,1.8147333848286311,1.8154198862734774,1.8161192794764573,1.8168316113234926,1.8175569249038226,1.8182952594229485,1.8190466501148952,1.8198111281538683,1.8205887205653895,1.8213794501370193,1.8221833353287744,1.8230003901833673,1.8238306242364097,1.8246740424267283,1.8255306450069588,1.8264004274545953,1.8272833803836814,1.8281794894573467,1.8290887353013983,1.830011093419192,1.8309465341080142,1.8318950223772181,1.8328565178683676,1.8338309747776451,1.834818341780793,1.8358185619608614,1.836831572739038,1.837857305808847,1.8388956870739996,1.8399466365901833,1.8410100685110822,1.842085891038909,1.8431740063797437,1.8442743107039508,1.8453866941119643,1.8465110406057037,1.8476472280658918,1.8487951282355288,1.8499546067097692,1.8511255229324393,1.8523077301994166,1.8535010756690842,1.8547054003800492,1.8559205392763123,1.8571463212400436,1.8583825691321136,1.8596290998405032,1.860885724336696,1.86215224774014,1.8634284693908414,1.8647141829301321,1.866009176389629,1.86731323228838,1.868626127738175,1.869947634556962,1.8712775193903022,1.872615543840764,1.8739614646051306,1.8753150336192823,1.8766759982105823,1.8780441012575777,1.879419081356804,1.8808006729964537,1.882188606736669,1.8835826093961638,1.8849824042449033,1.886387711202512,1.8877982470420964,1.8892137255991275,1.8906338579850277,1.8920583528050958,1.8934869163803745,1.8949192529730732,1.89635506501514,1.8977940533395656,1.899235917414006,1.900680355576291,1.902127065271398,1.903575743289454,1.9050260860043344,1.9064777896124319,1.9079305503711557,1.909384064836746,1.9108380301009746,1.9122921440263134,1.9137461054791711,1.9151996145607877,1.9166523728354021,1.918104083555307,1.9195544518824275,1.921003185106065,1.9224499928564598,1.9238945873138513,1.925336683412715,1.9267759990408848,1.928212255233275,1.9296451763599398,1.931074490308224,1.9324999286587723,1.9339212268551924,1.9353381243671708,1.9367503648468747,1.9381576962784797,1.939559871120686,1.9409566464421082,1.9423477840494325,1.9437330506082633,1.9451122177565954,1.9464850622108594,1.9478513658645191,1.9492109158792021,1.9505635047683705,1.9519089304735506,1.9532469964331585,1.9545775116439705,1.9559002907153031,1.957215153915982,1.958521927214191,1.9598204423103012,1.961110536662805,1.962392053507469,1.9636648418698524,1.9649287565713334,1.9661836582287961,1.9674294132481487,1.9686658938118353,1.9698929778605234,1.9711105490691527,1.972318496817525,1.9735167161556428,1.9747051077639803,1.9758835779088966,1.9770520383933914,1.9782104065034085,1.9793586049498975,1.9804965618068384,1.9816242104454382,1.9827414894647126,1.983848342618654,1.984944718740196,1.9860305716621833,1.9871058601355416,1.9881705477448586,1.9892246028215645,1.9902679983549187,1.9913007119009813,1.9923227254897704,1.9933340255307772,1.9943346027170328,1.9953244519278872,1.9963035721306845,1.9972719662814924,1.9982296412250482,1.999176607594083],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Ridge en fonction de alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Ridge pour tout les paramètres de GridSearchCV\n","FigRMSEGRidRidge_log = visuRMSEGrid(Ridge(), 'Ridge', alphasridge_log, 'alpha',\n","                                    GridRidge_log)\n","FigRMSEGRidRidge_log.show()\n","if write_data is True:\n","    FigRMSEGRidRidge_log.write_image(\n","        './Figures/EmissionsGraphRMSERidge_log.pdf')"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.2.3 Modèle Lasso"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre  Lasso()\n","0  lasso__alpha      0.1\n","       Lasso()\n","R²   -0.370634\n","RMSE  2.463609\n","MAE   1.552973\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_pred_logLasso=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4.7694281507709535,4.950019761766107,5.347480204730794,4.749758151470262,4.761830364064951,4.79970959148213,5.931456108872583,4.825945396658166,4.943648392631316,5.10722072805941,5.399133245178249,4.915813445166881,4.824885700098674,5.008755942325481,5.835736858487379,10.000801282230238,5.158228870916556,4.712922228065204,5.211121757158051,5.189319855731078,4.706982308806205,4.679744912305821,5.25131381962999,4.8608392530990185,4.79620234940499,4.650901719164121,4.736969868030728,4.951290641376401,5.433708335874738,4.673675970894652,4.937568045191943,5.011606779300082,4.979937523347942,5.0770795614641555,4.948854144845618,4.68808359594588,4.9156909890088505,4.7910604249405795,4.734487169749478,4.7666902307156525,4.688438433547584,4.65130502677459,4.746171599825715,4.827531876062499,4.819373508576595,5.342046298430462,4.783357492003574,4.74950540182474,4.945429566582167,4.765963504555052,5.756817356701806,4.795776808063237,4.718244792090774,4.728356913318843,5.512870748182502,4.970269407799568,5.215741894580744,4.727689516831713,4.6932730958708095,5.686209958455045,4.888867435170955,4.835751175653584,5.102992246639096,4.680365878108804,5.0322760695993765,4.731743357048367,5.056025312412685,5.16447525358446,4.654761423891573,4.773042881885085,4.790124588675711,5.024669238512834,4.706029185210638,4.700355063004832,4.721945271232914,4.799818800446517,4.658012401073133,4.764497627301885,4.7191318860950355,13.12103942738078,5.606157055191549,4.830654387330788,4.686627283288884,4.678702576850814,5.6682145235595165,4.747443696110641,4.6957206107890626,4.737610169620606,4.756606487400125,4.68585254568648,5.655370567139956,4.708935029455841,4.704287846423725,4.889557189737453,4.674481487880535,12.08949175357365,4.637158669503565,4.701294068180699,5.106965161027685,4.782627653004011,4.695926361517786,4.697383298090555,4.678902173001772,4.687137362341334,4.9624469865639185,5.224257997021655,5.615703274822759,4.721621232822432,4.841402961605234,4.705138423570942,5.743375339694762,4.763930133310246,4.817897188338032,5.293852014912166,4.84319529174206,4.6892663879515615,4.854919534507098,9.691302246304858,4.711756095586665,4.754009873384298,5.038329074544117,4.732142598157538,4.744794461800161,4.706202490982923,5.429099029376279,4.875915800185665,4.753728552261237,4.734426865218512,4.687107792541192,4.683498504525139,4.901372120218467,4.888940627045121,4.683603771224359,4.74159754175296,4.68917929234424,4.79518931382823,4.79766267676003,4.693599535118308,5.738876013793875,5.011606779300082,5.17492078548464,4.827660445066405,4.713380559967406,6.78816776428466,4.682628756064372,4.875915800185665,5.151112312334407,5.31053528138613,4.718533097642159,4.848004467564138,4.9554133765584165,5.0208103795942955,4.659416139944953,4.707276633125565,4.774123035321014,4.887302731952926,4.912283490342357,4.743286364639454,4.909148732781741,4.693886669223757,5.090303841771719,5.1823814052332615,4.951994062213703,4.694273954074888,4.823931498880118,4.6859065405625495,4.922888746344995,4.718473958041875,4.735005416517389,5.65466346826667,4.682708670831523,4.94115673046164,4.98862948722829,4.677793305496445,8.359289456556024,5.229610659694192,4.910374568513763,4.8311866437333455,5.267607324029905,12.906613084291163,10.82516784608384,5.990033030072901,4.969846829049467,4.925290310960953,4.806384973864199,4.755946287271891,5.091599710086258,4.859898919671279,4.699475361450606,4.688908929862238,4.7922787141769145,4.753728552261237,4.930053270508299,4.646416034184918,4.686180237063368,4.9134323582858155,4.953125107069136,4.6816521644149836,6.969804350524614,4.739920660600311,4.686841664339913,4.714164159671171,4.834056648644362,4.687137362341334,4.887739295274755,4.78767488799312,6.579414879151121,13.068803054185238,5.3905905450513,4.68032152340859,4.823104039535409,5.303578985902712,5.886008007111741,4.724661544349202,5.287932518793367,4.7354702767849295,4.94485405218249,5.077206821911785,4.767338988558192,4.7856963494348985,4.719755346591474,4.656406478530885,4.807084170145138,5.142428860371854,4.661816146365156,4.842437952687396,4.725323240327023,5.326268937222708,4.9878074679320425,4.646416034184918,4.685528846664752,4.721460507856222,4.9003658226678946,4.707566912432533,5.658584978167126,4.743286364639454,4.680380663008875,5.05711103260518,5.297718968782553,4.702365809414491,5.014244668671886,4.856299736115461,4.798615508876874,5.215638747740088,4.705529778029691,5.079883447828082,4.712730024364281,4.682921364130545,4.707391329355371,5.764889466621745,4.7824260432990995,5.623714146373627,4.931684895034481,4.712655721149087,4.6326300743893984,4.770287640340787,4.876613249201574,17.078681801819652,4.729954432947027,4.6951803479799725,6.1963964234447655,5.099554757372583,4.865744955822768,5.123210597486225,4.976768356193194,4.668200805001594,5.107679059961612,4.764173802753725,4.768129505571327,5.065506276523312,5.641169249757938,4.742560660627179,5.061188880515807,4.914482584897336,6.929178748712076,5.303253074359284,5.0443556289997185,4.903162804534674,4.899337640610741,4.773037631753998,5.085393654011314,4.750150606444049,4.7891531728314165,4.706483404084272,4.913054611937296,4.824774939926243,4.905850389092029,5.202264202055395,4.672147905055759,4.80102881245853,4.775960599508486,4.715749749073177,4.672437419406543,4.679552708604898,4.668205400943077,4.800825851437493,4.694470672776563,4.646406466640396,4.7595984207572615,5.027755737064253,4.894362521736841,5.090851533600367,5.242305470713502,6.986452338985405,5.017282706003959,4.664882263773369,4.664856496311643,4.65231779700738,4.9970878536975425,4.916771803189861,5.080649378110091,4.786846728420337,4.77940027431328,5.0808001116324855,4.686456845104411,5.559655206883706,5.7775331859157335,4.687137362341334,4.782098079794146,4.650596918071034,5.909177700760979,4.703814729621452,5.243995838616476,7.26076743516351,4.646406466640396,4.721260776737292,4.740266900746568,4.711369813557746,5.816565558300124,4.859651350757906,4.901865792649499,5.393399660880384,4.725459823325434,4.693064526306125,4.885126645776061,4.858670950166368,4.973703106276018,4.739535048193052,4.747808898240568,4.863462080588396,4.894362521736841,4.714017283823456,5.0667205991172795,5.19904234995052,5.060734983807257,5.555121883261122,4.739778999044224,4.957602718956593,4.8197569047060025,5.233432027515727,4.685954570335652,4.7784636900800646,4.832325132108093,5.204792924215467,4.757264911891389,5.203485212421863,5.030246974450439,4.985258791868203,5.1823814052332615,4.977956346738425,4.704857065076459,4.747223196229986,4.68045458750923,5.719402474908938,4.702332046516992,4.674954604682808,5.668381207287457,4.842831463180911,4.782627653004011,4.981735669904051,6.769829638020594,4.671632946504554,4.718591957402628,5.164472890330015,8.33339205422183,4.685865860935226,5.9293995564776045,7.291687693844768,4.707333535838356,4.7363303612156225,4.691631971962926,7.39468871989838,5.756776696978868,4.6837806882899065,5.4420142969368746,4.7354702767849295,4.874962097108258,5.027755737064253,5.004000883995113,4.722739401712365,4.767338988558192,5.853456021826109,4.723789129617408,4.6958826307333466,4.726651638353995,4.6423765021957095,5.03426463865893,4.832325132108093,5.56596660746322,7.387361889589673,4.8510505276649765,5.2274922454093575,5.178683412180629,5.398924275784128,4.773998650258615,4.675538608235613,5.0770795614641555,4.676669653091047,5.2270812097900015,4.807804324271017,5.515704534140768,4.918735429503928,4.8939159500912,4.7051048482965285,5.617744515827421,4.691957239764489,4.71070449305455,4.838460814568291,4.753771895671052,4.744710175051567,13.071536208114836,5.3142684961388555,5.231060463688113,4.855878289998513,5.542928538377807,6.513901480385229,5.579918272557502,4.794409292517304,4.826385859467846,4.983396080919914,4.810554315684228,4.830165652805284,4.784806412210535,4.679212655903264,4.67910689806758,5.458403924088636,4.769361633743061,4.725528975115351,5.173624721923125,4.716497954082304,5.496804056537685,4.86993047436947,4.6857771515348,4.654488995347468,5.608244085061067,4.70087237555981,4.765686385873273,4.719017966711273,4.708043211041765,5.661808727698906,6.987811482479705,4.819373508576595,5.587434282366351,4.681286584776858,4.712330832062364,4.689702542503657,4.762432271801717,5.008399914343828,4.788739195629428,4.637219934643059,4.699778451902062,4.758267516583042,6.705801113743856,4.836523992658986,4.695376082447448,4.680365878108804,4.682568828219387,4.678658222150601,4.708856380545672,6.804439743946963,4.819491573343867,4.854919534507098,4.958779878401116,5.931690013424167,4.736501081475975,4.941378503962706,7.1234277874931085,4.8868622765598735,4.814716265054222,4.717949094089353,4.824814266372079,4.772002688749026,4.820177522902391,4.7168248332232725,5.51765369849876,4.672147905055759,5.8237557377614015,5.491106061150364,5.122504169958782,4.729021638925263,4.880225616102089,4.722034935456678,4.894350373594567,4.701946286466968,5.6843766983785935,4.832650348840377,5.426739419131944,4.887508839707444,4.811214348684915,4.693665539408764,4.900009990573489,5.706006081949952,4.78767488799312,5.210586360406219,4.739639357005011,5.849154642760635,4.701212587208952,4.907269739498847,4.771204304145191,4.692164228365483,4.768809150333684,4.979937523347942,5.076492059796749,5.097650199745859,5.045515947612981,4.714330794486256,4.970238781940071,4.759409321804555,5.426894054998103,4.879333670814643,4.800744534487102,4.701057345758206,5.886008007111741,4.866348705472725,4.7561549583348635,4.746354233684175,4.738809658505303,5.578568411013455,5.007208271528951,5.249084768259864,4.70810235064205,4.736550361243303,5.2723322718145145,4.818425629187346,5.4920864618083005,4.948674852147757,7.318917134374443,6.416731592400063,5.2129551766355915,4.784229801107765,5.191343944137763,5.254640966667615,8.508471801821178,10.888455784978259,4.729021638925263,6.519876238402599,4.827401709315162,4.960828040006141,4.665679148996306,5.071184298181052,4.813518335756003,5.0322760695993765,5.032282683511372,4.758681482385115,5.330193963633203,4.758059067178425,4.7218692386812755,6.300765386150648,5.223637761939239,4.916045091022944,4.907438694648687,4.794440575239061,4.778419335379851,4.739610025068106,5.104714687497371,4.745962083579184,4.889856580795422,4.739838138644508,4.673942138783252,4.690505699069909,4.880744449701602,4.724883106984268,5.043224288101972,4.723224315322764,4.68746806789707,5.440664204151014,4.805495705549389,4.681695487999096,4.969377829853687,4.771778762277118,4.723035099713786,4.855306737799878,4.92549370997347,4.677515087661474,4.771778762277118,4.739639357005011,4.874962097108258,5.053610726929139,6.116570143903948,5.017444543329679,4.720713870402635,4.876555432329289,5.95605905823701,4.775926129097781,4.735513555373733,4.831654688123746,4.926580725682113,4.7912526286415025,4.6892663879515615,4.773895155958117,5.06523900430773,4.72046956507673,4.906678343496006,4.848146170321549,4.7547736386298265,4.719686319847699,4.760840089195401,4.646605400027603,5.57358342267656,8.859228424039692,4.763864328060751,4.740008340416598,5.071966133840047,4.817897188338032,6.027521617880525,5.042313386228637,4.681356466413563,5.021882284849445,4.892599574584537,4.7547736386298265,4.673828808979117,7.914523314248072,4.718525705192123,4.941943672893882,4.798157176974672,4.787504656473497,4.704339593573973,4.672794195672393,4.7484059882356675,4.677275833993959,4.941674201964126,4.706372580640749,5.146255118769924,5.061491319479096,5.1803764136108486,4.935490712103202,5.622704548483462,5.761269855386544,4.6930602210765455,5.297718968782553,4.685772475672889,4.721072753130015,5.858366941709523,4.719939787940094,4.8082904863501765,4.681695487999096,4.849889542323194,4.961153307807703,4.841402961605234,6.75345281891789,5.5520022693461355,4.951133951153275,4.783621507621413,4.7439196839740205,5.531600194101215,4.812172712358498,4.641953846703503,4.904650876568095,4.795993612092885,4.715849638279267,5.0050898027834805,4.845557566602383,4.688468003347726,4.794978423459401,4.803821086539399,5.3095949037052295,5.285807536017339,5.01250996531632,4.7758171929673505,4.687400832683572,5.663347351781036,4.758600176834641,5.847669373942976,5.426490681232494,4.693790567373296,7.387361889589673,6.524322377593817,4.907846350601617,4.74950540182474,4.689072652476042,4.708469363344017,4.67751067794152,5.05711103260518,5.338279146369412,4.8461933173054375,4.678059433697724,4.613653647404898,4.693565780610678,4.748701686237088,4.715066038575503,5.8084477041513125,4.856454037954729,4.723789129617408,4.686802917471242,5.444161135819084,4.856868015156718,5.031895930432051,4.833064326042366,4.889088394160559,4.67455541238089,4.81595922091838,4.701877907712148,6.704741538059391,73.42542004035927,4.865886804200044,5.43079565856386,4.748701686237088,4.7697011863418135,4.7064538342841304,4.693886669223757,5.425813147239924,7.702117466070366,5.466552337903917,5.053610726929139,5.143991376290737,4.763050431756019,5.019929090011115,4.706180313632816,4.748997384238509,4.696178328734766,4.745862985423451,4.715561332727883,5.004406532965492,4.685051044096697,4.752412696154916,4.742426350337762,5.1319189366070805,4.783002654401869,4.6853409969827045,4.863874790561389,4.69622376031237,5.851444342366395,6.614760528888329,5.170956092704861,5.018880950135027,4.847316969710835,4.821147696585118,5.940432258507087,4.718200437390561,4.934247659618064,5.371567348879328,5.156614452599205,4.741680002597107,4.832008298614353,5.995896716628599,4.704051288022589,4.685299676919666,4.689109879720355,4.741671395322417,6.521150416853178,5.508556840487424,4.797158626184431,4.666201440520441,4.8629979249980355,4.930291852883879,4.648361471356673,5.192129850347699,4.714999506525183,4.78069620999079,4.684180382327129,4.794076544555069,5.896422360057303,5.256701634041566,8.357257443677602,4.806244517313523,5.207523836079088,4.918735429503928,4.649330792644051,4.896380660596536,4.945170830830924,4.711532447458528,4.865783309899547,7.374751282152602,4.707119154787327,5.06775576175454,5.070811103396325,4.696658837987075,5.315072672745967,5.743375339694762,5.471387601708863,4.693029145019638,4.826337196510049,4.995872253692,5.332983107861125,4.794486353081286,7.675442754519608,4.866887089709308,4.81923305202592,4.917791326555173,5.245600743124682,4.694470672776563,6.0349887219568075,6.902854306578464,4.683498504525139,4.646464935087059,6.400686127002745,5.5240070610616465,4.733428294928806,4.785368238413234,5.795967777165719,4.837584617447998,4.720132733725817,4.696151581527126,6.079105858344134,4.77234274145066,5.008358983847134,6.4640312242984885,4.729950180372563,4.688793271149289,4.642052405840082,4.738628902980481,4.749677489641776,4.691809390763778,4.864319604792516,4.782862684445652,4.669234454524537,5.187386108941002,5.649196845641983,4.74606258157441,5.206050392584187,4.997302271807134,4.657037498472846,4.740008340416598,4.893346249085665,4.882578956380233,4.892632688428531,4.770979043822212,4.764817934750786,4.7553696761691215,4.778508044780278,6.686466847616305,4.744056776438555,4.739330968399152,4.712330832062364,4.953768083226739,4.761939917700009,4.964137018285727,5.75941525405598,4.723803881925283,5.149117310034001,5.6170187514643075,4.7039256163719845,4.785368238413234,4.67911069286306,4.623196972483741,4.9987217388881815,5.53668458840407,5.254604004417438,4.821374499689933,5.085393654011314,4.718244792090774,5.357985192802222,4.785190819612382,4.717948146123923,4.701567424810656,5.248843548712073,4.77590803059934,6.430196425132954,4.661626017268778,4.720684300602493,5.15856627774273,4.717076784985163,4.778145814728537,4.717090532677206,4.678177712898292,4.706180313632816,4.725215848639898,4.844364153812435,4.834299869630786,4.781042652877614,4.8618653113807255,6.667397308054481,4.657722522242334,4.714770340574082,4.902732893074405,6.400686127002745,4.906781837796504,4.803524095700454,4.864182115023749,5.183461107845345,4.762673416804208,4.76456305764477,5.316589697965216,5.369504963250566,4.831938894344421,6.180225146440941,4.677364543394385,4.787251864896587,4.721260776737292,4.776881705772465,4.936051995678246,6.342122114041862,4.75520704226834,4.848184211711142,4.866860809270422,4.690567459157812,5.118256685233086,18.882812752548897,5.118363059193696,4.827440190354154,7.078759210473031,5.102768089134629,4.657722522242334,5.004317823565065,4.7663696418219645,7.191795080493015,5.164634537448346,5.85489312465122,5.309598388087532,4.795603162938114,5.55458808089652,4.674644121781316,5.14846997919119,4.81827742018824,4.677263767213974,6.554839863813756,6.899209756268105,4.708264984542831,4.794833256739317,4.794076544555069,5.579227884065137,5.038678237943348,6.902854306578464,4.744916751818906,4.868873354014392,4.838505169268503,4.802296725549971,5.478647970006459,4.816608732263313,4.73995596025013,4.641304358230236,5.01125674688216,4.718126512890206,5.211228074390954,5.033202364255311,5.082352526139943,4.92722327621841,4.82506569510394,5.041571508481777,5.28652828728406,4.6958826307333466,4.794180038855566,4.716702491199401,4.763607461628592,4.688157520446235,5.342718196315518,5.25507712121971,4.783800370572719,4.791577896443065,4.891013741870754,5.39240666552944,4.707331674197037,4.717357698086512,4.718926662286832,5.090851533600367,6.196121432913255,5.858092268600162,4.6921158483832555,5.124614991455477,5.0630730164473245,6.045931034393869,4.707198082336534,4.716914864187348,6.209686371655643,5.0008324599345855,4.8007881532126815,4.741974556704771,4.859292738768366,4.758496682534143,5.2122507624685435,4.707419234223242,4.7074733372671265,4.772837064709169,5.579227884065137,6.081492671138193,5.265362520295569,6.6855316568652325,5.446590978351917,5.563671675222723,4.870669719373021,5.04629215486671,4.793068420454615,4.7757885780444225,4.802821812947081,4.67766956718404,4.722852516942564,4.667546768776479,4.691713288913316,4.684882665080503,4.745360298821036,4.94493237855912,4.751034218921959,4.969536897527528,4.9334252008774895,5.075084091246603,5.2862880452696475,17.356282659048265,4.717949094089353,4.7609369636618535,5.49099761259031,4.757869485075166,5.036700610301263,5.032895019262169,4.889113882211627,4.791430047442355,5.174746635507237,5.012239435592051,5.006109359034091,5.676861644350017,4.76082530429533,4.98628134070342,5.622581973933943,4.713277065666909,4.719161455895177,4.808357025496969,5.606157055191549,4.988726015465493,4.823054294384584,4.756808814188766,5.0755760050321195,4.824289487850211,4.735682580387828,4.982509433250425,5.0997922280941035,4.720395995051108,4.859788032920746,4.9816527439822895,4.689757845175078,4.803849363502017,6.0534703465723405,4.710459076381956,5.131859764027776,4.853772973336927,4.779940092721276,4.786218370167318,4.724543159521031,4.759684082498843,5.0102317835934755,4.752823909308786,4.674496950792135,6.0924609045384575,4.656016930796315,4.821233785497926,4.821201649361324,7.212105182488552,5.223409308418095,4.745849328505967,4.862988963786123,4.692992644176351,4.688994772422874,5.6479193085569195,4.746396642078396,5.530693894753201,4.807468402891692,5.285972977262845,4.839223116236463,4.689399452052201,4.705470638429407,6.122706577317178,4.788680056029144,5.261235032099293,4.717357698086512,4.782337333898673,4.71444507277252,4.685051044096697,4.693747974437869,4.94285812355264,5.002797200479507,4.758298582736078,4.671893969464546,4.844546753399313,4.696430471317616,4.760012134791424,4.788367401005185,5.3528016564323995,4.908355365744471,4.984309438006416,5.783348993207523,4.746967080218339,4.836050875856713,4.683352427923151,4.837691999764597,4.843147627890805,4.717357698086512,5.115396777798687,4.685110938212198,4.806540215314944,4.823158442994778,5.004628306466557,4.898886701158576,5.144938603586086,4.82279382803115,4.988512765389138,5.610654983012282,4.678222067598505,4.688157520446235,5.021512802435126,4.896797095084643,5.821070971039038,4.677882014896872,4.813363446697723,4.7891531728314165,4.687861700426678,5.250020685395419,4.902499899368248,4.705815702587721,4.955605029373708,4.7295758224519515,4.707262007633009,5.309428000078627,4.749987233058796,4.820970277784266,4.925935675838518,4.8228257827431795,5.522181628073914,4.835039157349664,5.576156214020804,4.731665687141729,4.81487130500775,4.935826773986035,4.8462007151173365,5.1879585125184935,8.482066028825866,5.407535918935796,5.565751418740678,5.046974495482634,4.910300644013408,4.6921158483832555,4.698378496670232,5.776467269028329,4.893520072248313,7.177086799149327,5.851444342366395,4.680380663008875,4.706180313632816,4.858115281351747,4.7758171929673505,4.698063403493823,4.6853409969827045,5.275932702450799,5.231038573729876,4.967346391010981,4.787504656473497,5.074601137783722,6.291079460831847,4.735876345996791,4.710618581860958,4.998014608699485,4.7043913407242215,4.819373508576595,5.8876116798950076,4.665278805074156,4.7498031612923794,4.827933965717719,5.303216241058903,4.762658631904137,4.705529778029691,4.886322996207127,4.678702576850814,4.822925388991658,4.80943066327883,4.743660035312868,6.408181047559209,4.67879128625124,4.908594312032788,4.8887373576841755,6.704741538059391,4.722739401712365,6.130278431755465,4.879122406019497,4.986039715910505,4.778145814728537,13.60991233389371,4.765751868549669,4.752063556375373,4.67751067794152,5.788820013614627,5.084104109716927,4.930627705804478,4.933113744823002,5.352674164678977,5.222643822426295,5.096627107151738,4.945429566582167,4.853370103459785,5.374108600632527,4.898979380959792,7.605391529595673,4.684113850276809,4.853193426779368,4.819473244557641,4.866860809270422,4.839223116236463,4.796042936264515,5.437782963727726,5.3905905450513,5.589375864927948,4.677519784845131,5.240330031046471,4.6705171692957865,4.895023406674287,5.102992246639096,4.71331967388603,4.7185848447924075,4.881514443575119,4.679249618153442,4.70087237555981,4.682901376356721,4.694273954074888,4.882242826056076,5.988188181769143,6.1963964234447655,4.731841476135719,4.774935477240157,4.881477481324942,4.6932730958708095,4.746254331474373,4.708841595645601,4.744125759665105,4.791577896443065,5.116306200972599,4.814938038555287,4.727359014912821,4.68794272987745,4.947144614990406,4.889719832028104,4.817384939517042,4.9746479672034765,4.6857771515348,4.895885366444157,4.744096189864964,4.826337196510049,4.725911190278514,5.546206793735687,4.840811613679583,4.794534876457271,4.8719338283290945,4.746897928428423,4.919082874655597,4.790217685636531,4.823212073144009,4.872392401052825,4.708884765184062,5.078324576652164,4.892566156378211,4.679212655903264,4.823413189339384,4.80949719532915,5.075084091246603,4.706838241685976,4.778641108880917,8.89480316785356,4.7278245058986395,4.861685870034994,8.451717417147012,5.095939411535973,6.194628881901266,4.921968643711608,5.060977510783794,4.671043865463951,4.845246716558838,4.75499266121731,4.731425514490336,4.903301115720436,4.632452798255954,4.886730610656858,4.686279838137215,4.737803718585724,4.787105427267357,5.0400529270367365,4.721608356856932,5.003393767310627,4.839792015397979,6.239621725500232,5.139305534601461,4.693599535118308,4.721932000752676,5.031972159121629,4.708942132382048,6.293827237959855,5.277801512628878,4.845246716558838,4.809615525634913,10.429440409394061,4.711310673957462,4.723906876516105,5.008058109926596,4.81038406205892,6.7020160545937895,4.680476764859336,5.344895389151196,4.710260946052419,4.701210688214169,4.644540028192073,4.908684692855077,5.609164954020873,4.856729931317281,4.695143385729795,4.8430967410312675,4.881239968830531,5.70942449285239,5.624978752135026,4.69536530337264,5.066771211462596,4.88049126841468,4.701610058348584,4.770360160794769,8.508471801821178,5.1327828837526175],"xaxis":"x","y":[6.235152624217932,4.478324711485028,6.139142019145689,4.729552770453971,2.689299160535892,2.4698859762744636,7.592232766722112,6.075532631167357,3.9020735793107426,6.768713657030485,8.05642095591347,6.192983169973992,4.934044647112246,4.088311235888661,7.071462362556624,8.345405246717796,8.140829770773001,5.7554217347342425,5.091276694365222,8.047178455318921,2.475084882948783,5.260778431893426,7.873013392989595,4.4495613746132365,3.4369613378336026,5.575917361118149,2.821710215034674,5.650190123496775,8.870210577263595,2.2868811477881614,5.54936113292434,3.682573297347578,10.919861451442168,7.606886339275915,3.4276061727818994,2.1009776477248208,3.548436624696042,6.819540460505722,4.475084882948782,6.762348815644129,3.929790997718597,4.173127433480656,2.7676547982373463,4.804260115634738,1.9335726382610239,4.846493315964563,3.10936055940423,5.617651119427331,5.739578112048051,5.089159131911238,5.6806056662025926,2.8579809951275723,2.1176950426697547,5.465648025982281,6.027463723408577,6.299391206126829,6.422064766172812,2.4356285940520905,4.153805336079036,8.148324042480084,6.387500406480984,5.944155663440363,6.496494306853713,5.703211467391146,3.192194165283345,5.185866545311334,4.718087583960517,6.035403969942067,1.3785116232537298,6.335211681705785,7.330737650206952,5.452858964713811,2.726831217032493,3.8369340113210915,2.4672794804599825,2.9616233282869446,2.0531113364595623,6.248496885959384,2.817623257511431,8.971342367308225,6.606294160697713,5.901832305785489,5.339137384919585,2.582556003014061,4.779259720372376,6.044831233447812,2.7949356628035362,2.8439838440483265,6.690277401023435,4.017921907997263,6.596487956856693,2.84197311892718,2.3074285251922473,5.976821852360685,5.10936055940423,10.16679075071818,5.2787282129389395,6.339493737901733,7.125981653854716,5.148934104526339,2.321928094887362,2.1210154009613658,2.430285272977781,2.5310694927259543,6.72805655144938,6.989025440631378,5.844988156682612,4.749534267669262,9.159820892013935,4.272769732436511,8.601733652922286,4.124328135002202,3.270528942380718,3.894332742277694,4.943921326553485,1.0565835283663676,3.7322691995014496,10.679004987548526,6.500961352460057,5.787902559391432,3.7782085763980877,4.6661884898727815,7.593353770980297,3.077242998932461,7.602439063741618,5.537296067090842,4.203984165855989,5.299757511839318,5.94251450533924,6.951051533288609,3.542258049766918,3.507160349117523,4.71259578044703,4.6194130105979365,2.550900664647523,6.872951851130105,4.712045448553693,2.867896463992655,8.377687734026882,3.678071905112638,4.0232553523003025,3.1538053360790355,4.635173946674924,7.127117349726852,2.4854268271702415,5.592756010441026,8.25559514727367,4.634593268445757,2.5582676340557358,4.990501111363251,3.727920454563199,3.8308637567517576,5.924337464075348,4.187451054027326,3.940166750482817,4.926473911256073,3.5434958834257717,4.708187236020708,5.568640195419223,4.831370587967541,6.12969515665378,5.751410160064371,6.279471295644468,2.523561956057013,4.552131108253784,2.6016965164809576,3.270528942380718,2.805292455600712,5.533874777258546,5.741197298571427,5.787641414483327,3.565597175854225,4.40735275114004,2.137503523749935,11.52102619171175,7.417936880550136,4.96624587322494,6.385085853547684,7.846744023244868,9.156361220297567,11.010164414182617,7.196430000954568,4.008988783227255,3.2464080872463845,5.763145958149476,7.976535484023556,4.4121040446775694,6.918624755574591,1.9745293124838819,4.546585829008101,0.22650852980867975,4.1715271060388135,6.920055054806967,1.3504972470841332,2.3589588258323295,5.726013748860151,3.2357270598380583,2.6848187375532224,9.236301266827088,2.4854268271702415,5.9618548076361595,4.088311235888661,2.440952198029637,5.456806149230474,6.205157883920094,5.588564737401351,6.620439787651722,8.334139178838237,5.331633567171029,2.211012193485512,3.8063240573900288,8.45889870671479,6.602884408718419,3.8439838440483265,6.132988042627442,2.541019153133559,5.953730849811491,4.18824265626441,1.4489009511451278,2.7070829917717063,3.0107798387532423,3.4019034716079584,8.574290766120185,3.542258049766918,4.313245851787562,3.3854310371935203,6.156032498888866,5.419538891513785,7.3107945270667845,1.3950627995175775,5.8346606579017255,6.025250321561575,6.2268938135713885,5.548128323585586,6.746447139362815,4.583158003874408,5.9576824861651225,5.778208576398088,7.834028534577023,1.570462931026041,7.0083165637349865,10.307269127730322,4.95093492831454,5.099295204337775,4.547203024756931,7.9684364043347875,2.8318772411916733,2.4436066514756147,2.4724877714627436,2.37851162325373,2.6507645591169022,8.202417721575182,4.963011647599427,9.01321099459576,2.176322772640463,2.73768676140986,5.5786368624717415,13.52966114241807,5.72328550477764,5.642412772905056,7.568108130120591,6.034743949324734,5.320845667645722,3.8469946869655733,3.8639384504239715,5.154210530588339,5.777945670695148,3.327687364176047,4.169123281476757,8.135247549809566,6.890446692679906,2.2479275134435857,4.9804823555382605,3.3015876466031866,8.873136468833458,5.8346606579017255,4.189033824390017,8.095608004811197,5.380244590478185,4.571676809970931,5.071247819461442,2.9373443921502322,2.944858445807539,6.706254253573203,6.849624029924543,7.541716163393138,7.907912142840534,6.091065077930535,4.388878338811989,4.078951341394822,2.698218478224414,2.5058909297299574,4.621758856709732,5.3175935046234715,1.996388746447621,2.7070829917717063,5.974529312483882,2.883620816285671,4.853995647176393,4.593951283948411,6.496494306853713,3.9845893503624565,6.212180210044431,6.897240425574799,5.452858964713811,2.232660756790275,3.773996325111173,6.075318692553354,4.161081482277184,3.2094533656289497,4.922673592849446,4.478971805032942,3.0840642647884744,6.054414387917307,3.486714373030702,7.582029046119882,7.461888664432819,5.6604951318885135,2.7311832415722,2.049630767724601,7.850749425896496,2.37851162325373,5.986638471730853,7.210817432089213,2.769771739249448,4.970393537914677,2.889473542531109,7.08533966935737,7.463197379833495,6.09950561709114,4.17951105027151,8.27891401949994,2.4594316186372973,6.221297103163338,4.867402305727543,2.980025300238734,3.4276061727818994,5.933572638261024,2.565597175854225,7.056366761024529,6.199672344836364,4.985955754607509,6.215484464192794,6.7410624357490905,4.59215800212536,5.19377174339668,4.099295204337775,6.714108111595169,2.1634987322828794,8.524737587012376,2.18269229751619,2.765534746362977,4.67581593117227,6.658497136656478,6.557808298936688,4.208673319629471,4.993674361750585,5.422905742612183,5.7824085649273735,5.3847405872923835,5.08278773170927,2.4276061727818994,2.411426245726465,7.603403804166527,5.174326515173915,5.896998342338911,6.425089989874059,0,5.106432077816929,2.9467308601403097,8.843732656611305,3.7311832415722,7.403949364232865,6.247927513443585,8.640931770341906,2.6915341649192004,5.336640446408735,7.390942772802543,2.1984941536390834,3.0125686735030555,2.0214797274104517,8.044284820139424,7.1855692560893685,2.636914580355878,8.491372117815171,2.5185351389821804,6.73226919950145,4.614709844115208,4.15623479785027,2.495695162624069,1.0426443374084937,8.060966074004773,3.5643781685650637,3.679198570566922,4.488000770834068,6.3103401206121505,3.605257262939004,4.821199978056451,7.003039979749236,8.211498981977794,8.84080938879262,7.640895629328568,5.95837871156634,6.804001915179304,3.997292407636508,2.3533232911628965,7.631540867341486,2.7990873060740036,4.706530552538117,3.452858964713811,8.983022086172278,6.187253085582297,4.564378168565064,4.241840183564671,5.187055089968057,5.496654082593496,6.564225720206398,6.793115555274348,8.126549613543878,6.3414521074811265,8.823876711241038,5.094658342654544,8.23429054170594,3.063502942306158,6.9910682753603695,7.485507332459785,6.9092930858238235,6.2992080183872785,5.189033824390017,5.562547724199122,3.2357270598380583,2.851998837112446,2.6507645591169022,6.425425734590699,2.889473542531109,5.012121672712217,4.938285791884053,5.642412772905056,6.199672344836364,2.0738202332916713,7.151270288790165,5.005849075696688,2.3923174227787602,2.1667154449664223,5.727920454563199,2.3103401206121505,6.984019633085816,3.9726926540042644,4.362469888750209,7.173227395028653,9.653203362033977,3.357552004618084,8.556735947914529,3.9420452599160467,4.1026581313637385,6.309067020588001,4.952799477899938,2.3015876466031866,4.580748491763774,5.046578366620329,6.999098033705607,2.752748591407134,6.73348992588162,3.1193561770396756,3.431622959713292,5.15623479785027,2.344828496997441,2.720278465233327,4.763411574470007,7.178117045706399,2.807354922057604,3.685940148445977,4.417515002885087,8.72584338467341,2.4724877714627436,3.619413010597937,7.5130959085510085,3.382667252745041,2.7463127664254587,4.509695841933387,1.6553518286125541,3.0992952043377753,7.846242565109514,2.344828496997441,5.39780296186249,4.658211482751795,4.872828759534886,7.290387539763883,6.802451741275622,5.377123749129487,4.270528942380718,6.325710207515785,3.572889668420581,2.548436624696042,6.845113646617966,3.2265085298086795,6.531849285751203,3.361768359419153,5.599912842187128,2.211012193485512,4.396433531250992,7.406162478777607,5.6519127446457835,5.7114949066500875,3.61706334388818,4.044394119358453,5.772149590784542,5.177917792195843,3.1795110502715103,2.321928094887362,4.854494418154875,8.167066838182594,6.5249725982866265,6.43095427138573,5.669310286202474,4.147306698780294,4.303050084681673,4.3298411765306275,6.912410353808268,4.904002316283692,3.2854022188622483,2.8659188145522125,6.582254908357664,5.198494153639083,5.103077946597573,4.3045110418099535,4.370861740085285,8.654779216959517,8.362951992360674,7.2600256559614555,2.6392321632492775,6.457791260256042,7.31786479839195,5.661920579135087,4.609991295212678,3.460742563789644,9.248900055578497,7.886854944198882,3.7059779016825223,3.0373822220030804,5.982537314399271,5.339850002884624,10.306392126750833,8.18685706717715,5.533874777258546,8.85835405767524,6.166715444966422,6.56727163085093,5.7737326506078555,3.396433531250992,7.532083141492505,4.876271360550127,3.817623257511431,5.643567621908798,4.28318097894557,2.6205864104518777,2.881664619320345,5.006746832440585,5.20163386116965,4.181102550753798,8.571145863602528,7.3433189836610735,7.006298023900369,2.9818526532897405,6.419370342528151,2.414135532984451,8.463115619892362,4.857483428286672,6.972003303808356,4.958842675243241,5.910252962799278,4.4963345134171115,4.707082991771706,2.8094144442358986,3.1651079851445365,6.320484678017694,3.8787253414801053,1.3504972470841332,6.207502459258789,6.152994605492435,2.333423733725192,3.0823619695574735,3.4620523187964327,6.266599165535096,6.2861418728343015,3.6064422281316078,6.75702324650746,5.29020320512607,5.342341397431548,7.833712369021444,2.440952198029637,6.642123916153487,6.722602636851373,4.393690764187454,2.729008870337863,7.036503333738988,7.122569192216447,3.18269229751619,1.0565835283663676,2.992768430768924,4.125981653854716,2.7949356628035362,6.039796407620025,3.4154882710497003,5.535430914871279,4.963011647599427,2.763411574470007,5.062639828286463,7.592232766722112,9.317322159829551,4.955126781261366,1.4489009511451278,6.436128517259249,3.2645364309990255,7.647314509547136,6.15319733085508,2.229587922740652,5.417515002885087,5.0925457415435655,5.34801990922869,2.4195388915137843,7.500961352460057,4.426264754702098,6.247547806890298,4.622344722724236,4.638073837180719,2.763411574470007,8.388663123619393,7.058749412335524,3.496973580998276,3.377123749129487,2.361768359419153,9.114445113289024,4.888986721186558,7.971428593590467,5.424922088210688,7.212958362728285,7.02136867825973,0.8318772411916731,7.720552091605108,3.060047383669939,4.532940288372874,7.506922418047323,2.356143810225275,3.382667252745041,1.372952097911829,9.10522758436473,4.751142324726947,9.036036206482532,7.013685569928653,6.974644026040249,3.521050736900963,2.7070829917717063,6.017253838968937,10.665637927637095,5.325530331567558,7.864928972289788,3.400537929583729,2.851998837112446,2.37851162325373,5.22573765346606,3.8349134297010385,4.126807703142035,2.389566811762726,5.632268215499513,6.783456654360239,5.589164236699772,3.596935142387232,4.365272599764531,2.8399595874895316,6.588864518190148,2.8379432418910273,4.650764559116903,5.37364821133469,4.8875252707415875,7.596264312101167,7.323370069061268,4.88508622529017,5.500164679492167,2.327687364176047,5.592457037268081,5.951401291674311,6.265849421440848,6.862203399053224,3.023255352300303,5.299391206126829,7.7627474419531515,2.7420062108667365,5.932628157022127,2.550900664647523,6.9532652390148435,3.6542063779442917,3.8257856274647914,1.996388746447621,5.713420884868084,6.884964164708779,3.84197311892718,6.009212786805205,7.490730563589641,2.3476656563009706,6.668317719915721,3.8196681834964554,10.872998007770835,13.44366362719111,3.2764966656403565,10.051385697621988,6.023698917266407,3.8787253414801053,6.258895755075436,4.826802684285827,7.317231700222606,7.7475216725404215,4.398487190018967,5.342341397431548,5.767654798237347,2.8155754288625725,5.8011586560936985,2.3757345385831563,2.430285272977781,2.150559676575381,4.505255800932163,3.344828496997441,3.4356285940520905,2.2172307162206693,2.823749360308273,5.130519083027338,4.868390453054454,4.839455765224629,2.2720231890610485,5.525755692829486,3.828834649468056,7.7780114016125745,10.806299887662895,5.515384460636948,3.9699332746978557,6.57334422759282,2.759155833800272,5.033863451866284,4.279471295644468,7.080870811759721,6.138323004056484,5.369815424283912,3.975446765640962,4.708739041359579,9.557597721454812,4.200849574637213,5.854494418154875,5.246788093844365,5.102238193930738,7.479861085016818,3.0373822220030804,6.836681593327116,2.877744249949002,7.240600549955795,3.228049047884462,2.0565835283663674,4.4924941516794314,5.307064162255372,2.9088129077395473,2.682573297347578,2.722466024471091,7.96087076379617,8.903429991567151,9.435816085556171,4.631686366311998,7.457709193357593,6.109569509370688,6.0413306068206225,7.586464525886388,5.687060688339892,2.6915341649192004,2.948600847493356,10.7317008703535,2.6016965164809576,5.20163386116965,8.56017995776418,4.4672794804599825,5.904002316283692,8.44795106244151,4.932155684432056,0.9411063109464314,3.4154882710497003,3.8257856274647914,6.542412837068272,2.5360529002402097,8.739409342229193,4.436961337833602,2.797012977836145,5.624685810525398,6.1705259991768475,5.926948247949772,7.881113960675097,9.500642736062806,7.073606040574941,7.318316841334983,7.971830914780502,6.137913322088407,4.721919445551544,1.922197848396367,8.226941966807852,6.570159301636119,5.3458930856874085,2.73768676140986,10.642557179595622,3.62993940943954,5.175524601089875,9.995102877826515,3.4646682670034443,1.8836208162856714,6.0083165637349865,2.408711861029429,2.950468414150123,4.443606651475615,6.456313341355182,4.8604662585171665,5.661065479806948,6.7997349224788675,4.824258696603374,3.269033146455237,5.9740703670240265,5.925049964727359,5.946496941203293,1.4750848829487828,3.6971065744769747,3.953265239014844,5.464341532761104,3.0908534304511135,3.7676547982373463,2.859969548221026,7.822411496360477,9.401604869714957,2.7092906357233577,2.257010618206024,4.178714641175443,6.423410093269615,6.9070108151861795,3.6484654430273142,8.622783966156124,4.569248029867182,5.366322214245816,5.570462931026041,5.130519083027338,2.773996325111173,2.18269229751619,5.724923070266613,6.9787676510367875,6.751410160064371,4.484138131201669,3.0686708106650986,5.111865963867557,2.0942360698457656,4.315421315945778,4.06177619758669,5.781884234541486,2.974529312483882,4.57349571549561,4.176322772640463,10.888134388352869,4.414812060565847,5.521364878446219,4.582556003014061,3.8369340113210915,3.929790997718597,4.598126959919604,3.61706334388818,2.4489009511451276,4.788163657037767,1.8953026213333068,2.7398481026993275,0.12432813500220177,3.400537929583729,7.515147882149842,5.381629467033664,2.1602748314085933,6.677226328235316,8.025084179448086,3.440952198029637,2.9763636357327616,3.4905701304462013,7.431622959713292,2.7803100990433753,5.659068274843228,8.878265537895738,6.403949364232865,3.452858964713811,8.162995472417697,2.1473066987802936,3.7398481026993275,5.212569338850806,2.8933622107638715,3.9155209007519587,8.562585882154105,4.814550423461808,4.5867647433551415,2.582556003014061,5.077670274232752,7.905507349949873,11.663771557482411,6.559338847606539,2.7949356628035362,8.834123370734224,6.0925457415435655,4.936402377725063,3.454175893185802,2.788685710613534,7.877437521970619,5.3377110921282895,8.303415462685624,7.285032249622911,2.6915341649192004,6.459267666703387,4.398487190018967,8.028624401408491,2.9392265777282085,5.342341397431548,7.217424613813984,7.563234413022304,2.5921580021253603,3.423578170981797,2.763411574470007,5.542258049766918,4.51853513898218,9.535217045453418,5.908092340818271,6.928844036712568,5.612352498752662,6.7786028451432125,6.4202128906107845,2.877744249949002,4.038260575175349,3.4222330006830477,3.777156666004501,4.766065051474954,5.59215800212536,4.534186139090097,6.1147834472498515,5.572889668420581,2.9726926540042644,1.944858445807539,6.412442824775613,3.7990873060740036,4.030336078370959,4.255500733148386,2.5459683691052923,3.7813597135246595,5.135042286233651,5.6061460780657235,7.027021314622255,5.524189078449365,3.145677455195635,7.669948004116052,2.4462562298895643,2.408711861029429,7.221587121264805,4.008092420948722,6.956289026583943,11.260837225974512,5.316869805064333,4.078951341394822,4.509062386361898,8.238500278879492,2.689299160535892,4.22881869049588,5.614415386508126,4.112700132749362,2.9523335663696857,3.693765712217783,7.474030370207135,2.521050736900963,5.727375938594897,2.3673710656485296,2.786596361890807,6.034963989749303,5.578334948681608,8.298429210890312,7.4944956156680504,10.089053172156433,7.896998342338911,8.325215494683038,7.3416300093299105,4.349789870241872,6.438459204423481,8.102815576392821,5.43229133824778,2.4905701304462013,2.873813198359087,6.096978636370538,4.032982416699428,5.604071323668861,2.5582676340557358,4.694323060125386,2.722466024471091,2.3757345385831563,7.995258373404689,7.1078970616646115,7.797726364623276,9.707290101958575,1.0976107966264221,3.1953475983222193,7.672354619892956,3.4208865749755315,6.180903709179382,5.908572758751745,3.372952097911829,7.12525847253797,4.314696525656286,8.915789767665157,5.459103696135399,7.07692245939234,5.79207446192887,4.488000770834068,5.685379551959075,6.442114122419225,5.125981653854716,3.1193561770396756,6.643134661857265,6.657640005207824,6.316507819103109,0,4.053111336459563,5.711219556894335,2.3161457422933562,4.726831217032493,5.8894735425311096,5.981167667105682,6.483976963236257,3.3840498067951597,4.672990993827748,5.239550796613645,6.48767927890162,2.733354340613827,7.754887502163468,5.065227622775619,6.546122758857796,4.556429415449573,3.33771109212829,4.72137265947682,5.533563348214512,7.015582311680932,3.3868109464722167,6.141391916170729,2.097610796626422,6.376255649313362,3.24031432933371,6.5787877956769725,6.052024560482831,6.4571345943180365,6.711081862306696,1.5109619192773793,2.0071955014042038,8.766462652425766,2.584962500721156,7.074676686294496,3.292781749227846,9.20006486151431,7.7525479058471385,4.009884588931844,2.169925001442312,8.76354436429392,6.257010618206024,4.3269687115878925,2.3950627995175777,2.3757345385831563,5.424922088210688,2.250961573533219,2.8011586560936985,3.4892860226258766,5.968551603363945,2.541019153133559,4.888012585402829,6.457791260256042,5.237257770900372,4.161081482277184,3.408711861029429,6.705839705883682,8.251198340973543,5.2671612180222604,7.824386002585445,3.1043366598147357,6.542103245856593,2.2234225499349374,5.316507819103109,3.363171077119244,4.631104282365877,3.9873208659292536,2.140778655782796,7.473705749619415,5.924575003392735,2.871843648509318,6.807870078299782,4.955591791753994,4.214901910160696,3.910732661902913,8.646486708605355,2.2539892662307865,3.816599706534912,6.460578760776026,7.281698250098829,4.901108243014512,2.867896463992655,5.9429835981871015,2.950468414150123,3.8429788317883253,6.045486656359465,4.990047219573794,6.705977901682522,7.860404179081675,2.565597175854225,0,9.146670494470888,6.7014104362328135,3.4019034716079584,4.462706750670158,4.311793717753649,6.649471757181635,2.788685710613534,7.693556651228518,2.8135246892978105,3.343407822297814,3.5360529002402097,3.2675357980687347,6.75127624861109,8.995371450572195,2.6870606883398924,7.605257262939004,5.275752048828529,6.982080909763738,5.305970520984387,2.017921907997262,7.062963556559086,7.2259304111710545,7.0414401300729725,7.778274295338579,2.4646682670034443,2.4620523187964327,3.2141248053528475,4.114367024952,6.111448698487674,2.189033824390017,6.304876050044889,6.0710332444568245,6.157650096861711,4.038260575175349,7.489045125114802,8.475936042446387,0.9411063109464314,2.01435529297707,8.154868729042667,5.955824240806441,2.996388746447621,6.1039172107520665,4.296457407371244,3.657640005207824,3.1259816538547156,8.325890061039784,3.085764553778314,5.309976492370556,3.145677455195635,2.594548549550354,3.631104282365877,5.121015400961366,4.371558862611963,8.492213725013189,2.1043366598147357,5.825022364044702,6.883987308105472,10.959639764364185,2.4620523187964327,5.522306892871389,0,6.593054921669878,3.9354597478052895,8.913068913647477,5.021035479511363,1.7484612330040354,5.982537314399271,7.690137692986635,5.791814071161826,3.578938713093386,4.896271848807375,6.766065051474954,7.801029283808189,6.976821852360685,5.795195490817005,6.883132015657282,4.464668267003444,4.808385050656093,7.161484638641659,2.3645724322958563,5.707082991771706,2.8439838440483265,2,7.723148957047578,1.1634987322828796,7.126807703142035,5.348728154231077,4.4924941516794314,6.174526265266431,5.563768278452033,5.6570683012015754,3.37851162325373,6.43895814789755,3.127633279725874,3.1953475983222193,6.676803353873854,2.6667565918848033,2.3589588258323295,5.587364990936461,2.454175893185802,3.2265085298086795,6.230164577884252,7.884597920990064,3.7687136570304847,3.127633279725874,6.450551443306483,4.084914659767938,4.135863165368679,2.319039815562536,3.097610796626422,5.624978179569036,3.968090752045256,5.105594275795191,5.185470146066942,5.509379148914689,5.266411766036114,3.1342209397606338,4.707082991771706,6.390598905552461,2.8698714061777126,1.4168397419128291,2.3645724322958563,3.5084286525318573,3.9354597478052895,7.11009175192513,1.4222330006830475,6.929672661579345,7.041001987167506,6.332349900373874,3.423578170981797,5.137503523749935,5.3139713710590195,5.999549087340949,3.2433644256936605,4.7114949066500875,3.7059779016825223,6.291677223176022,0.3895668117627256,5.758089934201813,7.001577085389535,7.931505781869729,2.8257856274647914,9.151270288790165,2.440952198029637,5.74604398299834,7.136683577697236,6.353146825498083,10.212070748654236,6.007419783571914,4.997744026059632,2.4982508675278257,8.25176642434399,2.8579809951275723,3.300123724569014,4.640967910449898,2.625270489374693,5.399854673572431,2.2660368939953175,1.516015147003665,6.199672344836364,5.708187236020708,5.402585758232587,5.269033146455237,4.529820946528695,5.212958362728285,5.648753032989572,2.9467308601403097,2.169925001442312,6.088735246190284,2.4356285940520905,8.627898615871748,5.300489844363109,8.091065077930535,7.119252412157343,10.142962207064079,1.7398481026993275,3.729008870337863,4.45022149589718,6.461724991502711,7.904544309292717,5.490890978894291,6.76593249347046,4.502075956045791,2.782408564927373,4.205548911173033,4.139960569545456,6.708325207143167,4.632268215499513,5.6194130105979365,5.541638734436141,3.581351247168777,3.496973580998276,4.862451391325582,4.784503982929566,5.359310316890443,3.41006969175638,2.5897634869849773,2.769771739249448,10.641220865859923,8.202368742775262],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Lasso()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_pred_logLasso"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression lasso\n","# réglage des paramètre pour la gridsearch\n","alphaslasso_log = np.linspace(0.1, 1, 5)\n","param_gridLasso_log = {'lasso__alpha': alphaslasso_log}\n","\n","GridLasso_log, \\\n","BestParametresLasso_log, \\\n","ScoresLasso_log, \\\n","TotalGHGEmissions_pred_logLasso_log, \\\n","figLasso_log = reg_modelGrid(model=Lasso(),\n","                            scaler=RobustScaler(quantile_range=(10, 90)),\n","                            X_train=BEBM_train,\n","                            X_test=BEBM_test,\n","                            y_train=TotalGHGEmissions_train_log,\n","                            y_test=TotalGHGEmissions_test_log,\n","                            y_test_name='TotalGHGEmissions_test_log',\n","                            y_pred_name='TotalGHGEmissions_pred_logLasso',\n","                            score=score,\n","                            param_grid=param_gridLasso_log)\n","\n","print(BestParametresLasso_log)\n","print(ScoresLasso_log)\n","figLasso_log.show()\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[1.7662000444242039,1.7988342878489825,1.8662668515448293,1.964488546093035,2.0509572805982423]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[1.7940814038105044,1.831760867639215,1.8991884699439023,1.9924643898974692,2.074805637696948]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[1.7383186850379033,1.76590770805875,1.8333452331457563,1.9365127022886008,2.0271089234995365]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.7237765639322644,1.7379381756339056,1.8039385402751518,1.9144258005730412,2.0109287829567917],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.7807506935949442,1.8297940443514065,1.901899608486122,1.9940480479962437,2.0803077884144865],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.7462479445606727,1.7926875587894222,1.8723444747170668,1.9811373817347322,2.062020564036573],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.8030291583322877,1.8225845295764944,1.8738220931806413,1.955267104816656,2.0394174472898876],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.7771958617008494,1.811167130893683,1.879329541065164,1.9775643953445026,2.062111820293473],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Lasso en fonction de alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Lasso pour tout les paramètres de GridSearchCV\n","FigRMSEGRidLasso_log = visuRMSEGrid(Lasso(), 'Lasso', alphaslasso_log, 'alpha',\n","                                    GridLasso_log, None, None)\n","FigRMSEGRidLasso_log.show()\n","if write_data is True:\n","    FigRMSEGRidLasso_log.write_image(\n","        './Figures/EmissionsGraphRMSELasso_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.2.4 Modèle ElasticNet"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.533e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.442e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.393e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.471e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.522e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.042e+02, tolerance: 1.766e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.888e+02, tolerance: 1.766e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.804e+02, tolerance: 1.766e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.534e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.012e+02, tolerance: 1.766e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.471e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.442e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.522e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.355e+01, tolerance: 1.766e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.992e+01, tolerance: 1.766e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.393e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.534e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.523e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.472e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.394e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.443e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+01, tolerance: 1.766e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.523e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.535e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.444e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.473e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.394e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.536e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.445e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.524e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.474e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.395e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.526e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.446e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.538e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.397e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.475e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.527e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.540e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.448e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.399e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.477e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.530e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.544e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.450e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.402e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.480e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.453e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.533e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.548e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.405e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.483e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.554e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.538e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.561e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.488e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.410e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.464e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.495e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.458e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.552e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.544e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.426e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.417e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.585e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.503e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.473e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.515e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.579e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.564e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.602e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.456e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.520e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.530e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.439e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.625e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.499e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.655e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.550e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.484e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.693e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.575e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.599e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.609e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.625e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.705e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.478e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.660e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.507e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.594e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.742e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.688e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.803e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.545e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.629e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.547e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.653e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.708e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.583e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.762e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.879e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.656e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.972e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.027e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.777e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.862e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.089e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.833e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.943e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.083e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.212e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.734e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.921e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.962e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.965e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.762e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.359e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.085e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.829e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.223e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.151e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.519e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.853e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.293e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.077e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.614e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.376e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.689e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.229e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.574e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.235e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.540e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.449e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.737e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.862e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.564e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.031e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.397e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.708e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.873e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.394e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.782e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.188e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.946e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.755e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.028e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.931e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.207e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.168e+03, tolerance: 1.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.243e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.328e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.236e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.099e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.906e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.096e+03, tolerance: 1.766e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.065e+03, tolerance: 1.756e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"name":"stdout","output_type":"stream","text":["              paramètre  ElasticNet()\n","0     elasticnet__alpha      0.062102\n","1  elasticnet__l1_ratio      1.000000\n","      ElasticNet()\n","R²       -0.435249\n","RMSE      2.521010\n","MAE       1.551599\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_pred_logEN=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4.75314668512322,4.942302248199516,5.356688102569182,4.732523752368974,4.745497498935891,4.784628156827848,5.966331202917768,4.812017823053129,4.93515020027481,5.105663754359323,5.4102572912745215,4.906014315681555,4.812248871433649,5.003477876461454,5.875488245065207,10.23143216995683,5.159726754984892,4.694023850971329,5.219509507464092,5.192519873245439,4.688013681053942,4.659387383792106,5.256487943630105,4.851090810949833,4.781181748182316,4.629580237847538,4.719129115617943,4.9462297675269085,5.449736703276382,4.653084643299797,4.930249593190485,5.005844666459134,4.972782584151695,5.076520548840194,4.941393462749416,4.668092806024317,4.9066640991166794,4.7755985965338,4.717131389472057,4.750222470914717,4.66846324952356,4.630063385353232,4.7290768860769825,4.81336132521085,4.805156900744231,5.352359229294942,4.767556885571066,4.732216889438451,4.9367569538503115,4.749448775894766,5.787077531062446,4.780522408044572,4.699580503459973,4.710980615448666,5.531992241100255,4.964827806641556,5.218957724544478,4.709640005750238,4.673510542200746,5.71409635984554,4.877521681362053,4.822355823222469,5.101249302660011,4.660035659915781,5.027423000290039,4.713741430825832,5.053500113240094,5.165436356476764,4.634095663917529,4.756530826977605,4.7749692691128764,5.019481617775017,4.687079220655207,4.680903977039804,4.703789422043978,4.784973220799877,4.636996903686409,4.748055403705873,4.7005066122080805,13.50727572288308,5.6292526762373685,4.816933916990999,4.666572444162841,4.65829920601308,5.694434764724877,4.7305256206276125,4.676452235897817,4.719869258920515,4.740044816452454,4.665957011148497,5.676524753201041,4.690781708250605,4.685009725823082,4.878391800846713,4.653892471886668,12.414241609203348,4.6153832318840005,4.683063696082958,5.1086243526058945,4.766889195200834,4.676475672671274,4.6778015127336445,4.6585075804814045,4.667104956693002,4.954522806668175,5.227848368526311,5.637352441914165,4.703104562994079,4.827451231059342,4.685909482132313,5.769758047257278,4.747275103987511,4.803728392888112,5.301703038411121,4.8294402686993845,4.66932761768846,4.8427891612648,9.910624957523492,4.693095070920593,4.736903087433625,5.034850836485102,4.714089540513658,4.727381373120705,4.687008577204414,5.443464611490913,4.864354507915209,4.736624853384274,4.716474270540035,4.667074086401399,4.663778239740038,4.8913394737796665,4.8785026562327305,4.663415956846374,4.723960316253904,4.669413467838802,4.780058841304766,4.7827308870663305,4.673905622959669,5.765099488449291,5.005844666459134,5.17634128698573,4.813808299966136,4.694502340491184,6.860539503575383,4.66250249676727,4.864354507915209,5.1525526384762275,5.317920161852669,4.699881488803109,4.835047060589402,4.947081519812903,5.01545304472075,4.638520907378598,4.688361109222349,4.7583167524560235,4.876073678130566,4.903869834922623,4.726205431483166,4.89801995860937,4.674151100751521,5.0888168620000105,5.184153659124584,4.943610158586307,4.67492364020176,4.810763169913637,4.666422115514265,4.913320304515447,4.699819748219901,4.716902105370146,5.677429191562494,4.662520665839247,4.932296196713594,4.982791363437244,4.65734994454627,8.502709995304773,5.235185077684393,4.900160223154263,4.817489582239864,5.273104216017165,13.270009090630781,11.084080839222008,6.030260946176698,4.962248097141972,4.916794196547903,4.791597125157358,4.738940125254543,5.089349920477787,4.847464635386944,4.679985585864598,4.66913011671914,4.778001000182481,4.736624853384274,4.920704402216448,4.625162281342174,4.666837562958112,4.903724757974424,4.944790947240145,4.661378517600538,7.050180818531296,4.7223231733683075,4.666796253776967,4.695320403218679,4.820759078643006,4.667104956693002,4.876167017413208,4.772387410349818,6.6422374944092795,13.431098999571235,5.402131803690836,4.659989354478377,4.8097831579896075,5.310657925752926,5.918963506632425,4.707390908090034,5.295260763268435,4.7175657677366205,4.936155952422301,5.074930293111775,4.75082070616237,4.770680238552153,4.7020369381505125,4.635549551306645,4.792493351846536,5.145812000832398,4.6408316521818955,4.8292357281950276,4.7071863489079195,5.335505881512637,4.9830329624190295,4.625162281342174,4.665406856368965,4.702937647671863,4.890206174402264,4.688547197320949,5.683925307805838,4.726205431483166,4.660051095061584,5.054230055438272,5.306053211850687,4.683003156868849,5.009238315725933,4.844087721367398,4.783485956038516,5.2225203784370855,4.686306278070432,5.07712416977181,4.693823194075905,4.663015937099081,4.688467805726164,5.792512898579306,4.766584471385553,5.644358290206112,4.9233450835416885,4.694528378747852,4.611086799049213,4.753912216682282,4.86491406771587,17.609196201490597,4.711805138934992,4.675501676009177,6.251533167739048,5.097660631261094,4.854169568417597,5.122356864543962,4.96983309512399,4.6478845566862645,5.106142243879179,4.747845710990841,4.751543042960403,5.062739748084257,5.664632727273668,4.725149596863202,5.057483510924749,4.903108179098153,7.007752206659976,5.313167302985529,5.04054435893695,4.8932483089491905,4.888637886813225,4.756783153801415,5.084237385222435,4.732889548100241,4.773607462725368,4.687301844974647,4.903162361929677,4.811582576455342,4.895437068538914,5.2046062364826735,4.652185321980384,4.786485043252766,4.760472474780062,4.697143895489027,4.652143371640558,4.659186726896683,4.647696344360796,4.785793510335884,4.6747607890106915,4.625339741724823,4.743952412220938,5.0235694724930156,4.883443960250922,5.088872235263423,5.249286672972083,7.07111162006781,5.0131541721286474,4.644090755794578,4.6443458471052415,4.631167471958672,4.990898272195742,4.906725278651646,5.079292216992136,4.771199579980289,4.763710482259524,5.078081148811521,4.666441943172115,5.588240846754379,5.80775912584973,4.667104956693002,4.766455126418996,4.629301383414223,5.949403144114491,4.684515801157424,5.248454288171703,7.363072881314492,4.625339741724823,4.703264994935427,4.722571153131743,4.69240316066214,5.843120104883504,4.846976435040735,4.8916104118658374,5.404989833574325,4.707112854611248,4.6739479039243745,4.874118786380271,4.846431336642636,4.964080104754398,4.721807113414554,4.730848383627996,4.851184505525176,4.883443960250922,4.695181121974719,5.066166718411557,5.201523727361354,5.058437319929679,5.573263778851717,4.722061793320283,4.949707788222632,4.805862838972136,5.237425876496323,4.665870145028859,4.762447852310673,4.817994270205655,5.210341536076039,4.741795339643794,5.2061619886747925,5.02683460285006,4.978453444992632,5.184153659124584,4.970714274614255,4.6856039789364505,4.729833389231486,4.660128270790593,5.747333501501715,4.682962067763415,4.654386396552326,5.697452784625723,4.829607193503153,4.766889195200834,4.977594373156804,6.849285229084781,4.6514371210258805,4.701268078411685,5.166517938151263,8.488319727267067,4.665777534154048,5.971154999920147,7.389338158600889,4.688189365858251,4.718026898926603,4.671797241016748,7.493735489802304,5.784575543442641,4.663696192063867,5.45920321079536,4.7175657677366205,4.863493869349338,5.0235694724930156,4.997891085011657,4.704272787783718,4.75082070616237,5.884398333222203,4.705368683135646,4.676234845434763,4.708563464179256,4.620815350743761,5.02949902740038,4.817994270205655,5.584585458297331,7.481729381974874,4.839035822518687,5.231644643652959,5.181277744822948,5.411971944773795,4.757786438278532,4.654996084811497,5.076520548840194,4.656176873465334,5.23141281721964,4.793078899154329,5.535561746416887,4.908888798105176,4.884414049804829,4.686083487342569,5.640853027656081,4.672136814224387,4.691708579101059,4.825083673974346,4.736550070877981,4.727162411259187,13.423564642872197,5.32258677008996,5.2394544011388575,4.844181240910023,5.5605867814970855,6.588257153711031,5.600941455621191,4.779210784121853,4.812454468059406,4.977302519069334,4.795949836273463,4.816425005315495,4.7690695298596415,4.658831718543242,4.659565399592615,5.4727704851348475,4.752953242847153,4.707356334035061,5.175657073122489,4.698949793254256,5.518686988344229,4.857937381813461,4.665684923279238,4.633338591479682,5.632768054762938,4.6816732362494715,4.749045748330031,4.701401456062238,4.6889302528567365,5.685403747081073,7.074127913436044,4.805156900744231,5.606997290001534,4.661240744980901,4.6934064451392565,4.669782954489613,4.7463657674865205,5.006191759401775,4.773175278642919,4.615675397558106,4.680302006353535,4.741363443145425,6.781642379113792,4.823061669874311,4.676601556467388,4.660035659915781,4.662335496640249,4.6582529005756745,4.689779185875835,6.8848281690870525,4.80587250262324,4.8427891612648,4.952449443536947,5.971374705357206,4.719016720810344,4.932527723900621,7.217739254236623,4.876814571392392,4.800294829816667,4.699271800543937,4.810531440703975,4.75570269359529,4.8074634320550835,4.698196323755669,5.538305206556708,4.652185321980384,5.855853905962741,5.507687297687067,5.124203399783764,4.7111281726173155,4.868962220805372,4.703284274280022,4.883820606224661,4.683378561154972,5.712839787966169,4.819017661674241,5.441944050089865,4.876863318520453,4.796869475511841,4.673972246327348,4.89016997464589,5.733899906328284,4.772387410349818,5.21324258154797,4.721727594827142,5.869335854941373,4.681799215496309,4.896918842535887,4.754869195721993,4.672352906265612,4.752368702102102,4.972782584151695,5.074691146877198,5.095898057013392,5.041245173355544,4.695664247583378,4.963937871138259,4.74292438977248,5.441308039326059,4.8677541345434,4.785708617033974,4.68163714646539,5.918963506632425,4.8566525372754,4.74003636509118,4.729218757605668,4.721730348499417,5.599175907188258,5.001252710583101,5.253430255706955,4.688991993439944,4.719045438420056,5.27817000123525,4.80494027370391,5.5074560347257755,4.940144968353805,7.415553750996011,6.486515745778462,5.217393043186845,4.768467559173372,5.193075699400915,5.259567593148994,8.656644172337842,11.156935760221156,4.7111281726173155,6.581702621743164,4.813538184914605,4.952832658202879,4.645063845831645,5.068406872661784,4.79799686520392,5.027423000290039,5.028924291518818,4.742742960384721,5.339424779493927,4.741182676143596,4.703600299912356,6.355714698181886,5.231347367266149,4.9075661999435205,4.8965403129833085,4.779014621726466,4.762401546873267,4.721973639810741,5.10304749714592,4.7291778743153445,4.881004482512133,4.722123533903491,4.654051703200883,4.670797112465733,4.869236622346411,4.706712101352846,5.038852725756266,4.704886674944254,4.667407297392521,5.454193833757277,4.791360655328685,4.661384833836673,4.962056816794747,4.755811491370771,4.704581490699754,4.842824074272864,4.915799968618625,4.6577896973006485,4.755811491370771,4.721727594827142,4.863493869349338,5.049816743064572,6.166244129779323,5.013943194441624,4.702158172808873,4.865085386146174,5.994721908528186,4.760296522916965,4.717608753756466,4.818704333606611,4.918120905627511,4.7757992534292235,4.66932761768846,4.757678392257919,5.061835657855135,4.702227509315709,4.896301436703815,4.835729452699582,4.737809012544984,4.701085430175648,4.744049158514937,4.625229578375095,5.599640712556431,9.021479668857149,4.746901954159431,4.722533292208106,5.068858649194951,4.803728392888112,6.066439839937679,5.038972521100759,4.6610698146845015,5.016572092791379,4.882970067796523,4.737809012544984,4.653605566856455,8.043491455577897,4.699873771230207,4.933016558501933,4.78300746651866,4.7718864439684685,4.685063748833388,4.652182182117384,4.7310682008956295,4.656809714443207,4.932836426816657,4.687336528069492,5.148882953016079,5.058454623511074,5.182036855786592,4.927807871338678,5.6479362538150575,5.792240018763494,4.673430165290254,5.306053211850687,4.665635199893268,4.7021615433214,5.891590071815578,4.701925216542804,4.793613365527804,4.661384833836673,4.83701504167913,4.953172231410518,4.827451231059342,6.824297781232775,5.570006963087539,4.944649950794371,4.768729294838096,4.726563252003,5.550237973843184,4.797482002943371,4.620427600728959,4.894407736703146,4.781037298755434,4.697080009840083,5.000134498732714,4.832492543959206,4.668494119815164,4.7796889101712745,4.7911772996517294,5.319046485137288,5.2921048804991715,5.008150343173007,4.759684961212152,4.667528462700845,5.686249046120835,4.741710733925965,5.878735860790928,5.443493105564466,4.674050772303809,7.481729381974874,6.585789777117364,4.897520813222156,4.732216889438451,4.669332995330783,4.689801870017203,4.6571262122967685,5.054230055438272,5.346884212949732,4.833156255228682,4.657627777170703,4.590470957654875,4.673879343507142,4.7313769038116655,4.696261947112588,5.838475010544669,4.843868246415127,4.705368683135646,4.666819442258189,5.45845614182714,4.844300430497577,5.0289666048075015,4.819449845756691,4.877940773157104,4.6539696476156776,4.80177405877864,4.682493797057389,6.7726831207932605,76.42852439159283,4.85371586943667,5.443469637804446,4.7313769038116655,4.75273077261827,4.687270974683044,4.674151100751521,5.438267993669243,7.821924079414329,5.479885345553317,5.049816743064572,5.144455298847461,4.746356712812305,5.014297964439866,4.6869854244857105,4.731685606727701,4.676543548350798,4.728413355817721,4.696779024496948,4.998327750453661,4.66511661387893,4.735251125407915,4.724900634706678,5.132131132411784,4.767186442071823,4.665229586478085,4.8518693359480745,4.676671921788892,5.883248809098145,6.683392631310346,5.172223417410745,5.013438758193615,4.834329326309619,4.807009118240447,5.977412399004874,4.699534198022568,4.92639250450139,5.381636443722467,5.157201242728759,4.724492911214095,4.819230596316745,6.033424063067646,4.6847627634902524,4.665625293454904,4.669057852972534,4.7239509842247775,6.58570451023844,5.524650787148972,4.782348081959218,4.646063060586335,4.8500620044797715,4.921213222996413,4.626915350701668,5.198522388429603,4.69619248895648,4.764778559326744,4.664017927532644,4.778747366277365,5.931395789026009,5.26422001655236,8.50691672057719,4.79145049127224,5.210376080533008,4.908888798105176,4.627943778845047,4.885550857652866,4.93648683879878,4.69257294726596,4.853607823416057,7.472921195688587,4.687965556244125,5.0646005276109705,5.0682552647564325,4.677045190589356,5.326563252877661,5.769758047257278,5.485846830603267,4.673255862295017,4.812426854416876,4.990913205422079,5.347887981528164,4.77956072430233,7.792007935941518,4.854762739151217,4.805010266859115,4.909934969831923,5.2506276053034195,4.6747607890106915,6.076882742234174,6.981261342359261,4.663778239740038,4.625068134837073,6.462930528625596,5.540780514511845,4.715436613887884,4.76965606540011,5.826993175788822,4.824585211014581,4.70249405635331,4.676348203401387,6.124991642246435,4.7560577019487305,5.002979617139596,6.536489456514849,4.712038216348449,4.668833693022803,4.620531285087404,4.720958309580049,4.732395623434583,4.671982462766369,4.85207974398168,4.767473055674491,4.648655395082407,5.189716342234563,5.671443255470935,4.728621730286045,5.2088399864714034,4.991208288719102,4.636453614535455,4.722533292208106,4.88505519090629,4.871142149046894,4.881638048192112,4.755357600710638,4.749071245128673,4.7383381545682735,4.762494157748078,6.756340397985854,4.726731188403444,4.722236601374721,4.6934064451392565,4.946096356369045,4.745891550585035,4.957276410975988,5.787408980922261,4.705966285088246,5.150063466382523,5.637407466395739,4.684631564750938,4.76965606540011,4.659148009825865,4.601055156365741,4.9923929368928714,5.554388896078223,5.259529005284489,4.8087217484205915,5.084237385222435,4.699580503459973,5.369258431864658,4.769470843650488,4.699726871669353,4.6821696589955515,5.258461204494404,4.761606278094047,6.490258991201651,4.64047161261201,4.702127302517269,5.162711464730241,4.698361126941632,4.762115996675934,4.698558008982644,4.657751258337116,4.6869854244857105,4.707234731530936,4.831240056469484,4.821002455888191,4.7657179019525975,4.8495175097785825,6.734457515093443,4.6367126612179375,4.695953244196552,4.892048904415102,6.462930528625596,4.896409482724427,4.788610424444711,4.852420811278553,5.188068097737483,4.745963116594359,4.748986684141419,5.324240854058503,5.38298780052082,4.818038366341872,6.229190518550688,4.656902325318018,4.771931245154173,4.703264994935427,4.760796291709881,4.927563734436168,6.394876589881115,4.7381683679644535,4.83523492955265,4.855081740432018,4.670685910519018,5.120387252900979,19.508208815433356,5.1191831490582205,4.814616421117269,7.169336923684979,5.102614310212039,4.6367126612179375,4.99823513957885,4.749821903044807,7.282903850797972,5.166320772734712,5.886219077555954,5.317553380069384,4.781309771986509,5.57144974124181,4.654062258490488,5.148136357795174,4.804227855754069,4.656909770650594,6.6169497501044,6.976465166119743,4.689161780043763,4.779780278086191,4.778747366277365,5.59635955876604,5.034686345044844,6.981261342359261,4.727425506486407,4.856833768888632,4.825129979411751,4.787670153427975,5.494811252784306,4.802270528479297,4.722251939456482,4.619739314237633,5.007933637430319,4.699457022293559,5.214729988684021,5.0283529656354595,5.079701839120709,4.918055843446088,4.811099431877921,5.03711372901053,5.2921965608065165,4.676234845434763,4.778855412297978,4.697972304028634,4.747648332635872,4.668169981753326,5.351642259401109,5.260022929950146,4.768438852148226,4.7761388266368625,4.879947899726816,5.403392281730094,4.688386070542323,4.698654394711866,4.700309842563557,5.088872235263423,6.2488089064398125,5.889386317218382,4.6730182874532495,5.125261829325149,5.059574408995172,6.09517935436101,4.688244319441677,4.6985987360733,6.256616271361562,4.9947338247682875,4.785539343526561,4.72435391247185,4.846831794409071,4.7416026879053526,5.216716901089743,4.68892026858196,4.688379235375848,4.757002119782219,5.59635955876604,6.129298211210649,5.273263434690412,6.753911535521668,5.4599617940393355,5.584791953760735,4.85870913910355,5.042055518510138,4.778027507361916,4.760333469424048,4.787877255019126,4.657187297464147,4.704181513290876,4.646753512590432,4.671882134318658,4.664751096958229,4.72788856086046,4.937791211250395,4.734055336603748,4.961388013280268,4.9242386856733145,5.071784207086318,5.29260652273773,17.911330434155474,4.699271800543937,4.745165011232361,5.506883312568333,4.741602571119278,5.033186373442943,5.028975847022688,4.877964483491286,4.775984475178845,5.1777854241100805,5.009297657562335,5.000756355465244,5.7019733717097125,4.744033723369135,4.980524870934932,5.647027206160576,4.694394294470571,4.700537482499684,4.794385039390023,5.6292526762373685,4.9830969590201155,4.809087614044393,4.740352014172417,5.074623944348221,4.810289086723328,4.718047259286004,4.976426898845421,5.107790567417501,4.701826317174134,4.8473488717934305,4.974921138617716,4.66999879983517,4.78894999765235,6.099372718363549,4.691652729970723,5.13138642483801,4.840998425257605,4.764718384291105,4.770543586283713,4.706155875571537,4.742264284768335,5.004409197899567,4.73676937228259,4.6540985669900765,6.141756869245013,4.634820413649002,4.80727467164518,4.807473632335822,7.309972520824749,5.229335518874308,4.728647595173172,4.850690580859519,4.673180380578824,4.669076908237843,5.670142471476666,4.729234922614959,5.547530464781591,4.792741160092617,5.292238070305629,4.825147538468218,4.6694665340006765,4.686244537487225,6.167035548921053,4.773113538059711,5.266451668176593,4.698654394711866,4.766491860510742,4.695613670988913,4.66511661387893,4.6745602196921014,4.9356665509967765,4.996580968988262,4.7417911517022535,4.6516038655613965,4.832521077945685,4.677171971462606,4.743184790350036,4.773313041657514,5.362738494595932,4.898032846522553,4.9774581689349615,5.811528407021082,4.729737295447659,4.822567745208653,4.663153559367744,4.824281046392652,4.829976615193513,4.698654394711866,5.114199389987715,4.665245393186864,4.791759194188276,4.809108298069491,4.9985592776406875,4.88816711486627,5.144178731091268,4.810159300975469,4.981734968716735,5.63090159258108,4.657797563774522,4.668169981753326,5.020749207672528,4.88598639764605,5.849196768789637,4.65744255542108,4.798882513975803,4.773607462725368,4.668032876959346,5.254744110085934,4.8924830350343145,4.687241655342871,4.949466567635902,4.712020824946649,4.688627057481759,5.322516942952491,4.733103888232608,4.806823896490825,4.916405714110649,4.80876100728895,5.5414508710138515,4.821927010025939,5.59549867128992,4.713902517458008,4.801712696930835,4.926731826652048,4.833715212883486,5.191791857692844,8.624497833317871,5.419227683020675,5.5839808133971225,5.0448829735182965,4.9000830474252535,4.6730182874532495,4.678924410831345,5.805988150887984,4.882688751876281,7.2766376985454615,5.883248809098145,4.660051095061584,4.6869854244857105,4.846381516521746,4.759684961212152,4.678511529440527,4.665229586478085,5.284715696347521,5.236339610941541,4.96042723090756,4.7718864439684685,5.073895071992483,6.348397735449735,4.71839595202629,4.691911525573607,4.992197245249822,4.685117771843694,4.805156900744231,5.920394439095504,4.644510490601285,4.732526822173899,4.814093850163469,5.310338518663276,4.745947681448557,4.686306278070432,4.876087330998527,4.65829920601308,4.808429839190593,4.794776765192527,4.726113519093254,6.463840821323522,4.658391816887891,4.898354154174238,4.878397877676768,6.7726831207932605,4.704272787783718,6.179444587282545,4.8683099005039265,4.979074449683691,4.762115996675934,14.012088155069627,4.750165237955066,4.734876836259877,4.6571262122967685,5.820800331901567,5.0817057849446625,4.922689939417406,4.923899477397419,5.362006756905415,5.226338880163837,5.097641709717722,4.9367569538503115,4.839740027346251,5.384412303757725,4.888263826842096,7.722794319035109,4.663948469376536,4.840826085587234,4.805521591240635,4.855081740432018,4.825147538468218,4.780800240669004,5.45256181247457,5.402131803690836,5.612275849265015,4.657064394348937,5.245144597799373,4.6498855597092295,4.885480441170054,5.101249302660011,4.694608393983343,4.699935511813415,4.870030818549164,4.658870306407747,4.6816732362494715,4.662600979981253,4.67492364020176,4.871364342007174,6.031765768843008,6.251533167739048,4.713958407459795,4.758964916183209,4.86999223068466,4.673510542200746,4.728369629722179,4.689763750730034,4.72659972618601,4.7761388266368625,5.116345398551471,4.800526357003694,4.709503630089091,4.668655133455338,4.93854743076332,4.878576397103439,4.803080873633891,4.967637001529978,4.665684923279238,4.885033780268507,4.726568855894407,4.812426854416876,4.707974571836479,5.569745060230993,4.827537862156831,4.779225855797221,4.860028844069603,4.729493816023846,4.909251524031518,4.774718793223098,4.809042014819554,4.86197780487102,4.690039923642859,5.075483562957267,4.881568590036004,4.658831718543242,4.810811296487831,4.794846223348634,5.071784207086318,4.687672288473891,4.762633074060294,9.06308160702664,4.710158376447496,4.849941536903075,8.605887108159106,5.093810563756705,6.247834971528423,4.912330089698039,5.0596253788801935,4.650829237846684,4.832387262178758,4.7379445583503275,4.713390065219588,4.893245616485838,4.610636751139516,4.876315258442346,4.666209718236499,4.720881500232785,4.771830636048401,5.035541886981782,4.703091999129881,4.997270442966238,4.827228838091814,6.287501158042936,5.140768933424803,4.673905622959669,4.703848838699108,5.028403872522121,4.689901876062142,6.3444576861195605,5.283746749047501,4.832387262178758,4.795137311015613,10.658606535194789,4.692341420078932,4.706293975157124,5.003943257535016,4.7957194352659105,6.777505786903042,4.660151423509295,5.353791440696034,4.691245524727005,4.682394363414624,4.623026464574973,4.898885680917512,5.63277648178352,4.846058869314784,4.675463088144673,4.830202818755356,4.869570350812216,5.734352678012121,5.649221106787735,4.675678164216602,5.064627404808463,4.869483573725067,4.682751199172029,4.754181172708785,8.656644172337842,5.132710284180703],"xaxis":"x","y":[6.235152624217932,4.478324711485028,6.139142019145689,4.729552770453971,2.689299160535892,2.4698859762744636,7.592232766722112,6.075532631167357,3.9020735793107426,6.768713657030485,8.05642095591347,6.192983169973992,4.934044647112246,4.088311235888661,7.071462362556624,8.345405246717796,8.140829770773001,5.7554217347342425,5.091276694365222,8.047178455318921,2.475084882948783,5.260778431893426,7.873013392989595,4.4495613746132365,3.4369613378336026,5.575917361118149,2.821710215034674,5.650190123496775,8.870210577263595,2.2868811477881614,5.54936113292434,3.682573297347578,10.919861451442168,7.606886339275915,3.4276061727818994,2.1009776477248208,3.548436624696042,6.819540460505722,4.475084882948782,6.762348815644129,3.929790997718597,4.173127433480656,2.7676547982373463,4.804260115634738,1.9335726382610239,4.846493315964563,3.10936055940423,5.617651119427331,5.739578112048051,5.089159131911238,5.6806056662025926,2.8579809951275723,2.1176950426697547,5.465648025982281,6.027463723408577,6.299391206126829,6.422064766172812,2.4356285940520905,4.153805336079036,8.148324042480084,6.387500406480984,5.944155663440363,6.496494306853713,5.703211467391146,3.192194165283345,5.185866545311334,4.718087583960517,6.035403969942067,1.3785116232537298,6.335211681705785,7.330737650206952,5.452858964713811,2.726831217032493,3.8369340113210915,2.4672794804599825,2.9616233282869446,2.0531113364595623,6.248496885959384,2.817623257511431,8.971342367308225,6.606294160697713,5.901832305785489,5.339137384919585,2.582556003014061,4.779259720372376,6.044831233447812,2.7949356628035362,2.8439838440483265,6.690277401023435,4.017921907997263,6.596487956856693,2.84197311892718,2.3074285251922473,5.976821852360685,5.10936055940423,10.16679075071818,5.2787282129389395,6.339493737901733,7.125981653854716,5.148934104526339,2.321928094887362,2.1210154009613658,2.430285272977781,2.5310694927259543,6.72805655144938,6.989025440631378,5.844988156682612,4.749534267669262,9.159820892013935,4.272769732436511,8.601733652922286,4.124328135002202,3.270528942380718,3.894332742277694,4.943921326553485,1.0565835283663676,3.7322691995014496,10.679004987548526,6.500961352460057,5.787902559391432,3.7782085763980877,4.6661884898727815,7.593353770980297,3.077242998932461,7.602439063741618,5.537296067090842,4.203984165855989,5.299757511839318,5.94251450533924,6.951051533288609,3.542258049766918,3.507160349117523,4.71259578044703,4.6194130105979365,2.550900664647523,6.872951851130105,4.712045448553693,2.867896463992655,8.377687734026882,3.678071905112638,4.0232553523003025,3.1538053360790355,4.635173946674924,7.127117349726852,2.4854268271702415,5.592756010441026,8.25559514727367,4.634593268445757,2.5582676340557358,4.990501111363251,3.727920454563199,3.8308637567517576,5.924337464075348,4.187451054027326,3.940166750482817,4.926473911256073,3.5434958834257717,4.708187236020708,5.568640195419223,4.831370587967541,6.12969515665378,5.751410160064371,6.279471295644468,2.523561956057013,4.552131108253784,2.6016965164809576,3.270528942380718,2.805292455600712,5.533874777258546,5.741197298571427,5.787641414483327,3.565597175854225,4.40735275114004,2.137503523749935,11.52102619171175,7.417936880550136,4.96624587322494,6.385085853547684,7.846744023244868,9.156361220297567,11.010164414182617,7.196430000954568,4.008988783227255,3.2464080872463845,5.763145958149476,7.976535484023556,4.4121040446775694,6.918624755574591,1.9745293124838819,4.546585829008101,0.22650852980867975,4.1715271060388135,6.920055054806967,1.3504972470841332,2.3589588258323295,5.726013748860151,3.2357270598380583,2.6848187375532224,9.236301266827088,2.4854268271702415,5.9618548076361595,4.088311235888661,2.440952198029637,5.456806149230474,6.205157883920094,5.588564737401351,6.620439787651722,8.334139178838237,5.331633567171029,2.211012193485512,3.8063240573900288,8.45889870671479,6.602884408718419,3.8439838440483265,6.132988042627442,2.541019153133559,5.953730849811491,4.18824265626441,1.4489009511451278,2.7070829917717063,3.0107798387532423,3.4019034716079584,8.574290766120185,3.542258049766918,4.313245851787562,3.3854310371935203,6.156032498888866,5.419538891513785,7.3107945270667845,1.3950627995175775,5.8346606579017255,6.025250321561575,6.2268938135713885,5.548128323585586,6.746447139362815,4.583158003874408,5.9576824861651225,5.778208576398088,7.834028534577023,1.570462931026041,7.0083165637349865,10.307269127730322,4.95093492831454,5.099295204337775,4.547203024756931,7.9684364043347875,2.8318772411916733,2.4436066514756147,2.4724877714627436,2.37851162325373,2.6507645591169022,8.202417721575182,4.963011647599427,9.01321099459576,2.176322772640463,2.73768676140986,5.5786368624717415,13.52966114241807,5.72328550477764,5.642412772905056,7.568108130120591,6.034743949324734,5.320845667645722,3.8469946869655733,3.8639384504239715,5.154210530588339,5.777945670695148,3.327687364176047,4.169123281476757,8.135247549809566,6.890446692679906,2.2479275134435857,4.9804823555382605,3.3015876466031866,8.873136468833458,5.8346606579017255,4.189033824390017,8.095608004811197,5.380244590478185,4.571676809970931,5.071247819461442,2.9373443921502322,2.944858445807539,6.706254253573203,6.849624029924543,7.541716163393138,7.907912142840534,6.091065077930535,4.388878338811989,4.078951341394822,2.698218478224414,2.5058909297299574,4.621758856709732,5.3175935046234715,1.996388746447621,2.7070829917717063,5.974529312483882,2.883620816285671,4.853995647176393,4.593951283948411,6.496494306853713,3.9845893503624565,6.212180210044431,6.897240425574799,5.452858964713811,2.232660756790275,3.773996325111173,6.075318692553354,4.161081482277184,3.2094533656289497,4.922673592849446,4.478971805032942,3.0840642647884744,6.054414387917307,3.486714373030702,7.582029046119882,7.461888664432819,5.6604951318885135,2.7311832415722,2.049630767724601,7.850749425896496,2.37851162325373,5.986638471730853,7.210817432089213,2.769771739249448,4.970393537914677,2.889473542531109,7.08533966935737,7.463197379833495,6.09950561709114,4.17951105027151,8.27891401949994,2.4594316186372973,6.221297103163338,4.867402305727543,2.980025300238734,3.4276061727818994,5.933572638261024,2.565597175854225,7.056366761024529,6.199672344836364,4.985955754607509,6.215484464192794,6.7410624357490905,4.59215800212536,5.19377174339668,4.099295204337775,6.714108111595169,2.1634987322828794,8.524737587012376,2.18269229751619,2.765534746362977,4.67581593117227,6.658497136656478,6.557808298936688,4.208673319629471,4.993674361750585,5.422905742612183,5.7824085649273735,5.3847405872923835,5.08278773170927,2.4276061727818994,2.411426245726465,7.603403804166527,5.174326515173915,5.896998342338911,6.425089989874059,0,5.106432077816929,2.9467308601403097,8.843732656611305,3.7311832415722,7.403949364232865,6.247927513443585,8.640931770341906,2.6915341649192004,5.336640446408735,7.390942772802543,2.1984941536390834,3.0125686735030555,2.0214797274104517,8.044284820139424,7.1855692560893685,2.636914580355878,8.491372117815171,2.5185351389821804,6.73226919950145,4.614709844115208,4.15623479785027,2.495695162624069,1.0426443374084937,8.060966074004773,3.5643781685650637,3.679198570566922,4.488000770834068,6.3103401206121505,3.605257262939004,4.821199978056451,7.003039979749236,8.211498981977794,8.84080938879262,7.640895629328568,5.95837871156634,6.804001915179304,3.997292407636508,2.3533232911628965,7.631540867341486,2.7990873060740036,4.706530552538117,3.452858964713811,8.983022086172278,6.187253085582297,4.564378168565064,4.241840183564671,5.187055089968057,5.496654082593496,6.564225720206398,6.793115555274348,8.126549613543878,6.3414521074811265,8.823876711241038,5.094658342654544,8.23429054170594,3.063502942306158,6.9910682753603695,7.485507332459785,6.9092930858238235,6.2992080183872785,5.189033824390017,5.562547724199122,3.2357270598380583,2.851998837112446,2.6507645591169022,6.425425734590699,2.889473542531109,5.012121672712217,4.938285791884053,5.642412772905056,6.199672344836364,2.0738202332916713,7.151270288790165,5.005849075696688,2.3923174227787602,2.1667154449664223,5.727920454563199,2.3103401206121505,6.984019633085816,3.9726926540042644,4.362469888750209,7.173227395028653,9.653203362033977,3.357552004618084,8.556735947914529,3.9420452599160467,4.1026581313637385,6.309067020588001,4.952799477899938,2.3015876466031866,4.580748491763774,5.046578366620329,6.999098033705607,2.752748591407134,6.73348992588162,3.1193561770396756,3.431622959713292,5.15623479785027,2.344828496997441,2.720278465233327,4.763411574470007,7.178117045706399,2.807354922057604,3.685940148445977,4.417515002885087,8.72584338467341,2.4724877714627436,3.619413010597937,7.5130959085510085,3.382667252745041,2.7463127664254587,4.509695841933387,1.6553518286125541,3.0992952043377753,7.846242565109514,2.344828496997441,5.39780296186249,4.658211482751795,4.872828759534886,7.290387539763883,6.802451741275622,5.377123749129487,4.270528942380718,6.325710207515785,3.572889668420581,2.548436624696042,6.845113646617966,3.2265085298086795,6.531849285751203,3.361768359419153,5.599912842187128,2.211012193485512,4.396433531250992,7.406162478777607,5.6519127446457835,5.7114949066500875,3.61706334388818,4.044394119358453,5.772149590784542,5.177917792195843,3.1795110502715103,2.321928094887362,4.854494418154875,8.167066838182594,6.5249725982866265,6.43095427138573,5.669310286202474,4.147306698780294,4.303050084681673,4.3298411765306275,6.912410353808268,4.904002316283692,3.2854022188622483,2.8659188145522125,6.582254908357664,5.198494153639083,5.103077946597573,4.3045110418099535,4.370861740085285,8.654779216959517,8.362951992360674,7.2600256559614555,2.6392321632492775,6.457791260256042,7.31786479839195,5.661920579135087,4.609991295212678,3.460742563789644,9.248900055578497,7.886854944198882,3.7059779016825223,3.0373822220030804,5.982537314399271,5.339850002884624,10.306392126750833,8.18685706717715,5.533874777258546,8.85835405767524,6.166715444966422,6.56727163085093,5.7737326506078555,3.396433531250992,7.532083141492505,4.876271360550127,3.817623257511431,5.643567621908798,4.28318097894557,2.6205864104518777,2.881664619320345,5.006746832440585,5.20163386116965,4.181102550753798,8.571145863602528,7.3433189836610735,7.006298023900369,2.9818526532897405,6.419370342528151,2.414135532984451,8.463115619892362,4.857483428286672,6.972003303808356,4.958842675243241,5.910252962799278,4.4963345134171115,4.707082991771706,2.8094144442358986,3.1651079851445365,6.320484678017694,3.8787253414801053,1.3504972470841332,6.207502459258789,6.152994605492435,2.333423733725192,3.0823619695574735,3.4620523187964327,6.266599165535096,6.2861418728343015,3.6064422281316078,6.75702324650746,5.29020320512607,5.342341397431548,7.833712369021444,2.440952198029637,6.642123916153487,6.722602636851373,4.393690764187454,2.729008870337863,7.036503333738988,7.122569192216447,3.18269229751619,1.0565835283663676,2.992768430768924,4.125981653854716,2.7949356628035362,6.039796407620025,3.4154882710497003,5.535430914871279,4.963011647599427,2.763411574470007,5.062639828286463,7.592232766722112,9.317322159829551,4.955126781261366,1.4489009511451278,6.436128517259249,3.2645364309990255,7.647314509547136,6.15319733085508,2.229587922740652,5.417515002885087,5.0925457415435655,5.34801990922869,2.4195388915137843,7.500961352460057,4.426264754702098,6.247547806890298,4.622344722724236,4.638073837180719,2.763411574470007,8.388663123619393,7.058749412335524,3.496973580998276,3.377123749129487,2.361768359419153,9.114445113289024,4.888986721186558,7.971428593590467,5.424922088210688,7.212958362728285,7.02136867825973,0.8318772411916731,7.720552091605108,3.060047383669939,4.532940288372874,7.506922418047323,2.356143810225275,3.382667252745041,1.372952097911829,9.10522758436473,4.751142324726947,9.036036206482532,7.013685569928653,6.974644026040249,3.521050736900963,2.7070829917717063,6.017253838968937,10.665637927637095,5.325530331567558,7.864928972289788,3.400537929583729,2.851998837112446,2.37851162325373,5.22573765346606,3.8349134297010385,4.126807703142035,2.389566811762726,5.632268215499513,6.783456654360239,5.589164236699772,3.596935142387232,4.365272599764531,2.8399595874895316,6.588864518190148,2.8379432418910273,4.650764559116903,5.37364821133469,4.8875252707415875,7.596264312101167,7.323370069061268,4.88508622529017,5.500164679492167,2.327687364176047,5.592457037268081,5.951401291674311,6.265849421440848,6.862203399053224,3.023255352300303,5.299391206126829,7.7627474419531515,2.7420062108667365,5.932628157022127,2.550900664647523,6.9532652390148435,3.6542063779442917,3.8257856274647914,1.996388746447621,5.713420884868084,6.884964164708779,3.84197311892718,6.009212786805205,7.490730563589641,2.3476656563009706,6.668317719915721,3.8196681834964554,10.872998007770835,13.44366362719111,3.2764966656403565,10.051385697621988,6.023698917266407,3.8787253414801053,6.258895755075436,4.826802684285827,7.317231700222606,7.7475216725404215,4.398487190018967,5.342341397431548,5.767654798237347,2.8155754288625725,5.8011586560936985,2.3757345385831563,2.430285272977781,2.150559676575381,4.505255800932163,3.344828496997441,3.4356285940520905,2.2172307162206693,2.823749360308273,5.130519083027338,4.868390453054454,4.839455765224629,2.2720231890610485,5.525755692829486,3.828834649468056,7.7780114016125745,10.806299887662895,5.515384460636948,3.9699332746978557,6.57334422759282,2.759155833800272,5.033863451866284,4.279471295644468,7.080870811759721,6.138323004056484,5.369815424283912,3.975446765640962,4.708739041359579,9.557597721454812,4.200849574637213,5.854494418154875,5.246788093844365,5.102238193930738,7.479861085016818,3.0373822220030804,6.836681593327116,2.877744249949002,7.240600549955795,3.228049047884462,2.0565835283663674,4.4924941516794314,5.307064162255372,2.9088129077395473,2.682573297347578,2.722466024471091,7.96087076379617,8.903429991567151,9.435816085556171,4.631686366311998,7.457709193357593,6.109569509370688,6.0413306068206225,7.586464525886388,5.687060688339892,2.6915341649192004,2.948600847493356,10.7317008703535,2.6016965164809576,5.20163386116965,8.56017995776418,4.4672794804599825,5.904002316283692,8.44795106244151,4.932155684432056,0.9411063109464314,3.4154882710497003,3.8257856274647914,6.542412837068272,2.5360529002402097,8.739409342229193,4.436961337833602,2.797012977836145,5.624685810525398,6.1705259991768475,5.926948247949772,7.881113960675097,9.500642736062806,7.073606040574941,7.318316841334983,7.971830914780502,6.137913322088407,4.721919445551544,1.922197848396367,8.226941966807852,6.570159301636119,5.3458930856874085,2.73768676140986,10.642557179595622,3.62993940943954,5.175524601089875,9.995102877826515,3.4646682670034443,1.8836208162856714,6.0083165637349865,2.408711861029429,2.950468414150123,4.443606651475615,6.456313341355182,4.8604662585171665,5.661065479806948,6.7997349224788675,4.824258696603374,3.269033146455237,5.9740703670240265,5.925049964727359,5.946496941203293,1.4750848829487828,3.6971065744769747,3.953265239014844,5.464341532761104,3.0908534304511135,3.7676547982373463,2.859969548221026,7.822411496360477,9.401604869714957,2.7092906357233577,2.257010618206024,4.178714641175443,6.423410093269615,6.9070108151861795,3.6484654430273142,8.622783966156124,4.569248029867182,5.366322214245816,5.570462931026041,5.130519083027338,2.773996325111173,2.18269229751619,5.724923070266613,6.9787676510367875,6.751410160064371,4.484138131201669,3.0686708106650986,5.111865963867557,2.0942360698457656,4.315421315945778,4.06177619758669,5.781884234541486,2.974529312483882,4.57349571549561,4.176322772640463,10.888134388352869,4.414812060565847,5.521364878446219,4.582556003014061,3.8369340113210915,3.929790997718597,4.598126959919604,3.61706334388818,2.4489009511451276,4.788163657037767,1.8953026213333068,2.7398481026993275,0.12432813500220177,3.400537929583729,7.515147882149842,5.381629467033664,2.1602748314085933,6.677226328235316,8.025084179448086,3.440952198029637,2.9763636357327616,3.4905701304462013,7.431622959713292,2.7803100990433753,5.659068274843228,8.878265537895738,6.403949364232865,3.452858964713811,8.162995472417697,2.1473066987802936,3.7398481026993275,5.212569338850806,2.8933622107638715,3.9155209007519587,8.562585882154105,4.814550423461808,4.5867647433551415,2.582556003014061,5.077670274232752,7.905507349949873,11.663771557482411,6.559338847606539,2.7949356628035362,8.834123370734224,6.0925457415435655,4.936402377725063,3.454175893185802,2.788685710613534,7.877437521970619,5.3377110921282895,8.303415462685624,7.285032249622911,2.6915341649192004,6.459267666703387,4.398487190018967,8.028624401408491,2.9392265777282085,5.342341397431548,7.217424613813984,7.563234413022304,2.5921580021253603,3.423578170981797,2.763411574470007,5.542258049766918,4.51853513898218,9.535217045453418,5.908092340818271,6.928844036712568,5.612352498752662,6.7786028451432125,6.4202128906107845,2.877744249949002,4.038260575175349,3.4222330006830477,3.777156666004501,4.766065051474954,5.59215800212536,4.534186139090097,6.1147834472498515,5.572889668420581,2.9726926540042644,1.944858445807539,6.412442824775613,3.7990873060740036,4.030336078370959,4.255500733148386,2.5459683691052923,3.7813597135246595,5.135042286233651,5.6061460780657235,7.027021314622255,5.524189078449365,3.145677455195635,7.669948004116052,2.4462562298895643,2.408711861029429,7.221587121264805,4.008092420948722,6.956289026583943,11.260837225974512,5.316869805064333,4.078951341394822,4.509062386361898,8.238500278879492,2.689299160535892,4.22881869049588,5.614415386508126,4.112700132749362,2.9523335663696857,3.693765712217783,7.474030370207135,2.521050736900963,5.727375938594897,2.3673710656485296,2.786596361890807,6.034963989749303,5.578334948681608,8.298429210890312,7.4944956156680504,10.089053172156433,7.896998342338911,8.325215494683038,7.3416300093299105,4.349789870241872,6.438459204423481,8.102815576392821,5.43229133824778,2.4905701304462013,2.873813198359087,6.096978636370538,4.032982416699428,5.604071323668861,2.5582676340557358,4.694323060125386,2.722466024471091,2.3757345385831563,7.995258373404689,7.1078970616646115,7.797726364623276,9.707290101958575,1.0976107966264221,3.1953475983222193,7.672354619892956,3.4208865749755315,6.180903709179382,5.908572758751745,3.372952097911829,7.12525847253797,4.314696525656286,8.915789767665157,5.459103696135399,7.07692245939234,5.79207446192887,4.488000770834068,5.685379551959075,6.442114122419225,5.125981653854716,3.1193561770396756,6.643134661857265,6.657640005207824,6.316507819103109,0,4.053111336459563,5.711219556894335,2.3161457422933562,4.726831217032493,5.8894735425311096,5.981167667105682,6.483976963236257,3.3840498067951597,4.672990993827748,5.239550796613645,6.48767927890162,2.733354340613827,7.754887502163468,5.065227622775619,6.546122758857796,4.556429415449573,3.33771109212829,4.72137265947682,5.533563348214512,7.015582311680932,3.3868109464722167,6.141391916170729,2.097610796626422,6.376255649313362,3.24031432933371,6.5787877956769725,6.052024560482831,6.4571345943180365,6.711081862306696,1.5109619192773793,2.0071955014042038,8.766462652425766,2.584962500721156,7.074676686294496,3.292781749227846,9.20006486151431,7.7525479058471385,4.009884588931844,2.169925001442312,8.76354436429392,6.257010618206024,4.3269687115878925,2.3950627995175777,2.3757345385831563,5.424922088210688,2.250961573533219,2.8011586560936985,3.4892860226258766,5.968551603363945,2.541019153133559,4.888012585402829,6.457791260256042,5.237257770900372,4.161081482277184,3.408711861029429,6.705839705883682,8.251198340973543,5.2671612180222604,7.824386002585445,3.1043366598147357,6.542103245856593,2.2234225499349374,5.316507819103109,3.363171077119244,4.631104282365877,3.9873208659292536,2.140778655782796,7.473705749619415,5.924575003392735,2.871843648509318,6.807870078299782,4.955591791753994,4.214901910160696,3.910732661902913,8.646486708605355,2.2539892662307865,3.816599706534912,6.460578760776026,7.281698250098829,4.901108243014512,2.867896463992655,5.9429835981871015,2.950468414150123,3.8429788317883253,6.045486656359465,4.990047219573794,6.705977901682522,7.860404179081675,2.565597175854225,0,9.146670494470888,6.7014104362328135,3.4019034716079584,4.462706750670158,4.311793717753649,6.649471757181635,2.788685710613534,7.693556651228518,2.8135246892978105,3.343407822297814,3.5360529002402097,3.2675357980687347,6.75127624861109,8.995371450572195,2.6870606883398924,7.605257262939004,5.275752048828529,6.982080909763738,5.305970520984387,2.017921907997262,7.062963556559086,7.2259304111710545,7.0414401300729725,7.778274295338579,2.4646682670034443,2.4620523187964327,3.2141248053528475,4.114367024952,6.111448698487674,2.189033824390017,6.304876050044889,6.0710332444568245,6.157650096861711,4.038260575175349,7.489045125114802,8.475936042446387,0.9411063109464314,2.01435529297707,8.154868729042667,5.955824240806441,2.996388746447621,6.1039172107520665,4.296457407371244,3.657640005207824,3.1259816538547156,8.325890061039784,3.085764553778314,5.309976492370556,3.145677455195635,2.594548549550354,3.631104282365877,5.121015400961366,4.371558862611963,8.492213725013189,2.1043366598147357,5.825022364044702,6.883987308105472,10.959639764364185,2.4620523187964327,5.522306892871389,0,6.593054921669878,3.9354597478052895,8.913068913647477,5.021035479511363,1.7484612330040354,5.982537314399271,7.690137692986635,5.791814071161826,3.578938713093386,4.896271848807375,6.766065051474954,7.801029283808189,6.976821852360685,5.795195490817005,6.883132015657282,4.464668267003444,4.808385050656093,7.161484638641659,2.3645724322958563,5.707082991771706,2.8439838440483265,2,7.723148957047578,1.1634987322828796,7.126807703142035,5.348728154231077,4.4924941516794314,6.174526265266431,5.563768278452033,5.6570683012015754,3.37851162325373,6.43895814789755,3.127633279725874,3.1953475983222193,6.676803353873854,2.6667565918848033,2.3589588258323295,5.587364990936461,2.454175893185802,3.2265085298086795,6.230164577884252,7.884597920990064,3.7687136570304847,3.127633279725874,6.450551443306483,4.084914659767938,4.135863165368679,2.319039815562536,3.097610796626422,5.624978179569036,3.968090752045256,5.105594275795191,5.185470146066942,5.509379148914689,5.266411766036114,3.1342209397606338,4.707082991771706,6.390598905552461,2.8698714061777126,1.4168397419128291,2.3645724322958563,3.5084286525318573,3.9354597478052895,7.11009175192513,1.4222330006830475,6.929672661579345,7.041001987167506,6.332349900373874,3.423578170981797,5.137503523749935,5.3139713710590195,5.999549087340949,3.2433644256936605,4.7114949066500875,3.7059779016825223,6.291677223176022,0.3895668117627256,5.758089934201813,7.001577085389535,7.931505781869729,2.8257856274647914,9.151270288790165,2.440952198029637,5.74604398299834,7.136683577697236,6.353146825498083,10.212070748654236,6.007419783571914,4.997744026059632,2.4982508675278257,8.25176642434399,2.8579809951275723,3.300123724569014,4.640967910449898,2.625270489374693,5.399854673572431,2.2660368939953175,1.516015147003665,6.199672344836364,5.708187236020708,5.402585758232587,5.269033146455237,4.529820946528695,5.212958362728285,5.648753032989572,2.9467308601403097,2.169925001442312,6.088735246190284,2.4356285940520905,8.627898615871748,5.300489844363109,8.091065077930535,7.119252412157343,10.142962207064079,1.7398481026993275,3.729008870337863,4.45022149589718,6.461724991502711,7.904544309292717,5.490890978894291,6.76593249347046,4.502075956045791,2.782408564927373,4.205548911173033,4.139960569545456,6.708325207143167,4.632268215499513,5.6194130105979365,5.541638734436141,3.581351247168777,3.496973580998276,4.862451391325582,4.784503982929566,5.359310316890443,3.41006969175638,2.5897634869849773,2.769771739249448,10.641220865859923,8.202368742775262],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle ElasticNet()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_pred_logEN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression elasticnet\n","# réglage des paramètre pour la gridsearch\n","alphasEN_log = np.logspace(-3, 1, 30)\n","l1ratioEN_log = np.linspace(0, 1, 6)\n","param_gridEN_log = {\n","    'elasticnet__alpha': alphasEN_log,\n","    'elasticnet__l1_ratio': l1ratioEN_log\n","}\n","\n","GridEN_log, \\\n","BestParametresEN_log, \\\n","ScoresEN_log, \\\n","TotalGHGEmissions_pred_logEN, \\\n","figEN_log = reg_modelGrid(model=ElasticNet(),\n","                         scaler=scaler,\n","                         X_train=BEBM_train,\n","                         X_test=BEBM_test,\n","                         y_train=TotalGHGEmissions_train_log,\n","                         y_test=TotalGHGEmissions_test_log,\n","                         y_test_name='TotalGHGEmissions_test_log',\n","                         y_pred_name='TotalGHGEmissions_pred_logEN',\n","                         score=score,\n","                         param_grid=param_gridEN_log)\n","\n","print(BestParametresEN_log)\n","print(ScoresEN_log)\n","figEN_log.show()\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"y":[1.7704414093907068,1.770361052481188,1.7702515116753095,1.7700990997009793,1.7699040159758048,1.7696457813565936,1.7693041789412163,1.768861406992396,1.7682915599840385,1.767656771548867,1.766888212194979,1.7661066144300293,1.7653495772225583,1.764787802382223,1.7653549178345203,1.7673927487797514,1.7714136116363406,1.779228236009838,1.79423152354756,1.8225379895512426,1.8752213716808914,1.970937694890494,2.0509572805982423,2.0509572805982423,2.0509572805982423,2.0509572805982423,2.0509572805982423,2.0509572805982423,2.0509572805982423,2.0509572805982423]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"y":[1.8012494107846824,1.8011187728337048,1.8009414279350404,1.8007001885778495,1.8003833372950129,1.7999598340578467,1.799397620748921,1.798648126171018,1.797671974325691,1.7965781027932806,1.7952101933926088,1.7939299889496074,1.7927970458542921,1.7920354467774446,1.7928999437235453,1.79568929117661,1.8008195475247892,1.8101313090850382,1.826828292437309,1.8562270268286032,1.9077734962471011,1.9985419181696988,2.074805637696948,2.074805637696948,2.074805637696948,2.074805637696948,2.074805637696948,2.074805637696948,2.074805637696948,2.074805637696948]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"y":[1.7396334079967313,1.739603332128671,1.7395615954155785,1.7394980108241092,1.7394246946565968,1.7393317286553405,1.7392107371335115,1.739074687813774,1.738911145642386,1.7387354403044533,1.738566230997349,1.738283239910451,1.7379021085908244,1.7375401579870013,1.7378098919454952,1.7390962063828928,1.742007675747892,1.7483251629346377,1.7616347546578113,1.788848952273882,1.8426692471146817,1.943333471611289,2.0271089234995365,2.0271089234995365,2.0271089234995365,2.0271089234995365,2.0271089234995365,2.0271089234995365,2.0271089234995365,2.0271089234995365]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"xaxis":"x","y":[1.735545782500404,1.735446138062868,1.7353103141898292,1.7351257361696566,1.7348759723722542,1.7345418737771578,1.7341060186894812,1.73354421769592,1.7327958173635356,1.7319684431793545,1.7310714257162956,1.7298976800157158,1.7284208116483981,1.726652136384039,1.7247101212067353,1.7229710135005438,1.7223363559826594,1.724779259291132,1.734356309502161,1.7590346479674952,1.8136819359540441,1.9219315546593236,2.0109287829567917,2.0109287829567917,2.0109287829567917,2.0109287829567917,2.0109287829567917,2.0109287829567917,2.0109287829567917,2.0109287829567917],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"xaxis":"x","y":[1.7670174271849384,1.7670366221582607,1.7670602175240337,1.7670939459427764,1.7671987677418595,1.7673652121698873,1.7675996419866034,1.7679253327243267,1.768380249640755,1.7690043778439917,1.7698043345450718,1.770937952739258,1.7725676205364207,1.7749292743152483,1.7784097008683184,1.7836308776282046,1.7916280225182237,1.804146943676598,1.8241757267936638,1.8568489394965169,1.910622649895309,2.0000186587370035,2.0803077884144865,2.0803077884144865,2.0803077884144865,2.0803077884144865,2.0803077884144865,2.0803077884144865,2.0803077884144865,2.0803077884144865],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"xaxis":"x","y":[1.7388537592557443,1.7388582516227153,1.7388649736641622,1.73885713215692,1.7388477736480465,1.7388375861636147,1.7388237527218482,1.7388551636730234,1.7389609484961057,1.7391262429983745,1.7393907811494909,1.739826221735963,1.7405595427965463,1.7419118163962752,1.7444460799350956,1.7485554788875974,1.7553761980736247,1.7669384742201704,1.7868724280892856,1.8216323659758982,1.8824755994219877,1.9882479614490556,2.062020564036573,2.062020564036573,2.062020564036573,2.062020564036573,2.062020564036573,2.062020564036573,2.062020564036573,2.062020564036573],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"xaxis":"x","y":[1.8115164862042736,1.8113832968057548,1.8112045977690954,1.8109610538883056,1.8106301628780817,1.81018255548895,1.8095897236694862,1.8087929005756929,1.8077427913377333,1.8067198487054668,1.8054315232948848,1.804511391060563,1.803808696547755,1.803218926558693,1.8029828076703391,1.8032761417680476,1.8048439654038682,1.8091900699031027,1.8192647629180636,1.8399401940221982,1.8809941269682369,1.9608522294006336,2.0394174472898876,2.0394174472898876,2.0394174472898876,2.0394174472898876,2.0394174472898876,2.0394174472898876,2.0394174472898876,2.0394174472898876],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"xaxis":"x","y":[1.799273591808175,1.7990809537563401,1.7988174552294256,1.798457630347239,1.7979674032387827,1.7973016791833585,1.7964017576386633,1.7951894202930176,1.7935779930820637,1.7914649450171471,1.788742996269152,1.785359826598646,1.7813912145836703,1.77722685825686,1.7762258794921124,1.7785302321143626,1.7828835162033272,1.7910864329581866,1.806488390434626,1.8352338002941049,1.8883325461648781,1.9836380702064527,2.062111820293473,2.062111820293473,2.062111820293473,2.062111820293473,2.062111820293473,2.062111820293473,2.062111820293473,2.062111820293473],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle EN pour le paramètre<br>elasticnet__l1_ratio=1.0<br>en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE ElasticNet pour tout le meilleur paramètre l1 ratio\n","FigRMSEGRidEN_log = visuRMSEGrid(ElasticNet(), 'EN', alphasEN_log, 'alpha',\n","                                 GridEN_log, BestParametresEN_log,\n","                                 'elasticnet__l1_ratio')\n","FigRMSEGRidEN_log.show()\n","if write_data is True:\n","    FigRMSEGRidEN_log.write_image('./Figures/EmissionsGraphRMSEEN_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.2.5 Modèle kNeighborsRegressor"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                          paramètre  KNeighborsRegressor()\n","0  kneighborsregressor__n_neighbors                     45\n","      KNeighborsRegressor()\n","R²                 0.432091\n","RMSE               1.585807\n","MAE                1.284268\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_pred_logkNN=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4.557655123712581,5.267610423928948,6.213254392511176,4.113985725955611,4.568915172240463,4.926408871688616,7.104841153754558,4.444277269354001,4.86571180611927,6.582365408842386,6.160879560124195,4.9322007357294755,5.144887881344649,5.13857644400802,7.359026932433868,9.437437265249324,6.594723332721179,4.00267833413953,5.5488367419445,6.470645861964651,3.7365018972530213,3.98187377125582,6.492234535025732,4.581205556832596,4.50362697786899,4.077307592486575,4.231042689228814,5.331324098954732,6.3899068622932536,4.038718034420873,4.71954196093635,5.126006448833328,6.0607841715237205,5.550393116762178,4.575259564789022,2.6734316963573357,4.2956585891562975,4.363553893212531,4.493000775245593,4.277534042020772,2.8079897538322296,4.637888136186038,3.4440919121900895,4.392426709351551,4.786119810075628,5.335249426642983,4.136076457975722,5.150342616384042,4.8782692669566945,3.737361434715145,7.1889611249643,4.079625718606424,4.2192051331923,4.910460944428096,6.310430884436452,6.4017995812918205,5.732065074877489,4.057089859704089,3.8718289048040333,6.960151911207836,5.587864838121596,5.558496900619809,5.142938861510274,4.002639999789424,5.387092308981582,4.813071847925861,5.301214024883465,5.977999681613145,4.800555067617463,4.003115162242421,4.170083216348743,5.3347010725833055,3.355039842121127,4.0894249515592715,4.348367193319843,3.479478216243362,2.9308091399891634,4.413140399335945,4.147595157795158,8.963641615288406,6.574995579108224,4.694253800001521,3.5866338607230355,4.140133739260308,6.2644002279909285,4.259921410341991,4.026004844088625,4.307648127612166,3.914785456972006,4.278769012903663,6.727657117551507,3.672399882377144,3.940298837213561,4.750366472383143,4.105342688040536,9.109550445652115,4.046874769773249,4.991972597196552,5.475359166166323,4.406772507871727,3.9538650373203073,3.094337264185143,3.3540675070357713,4.031235583132591,4.906096360521835,5.774157921402126,6.134295584342022,4.231185009029654,5.233456272800994,4.110355307625789,6.838655764022207,5.260117536691482,4.464728206917414,5.55473455025868,4.3162725866441125,2.726268246486235,4.6798076400138955,9.275167730203558,3.696689287325225,3.817743304530039,4.920112039365311,4.7288018006335575,4.9549163688860585,4.093452689768907,6.265431700684859,4.894015806082675,4.295770548318879,4.88494155312719,4.447250206921267,4.342940521387442,4.9152557012022315,4.601367617122478,3.863608759065025,4.933104788400878,3.977750874793984,4.207257898606452,4.162427520668441,3.758903866232342,7.398921302412637,5.3541214076315296,5.6254716499356725,4.444277269354001,4.0043458242178565,7.51604206826509,2.888202937369795,4.2558578897199455,5.63466125073383,5.859928585938973,3.2352585638624363,4.637596122520554,5.005496845525958,5.496463186655475,4.242923755301144,4.500373895184981,4.065175373366525,5.894607601397637,4.640232248670226,4.19601196290779,5.837052904776395,2.7720653676150557,5.245524492192383,6.1455354209619895,5.855765787055879,3.9221602094298516,4.538213399110563,4.213469955895814,4.480765136672781,4.5399673864446966,4.809848936095762,6.869149628894535,3.7025390213150615,5.046430497416893,5.368115762572465,3.9818737712558208,9.060854230387072,5.573837602992019,4.826509727162031,4.7606108133162115,5.9249449641211624,8.963375754996118,8.808201498660619,7.622926361579252,4.873742485989666,5.002296851255992,4.629699603698793,4.8677425003848915,5.7626197504072225,4.955913226779071,3.983385757352922,4.180572303535859,4.690202976129798,4.408741066840036,4.987135540280552,4.7608623850146365,4.006342869098361,5.324866509997244,4.65919674638002,3.697609158067036,8.220275181891923,3.2731136274377186,3.8993982327970746,3.401699006295627,4.66377003024223,3.918398758806699,4.749614995863055,5.151906985030347,7.83507403657418,8.922512319421273,6.473390960860672,2.6904840359050803,4.762940134782904,6.192214728585993,7.657255823467226,3.38629032445013,6.26135791255781,4.313629049989893,5.59078969928187,5.282367402343903,5.096706469835831,3.7875597323236883,4.642349620015312,4.319996077995412,4.379609577419956,6.09476982937979,2.8532110793093364,4.787110550367097,4.592344709096564,5.998890074739464,5.117232208184582,4.735900931569353,3.8701434849066643,3.291401757912931,5.5000285881162885,3.9764092803441575,7.325065655177228,4.491623350792161,3.9818737712558208,5.506182721237296,5.982346201424856,4.046187381880618,5.884862498200659,4.458799303974758,4.056557671210824,5.749260843642928,4.232066444048332,5.286635443644076,4.00267833413953,4.004301528525759,4.061657671627115,6.632582059267917,3.5514654673829695,6.441160515143065,5.195309421261035,4.3253134030400195,2.8820407995148254,4.479801917977968,4.551029978459155,9.760666410308591,4.118935183234206,3.940580541691225,7.703390137593207,5.374300597345602,4.7846696050732955,5.417851580394056,4.454332015333124,4.1424932855565295,6.306442807606285,3.7123198743099906,4.358430512761329,5.4776270620339425,7.270953379136738,4.009152669301527,5.482939230003876,5.662998923447059,7.635271443782606,5.750403781418797,5.449865358278003,5.8039456344388,5.589408033469238,4.874022240865476,4.935243598462295,3.9110842067220353,3.770997929859435,4.118381146340055,4.426389888198414,3.9511428816176495,5.47729923083757,5.375666151627331,4.418731717079012,4.224913407139581,4.015896867106571,4.144220238046618,4.203895967024712,3.9818737712558208,3.431272372982695,4.113999317085581,3.970343960431055,4.699226867271653,5.118387663172281,5.006453507517137,4.75952802433247,5.018565084532127,6.214900400634354,7.396655286994357,5.34833314126253,2.7471744619724827,4.665255421030858,3.876090138633522,6.026023367554857,4.711574235892955,6.474220631458166,4.336702399076427,3.5719626058424043,5.166447990239578,3.8342778906234876,7.42653906686547,6.992564812686548,4.031235583132591,3.6765044757753143,4.6820534541873915,7.2314302660131995,3.2865117483300392,5.884694991380358,7.44129044624223,4.8412247831708175,3.9499188585160545,3.918486193945214,3.984272943115558,6.914731196892914,5.370158059377305,4.256235547970939,6.08758311158483,3.986249690432061,3.315319211660931,3.9872526259796706,4.033747147842643,4.9955172182747685,4.832489608369337,4.141508205884373,4.361965107759067,4.741150712228958,4.252608343151949,5.580091687541304,6.501848390490727,5.10197601356327,6.818490358976315,3.741329069178242,5.072000176386363,5.3340521561621586,5.953287821922434,3.8794432789935525,4.314867142902716,4.499612893736807,6.179617804089757,5.323401438919688,5.764920786421788,5.082445319144797,5.037649307481352,6.125479035000089,5.334518923201416,4.0813626655248605,4.3010852158608115,2.8359523143348953,7.472516257939311,3.9598017099134575,4.096826478167204,6.753861875627095,5.380473677535042,4.239352575695286,5.3125554421805266,7.6824416908755,4.531408807650672,4.937473136854936,6.480823281238223,8.900372839176802,2.7056331005453758,7.585862523252227,8.009551766695804,3.2854013295616578,4.220505782168834,4.056547060616462,7.855881326900587,7.471972708476205,2.915629751723402,6.414767280945481,4.393586952992365,5.633204263086715,5.184637427520156,5.459773683847152,3.37634284801198,4.745520212579698,6.931240984925738,3.3220008508319,4.072674281319465,3.8014649263266267,3.8681629623115565,5.294155142058434,4.34322064396982,6.479230557783206,7.933810278348122,5.923068865441262,5.680632294240716,5.933314739075154,6.049938011498626,5.563091787488161,3.9038595314413302,5.469608760636004,3.6813886311716306,5.722385950632256,4.3879175741297605,6.944681812101578,4.87010744299304,5.371409007877369,3.3540675070357717,6.546961264825992,3.8644193687637736,3.367987313005947,4.638310232838412,4.6659498695824695,4.811862098219401,8.791816926741944,6.837777348816389,6.097716789386308,3.908512288265612,6.419937694943725,7.367483918385215,6.612907066342422,3.9931371827533457,4.918745797550058,4.962055603345736,4.240278940395553,4.3170378955881255,3.6517153343857123,4.140133739260308,3.24573162376275,6.160570821557126,4.068896519357845,4.638418245354927,6.674176096329755,3.7705009812631967,6.249947078636795,4.555018618658158,2.8359523143348957,4.071863959138761,7.255088199578019,3.3649878199312413,4.989404753582173,4.644283634699813,3.3238110598912898,7.234831864980774,7.819945059669315,4.786119810075628,6.531975692768184,4.174642978149309,4.0835555212391705,3.719942800181675,5.1144232054376815,5.539659206500858,3.7226026605435396,4.176856744812213,4.015003664335192,3.594492916768556,7.811641850817056,4.492394527620699,2.943945103206623,4.3193275339252715,4.096267418165799,4.140133739260309,4.083942052813752,7.449746465551866,3.7011749948872668,4.227114117846492,5.003913048761519,6.822345778365455,3.7050004848654328,4.552472176458634,7.79091126070411,4.1663638410015285,4.267728692457542,4.576606017803685,5.361678778992744,3.39587330156575,4.3830391969123506,3.8055033111010275,6.134089085997019,4.474962965870893,6.982646335204715,6.055285612510503,5.455259742744305,4.24087984633542,4.948790252472967,4.387971481435084,4.870669714376738,3.5524263390836395,6.401551373013415,4.382271675681268,7.230384264198368,5.488250989021673,4.2331558152089315,3.5613986456813795,5.463477097702701,6.398464319316932,4.9180606387449,5.942635809218371,4.751227650539311,6.443321749494562,3.1519269900989935,5.131556966041497,3.730782172189949,3.858658012039106,4.068896519357844,5.677685873240378,6.245823293889711,5.01510010300927,5.180631169231413,4.269593598540778,5.217146315555092,4.977643080967356,6.166327306418835,4.688792714009481,4.345164282264676,3.9906600912376295,7.652213080021438,4.514903121109782,5.565088965781367,3.367998417450801,4.211739809622188,6.431808807536868,5.594823680671949,5.904745610389231,4.140560127806196,3.073637335597331,5.909299660697716,4.738727599779759,6.3460708204824945,4.944764690019039,8.166990651431703,7.7111986318831205,6.07999567772077,4.825964892380641,5.844760158952157,5.830714909041825,9.046420405965616,8.588654184311716,4.348367193319843,7.761925469540278,4.547844419840666,4.907289262744868,3.9897498708362056,5.16614057103496,4.788300553873682,5.294155142058434,5.490828580674792,5.1534920916707385,6.299424878550255,3.532695368554596,4.045629354496972,7.631002154794125,6.285088950070836,4.834334957535154,4.394962378289604,4.204709576860754,4.314867142902716,4.635614639195394,6.511623462211028,3.237928018096775,5.60635995199417,4.026365773577856,4.418731717079012,4.202779066262635,5.7827662504949435,3.308204429066595,5.067843603511445,3.428492109260274,2.69289154538565,6.161447985499813,4.6423317048101795,4.0226114812152645,4.8971161123769535,5.33634550184393,3.377646757063705,4.592257724689938,4.343761617468589,4.390040583549033,4.977651141084459,4.8213637146097925,5.819131368475062,5.785476991988999,7.376289955964207,5.309033609240676,3.29140175791293,4.65850244576325,7.466348327187583,4.517385984537689,3.7823471235761366,6.206215009705706,4.6284464153689635,3.6915562283146404,2.853459229231779,4.517474373490026,5.099954584188211,4.02384773372113,5.796579807064394,4.69706652707144,4.899797377383223,4.33523772796993,3.570986623103145,4.024171021040609,6.351653372851404,9.041732087981986,5.101950973932692,3.4701974894656065,5.486132390443842,4.656850443278352,7.2944180410457715,6.090619187541498,4.140133739260308,5.041122545755184,3.7956235641669838,5.188518716090494,4.005706962238896,8.140603211842485,4.001759533830234,5.317409776750918,4.207406633640377,4.9659168150162625,3.9402988372135606,3.651796013182271,5.150342616384043,2.7104760822674385,4.9839834132183665,3.391196406968192,6.067567212388382,5.524980239951417,6.350442508611867,4.773336587790047,6.207845637204823,6.914386585101526,4.099029955112291,6.224976940522116,3.6811528293927926,4.058697093317315,7.372836687422477,3.743038640524328,4.403459898251688,4.3193275339252715,5.428433407964249,4.634106238615351,5.432553362594084,7.4351371411005305,6.3603147969115446,5.0239196650991,4.4310489896046485,3.49818780165545,6.245624810002118,4.9734507957764995,4.764297637971447,4.741446356134163,4.026098194930833,3.428718378637029,5.397485596740739,4.345153745426483,3.9192696305776784,4.835354801977385,4.40135871764621,6.27378692474693,5.554828270105941,5.257014547483863,4.603548743281772,3.654525556626953,6.616039164537234,3.3451508360542825,6.796434951541899,6.274167595486924,3.94260953365619,8.056890639584518,7.470163055916442,4.387833146999562,4.869539501778008,3.647422988443503,3.3510384367725616,3.890584675576295,5.2553632972345925,6.067953271570451,4.876973750851085,4.105342688040535,4.584660765404298,3.9943143910359558,4.149474738792094,3.4349004186314054,7.010807423795202,4.53247473737902,3.3017602001961577,2.8687218547716826,6.3538516565547765,4.5367564632023,5.411736204995538,4.4923945276207,5.894607601397637,3.8648332910149983,4.200876209365153,4.1196606711483055,7.799685922266525,9.582130832426307,4.99663566333006,5.90729648503124,4.080280024005204,5.2198248818873045,3.980636386983836,2.965183254218501,6.010052030206342,7.923532147731146,6.942414262435124,5.539621720086532,5.64915086391048,4.3021231121847014,5.4369277232046205,4.153993950905636,4.149474738792094,2.9473455959309725,4.142519890797581,3.2881949718335264,4.982537189891208,4.072131190104431,4.097458615734994,4.137869230076626,5.416948815899738,3.7312578627904665,3.8466277119010495,4.6259542439477075,3.5613986456813795,7.368304321174865,7.60931195612071,5.19667813851246,5.041122545755184,4.637596122520555,5.066661853662786,7.094622233593898,4.001759533830234,4.995059435108696,5.767034330519891,5.530657837434635,3.5496862323564113,5.4443045631504035,7.481826046866528,3.88944498148021,4.3144779769506645,4.275749295057272,4.830438862876651,7.838778910194638,6.3721466735183485,3.383175332754698,3.28425615496392,5.410046674717633,4.502203611466963,3.573142068546697,6.278358745909262,4.174845521764491,3.722767701074468,2.8359523143348953,4.151937554592239,6.961182871024957,6.2198442113624814,8.498558756701634,4.289753652927069,5.92370437507129,4.755902719873835,3.9124087267859666,4.828432668514433,4.659196746380019,4.536742387325945,4.783656901326681,8.268660508984736,3.432841640192074,5.341679032583272,5.908273370438509,4.150977650278473,6.248241773755499,6.621134591529342,6.078800202385056,4.244042397143849,5.233538304867867,5.019542186114814,6.712783121961398,3.871565689848799,8.470340353098836,5.73349072557645,4.429843627824922,4.852101910792777,5.980749140983161,4.010235090633527,7.6312697548825374,7.800784830459089,4.3243972189487545,4.647992409793511,7.87334368743985,6.307190743391064,3.7496780598849266,5.210578442017061,6.949871017025197,4.489874167676244,3.3426898665873592,2.831610368450225,7.260879859677491,4.432199379087289,4.980437342074885,7.4195457888829175,3.960346102344017,3.9860318171691125,4.078774047730267,4.408573651891473,3.37368018098234,2.7736257004738394,4.404912144618294,5.154276806116565,4.146207234152366,5.866267313664758,6.193089285369049,3.3400292532669824,5.777010841333366,5.137814982881313,4.059935630498358,3.259389285643094,5.874925998919039,4.941233298928009,5.0029813100676925,4.24033033377569,3.6797120430974473,4.1987659228270315,4.950524413660766,8.015831509518703,3.374709339280598,5.365761370984176,4.00267833413953,6.034523741249149,4.983657933311337,4.688900816796844,7.270233532018842,3.4641285789329377,5.626312707288276,6.637150584252649,4.046923804195414,4.345976472762396,2.915629751723402,3.902184270736665,5.13384007100104,6.1269591385060735,5.806923038783875,4.45836729281481,5.155145629732626,4.576606017803684,5.794253421300824,4.844633085926322,4.999627650105546,4.132079403028172,5.864540343529603,4.571188004496024,7.603067721386333,4.230579948464707,4.084469884496492,6.044227289864454,4.1890821508374225,4.277534042020772,4.040904184857144,3.697609158067036,4.153993950905636,3.5662879083493086,5.331160214835963,4.733174266993402,4.556823236756203,5.062546733348651,7.885281606937867,4.01723210660607,4.025850081482902,4.756492023681113,7.635101423991817,4.387833146999562,4.183435996733994,4.765672834828822,6.395177227489036,4.504671117792693,5.043807256065635,6.926477390108143,5.9159810922102185,4.087527368593725,7.821911936809026,4.140133739260308,3.9836964387813634,4.459459735163977,5.123302162870524,4.897505034142126,7.407341337712992,3.360026873184157,4.38462545212255,5.404709157486076,4.056547060616462,5.760620551568627,9.740691045159352,5.756282130048341,4.2760088019553555,7.952454343630298,5.696942310750373,3.8000327037464268,5.105398946033938,3.3891511320889127,8.16612935909551,5.555830439280306,7.627888874761382,6.250136983467584,4.320724167887263,7.299072470376261,4.096826478167204,5.256915561646615,4.2539911334582765,2.7672729534740528,7.619444479330185,7.713626502295113,3.323811059891289,4.4612650764205934,4.319609286055251,7.422174205651893,6.03863100338056,8.09916638742082,4.170748121895354,4.916191857988987,4.30215408174394,4.236765539052254,6.701946190053571,4.4457682505604454,4.7961738053266725,4.660368426174942,5.289376622073946,3.993041368879595,5.331343653549948,5.727249414639439,5.354579047868058,4.409718761682289,4.588347608743447,5.407926843396555,6.06020819535006,3.940580541691225,3.882174796903105,4.640631349092449,4.234824201574269,4.357131806647166,6.067953271570452,5.949484107505101,3.8093218368490316,4.168222022252071,4.806142253219008,6.226535462913868,3.4003434842789746,4.394418142191004,4.271139540921379,5.4198715977709115,7.285884796889889,7.075357951034089,4.897525538611638,5.475117433619537,5.058983229446939,7.695058175404582,3.3270001741025688,4.276875127285418,7.587782121511332,5.1372322887789,4.093684657143887,3.347089310189025,5.4016778454881695,5.060775535574359,5.5650258334418075,4.117039149498552,4.311034764862575,3.9676280501676375,7.248221791496576,6.899525807414563,5.6948520913648695,8.085286725129079,6.161447985499813,6.490500459875711,5.4536918872805975,5.005383000423763,4.41433238712499,5.74566053285529,4.3578585982529745,3.685317812600091,4.228458089965133,3.6511000283419057,3.8336551049758523,4.141147727630387,3.3319735043213736,5.195834545602774,4.367649611132879,5.3407653250270855,4.933377548628096,5.332140823324813,6.427229452575976,9.14170647854245,4.640631349092449,4.868465825900699,7.2084801187728305,4.24321544252427,5.709982993429896,5.4377546076544405,4.750366472383142,4.363553893212531,5.740078124428479,5.444858516413261,5.079129255846061,7.285122141706321,4.820666691471377,6.193089285369049,6.693220172618141,4.0835555212391705,4.0672390160414045,4.298313972167483,6.9028409586444806,5.5862923099567485,3.7246766969404423,3.717006638023382,5.235391474806078,4.845837457100558,3.8058080312241556,5.096999692572889,7.549219787986885,4.237403985948843,4.468727213944961,4.316760779296591,4.180572303535858,4.926408871688618,7.038164461127777,4.162512463785223,5.137415943288561,4.500682253863707,5.417547368234734,4.345976472762396,4.052091769149032,4.21491024618517,5.293755486593216,3.5916474500932813,4.1929507240999495,7.526124920841341,3.982054322964402,4.869263561974075,4.127417622859943,7.362829298108379,5.93315959199541,3.347834999275549,5.062546733348651,4.01616986868042,2.8246817841195764,7.19644213485712,4.066101881175775,6.756701408881133,4.830189652502141,5.582228053291966,5.484719352103322,3.8399812500496524,3.9721231466113207,7.900818028198896,4.329623884448752,5.789456346748299,4.230400173918911,3.7312578627904665,4.4701144763417116,4.317910028743954,4.484385681352875,4.8179598665309875,5.106177122535324,4.500187343952437,4.71632751888289,4.638056735358068,4.212316228239487,3.9695679851296046,3.321392092206403,5.769045718920536,5.794118568279148,4.872289507758752,7.717469649094805,5.037156806695431,4.479864252859591,2.8359523143348953,4.378024635339669,5.0037302387360025,3.993041368879596,5.657353922751962,3.03792914871535,4.676678679808271,4.605239636706719,4.982537189891208,4.743783572768445,5.6214387691519825,4.538269343612875,5.152515564915279,6.775988477219321,2.695028406301404,3.99431520281278,5.706125001532967,5.623934696579013,6.817403073177852,4.105342688040535,5.213703133951659,3.8195381960475263,3.888131316683495,5.718580217409052,4.346500042752862,4.7241363481231735,4.165582051069927,3.423688489617481,4.185920992158085,5.853890220217514,4.093713652977127,4.681600898712565,4.825915966556418,4.5389077004390606,6.589801449800449,4.520989264136317,6.525740055342943,3.4990711018616922,3.974280815634176,4.933377548628096,4.53515939780555,5.3750788042861,8.46205651560768,6.270514846748689,6.864524575694684,5.927988918009984,4.826509727162031,4.735656594571395,4.073605177161081,7.596176893900267,5.044662037496233,7.623082962443411,7.545215427868213,3.3540675070357713,3.307005647461997,4.284650283215913,4.315282200580217,4.132844540454024,4.154069173330052,6.306333476760148,5.966719650414347,5.155939882679753,5.248518082602153,5.287882388742536,7.3860710404021575,3.803787030337542,3.219496845700054,5.772433815540236,4.0774268581061115,5.102582283650847,7.115082080285721,4.061517965791542,3.9110842067220353,4.610741613163925,6.683906588547627,5.097604619990312,4.131832330071636,4.135415454090175,3.9818737712558208,5.546828700090097,4.503027859008861,4.408573651891472,7.506929454704097,4.140133739260308,5.609451497465679,5.857588194439051,6.796624487206258,3.2381360813569384,7.435914148580226,5.935405958661576,5.7946870118091205,4.32042227229317,9.050283681521538,3.882154208980656,5.147109364950631,4.031591446192473,6.792722096746696,4.989112878207989,4.785284545493709,5.344328191577198,6.23483504751271,5.850072741929416,5.922723889588935,4.861436292079091,5.33240235016282,5.9625915172244115,5.5386971793563955,7.849198836189191,4.054512524992715,4.3309028178722615,3.7443858105192667,5.41330205556695,5.215585702898403,5.301351396017292,5.680340050022377,6.196615015234179,6.38760938254677,4.140133739260308,5.423690262126759,3.7084462021974773,4.4984595675892365,5.485769948123387,3.3537196163922323,3.2352585638624363,5.111985388244734,3.6976091580670363,3.0098219173244933,4.069934715435853,3.8261216114601506,4.297722901694924,7.411050724826932,7.7000978579444,4.853778557694449,4.943768269593336,5.7827662504949435,3.9360139405486128,3.9681695428923085,3.345426068872405,3.3472802913694952,4.235905406607403,5.527020771823147,5.209242942242094,4.369797085502774,4.694435284949505,5.779276443171503,4.837032592240575,4.110162546004161,5.149895548836718,3.5866338607230355,5.9528925904051,4.8075728019044615,5.349757592735874,3.71038028197847,6.162368429347679,5.009795708293278,5.5592429964126335,5.012781620741535,3.317943495833155,4.80760410857953,4.334600589144231,4.80476881460923,4.570488623481296,4.071363113296908,5.3552974419187,5.588309229489866,3.9818737712558208,5.105907837095493,4.830189652502141,5.278887947726569,4.2547414172761835,4.322315150713791,8.280292671272026,3.440156083919732,4.84928717552522,8.267368301650704,5.872906892404919,7.485859954984705,5.39218314387713,5.68903578208337,4.202528628226629,4.728663672870936,4.1987659228270315,4.743321878121941,4.941324037578782,3.4487942541059584,5.001154764858094,3.5866338607230355,5.059442292881736,3.9333795952216764,5.331599211780027,3.291401757912931,5.459773683847152,5.618777080950702,7.558217620381322,6.285065152850219,3.9125546913936318,3.0894032517495473,5.280185282493305,3.3028718794779586,7.694574014317641,6.046969259145678,4.377348675981303,4.348083517768206,8.296716019506432,4.423844621475742,4.9648620402458175,5.408307702011873,5.029434274289932,7.520774723789732,4.002639999789425,6.067953271570452,3.4422570948213513,3.378817902711148,3.844230825749505,4.655948666598568,6.575768902206392,4.560086701617333,2.886612517045812,4.489874167676245,5.566622455903861,6.697610673001279,6.274931000829234,4.045803547745332,5.68180819788595,4.407895444637709,2.9452964863582123,4.369954979875667,9.159467845819336,5.438487266271622],"xaxis":"x","y":[6.235152624217932,4.478324711485028,6.139142019145689,4.729552770453971,2.689299160535892,2.4698859762744636,7.592232766722112,6.075532631167357,3.9020735793107426,6.768713657030485,8.05642095591347,6.192983169973992,4.934044647112246,4.088311235888661,7.071462362556624,8.345405246717796,8.140829770773001,5.7554217347342425,5.091276694365222,8.047178455318921,2.475084882948783,5.260778431893426,7.873013392989595,4.4495613746132365,3.4369613378336026,5.575917361118149,2.821710215034674,5.650190123496775,8.870210577263595,2.2868811477881614,5.54936113292434,3.682573297347578,10.919861451442168,7.606886339275915,3.4276061727818994,2.1009776477248208,3.548436624696042,6.819540460505722,4.475084882948782,6.762348815644129,3.929790997718597,4.173127433480656,2.7676547982373463,4.804260115634738,1.9335726382610239,4.846493315964563,3.10936055940423,5.617651119427331,5.739578112048051,5.089159131911238,5.6806056662025926,2.8579809951275723,2.1176950426697547,5.465648025982281,6.027463723408577,6.299391206126829,6.422064766172812,2.4356285940520905,4.153805336079036,8.148324042480084,6.387500406480984,5.944155663440363,6.496494306853713,5.703211467391146,3.192194165283345,5.185866545311334,4.718087583960517,6.035403969942067,1.3785116232537298,6.335211681705785,7.330737650206952,5.452858964713811,2.726831217032493,3.8369340113210915,2.4672794804599825,2.9616233282869446,2.0531113364595623,6.248496885959384,2.817623257511431,8.971342367308225,6.606294160697713,5.901832305785489,5.339137384919585,2.582556003014061,4.779259720372376,6.044831233447812,2.7949356628035362,2.8439838440483265,6.690277401023435,4.017921907997263,6.596487956856693,2.84197311892718,2.3074285251922473,5.976821852360685,5.10936055940423,10.16679075071818,5.2787282129389395,6.339493737901733,7.125981653854716,5.148934104526339,2.321928094887362,2.1210154009613658,2.430285272977781,2.5310694927259543,6.72805655144938,6.989025440631378,5.844988156682612,4.749534267669262,9.159820892013935,4.272769732436511,8.601733652922286,4.124328135002202,3.270528942380718,3.894332742277694,4.943921326553485,1.0565835283663676,3.7322691995014496,10.679004987548526,6.500961352460057,5.787902559391432,3.7782085763980877,4.6661884898727815,7.593353770980297,3.077242998932461,7.602439063741618,5.537296067090842,4.203984165855989,5.299757511839318,5.94251450533924,6.951051533288609,3.542258049766918,3.507160349117523,4.71259578044703,4.6194130105979365,2.550900664647523,6.872951851130105,4.712045448553693,2.867896463992655,8.377687734026882,3.678071905112638,4.0232553523003025,3.1538053360790355,4.635173946674924,7.127117349726852,2.4854268271702415,5.592756010441026,8.25559514727367,4.634593268445757,2.5582676340557358,4.990501111363251,3.727920454563199,3.8308637567517576,5.924337464075348,4.187451054027326,3.940166750482817,4.926473911256073,3.5434958834257717,4.708187236020708,5.568640195419223,4.831370587967541,6.12969515665378,5.751410160064371,6.279471295644468,2.523561956057013,4.552131108253784,2.6016965164809576,3.270528942380718,2.805292455600712,5.533874777258546,5.741197298571427,5.787641414483327,3.565597175854225,4.40735275114004,2.137503523749935,11.52102619171175,7.417936880550136,4.96624587322494,6.385085853547684,7.846744023244868,9.156361220297567,11.010164414182617,7.196430000954568,4.008988783227255,3.2464080872463845,5.763145958149476,7.976535484023556,4.4121040446775694,6.918624755574591,1.9745293124838819,4.546585829008101,0.22650852980867975,4.1715271060388135,6.920055054806967,1.3504972470841332,2.3589588258323295,5.726013748860151,3.2357270598380583,2.6848187375532224,9.236301266827088,2.4854268271702415,5.9618548076361595,4.088311235888661,2.440952198029637,5.456806149230474,6.205157883920094,5.588564737401351,6.620439787651722,8.334139178838237,5.331633567171029,2.211012193485512,3.8063240573900288,8.45889870671479,6.602884408718419,3.8439838440483265,6.132988042627442,2.541019153133559,5.953730849811491,4.18824265626441,1.4489009511451278,2.7070829917717063,3.0107798387532423,3.4019034716079584,8.574290766120185,3.542258049766918,4.313245851787562,3.3854310371935203,6.156032498888866,5.419538891513785,7.3107945270667845,1.3950627995175775,5.8346606579017255,6.025250321561575,6.2268938135713885,5.548128323585586,6.746447139362815,4.583158003874408,5.9576824861651225,5.778208576398088,7.834028534577023,1.570462931026041,7.0083165637349865,10.307269127730322,4.95093492831454,5.099295204337775,4.547203024756931,7.9684364043347875,2.8318772411916733,2.4436066514756147,2.4724877714627436,2.37851162325373,2.6507645591169022,8.202417721575182,4.963011647599427,9.01321099459576,2.176322772640463,2.73768676140986,5.5786368624717415,13.52966114241807,5.72328550477764,5.642412772905056,7.568108130120591,6.034743949324734,5.320845667645722,3.8469946869655733,3.8639384504239715,5.154210530588339,5.777945670695148,3.327687364176047,4.169123281476757,8.135247549809566,6.890446692679906,2.2479275134435857,4.9804823555382605,3.3015876466031866,8.873136468833458,5.8346606579017255,4.189033824390017,8.095608004811197,5.380244590478185,4.571676809970931,5.071247819461442,2.9373443921502322,2.944858445807539,6.706254253573203,6.849624029924543,7.541716163393138,7.907912142840534,6.091065077930535,4.388878338811989,4.078951341394822,2.698218478224414,2.5058909297299574,4.621758856709732,5.3175935046234715,1.996388746447621,2.7070829917717063,5.974529312483882,2.883620816285671,4.853995647176393,4.593951283948411,6.496494306853713,3.9845893503624565,6.212180210044431,6.897240425574799,5.452858964713811,2.232660756790275,3.773996325111173,6.075318692553354,4.161081482277184,3.2094533656289497,4.922673592849446,4.478971805032942,3.0840642647884744,6.054414387917307,3.486714373030702,7.582029046119882,7.461888664432819,5.6604951318885135,2.7311832415722,2.049630767724601,7.850749425896496,2.37851162325373,5.986638471730853,7.210817432089213,2.769771739249448,4.970393537914677,2.889473542531109,7.08533966935737,7.463197379833495,6.09950561709114,4.17951105027151,8.27891401949994,2.4594316186372973,6.221297103163338,4.867402305727543,2.980025300238734,3.4276061727818994,5.933572638261024,2.565597175854225,7.056366761024529,6.199672344836364,4.985955754607509,6.215484464192794,6.7410624357490905,4.59215800212536,5.19377174339668,4.099295204337775,6.714108111595169,2.1634987322828794,8.524737587012376,2.18269229751619,2.765534746362977,4.67581593117227,6.658497136656478,6.557808298936688,4.208673319629471,4.993674361750585,5.422905742612183,5.7824085649273735,5.3847405872923835,5.08278773170927,2.4276061727818994,2.411426245726465,7.603403804166527,5.174326515173915,5.896998342338911,6.425089989874059,0,5.106432077816929,2.9467308601403097,8.843732656611305,3.7311832415722,7.403949364232865,6.247927513443585,8.640931770341906,2.6915341649192004,5.336640446408735,7.390942772802543,2.1984941536390834,3.0125686735030555,2.0214797274104517,8.044284820139424,7.1855692560893685,2.636914580355878,8.491372117815171,2.5185351389821804,6.73226919950145,4.614709844115208,4.15623479785027,2.495695162624069,1.0426443374084937,8.060966074004773,3.5643781685650637,3.679198570566922,4.488000770834068,6.3103401206121505,3.605257262939004,4.821199978056451,7.003039979749236,8.211498981977794,8.84080938879262,7.640895629328568,5.95837871156634,6.804001915179304,3.997292407636508,2.3533232911628965,7.631540867341486,2.7990873060740036,4.706530552538117,3.452858964713811,8.983022086172278,6.187253085582297,4.564378168565064,4.241840183564671,5.187055089968057,5.496654082593496,6.564225720206398,6.793115555274348,8.126549613543878,6.3414521074811265,8.823876711241038,5.094658342654544,8.23429054170594,3.063502942306158,6.9910682753603695,7.485507332459785,6.9092930858238235,6.2992080183872785,5.189033824390017,5.562547724199122,3.2357270598380583,2.851998837112446,2.6507645591169022,6.425425734590699,2.889473542531109,5.012121672712217,4.938285791884053,5.642412772905056,6.199672344836364,2.0738202332916713,7.151270288790165,5.005849075696688,2.3923174227787602,2.1667154449664223,5.727920454563199,2.3103401206121505,6.984019633085816,3.9726926540042644,4.362469888750209,7.173227395028653,9.653203362033977,3.357552004618084,8.556735947914529,3.9420452599160467,4.1026581313637385,6.309067020588001,4.952799477899938,2.3015876466031866,4.580748491763774,5.046578366620329,6.999098033705607,2.752748591407134,6.73348992588162,3.1193561770396756,3.431622959713292,5.15623479785027,2.344828496997441,2.720278465233327,4.763411574470007,7.178117045706399,2.807354922057604,3.685940148445977,4.417515002885087,8.72584338467341,2.4724877714627436,3.619413010597937,7.5130959085510085,3.382667252745041,2.7463127664254587,4.509695841933387,1.6553518286125541,3.0992952043377753,7.846242565109514,2.344828496997441,5.39780296186249,4.658211482751795,4.872828759534886,7.290387539763883,6.802451741275622,5.377123749129487,4.270528942380718,6.325710207515785,3.572889668420581,2.548436624696042,6.845113646617966,3.2265085298086795,6.531849285751203,3.361768359419153,5.599912842187128,2.211012193485512,4.396433531250992,7.406162478777607,5.6519127446457835,5.7114949066500875,3.61706334388818,4.044394119358453,5.772149590784542,5.177917792195843,3.1795110502715103,2.321928094887362,4.854494418154875,8.167066838182594,6.5249725982866265,6.43095427138573,5.669310286202474,4.147306698780294,4.303050084681673,4.3298411765306275,6.912410353808268,4.904002316283692,3.2854022188622483,2.8659188145522125,6.582254908357664,5.198494153639083,5.103077946597573,4.3045110418099535,4.370861740085285,8.654779216959517,8.362951992360674,7.2600256559614555,2.6392321632492775,6.457791260256042,7.31786479839195,5.661920579135087,4.609991295212678,3.460742563789644,9.248900055578497,7.886854944198882,3.7059779016825223,3.0373822220030804,5.982537314399271,5.339850002884624,10.306392126750833,8.18685706717715,5.533874777258546,8.85835405767524,6.166715444966422,6.56727163085093,5.7737326506078555,3.396433531250992,7.532083141492505,4.876271360550127,3.817623257511431,5.643567621908798,4.28318097894557,2.6205864104518777,2.881664619320345,5.006746832440585,5.20163386116965,4.181102550753798,8.571145863602528,7.3433189836610735,7.006298023900369,2.9818526532897405,6.419370342528151,2.414135532984451,8.463115619892362,4.857483428286672,6.972003303808356,4.958842675243241,5.910252962799278,4.4963345134171115,4.707082991771706,2.8094144442358986,3.1651079851445365,6.320484678017694,3.8787253414801053,1.3504972470841332,6.207502459258789,6.152994605492435,2.333423733725192,3.0823619695574735,3.4620523187964327,6.266599165535096,6.2861418728343015,3.6064422281316078,6.75702324650746,5.29020320512607,5.342341397431548,7.833712369021444,2.440952198029637,6.642123916153487,6.722602636851373,4.393690764187454,2.729008870337863,7.036503333738988,7.122569192216447,3.18269229751619,1.0565835283663676,2.992768430768924,4.125981653854716,2.7949356628035362,6.039796407620025,3.4154882710497003,5.535430914871279,4.963011647599427,2.763411574470007,5.062639828286463,7.592232766722112,9.317322159829551,4.955126781261366,1.4489009511451278,6.436128517259249,3.2645364309990255,7.647314509547136,6.15319733085508,2.229587922740652,5.417515002885087,5.0925457415435655,5.34801990922869,2.4195388915137843,7.500961352460057,4.426264754702098,6.247547806890298,4.622344722724236,4.638073837180719,2.763411574470007,8.388663123619393,7.058749412335524,3.496973580998276,3.377123749129487,2.361768359419153,9.114445113289024,4.888986721186558,7.971428593590467,5.424922088210688,7.212958362728285,7.02136867825973,0.8318772411916731,7.720552091605108,3.060047383669939,4.532940288372874,7.506922418047323,2.356143810225275,3.382667252745041,1.372952097911829,9.10522758436473,4.751142324726947,9.036036206482532,7.013685569928653,6.974644026040249,3.521050736900963,2.7070829917717063,6.017253838968937,10.665637927637095,5.325530331567558,7.864928972289788,3.400537929583729,2.851998837112446,2.37851162325373,5.22573765346606,3.8349134297010385,4.126807703142035,2.389566811762726,5.632268215499513,6.783456654360239,5.589164236699772,3.596935142387232,4.365272599764531,2.8399595874895316,6.588864518190148,2.8379432418910273,4.650764559116903,5.37364821133469,4.8875252707415875,7.596264312101167,7.323370069061268,4.88508622529017,5.500164679492167,2.327687364176047,5.592457037268081,5.951401291674311,6.265849421440848,6.862203399053224,3.023255352300303,5.299391206126829,7.7627474419531515,2.7420062108667365,5.932628157022127,2.550900664647523,6.9532652390148435,3.6542063779442917,3.8257856274647914,1.996388746447621,5.713420884868084,6.884964164708779,3.84197311892718,6.009212786805205,7.490730563589641,2.3476656563009706,6.668317719915721,3.8196681834964554,10.872998007770835,13.44366362719111,3.2764966656403565,10.051385697621988,6.023698917266407,3.8787253414801053,6.258895755075436,4.826802684285827,7.317231700222606,7.7475216725404215,4.398487190018967,5.342341397431548,5.767654798237347,2.8155754288625725,5.8011586560936985,2.3757345385831563,2.430285272977781,2.150559676575381,4.505255800932163,3.344828496997441,3.4356285940520905,2.2172307162206693,2.823749360308273,5.130519083027338,4.868390453054454,4.839455765224629,2.2720231890610485,5.525755692829486,3.828834649468056,7.7780114016125745,10.806299887662895,5.515384460636948,3.9699332746978557,6.57334422759282,2.759155833800272,5.033863451866284,4.279471295644468,7.080870811759721,6.138323004056484,5.369815424283912,3.975446765640962,4.708739041359579,9.557597721454812,4.200849574637213,5.854494418154875,5.246788093844365,5.102238193930738,7.479861085016818,3.0373822220030804,6.836681593327116,2.877744249949002,7.240600549955795,3.228049047884462,2.0565835283663674,4.4924941516794314,5.307064162255372,2.9088129077395473,2.682573297347578,2.722466024471091,7.96087076379617,8.903429991567151,9.435816085556171,4.631686366311998,7.457709193357593,6.109569509370688,6.0413306068206225,7.586464525886388,5.687060688339892,2.6915341649192004,2.948600847493356,10.7317008703535,2.6016965164809576,5.20163386116965,8.56017995776418,4.4672794804599825,5.904002316283692,8.44795106244151,4.932155684432056,0.9411063109464314,3.4154882710497003,3.8257856274647914,6.542412837068272,2.5360529002402097,8.739409342229193,4.436961337833602,2.797012977836145,5.624685810525398,6.1705259991768475,5.926948247949772,7.881113960675097,9.500642736062806,7.073606040574941,7.318316841334983,7.971830914780502,6.137913322088407,4.721919445551544,1.922197848396367,8.226941966807852,6.570159301636119,5.3458930856874085,2.73768676140986,10.642557179595622,3.62993940943954,5.175524601089875,9.995102877826515,3.4646682670034443,1.8836208162856714,6.0083165637349865,2.408711861029429,2.950468414150123,4.443606651475615,6.456313341355182,4.8604662585171665,5.661065479806948,6.7997349224788675,4.824258696603374,3.269033146455237,5.9740703670240265,5.925049964727359,5.946496941203293,1.4750848829487828,3.6971065744769747,3.953265239014844,5.464341532761104,3.0908534304511135,3.7676547982373463,2.859969548221026,7.822411496360477,9.401604869714957,2.7092906357233577,2.257010618206024,4.178714641175443,6.423410093269615,6.9070108151861795,3.6484654430273142,8.622783966156124,4.569248029867182,5.366322214245816,5.570462931026041,5.130519083027338,2.773996325111173,2.18269229751619,5.724923070266613,6.9787676510367875,6.751410160064371,4.484138131201669,3.0686708106650986,5.111865963867557,2.0942360698457656,4.315421315945778,4.06177619758669,5.781884234541486,2.974529312483882,4.57349571549561,4.176322772640463,10.888134388352869,4.414812060565847,5.521364878446219,4.582556003014061,3.8369340113210915,3.929790997718597,4.598126959919604,3.61706334388818,2.4489009511451276,4.788163657037767,1.8953026213333068,2.7398481026993275,0.12432813500220177,3.400537929583729,7.515147882149842,5.381629467033664,2.1602748314085933,6.677226328235316,8.025084179448086,3.440952198029637,2.9763636357327616,3.4905701304462013,7.431622959713292,2.7803100990433753,5.659068274843228,8.878265537895738,6.403949364232865,3.452858964713811,8.162995472417697,2.1473066987802936,3.7398481026993275,5.212569338850806,2.8933622107638715,3.9155209007519587,8.562585882154105,4.814550423461808,4.5867647433551415,2.582556003014061,5.077670274232752,7.905507349949873,11.663771557482411,6.559338847606539,2.7949356628035362,8.834123370734224,6.0925457415435655,4.936402377725063,3.454175893185802,2.788685710613534,7.877437521970619,5.3377110921282895,8.303415462685624,7.285032249622911,2.6915341649192004,6.459267666703387,4.398487190018967,8.028624401408491,2.9392265777282085,5.342341397431548,7.217424613813984,7.563234413022304,2.5921580021253603,3.423578170981797,2.763411574470007,5.542258049766918,4.51853513898218,9.535217045453418,5.908092340818271,6.928844036712568,5.612352498752662,6.7786028451432125,6.4202128906107845,2.877744249949002,4.038260575175349,3.4222330006830477,3.777156666004501,4.766065051474954,5.59215800212536,4.534186139090097,6.1147834472498515,5.572889668420581,2.9726926540042644,1.944858445807539,6.412442824775613,3.7990873060740036,4.030336078370959,4.255500733148386,2.5459683691052923,3.7813597135246595,5.135042286233651,5.6061460780657235,7.027021314622255,5.524189078449365,3.145677455195635,7.669948004116052,2.4462562298895643,2.408711861029429,7.221587121264805,4.008092420948722,6.956289026583943,11.260837225974512,5.316869805064333,4.078951341394822,4.509062386361898,8.238500278879492,2.689299160535892,4.22881869049588,5.614415386508126,4.112700132749362,2.9523335663696857,3.693765712217783,7.474030370207135,2.521050736900963,5.727375938594897,2.3673710656485296,2.786596361890807,6.034963989749303,5.578334948681608,8.298429210890312,7.4944956156680504,10.089053172156433,7.896998342338911,8.325215494683038,7.3416300093299105,4.349789870241872,6.438459204423481,8.102815576392821,5.43229133824778,2.4905701304462013,2.873813198359087,6.096978636370538,4.032982416699428,5.604071323668861,2.5582676340557358,4.694323060125386,2.722466024471091,2.3757345385831563,7.995258373404689,7.1078970616646115,7.797726364623276,9.707290101958575,1.0976107966264221,3.1953475983222193,7.672354619892956,3.4208865749755315,6.180903709179382,5.908572758751745,3.372952097911829,7.12525847253797,4.314696525656286,8.915789767665157,5.459103696135399,7.07692245939234,5.79207446192887,4.488000770834068,5.685379551959075,6.442114122419225,5.125981653854716,3.1193561770396756,6.643134661857265,6.657640005207824,6.316507819103109,0,4.053111336459563,5.711219556894335,2.3161457422933562,4.726831217032493,5.8894735425311096,5.981167667105682,6.483976963236257,3.3840498067951597,4.672990993827748,5.239550796613645,6.48767927890162,2.733354340613827,7.754887502163468,5.065227622775619,6.546122758857796,4.556429415449573,3.33771109212829,4.72137265947682,5.533563348214512,7.015582311680932,3.3868109464722167,6.141391916170729,2.097610796626422,6.376255649313362,3.24031432933371,6.5787877956769725,6.052024560482831,6.4571345943180365,6.711081862306696,1.5109619192773793,2.0071955014042038,8.766462652425766,2.584962500721156,7.074676686294496,3.292781749227846,9.20006486151431,7.7525479058471385,4.009884588931844,2.169925001442312,8.76354436429392,6.257010618206024,4.3269687115878925,2.3950627995175777,2.3757345385831563,5.424922088210688,2.250961573533219,2.8011586560936985,3.4892860226258766,5.968551603363945,2.541019153133559,4.888012585402829,6.457791260256042,5.237257770900372,4.161081482277184,3.408711861029429,6.705839705883682,8.251198340973543,5.2671612180222604,7.824386002585445,3.1043366598147357,6.542103245856593,2.2234225499349374,5.316507819103109,3.363171077119244,4.631104282365877,3.9873208659292536,2.140778655782796,7.473705749619415,5.924575003392735,2.871843648509318,6.807870078299782,4.955591791753994,4.214901910160696,3.910732661902913,8.646486708605355,2.2539892662307865,3.816599706534912,6.460578760776026,7.281698250098829,4.901108243014512,2.867896463992655,5.9429835981871015,2.950468414150123,3.8429788317883253,6.045486656359465,4.990047219573794,6.705977901682522,7.860404179081675,2.565597175854225,0,9.146670494470888,6.7014104362328135,3.4019034716079584,4.462706750670158,4.311793717753649,6.649471757181635,2.788685710613534,7.693556651228518,2.8135246892978105,3.343407822297814,3.5360529002402097,3.2675357980687347,6.75127624861109,8.995371450572195,2.6870606883398924,7.605257262939004,5.275752048828529,6.982080909763738,5.305970520984387,2.017921907997262,7.062963556559086,7.2259304111710545,7.0414401300729725,7.778274295338579,2.4646682670034443,2.4620523187964327,3.2141248053528475,4.114367024952,6.111448698487674,2.189033824390017,6.304876050044889,6.0710332444568245,6.157650096861711,4.038260575175349,7.489045125114802,8.475936042446387,0.9411063109464314,2.01435529297707,8.154868729042667,5.955824240806441,2.996388746447621,6.1039172107520665,4.296457407371244,3.657640005207824,3.1259816538547156,8.325890061039784,3.085764553778314,5.309976492370556,3.145677455195635,2.594548549550354,3.631104282365877,5.121015400961366,4.371558862611963,8.492213725013189,2.1043366598147357,5.825022364044702,6.883987308105472,10.959639764364185,2.4620523187964327,5.522306892871389,0,6.593054921669878,3.9354597478052895,8.913068913647477,5.021035479511363,1.7484612330040354,5.982537314399271,7.690137692986635,5.791814071161826,3.578938713093386,4.896271848807375,6.766065051474954,7.801029283808189,6.976821852360685,5.795195490817005,6.883132015657282,4.464668267003444,4.808385050656093,7.161484638641659,2.3645724322958563,5.707082991771706,2.8439838440483265,2,7.723148957047578,1.1634987322828796,7.126807703142035,5.348728154231077,4.4924941516794314,6.174526265266431,5.563768278452033,5.6570683012015754,3.37851162325373,6.43895814789755,3.127633279725874,3.1953475983222193,6.676803353873854,2.6667565918848033,2.3589588258323295,5.587364990936461,2.454175893185802,3.2265085298086795,6.230164577884252,7.884597920990064,3.7687136570304847,3.127633279725874,6.450551443306483,4.084914659767938,4.135863165368679,2.319039815562536,3.097610796626422,5.624978179569036,3.968090752045256,5.105594275795191,5.185470146066942,5.509379148914689,5.266411766036114,3.1342209397606338,4.707082991771706,6.390598905552461,2.8698714061777126,1.4168397419128291,2.3645724322958563,3.5084286525318573,3.9354597478052895,7.11009175192513,1.4222330006830475,6.929672661579345,7.041001987167506,6.332349900373874,3.423578170981797,5.137503523749935,5.3139713710590195,5.999549087340949,3.2433644256936605,4.7114949066500875,3.7059779016825223,6.291677223176022,0.3895668117627256,5.758089934201813,7.001577085389535,7.931505781869729,2.8257856274647914,9.151270288790165,2.440952198029637,5.74604398299834,7.136683577697236,6.353146825498083,10.212070748654236,6.007419783571914,4.997744026059632,2.4982508675278257,8.25176642434399,2.8579809951275723,3.300123724569014,4.640967910449898,2.625270489374693,5.399854673572431,2.2660368939953175,1.516015147003665,6.199672344836364,5.708187236020708,5.402585758232587,5.269033146455237,4.529820946528695,5.212958362728285,5.648753032989572,2.9467308601403097,2.169925001442312,6.088735246190284,2.4356285940520905,8.627898615871748,5.300489844363109,8.091065077930535,7.119252412157343,10.142962207064079,1.7398481026993275,3.729008870337863,4.45022149589718,6.461724991502711,7.904544309292717,5.490890978894291,6.76593249347046,4.502075956045791,2.782408564927373,4.205548911173033,4.139960569545456,6.708325207143167,4.632268215499513,5.6194130105979365,5.541638734436141,3.581351247168777,3.496973580998276,4.862451391325582,4.784503982929566,5.359310316890443,3.41006969175638,2.5897634869849773,2.769771739249448,10.641220865859923,8.202368742775262],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle KNeighborsRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_pred_logkNN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle kNN\n","# réglage des paramètre pour la gridsearch\n","n_neighbors_log = np.linspace(1, 100, dtype=int)\n","param_gridkNN_log = {'kneighborsregressor__n_neighbors': n_neighbors_log}\n","\n","\n","GridkNN_log, \\\n","BestParametreskNN_log, \\\n","ScoreskNN_log, \\\n","TotalGHGEmissions_pred_logkNN_log, \\\n","figkNN_log = reg_modelGrid(model=KNeighborsRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBM_train,\n","                         X_test=BEBM_test,\n","                         y_train=TotalGHGEmissions_train_log,\n","                         y_test=TotalGHGEmissions_test_log,\n","                         y_test_name='TotalGHGEmissions_test_log',\n","                         y_pred_name='TotalGHGEmissions_pred_logkNN',\n","                         score=score,\n","                         param_grid=param_gridkNN_log)\n","\n","print(BestParametreskNN_log)\n","print(ScoreskNN_log)\n","figkNN_log.show()\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[2.1066260872787974,1.7419025561502655,1.6481829656109297,1.6060415186444417,1.583245472733673,1.572171091968285,1.5657118387242899,1.557206047192335,1.5522514248817618,1.549604708066066,1.5445826999654315,1.5442840965465365,1.5436158992799356,1.544452048137493,1.5454281070019629,1.5452708659760006,1.5451222296004028,1.5453573233913676,1.54546016586827,1.5458281579699011,1.5447117226555633,1.5440909240173974,1.543079198497454,1.5439581522327703,1.5455749275972945,1.5463789986319587,1.5465375050893537,1.5462696894050354,1.545662450324375,1.5466235059709976,1.546563059742758,1.5465326289899441,1.546964734718922,1.5470198677914544,1.547475092168877,1.547831162202338,1.5475485467468704,1.5479255213659884,1.548116107055655,1.5482242416151115,1.5479584085225322,1.548659735027704,1.548355360872606,1.5489613133724673,1.5496950879785893,1.5501598540219164,1.550736128368451,1.5509190179953536,1.551667371815465,1.5519810370959526]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[2.1294883993834004,1.771078763007565,1.664865051842282,1.623609156365236,1.5985302786172826,1.5876120915527259,1.5818878231077,1.5723937491878806,1.5650714496848075,1.56286411747819,1.5578190047790277,1.5587163696428286,1.5590137148193945,1.5593053515207647,1.5607816178834173,1.5615096824646488,1.5624892877577679,1.561408737385698,1.562323779037493,1.5615537732313518,1.5603499997233554,1.5594099537585155,1.5572917396485428,1.5578401587415978,1.5598509125292555,1.5615516128101161,1.5620217052395788,1.5623395802488056,1.5610589968500177,1.5622559937633067,1.562508548315138,1.5626794246257987,1.5635602741384167,1.5633264534143025,1.5640266738081383,1.5643995347851123,1.5637400887833988,1.5637763368060356,1.5634552411006197,1.56363744091322,1.5633229861078852,1.5644056112719738,1.5646845107618388,1.5656342103915502,1.5666262022964987,1.567568410641032,1.5681718974792405,1.5687615837431006,1.5692343570200509,1.5696482433456815]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[2.0837637751741944,1.7127263492929659,1.6315008793795773,1.5884738809236474,1.5679606668500636,1.556730092383844,1.5495358543408797,1.5420183451967895,1.5394314000787162,1.5363452986539419,1.5313463951518353,1.5298518234502445,1.5282180837404766,1.5295987447542214,1.5300745961205084,1.5290320494873524,1.5277551714430377,1.5293059093970371,1.528596552699047,1.5301025427084505,1.5290734455877713,1.5287718942762794,1.528866657346365,1.530076145723943,1.5312989426653336,1.5312063844538013,1.5310533049391286,1.5301997985612652,1.5302659037987323,1.5309910181786885,1.530617571170378,1.5303858333540896,1.5303691952994272,1.5307132821686062,1.5309235105296155,1.5312627896195636,1.531357004710342,1.5320747059259412,1.5327769730106904,1.532811042317003,1.5325938309371792,1.5329138587834343,1.532026210983373,1.5322884163533845,1.53276397366068,1.5327512974028008,1.5333003592576617,1.5330764522476066,1.534100386610879,1.5343138308462236]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[2.140639940178953,1.7702105409171793,1.6376124133854486,1.5898523387820354,1.5705195260206195,1.5474999076344589,1.5394822895189484,1.5300032439643627,1.5297524369068323,1.528207573786708,1.5229255864819466,1.5236880418077012,1.5219945509518424,1.5216587349537989,1.520747658954288,1.5192031107695934,1.5160799233609397,1.5181168254823147,1.5165795142598752,1.5195709693729562,1.5184140791584333,1.517956550840978,1.5211793407644396,1.522094822162551,1.524276411465606,1.524368372072073,1.5246601543222447,1.5227144640884454,1.5228203297149685,1.5229447628751764,1.5215736997425613,1.521268818688881,1.5204591467573347,1.521917760989194,1.522763194734034,1.5228059980705615,1.5233697152268704,1.5235221042689615,1.5240342850031199,1.5241212956743995,1.5246510316416573,1.5247997955098795,1.5230710679637558,1.522409621833077,1.522262858516726,1.52241647538312,1.523196323765539,1.522918501514821,1.523929689321724,1.5243915959137075],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[2.083379115110446,1.7297234673508168,1.6469448202297223,1.6136744244192733,1.5900358119876488,1.5767502026280549,1.5710464272372189,1.560786991682724,1.5552416554151103,1.5540552606037177,1.55252644567492,1.549841855711025,1.5459756403637548,1.5493383321091654,1.5526174423779668,1.549100725863196,1.5482625837481379,1.5462480202869857,1.5488278086236662,1.5468630015966784,1.5449364038223437,1.5443124015265752,1.541228254928659,1.5418429930526125,1.541910454761183,1.5441440967463493,1.5433890725080455,1.5431982268117983,1.542439592435344,1.5435619533629203,1.54468910207057,1.5446825926925642,1.5459692901897397,1.5459558903680881,1.5464506554133504,1.5482467952238295,1.5505022795183412,1.5517772162481087,1.552844738420965,1.554161484726495,1.5547221459158913,1.5549975338044493,1.5549529899251118,1.5565510665203353,1.5567082652907007,1.5568923335217342,1.5578371630825143,1.5583311577140684,1.5598507365913825,1.5577608135394803],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[2.081014090763973,1.7729957540999348,1.674661938949519,1.6171496096215086,1.594754975047931,1.5836518876739911,1.579054658758011,1.5697643253013502,1.5634673423551964,1.560621237947535,1.5560008396024336,1.5545618877323342,1.5594826546977136,1.5608120532364425,1.5611676025009324,1.5616623555749025,1.5606890812305707,1.561542720209096,1.5609228181629469,1.5616075519504211,1.5613147746193343,1.560013757472958,1.5612177421957687,1.5628108692319176,1.5662604347948874,1.5687386131607965,1.5680545571103803,1.5693124136181118,1.5677690291723265,1.568929911730469,1.568161219356416,1.5679754866088014,1.568490404640835,1.5684819852829974,1.56977175316308,1.5699418832821148,1.5696984971863017,1.5686260658200535,1.5675629956293267,1.5672857855589692,1.56737649630937,1.5691904825334886,1.5683989479458447,1.5693076135987702,1.5705271296657177,1.5729318708525273,1.5729990852658329,1.5738486746542915,1.5736922647472813,1.5740351613761256],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[2.1227059996435536,1.6935444589467263,1.6255271570233034,1.5813550024503547,1.5603082742887835,1.5624481703383912,1.555607761233325,1.5532088282349616,1.5478410686307276,1.541113988093571,1.535462470628712,1.5313443914429161,1.5302319443134516,1.5331664184046045,1.534715675363195,1.535183900595332,1.536891605536235,1.5396743788966325,1.5385087891107823,1.5393159767510918,1.53918361110836,1.5394197368688036,1.5363407900176933,1.5390207145452692,1.5401332679276585,1.5383819349116106,1.5373116147971184,1.5383268137961403,1.5387933834996836,1.5403430405733478,1.5401686859336996,1.5397235369039777,1.5401464077528417,1.5387278453155722,1.5379035568181936,1.5378085388481002,1.5364455909134165,1.5374262405500625,1.5381706422131198,1.5375810530119187,1.5362699429485824,1.5370519363122515,1.53654775957505,1.53796377858312,1.5395502319280328,1.539788447980034,1.5394832995926688,1.5391581213536822,1.5399797979248508,1.5400339706483912],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[2.1053912906970624,1.74303855943667,1.6561684984666545,1.6281762179490367,1.6006087763233823,1.590505291566529,1.5833680568739457,1.5722668467782768,1.5649546211009426,1.5640254798987983,1.5559981574391448,1.5619843060387064,1.5603947060729162,1.5572847019834533,1.5578921558134327,1.56120423707698,1.563687954126131,1.5612046720818078,1.5624618991840793,1.5617832901783584,1.559709744569346,1.5587521733776724,1.5554298645807085,1.554021362171501,1.5552940690371373,1.5562619762689636,1.55927212670898,1.5577965287106816,1.556489916799552,1.5573378613130744,1.5582225916105434,1.5590127100554965,1.5597584242538591,1.5600158570014202,1.5604863007157264,1.560352595587084,1.557726650889423,1.5582759799427566,1.5579678740117446,1.557971589103775,1.556772425797161,1.5572589269784516,1.558806038953267,1.5585744863270352,1.5594269544917705,1.5587701423721654,1.5601647701357009,1.5603386347399035,1.5608843704920867,1.5636836440020578],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle kNN en fonction de n neighbors"},"xaxis":{"title":{"text":"n neighbors"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE kNN pour les paramètres de GridSearchCV\n","FigRMSEGRidkNN_log = visuRMSEGrid(KNeighborsRegressor(), 'kNN',\n","                                  n_neighbors_log, 'n neighbors', GridkNN_log)\n","FigRMSEGRidkNN_log.show()\n","if write_data is True:\n","    FigRMSEGRidkNN_log.write_image('./Figures/EmissionsGraphRMSEkNN_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.2.5 Modèle RandomForestRegressor"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                             paramètre RandomForestRegressor()\n","0  randomforestregressor__n_estimators                     464\n","1  randomforestregressor__max_features                    auto\n","      RandomForestRegressor()\n","R²                   0.685788\n","RMSE                 1.179566\n","MAE                  0.861599\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_pred_log_logRF=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4.461225755000088,4.954490890951887,5.016001639614646,4.665945075252624,3.1881141592789333,3.056399457377179,6.950425436330877,5.550105436366905,4.132288711281424,5.872481267346308,6.541726952777725,5.238278855763902,5.136021262590358,4.493836017782308,7.138728550297497,9.873654995687952,7.511449515490891,5.295988679984695,6.2002794831435555,7.678231693241143,2.987319510772994,5.558714292406418,8.206749152548523,4.701868049414976,4.006656910268226,4.769585661234698,2.9976571009405157,5.717144780547345,7.729557468135035,4.185229726266013,5.205658388099196,4.7331128561675015,5.623457790101758,5.597659651315041,4.703123395081906,2.2649439094371084,3.8249634011703892,5.852051667941578,4.440815586481471,5.621171930443536,3.196819575243048,4.559963346083881,4.677028756342599,5.227949168762381,2.5832088635543444,5.53903170591357,3.0596908135697096,4.806625730771767,5.821698186349572,4.6632296060217655,7.36879570580841,3.2944013236183918,4.6175263899888925,5.717460481664914,6.054741860769216,6.258471829202404,7.1683552465560485,3.6347616392004105,4.450417059452659,7.5569984911594945,5.5062797665397625,5.782946943146519,5.8781059013312715,5.199193048489602,4.214600248665968,5.009095132844979,4.715546284941433,6.006706475538931,2.9948005714308454,5.480611775386692,3.9327852556594567,6.232223241627405,2.8125024931417206,4.299395620115605,3.721000318556621,3.4114237113625876,3.049127509168927,4.7659337205424475,3.861735166723065,8.926463972451257,6.734094760476578,5.212412217093768,4.983954111976538,3.2706405165721892,6.673872404364581,5.260349441909211,3.5443156538421126,4.436006087734641,4.333537322663945,4.709359219151838,6.411008781984746,3.4333954589737994,3.9072939446876593,5.401815401590257,2.772937798339211,9.406746183281955,4.743607636501728,6.191589850770416,5.646317199240495,5.052818642587747,3.495538043803368,2.3360733331356145,3.023413318742638,3.671570606205515,6.242063089728393,5.885222611590442,6.959291190821684,4.885260426234779,5.107244015522878,3.3758919203482693,6.835615105841119,5.1659368810893875,5.135373546846921,6.669076100434587,4.497765388843977,2.9722884067619386,4.420221285683994,10.160492598157411,4.3921190444924,3.2734240562490773,4.693300445362633,4.919861650476066,6.549753321123384,3.5683072573862566,7.195192806734196,5.347670917080987,3.5347984030942885,4.762210394160878,4.912434874813554,5.07180267353133,4.087550754297233,4.595917965181553,3.5037388487869454,5.763072256616818,2.773739728376425,5.333179309219393,4.404622331709882,3.383452555150393,8.19314833543548,4.835521481804788,4.4735248463778055,3.859802452860109,4.691260558149342,7.301841126548186,3.032398939643562,5.165448397832542,7.53529887180339,4.757586051655862,2.645894425900614,4.796937176763859,4.1526795967000565,4.943680615881528,5.214117530564994,4.179673945617288,3.976910226498313,5.513415543742962,4.9407260307100955,4.648511002625549,4.589851855939297,4.478131273473514,6.003191076118244,6.049417016342914,5.31387075051773,3.946948612187315,4.284516439098071,3.1866937983385104,5.491319654353278,4.100009727783687,5.331367419349459,6.175261553084188,5.4726842985661195,4.654319303051178,5.851602944947394,3.393906949070405,10.515163090030539,6.821442325574511,5.287644219885555,5.746581387676414,7.286275060222861,9.109493742703414,9.287941417471828,7.172632729208276,4.726324691219455,4.421099976677507,5.32457316815626,6.064480975394137,6.317847777455786,5.164708350962616,2.75740087637041,4.389655625296035,3.9536995546676836,3.4030795902471254,6.370143968659186,5.07253532064505,2.668608979000482,5.423871757793234,4.7890506275323865,5.320574439693284,9.225790706744759,3.1142702748084754,4.571143108271419,3.705760464855145,4.907633647005969,3.23179390463478,5.797674666971457,3.632884471705773,7.083587376464881,8.632417613532716,5.667714330984504,2.3030856384943132,5.091447858717559,6.51015023413601,8.020092499826776,4.103138048877526,6.275748832088158,2.6204218449967933,5.733039843772859,4.461176725688738,5.289729435911976,3.6387780334520596,4.002390179716203,3.521958325693221,6.32039881491653,4.472509418647185,2.41809806641732,3.2438724264550256,5.075702858687191,5.88448934413271,6.275972627050594,5.0356427580562375,4.93297192181787,4.280943348429263,6.568689590388439,5.146704590832432,6.793189803798893,4.967367420476996,5.834895560709641,5.053706256829199,6.571130729884212,2.316006593964312,6.191550540593306,7.075545054207461,4.947914864299182,5.209879030041946,2.945036790614826,7.082355898138016,4.213182852585217,3.475539988037596,3.128565402982674,4.86271669090079,2.7605818010598004,7.709094598954501,5.40909800031535,7.015211083423696,2.866386505207942,5.076067454582172,4.971556345159647,11.076374499577666,5.372856551647611,4.343383029436061,8.264902073012594,5.917789387843321,4.281890442525099,5.338224638090698,4.96862097795095,4.6721553305825685,5.861105882987535,3.292829730758449,4.094000046730118,6.680225048541444,6.599383586162235,5.05713956741454,6.466508705358969,4.3354563361968195,7.431820503540182,5.844209209700647,4.657587781679951,7.268592513102652,5.41433416693804,5.250797234356058,5.2090875502577125,3.3110001961113174,3.692243265721811,5.532672505277076,6.051315813908166,4.802564982526386,6.978873014974565,5.875297331454999,5.830078416503247,3.9964150429697596,3.8333014978931113,2.8635903858423033,4.4294227972716405,5.6159798284373625,2.1282907100714907,3.120202017001347,3.597055958583966,3.5780506700954136,4.90832030387592,4.660594444748638,5.917015740153838,5.814357038886204,6.981673359391443,7.885910225974671,5.268020797912021,5.113814706259615,3.7937027916131014,5.264412840285475,4.678798874688893,4.007710346546435,7.285176462587559,4.293002010225598,3.6375985212626842,4.801776726235325,3.515709592323499,7.3109530069855655,6.910540725815045,3.671570606205515,3.987760856718805,3.7791157541472478,7.400461279028568,2.4413753478001574,5.1072485795439215,7.727399304206455,3.5666740934879444,5.54851246261423,3.9632764041734547,3.2259914177103846,6.617559448675432,6.198119356207314,4.746020612161552,7.3384980637665755,3.1349574960222895,5.413915727148706,4.7069548038775695,4.904346553979458,6.066740333272793,5.484298922969438,4.0590218945953875,5.2405612592854975,5.941724327216673,5.2114731188383345,6.093890974806234,7.514919780670035,4.981684993729079,6.009218940406127,3.9906335882687958,6.398219688117191,2.9543021896445945,7.60763794537557,2.2031624419727573,4.490581272178317,5.617013720981904,5.463634538873555,6.339855866958265,5.007122823392395,5.582505498785246,5.465077674610313,6.095617213437795,6.120117663404719,4.417077210754147,3.8000047147680225,2.255401669621538,7.109719147294896,2.865534083592611,3.587825491745831,6.418135605887244,5.670890781349116,4.9837997108500955,4.876087503678679,7.407452517424615,3.7216498820713797,6.327546166401279,7.0074712472936955,8.231981802631722,2.4068090151675485,6.070513696882793,7.654867743642514,2.488489168973027,3.7446473960926934,2.723327672392257,7.87388077876359,8.2798472956157,2.7316910640815455,7.291105370120063,2.609119908156131,6.540845826986299,4.655590775745018,4.8468201526728665,2.488259986349902,5.446970814219454,8.008836091810927,2.609345395325555,4.270699850361271,4.19961870369128,5.444481341893302,4.241025302248759,5.5404515183611105,6.544195320592489,8.114663398731317,6.358149098440511,7.010348897467842,6.407400198656593,5.9876803173612725,4.076575522072068,2.3758999072014757,5.624587352853892,2.916472650056774,5.1064703061059875,4.068392681321279,6.4381948491693475,4.640498948808677,5.50476325281306,3.8438400897386282,6.67699854867321,5.405250164426945,5.858135475610509,5.985190539845793,7.00902428279911,6.225487640351496,8.741117747260247,5.9953141791801,7.152803785847929,4.536469283821646,6.9238831311294895,7.412356760023671,6.970894961390999,5.4063169381561345,5.028033025082626,5.723638125310957,4.453604927496531,4.3943578662336815,2.79890562467032,5.208103515194516,3.7982740079719814,4.827797183398605,4.713225761199135,4.618991181663804,7.033582723158343,3.1719311362965343,6.449375520705581,5.851364285388702,2.2494332150025604,2.971735639714149,6.1466657297957505,2.9005113839271144,6.390788579020057,4.408313556192121,3.5829466581664247,6.97050898942165,8.93201616365418,2.5832088635543444,7.593520481675749,3.8083526931095606,2.7449331971583186,4.78125692383992,3.5415447508147584,4.9677428387700715,3.7309675253593855,5.3429746839331855,6.061369021537312,2.783324424060303,7.431082388264076,3.5580689682163036,3.7659566063690035,5.385500699514448,3.9366377691820644,3.2466876405840526,3.7243108794081357,7.122551374600468,3.785778007036603,4.347806241647831,4.724481759514129,7.971936673012061,5.530142915440066,4.130631967171592,7.751025009477769,4.168102163630483,3.6877403528577073,3.2948790123369642,3.2315046742019002,3.5409229634190575,5.307136441163654,2.995116571923958,5.328386349813565,5.975391194814886,6.196019062110292,6.730190849521758,6.064966365587799,4.10932252882137,4.661523816129952,5.42290641094858,4.320194630627365,2.80049752939911,6.671612484690015,3.9740585661275376,6.994480693779116,4.728594341669254,5.590441024229242,2.651724083394248,5.045875157236804,7.21234863236317,3.6415514627811905,6.106432265467327,5.614372305760679,4.206207640949431,5.636123345915989,5.166962239428308,3.447381000159965,2.503128805615363,4.277341109851555,5.594552194666083,6.2791153605658465,5.936537029178207,4.755368105528651,4.19610437638806,4.246112866795636,4.119244151365026,6.696074705121478,2.566979432710675,3.6956993945884364,3.050307485320596,7.677475927855909,5.693724564249429,5.126644620544116,3.92811956605675,3.8287962590559252,7.213281348568819,7.078212207064718,6.5434440537983285,2.6430822429620315,2.56749112309332,6.080736676878432,5.83846814476448,5.449246013942431,4.772185229490682,8.380314330827185,7.488798165446487,4.9353280385732035,3.2139007846742413,5.95091179089937,5.466338469929251,11.074252358850936,8.711621786069175,4.084752145742789,8.275202281021855,5.339398238509114,6.543680266590479,4.966106785452184,4.031517409866468,6.0503701526674485,4.293095388208057,5.3601168314934915,5.897821448262043,5.123786980835227,3.478760291443744,3.272801866433994,6.204460588090243,5.212850544467906,4.699575344294462,5.848416182095654,5.828493190376564,5.637423443797618,3.8720062847939802,5.986323992132688,3.8819431466415217,7.756643575652619,4.046148066697901,6.267794307330233,4.432407265364419,5.538783141149879,3.101948639221904,4.725087417852602,3.2964417679900917,3.135286578147758,5.418350439454862,4.0143819657320075,4.453498968147988,5.93230686592559,6.3039301010807405,2.4819357290646193,4.254191033755546,4.0807771171920635,6.203628858540916,6.393083523905914,5.607377930952275,6.735426651622876,5.353494414130296,6.67857251114196,7.343673163138002,3.25626131834502,5.275447601514757,7.719989084128928,4.707490023081998,3.552754424821899,5.464747981541314,5.4629129197774,3.110263080265044,2.8184858632851095,4.799400728920495,4.3177140339509235,2.82060928945425,6.239983862965931,3.7911113241959113,5.298506968312853,4.622056221051254,2.978409632850194,5.131883729577795,4.74739452714652,10.124597406503515,4.960703762352239,3.2420902132791194,6.742481016325316,5.008134111695089,7.255193261873191,6.0659013353159725,4.0602857827814836,4.627904393984714,5.170835868139402,5.483598573866357,2.6180448660993125,7.86192783665668,3.40315070521061,5.646131863343227,5.177903537041231,3.4516863864746163,3.9068347679197846,6.346257389312579,5.543218240907925,3.130183320815743,4.038200347425846,2.6686740214838385,6.989563496298597,4.6415672702997695,7.56327302020501,5.054223994300786,6.698061986185101,7.307336788135589,2.118793072983972,6.556995109308727,3.844457457034711,4.6668797197203995,7.392023440751933,3.173317992438921,4.30944731180559,4.538486335095073,7.897265520610151,6.015419597268538,5.154759707899891,7.602219776012678,6.773309837195529,4.365181525131109,3.3704079384179977,4.935791272234755,8.850725480422033,4.772926091870607,4.225629445157393,4.104332280219587,3.736377501128775,3.1267188738579685,5.152008676390169,4.419242026505105,3.88162661288077,3.323900696382852,6.080251829807071,6.244030525481067,6.675865400837656,4.264129398897208,4.806524935121685,3.470454432521371,6.942505950817102,2.747252483968771,5.7449938352310035,5.927759413816888,4.875296805886338,8.200640919352294,8.027506585392784,5.592543041332863,4.6045280627437135,3.297989832422805,4.332168524514283,4.046645022380343,5.081583120127065,6.3977087113563025,2.945893944541393,4.784334992609227,4.838988220085721,2.822473018442406,4.773368446206859,3.5492937378109235,6.9298539637754475,3.528713311008788,2.5658431489275717,2.285008973545474,5.8006306645250865,3.3184070212175274,4.646222025493199,5.022486104920213,6.467349409364345,4.203929563692156,5.406324310372787,3.0698893883593543,7.576818542166228,10.303132570680626,4.743515928993973,6.3699272039609784,4.831490631061908,4.336718672886408,5.594554232168218,4.473510143331611,6.625565562411933,7.677704780917039,5.345447491234654,5.336339135389824,5.842156786431078,3.4416085918281456,5.767613939397271,3.0054771744046667,4.855710204258107,2.3379711030634134,4.150320562509877,3.1095216881328924,4.705497191556304,4.803801462715968,2.977879797862827,4.5437219512344775,5.44643197757836,4.133612229040733,2.2299938250349025,4.381441491400121,3.6499220511617607,7.378410933590666,9.058775245721293,5.165670279685385,4.232947497697167,5.747494883897324,3.4115588592609596,7.736056288893933,4.0778235462220165,6.4162140179480085,5.623846215532241,6.093455853193906,3.6905640324918725,5.143595156391627,8.678320564780517,3.4748069978699725,5.303796074744515,4.701892652109297,5.403515373697737,7.485382644562935,5.151324521261933,3.8472219462456105,3.6983375859369625,6.369059743289145,4.843391868881782,3.7650131657841097,4.884007766353235,4.404873611217257,3.356135502847857,2.1441482472995643,5.40793750477524,7.616893210730314,6.422484889110607,8.308952833378969,5.162488409143341,6.603204746283181,4.421628804767922,5.148640766842892,6.291825525950362,5.5887914789793,3.4231459638287554,4.515339809265974,9.489542983995962,2.5215747492184923,5.208305085568117,6.172911200557043,3.2565388892048457,5.551095087755977,7.035829860306719,5.2978958580484266,2.8046152695175772,4.927437848670564,4.544232492494532,6.115892890244309,4.038077932190894,7.578867931519813,4.724143580072822,4.870078919974779,5.615191236327701,6.361394821089497,3.3489486861861653,7.1845684735684765,9.0286289197954,4.900641923652906,6.08244014620625,7.4287452795698465,6.71921703890103,3.0223388066121655,2.594112940631922,8.009533802616051,3.9951499546267244,4.7018559443977646,3.023615097128769,9.445687793850098,4.684717083243436,5.005052626392277,7.4330286205099565,2.988490360657162,3.371721034727252,4.073716043372147,2.901626788015682,3.1295548558930015,2.7275106756993512,5.926518948249574,4.923269821484695,4.6930597932142115,6.417438230229342,5.454972053474944,3.291372803124916,6.310257865263442,4.811079473291749,5.6032936852707715,3.3278746716128262,4.212125901741112,3.9706897819804405,5.75113225413995,3.4889931631968367,3.614271738399525,2.70328160526096,6.0860647705660895,8.942188394535073,4.159591678100988,3.159469680190077,2.8498010044087985,6.135977618008789,5.266133857482863,4.167473693765644,8.439622793541876,3.4931904197489887,6.046622435212064,6.60590948195113,4.397693401742293,3.2425800456865437,2.6970377181307104,6.434931003329845,5.455227831515702,6.6010545183775635,5.207747277025129,4.38544767022127,5.3247938706890015,4.63795193338587,5.81906791352615,3.4180756833403985,6.018383757241515,4.171718413476599,6.27770352061356,4.268422917671909,9.857212075857811,2.766623635517852,4.620791190436884,5.12824302804451,4.5902529389784394,4.016766580701236,3.671925546807996,4.056282078981169,3.0054771744046667,4.0139863035273144,3.5071005304102782,4.387077906832371,4.097923957335342,4.798348338331828,7.561987411319478,5.008754326933304,3.640753439234754,5.921333269679566,7.4231214819685345,5.940536415115219,3.8533068708795417,4.212103172197657,6.514092870547545,3.865598715930913,5.3598389090432645,8.113509451902127,6.374164672232288,3.93124032593197,8.216980432120097,3.382011659247629,3.610468378368945,5.710474944663013,4.516568602746357,4.906024508477778,8.344660225463967,3.6027298863399304,4.5996195815142835,5.628030245589649,4.093000305067883,6.8915667899402715,8.747014169498797,6.449558403467949,3.454559223928558,8.634973077527452,6.259052826157201,4.941555084430129,4.762158645938638,2.554961368075725,8.05981966316338,5.730565503408662,8.278806166467955,4.978191306685888,4.033257004591464,6.945024173842006,3.6027808970937767,6.863029827631359,3.5428289031862317,4.208897295919847,7.448267290880549,8.118359117020379,2.9223434563066095,3.794116173319114,5.80496875648172,6.896690829958144,6.333432602403648,9.055951375837765,5.552726608718253,6.844386601947756,4.797943689540034,5.7091389421875816,6.313980864159302,3.296348455145923,4.581709977339387,3.759436494084867,5.648739465855127,4.288272941903928,6.02469847486618,7.982346792543472,5.477191336293234,5.366412948913924,4.431867014339076,4.143453327198327,6.672386316690105,4.088891735107956,3.7527338026363433,5.613457152855205,4.2575441559337,4.215757130357277,6.023477091211422,5.661140684845124,3.6484630857584284,5.317588808760446,3.6367781926702882,7.049255476349682,2.8096661509479373,4.2670562242261125,5.8878950581451726,6.060756627489438,7.0107774526131905,9.814663592977503,5.492400701539416,5.635545530654244,4.4638533540662815,7.79530508166033,2.831762728636961,3.9521091583074184,6.339568656526193,4.6441155130910206,3.3486621335164846,3.9932411697966126,6.719557706598698,3.5018456732027867,5.548665039996621,2.824868463601963,3.527745240126096,4.649586034572041,6.908631047849491,7.656679429624762,6.17509601131539,9.303737980093613,5.563155060803971,7.690892769520328,6.1337289805941095,4.49853806003093,5.342502918325942,5.691322954972424,4.729372147558104,3.155883772055515,4.943212303789351,4.483964207705639,4.755842366816837,4.400876638348179,3.3861523797182977,4.861579269261891,4.079552114511421,4.966303238133738,6.7312476755903115,6.373801692366615,7.423073266654549,9.666746114846875,2.6329174856783664,3.8107208781854145,7.383405442293847,4.104338783761084,5.978173354561138,5.555673518727159,3.7210711478768657,5.96036457726983,5.217424376837706,5.9685822442940255,4.544351168082711,6.817975540691756,5.996206411947533,5.486051916293472,6.259343070820093,5.170828864992739,4.02123136253659,4.797473120616991,6.785025691330336,6.283552461854283,4.570751432042379,3.125623017819841,5.414181040986168,5.053644382018766,4.850168109491593,4.671583569637922,6.738772368082326,5.4323190382024995,4.9675087127228155,4.71440139219199,4.734059357278571,5.410117084021212,6.83162094970455,3.092373965806967,6.113841849505602,5.064120696824533,6.437493934732352,4.334644911805437,3.1387013178740464,4.788019425590165,5.441170103196302,3.9325540460905732,3.5329277971993793,6.706882320705309,3.7673987349843974,5.59937638053697,3.530997465366019,7.770031084705085,5.708647250447433,3.174756614601962,5.7658501031561835,2.3408810102623305,2.5304608276748946,8.545992399388947,4.808182175181033,6.549349853913181,4.245411962671624,6.962328052581588,5.815123495556902,3.1158890741895613,2.29568986063605,8.527353965236976,5.7118754412461055,5.116825426543587,4.180445372188579,2.9253802167558516,5.220254353506649,4.70301221059442,3.7709123838844003,4.269344575681142,5.298135786616335,3.5132829750742887,5.004896006055297,5.584200879708009,5.162241270898121,4.168675107329239,3.9695149215275096,6.1225291579245695,7.041555333147559,5.055397575605277,7.9290540307466335,4.591011309206919,3.236133528029475,2.2488592227820097,5.487969319174038,3.529773015488061,4.1467909569270915,5.03821838900692,2.334998464147022,7.07363660704494,5.281729563871751,3.779586759376812,5.661677216420762,6.548392671742487,3.95184233061147,5.626807264945834,7.895317113896053,2.2652706184331928,4.4277393469161765,6.201336025073973,6.270541720806063,6.733086763392956,4.426609679980606,6.263491981787492,3.9608578772687677,3.953691169601258,6.446516516424337,4.709547805128315,6.241438447585994,5.303726067822972,4.333837299228657,4.753133551087409,8.248592763860863,2.472637019000505,4.44017596431441,4.905445208259147,4.422590782989309,6.4972834468494725,3.392627303361721,7.206849537237433,3.0782581728824376,3.5712299078679277,4.719079963816741,3.614758798480067,5.9732140312157185,8.848572032403144,3.9993387053267875,6.607075282574783,5.472712782610926,5.945372711900388,5.362106634166031,2.700538306720783,7.373566646278073,5.47676381930944,7.4033526303296755,7.612802449686724,2.661592692594947,3.165874663778437,3.941933452885457,4.530990345814664,4.481649120789199,2.348381944883694,6.381306475837139,6.1731283407613375,5.72169662213224,2.825525912448416,6.375955286367994,7.98898641130158,2.0940773168769335,2.4473362210560063,7.3552804967302565,4.179208806165236,2.6721245501551327,6.80509816703101,3.795425591789967,3.59890330550803,3.883055276833977,8.3782088871105,4.069592343449001,2.756055812335739,4.109343070199337,2.9405333355486034,4.842881059749561,5.010246646009519,3.8302458747243238,8.292122500855111,3.0833053666058174,5.533295925184539,6.654049299797944,7.809973958136737,2.3253989887363335,6.610530737401053,6.097629458709598,6.90274498215884,4.128253629767045,6.240610310371115,4.222859191492798,4.229675540100805,4.174930722438449,7.162220467123374,5.9528425181479125,4.857863052202067,5.735899362254393,6.509698815841197,7.2063584963142935,6.704541898240708,5.866070621250177,7.077567305812182,5.650665568713505,5.127038465601441,7.899134455675397,3.0920980210682485,5.2585615322918295,3.734720982945395,5.731721247602821,5.77650118182795,2.511345464799229,6.37133522855356,5.713718447978005,6.146705810138181,5.949510699165329,6.277129189181051,4.601529757444919,5.640889035391446,5.93012546380726,2.9415997625405006,2.752934600809102,5.848320383737016,3.9351995079447906,2.8796961688336635,3.999381394295469,3.910851797393559,4.268466726140135,6.808991535620919,8.279411709787489,3.8062338464627663,4.154204030561108,5.691093460953341,4.32080499119834,4.565723546973185,3.034804412054557,3.267009235172632,5.8273976293983205,4.602044203007664,5.733807236079705,3.964222865509841,5.731244733349204,5.576303157195967,3.927315995846692,3.9317599610630904,6.126206616349203,3.140862256639139,3.738105385316394,3.7788245422058164,4.911095738992587,3.916371433407548,6.612348897378825,2.4732402323931724,5.893095785663512,6.16618402858736,5.025458242880965,4.624502544002461,5.663585533513188,5.00335489647798,5.556090923615023,3.54893597623788,4.572569092515496,4.885245959715809,4.988003541661815,2.7807528541144304,5.898832406172642,6.380502481091458,5.864582238383122,4.387783254047904,8.869020541969984,3.304485688655508,4.718338232197225,7.775612934761869,6.304643358572408,7.890769766806289,6.234454796118761,5.421181141334255,2.7757265559735784,5.33750004813449,2.712697178399056,3.6276459920483872,4.864666509401169,3.834322424321268,5.027516131122876,3.8848417449332406,2.3438301125281575,4.599212018335023,5.903067016242134,4.45380135685544,5.111787310451679,5.455517109066147,6.197339397331642,5.620967334126763,3.3940560112172395,3.0116215696252535,5.498068875037998,2.6707216253338713,8.089011814992066,6.205691847931916,5.262759265594556,6.178701436476153,9.910242225221566,3.194053229105563,4.094582332086031,4.8420954465783845,6.242505413139413,7.85331864109678,5.198184949695807,6.363441859041896,3.9011776662415043,3.4511310494786467,4.26157312177963,4.687572896100983,6.559457293577215,4.4887142644919305,4.548690715182181,4.927941736735304,4.5786621318925524,5.199572544209215,6.575027743032522,4.250447264686293,5.617684276746297,3.523308568978459,2.5207340733987023,3.2093545902450473,11.044794976778725,6.894212808658047],"xaxis":"x","y":[6.235152624217932,4.478324711485028,6.139142019145689,4.729552770453971,2.689299160535892,2.4698859762744636,7.592232766722112,6.075532631167357,3.9020735793107426,6.768713657030485,8.05642095591347,6.192983169973992,4.934044647112246,4.088311235888661,7.071462362556624,8.345405246717796,8.140829770773001,5.7554217347342425,5.091276694365222,8.047178455318921,2.475084882948783,5.260778431893426,7.873013392989595,4.4495613746132365,3.4369613378336026,5.575917361118149,2.821710215034674,5.650190123496775,8.870210577263595,2.2868811477881614,5.54936113292434,3.682573297347578,10.919861451442168,7.606886339275915,3.4276061727818994,2.1009776477248208,3.548436624696042,6.819540460505722,4.475084882948782,6.762348815644129,3.929790997718597,4.173127433480656,2.7676547982373463,4.804260115634738,1.9335726382610239,4.846493315964563,3.10936055940423,5.617651119427331,5.739578112048051,5.089159131911238,5.6806056662025926,2.8579809951275723,2.1176950426697547,5.465648025982281,6.027463723408577,6.299391206126829,6.422064766172812,2.4356285940520905,4.153805336079036,8.148324042480084,6.387500406480984,5.944155663440363,6.496494306853713,5.703211467391146,3.192194165283345,5.185866545311334,4.718087583960517,6.035403969942067,1.3785116232537298,6.335211681705785,7.330737650206952,5.452858964713811,2.726831217032493,3.8369340113210915,2.4672794804599825,2.9616233282869446,2.0531113364595623,6.248496885959384,2.817623257511431,8.971342367308225,6.606294160697713,5.901832305785489,5.339137384919585,2.582556003014061,4.779259720372376,6.044831233447812,2.7949356628035362,2.8439838440483265,6.690277401023435,4.017921907997263,6.596487956856693,2.84197311892718,2.3074285251922473,5.976821852360685,5.10936055940423,10.16679075071818,5.2787282129389395,6.339493737901733,7.125981653854716,5.148934104526339,2.321928094887362,2.1210154009613658,2.430285272977781,2.5310694927259543,6.72805655144938,6.989025440631378,5.844988156682612,4.749534267669262,9.159820892013935,4.272769732436511,8.601733652922286,4.124328135002202,3.270528942380718,3.894332742277694,4.943921326553485,1.0565835283663676,3.7322691995014496,10.679004987548526,6.500961352460057,5.787902559391432,3.7782085763980877,4.6661884898727815,7.593353770980297,3.077242998932461,7.602439063741618,5.537296067090842,4.203984165855989,5.299757511839318,5.94251450533924,6.951051533288609,3.542258049766918,3.507160349117523,4.71259578044703,4.6194130105979365,2.550900664647523,6.872951851130105,4.712045448553693,2.867896463992655,8.377687734026882,3.678071905112638,4.0232553523003025,3.1538053360790355,4.635173946674924,7.127117349726852,2.4854268271702415,5.592756010441026,8.25559514727367,4.634593268445757,2.5582676340557358,4.990501111363251,3.727920454563199,3.8308637567517576,5.924337464075348,4.187451054027326,3.940166750482817,4.926473911256073,3.5434958834257717,4.708187236020708,5.568640195419223,4.831370587967541,6.12969515665378,5.751410160064371,6.279471295644468,2.523561956057013,4.552131108253784,2.6016965164809576,3.270528942380718,2.805292455600712,5.533874777258546,5.741197298571427,5.787641414483327,3.565597175854225,4.40735275114004,2.137503523749935,11.52102619171175,7.417936880550136,4.96624587322494,6.385085853547684,7.846744023244868,9.156361220297567,11.010164414182617,7.196430000954568,4.008988783227255,3.2464080872463845,5.763145958149476,7.976535484023556,4.4121040446775694,6.918624755574591,1.9745293124838819,4.546585829008101,0.22650852980867975,4.1715271060388135,6.920055054806967,1.3504972470841332,2.3589588258323295,5.726013748860151,3.2357270598380583,2.6848187375532224,9.236301266827088,2.4854268271702415,5.9618548076361595,4.088311235888661,2.440952198029637,5.456806149230474,6.205157883920094,5.588564737401351,6.620439787651722,8.334139178838237,5.331633567171029,2.211012193485512,3.8063240573900288,8.45889870671479,6.602884408718419,3.8439838440483265,6.132988042627442,2.541019153133559,5.953730849811491,4.18824265626441,1.4489009511451278,2.7070829917717063,3.0107798387532423,3.4019034716079584,8.574290766120185,3.542258049766918,4.313245851787562,3.3854310371935203,6.156032498888866,5.419538891513785,7.3107945270667845,1.3950627995175775,5.8346606579017255,6.025250321561575,6.2268938135713885,5.548128323585586,6.746447139362815,4.583158003874408,5.9576824861651225,5.778208576398088,7.834028534577023,1.570462931026041,7.0083165637349865,10.307269127730322,4.95093492831454,5.099295204337775,4.547203024756931,7.9684364043347875,2.8318772411916733,2.4436066514756147,2.4724877714627436,2.37851162325373,2.6507645591169022,8.202417721575182,4.963011647599427,9.01321099459576,2.176322772640463,2.73768676140986,5.5786368624717415,13.52966114241807,5.72328550477764,5.642412772905056,7.568108130120591,6.034743949324734,5.320845667645722,3.8469946869655733,3.8639384504239715,5.154210530588339,5.777945670695148,3.327687364176047,4.169123281476757,8.135247549809566,6.890446692679906,2.2479275134435857,4.9804823555382605,3.3015876466031866,8.873136468833458,5.8346606579017255,4.189033824390017,8.095608004811197,5.380244590478185,4.571676809970931,5.071247819461442,2.9373443921502322,2.944858445807539,6.706254253573203,6.849624029924543,7.541716163393138,7.907912142840534,6.091065077930535,4.388878338811989,4.078951341394822,2.698218478224414,2.5058909297299574,4.621758856709732,5.3175935046234715,1.996388746447621,2.7070829917717063,5.974529312483882,2.883620816285671,4.853995647176393,4.593951283948411,6.496494306853713,3.9845893503624565,6.212180210044431,6.897240425574799,5.452858964713811,2.232660756790275,3.773996325111173,6.075318692553354,4.161081482277184,3.2094533656289497,4.922673592849446,4.478971805032942,3.0840642647884744,6.054414387917307,3.486714373030702,7.582029046119882,7.461888664432819,5.6604951318885135,2.7311832415722,2.049630767724601,7.850749425896496,2.37851162325373,5.986638471730853,7.210817432089213,2.769771739249448,4.970393537914677,2.889473542531109,7.08533966935737,7.463197379833495,6.09950561709114,4.17951105027151,8.27891401949994,2.4594316186372973,6.221297103163338,4.867402305727543,2.980025300238734,3.4276061727818994,5.933572638261024,2.565597175854225,7.056366761024529,6.199672344836364,4.985955754607509,6.215484464192794,6.7410624357490905,4.59215800212536,5.19377174339668,4.099295204337775,6.714108111595169,2.1634987322828794,8.524737587012376,2.18269229751619,2.765534746362977,4.67581593117227,6.658497136656478,6.557808298936688,4.208673319629471,4.993674361750585,5.422905742612183,5.7824085649273735,5.3847405872923835,5.08278773170927,2.4276061727818994,2.411426245726465,7.603403804166527,5.174326515173915,5.896998342338911,6.425089989874059,0,5.106432077816929,2.9467308601403097,8.843732656611305,3.7311832415722,7.403949364232865,6.247927513443585,8.640931770341906,2.6915341649192004,5.336640446408735,7.390942772802543,2.1984941536390834,3.0125686735030555,2.0214797274104517,8.044284820139424,7.1855692560893685,2.636914580355878,8.491372117815171,2.5185351389821804,6.73226919950145,4.614709844115208,4.15623479785027,2.495695162624069,1.0426443374084937,8.060966074004773,3.5643781685650637,3.679198570566922,4.488000770834068,6.3103401206121505,3.605257262939004,4.821199978056451,7.003039979749236,8.211498981977794,8.84080938879262,7.640895629328568,5.95837871156634,6.804001915179304,3.997292407636508,2.3533232911628965,7.631540867341486,2.7990873060740036,4.706530552538117,3.452858964713811,8.983022086172278,6.187253085582297,4.564378168565064,4.241840183564671,5.187055089968057,5.496654082593496,6.564225720206398,6.793115555274348,8.126549613543878,6.3414521074811265,8.823876711241038,5.094658342654544,8.23429054170594,3.063502942306158,6.9910682753603695,7.485507332459785,6.9092930858238235,6.2992080183872785,5.189033824390017,5.562547724199122,3.2357270598380583,2.851998837112446,2.6507645591169022,6.425425734590699,2.889473542531109,5.012121672712217,4.938285791884053,5.642412772905056,6.199672344836364,2.0738202332916713,7.151270288790165,5.005849075696688,2.3923174227787602,2.1667154449664223,5.727920454563199,2.3103401206121505,6.984019633085816,3.9726926540042644,4.362469888750209,7.173227395028653,9.653203362033977,3.357552004618084,8.556735947914529,3.9420452599160467,4.1026581313637385,6.309067020588001,4.952799477899938,2.3015876466031866,4.580748491763774,5.046578366620329,6.999098033705607,2.752748591407134,6.73348992588162,3.1193561770396756,3.431622959713292,5.15623479785027,2.344828496997441,2.720278465233327,4.763411574470007,7.178117045706399,2.807354922057604,3.685940148445977,4.417515002885087,8.72584338467341,2.4724877714627436,3.619413010597937,7.5130959085510085,3.382667252745041,2.7463127664254587,4.509695841933387,1.6553518286125541,3.0992952043377753,7.846242565109514,2.344828496997441,5.39780296186249,4.658211482751795,4.872828759534886,7.290387539763883,6.802451741275622,5.377123749129487,4.270528942380718,6.325710207515785,3.572889668420581,2.548436624696042,6.845113646617966,3.2265085298086795,6.531849285751203,3.361768359419153,5.599912842187128,2.211012193485512,4.396433531250992,7.406162478777607,5.6519127446457835,5.7114949066500875,3.61706334388818,4.044394119358453,5.772149590784542,5.177917792195843,3.1795110502715103,2.321928094887362,4.854494418154875,8.167066838182594,6.5249725982866265,6.43095427138573,5.669310286202474,4.147306698780294,4.303050084681673,4.3298411765306275,6.912410353808268,4.904002316283692,3.2854022188622483,2.8659188145522125,6.582254908357664,5.198494153639083,5.103077946597573,4.3045110418099535,4.370861740085285,8.654779216959517,8.362951992360674,7.2600256559614555,2.6392321632492775,6.457791260256042,7.31786479839195,5.661920579135087,4.609991295212678,3.460742563789644,9.248900055578497,7.886854944198882,3.7059779016825223,3.0373822220030804,5.982537314399271,5.339850002884624,10.306392126750833,8.18685706717715,5.533874777258546,8.85835405767524,6.166715444966422,6.56727163085093,5.7737326506078555,3.396433531250992,7.532083141492505,4.876271360550127,3.817623257511431,5.643567621908798,4.28318097894557,2.6205864104518777,2.881664619320345,5.006746832440585,5.20163386116965,4.181102550753798,8.571145863602528,7.3433189836610735,7.006298023900369,2.9818526532897405,6.419370342528151,2.414135532984451,8.463115619892362,4.857483428286672,6.972003303808356,4.958842675243241,5.910252962799278,4.4963345134171115,4.707082991771706,2.8094144442358986,3.1651079851445365,6.320484678017694,3.8787253414801053,1.3504972470841332,6.207502459258789,6.152994605492435,2.333423733725192,3.0823619695574735,3.4620523187964327,6.266599165535096,6.2861418728343015,3.6064422281316078,6.75702324650746,5.29020320512607,5.342341397431548,7.833712369021444,2.440952198029637,6.642123916153487,6.722602636851373,4.393690764187454,2.729008870337863,7.036503333738988,7.122569192216447,3.18269229751619,1.0565835283663676,2.992768430768924,4.125981653854716,2.7949356628035362,6.039796407620025,3.4154882710497003,5.535430914871279,4.963011647599427,2.763411574470007,5.062639828286463,7.592232766722112,9.317322159829551,4.955126781261366,1.4489009511451278,6.436128517259249,3.2645364309990255,7.647314509547136,6.15319733085508,2.229587922740652,5.417515002885087,5.0925457415435655,5.34801990922869,2.4195388915137843,7.500961352460057,4.426264754702098,6.247547806890298,4.622344722724236,4.638073837180719,2.763411574470007,8.388663123619393,7.058749412335524,3.496973580998276,3.377123749129487,2.361768359419153,9.114445113289024,4.888986721186558,7.971428593590467,5.424922088210688,7.212958362728285,7.02136867825973,0.8318772411916731,7.720552091605108,3.060047383669939,4.532940288372874,7.506922418047323,2.356143810225275,3.382667252745041,1.372952097911829,9.10522758436473,4.751142324726947,9.036036206482532,7.013685569928653,6.974644026040249,3.521050736900963,2.7070829917717063,6.017253838968937,10.665637927637095,5.325530331567558,7.864928972289788,3.400537929583729,2.851998837112446,2.37851162325373,5.22573765346606,3.8349134297010385,4.126807703142035,2.389566811762726,5.632268215499513,6.783456654360239,5.589164236699772,3.596935142387232,4.365272599764531,2.8399595874895316,6.588864518190148,2.8379432418910273,4.650764559116903,5.37364821133469,4.8875252707415875,7.596264312101167,7.323370069061268,4.88508622529017,5.500164679492167,2.327687364176047,5.592457037268081,5.951401291674311,6.265849421440848,6.862203399053224,3.023255352300303,5.299391206126829,7.7627474419531515,2.7420062108667365,5.932628157022127,2.550900664647523,6.9532652390148435,3.6542063779442917,3.8257856274647914,1.996388746447621,5.713420884868084,6.884964164708779,3.84197311892718,6.009212786805205,7.490730563589641,2.3476656563009706,6.668317719915721,3.8196681834964554,10.872998007770835,13.44366362719111,3.2764966656403565,10.051385697621988,6.023698917266407,3.8787253414801053,6.258895755075436,4.826802684285827,7.317231700222606,7.7475216725404215,4.398487190018967,5.342341397431548,5.767654798237347,2.8155754288625725,5.8011586560936985,2.3757345385831563,2.430285272977781,2.150559676575381,4.505255800932163,3.344828496997441,3.4356285940520905,2.2172307162206693,2.823749360308273,5.130519083027338,4.868390453054454,4.839455765224629,2.2720231890610485,5.525755692829486,3.828834649468056,7.7780114016125745,10.806299887662895,5.515384460636948,3.9699332746978557,6.57334422759282,2.759155833800272,5.033863451866284,4.279471295644468,7.080870811759721,6.138323004056484,5.369815424283912,3.975446765640962,4.708739041359579,9.557597721454812,4.200849574637213,5.854494418154875,5.246788093844365,5.102238193930738,7.479861085016818,3.0373822220030804,6.836681593327116,2.877744249949002,7.240600549955795,3.228049047884462,2.0565835283663674,4.4924941516794314,5.307064162255372,2.9088129077395473,2.682573297347578,2.722466024471091,7.96087076379617,8.903429991567151,9.435816085556171,4.631686366311998,7.457709193357593,6.109569509370688,6.0413306068206225,7.586464525886388,5.687060688339892,2.6915341649192004,2.948600847493356,10.7317008703535,2.6016965164809576,5.20163386116965,8.56017995776418,4.4672794804599825,5.904002316283692,8.44795106244151,4.932155684432056,0.9411063109464314,3.4154882710497003,3.8257856274647914,6.542412837068272,2.5360529002402097,8.739409342229193,4.436961337833602,2.797012977836145,5.624685810525398,6.1705259991768475,5.926948247949772,7.881113960675097,9.500642736062806,7.073606040574941,7.318316841334983,7.971830914780502,6.137913322088407,4.721919445551544,1.922197848396367,8.226941966807852,6.570159301636119,5.3458930856874085,2.73768676140986,10.642557179595622,3.62993940943954,5.175524601089875,9.995102877826515,3.4646682670034443,1.8836208162856714,6.0083165637349865,2.408711861029429,2.950468414150123,4.443606651475615,6.456313341355182,4.8604662585171665,5.661065479806948,6.7997349224788675,4.824258696603374,3.269033146455237,5.9740703670240265,5.925049964727359,5.946496941203293,1.4750848829487828,3.6971065744769747,3.953265239014844,5.464341532761104,3.0908534304511135,3.7676547982373463,2.859969548221026,7.822411496360477,9.401604869714957,2.7092906357233577,2.257010618206024,4.178714641175443,6.423410093269615,6.9070108151861795,3.6484654430273142,8.622783966156124,4.569248029867182,5.366322214245816,5.570462931026041,5.130519083027338,2.773996325111173,2.18269229751619,5.724923070266613,6.9787676510367875,6.751410160064371,4.484138131201669,3.0686708106650986,5.111865963867557,2.0942360698457656,4.315421315945778,4.06177619758669,5.781884234541486,2.974529312483882,4.57349571549561,4.176322772640463,10.888134388352869,4.414812060565847,5.521364878446219,4.582556003014061,3.8369340113210915,3.929790997718597,4.598126959919604,3.61706334388818,2.4489009511451276,4.788163657037767,1.8953026213333068,2.7398481026993275,0.12432813500220177,3.400537929583729,7.515147882149842,5.381629467033664,2.1602748314085933,6.677226328235316,8.025084179448086,3.440952198029637,2.9763636357327616,3.4905701304462013,7.431622959713292,2.7803100990433753,5.659068274843228,8.878265537895738,6.403949364232865,3.452858964713811,8.162995472417697,2.1473066987802936,3.7398481026993275,5.212569338850806,2.8933622107638715,3.9155209007519587,8.562585882154105,4.814550423461808,4.5867647433551415,2.582556003014061,5.077670274232752,7.905507349949873,11.663771557482411,6.559338847606539,2.7949356628035362,8.834123370734224,6.0925457415435655,4.936402377725063,3.454175893185802,2.788685710613534,7.877437521970619,5.3377110921282895,8.303415462685624,7.285032249622911,2.6915341649192004,6.459267666703387,4.398487190018967,8.028624401408491,2.9392265777282085,5.342341397431548,7.217424613813984,7.563234413022304,2.5921580021253603,3.423578170981797,2.763411574470007,5.542258049766918,4.51853513898218,9.535217045453418,5.908092340818271,6.928844036712568,5.612352498752662,6.7786028451432125,6.4202128906107845,2.877744249949002,4.038260575175349,3.4222330006830477,3.777156666004501,4.766065051474954,5.59215800212536,4.534186139090097,6.1147834472498515,5.572889668420581,2.9726926540042644,1.944858445807539,6.412442824775613,3.7990873060740036,4.030336078370959,4.255500733148386,2.5459683691052923,3.7813597135246595,5.135042286233651,5.6061460780657235,7.027021314622255,5.524189078449365,3.145677455195635,7.669948004116052,2.4462562298895643,2.408711861029429,7.221587121264805,4.008092420948722,6.956289026583943,11.260837225974512,5.316869805064333,4.078951341394822,4.509062386361898,8.238500278879492,2.689299160535892,4.22881869049588,5.614415386508126,4.112700132749362,2.9523335663696857,3.693765712217783,7.474030370207135,2.521050736900963,5.727375938594897,2.3673710656485296,2.786596361890807,6.034963989749303,5.578334948681608,8.298429210890312,7.4944956156680504,10.089053172156433,7.896998342338911,8.325215494683038,7.3416300093299105,4.349789870241872,6.438459204423481,8.102815576392821,5.43229133824778,2.4905701304462013,2.873813198359087,6.096978636370538,4.032982416699428,5.604071323668861,2.5582676340557358,4.694323060125386,2.722466024471091,2.3757345385831563,7.995258373404689,7.1078970616646115,7.797726364623276,9.707290101958575,1.0976107966264221,3.1953475983222193,7.672354619892956,3.4208865749755315,6.180903709179382,5.908572758751745,3.372952097911829,7.12525847253797,4.314696525656286,8.915789767665157,5.459103696135399,7.07692245939234,5.79207446192887,4.488000770834068,5.685379551959075,6.442114122419225,5.125981653854716,3.1193561770396756,6.643134661857265,6.657640005207824,6.316507819103109,0,4.053111336459563,5.711219556894335,2.3161457422933562,4.726831217032493,5.8894735425311096,5.981167667105682,6.483976963236257,3.3840498067951597,4.672990993827748,5.239550796613645,6.48767927890162,2.733354340613827,7.754887502163468,5.065227622775619,6.546122758857796,4.556429415449573,3.33771109212829,4.72137265947682,5.533563348214512,7.015582311680932,3.3868109464722167,6.141391916170729,2.097610796626422,6.376255649313362,3.24031432933371,6.5787877956769725,6.052024560482831,6.4571345943180365,6.711081862306696,1.5109619192773793,2.0071955014042038,8.766462652425766,2.584962500721156,7.074676686294496,3.292781749227846,9.20006486151431,7.7525479058471385,4.009884588931844,2.169925001442312,8.76354436429392,6.257010618206024,4.3269687115878925,2.3950627995175777,2.3757345385831563,5.424922088210688,2.250961573533219,2.8011586560936985,3.4892860226258766,5.968551603363945,2.541019153133559,4.888012585402829,6.457791260256042,5.237257770900372,4.161081482277184,3.408711861029429,6.705839705883682,8.251198340973543,5.2671612180222604,7.824386002585445,3.1043366598147357,6.542103245856593,2.2234225499349374,5.316507819103109,3.363171077119244,4.631104282365877,3.9873208659292536,2.140778655782796,7.473705749619415,5.924575003392735,2.871843648509318,6.807870078299782,4.955591791753994,4.214901910160696,3.910732661902913,8.646486708605355,2.2539892662307865,3.816599706534912,6.460578760776026,7.281698250098829,4.901108243014512,2.867896463992655,5.9429835981871015,2.950468414150123,3.8429788317883253,6.045486656359465,4.990047219573794,6.705977901682522,7.860404179081675,2.565597175854225,0,9.146670494470888,6.7014104362328135,3.4019034716079584,4.462706750670158,4.311793717753649,6.649471757181635,2.788685710613534,7.693556651228518,2.8135246892978105,3.343407822297814,3.5360529002402097,3.2675357980687347,6.75127624861109,8.995371450572195,2.6870606883398924,7.605257262939004,5.275752048828529,6.982080909763738,5.305970520984387,2.017921907997262,7.062963556559086,7.2259304111710545,7.0414401300729725,7.778274295338579,2.4646682670034443,2.4620523187964327,3.2141248053528475,4.114367024952,6.111448698487674,2.189033824390017,6.304876050044889,6.0710332444568245,6.157650096861711,4.038260575175349,7.489045125114802,8.475936042446387,0.9411063109464314,2.01435529297707,8.154868729042667,5.955824240806441,2.996388746447621,6.1039172107520665,4.296457407371244,3.657640005207824,3.1259816538547156,8.325890061039784,3.085764553778314,5.309976492370556,3.145677455195635,2.594548549550354,3.631104282365877,5.121015400961366,4.371558862611963,8.492213725013189,2.1043366598147357,5.825022364044702,6.883987308105472,10.959639764364185,2.4620523187964327,5.522306892871389,0,6.593054921669878,3.9354597478052895,8.913068913647477,5.021035479511363,1.7484612330040354,5.982537314399271,7.690137692986635,5.791814071161826,3.578938713093386,4.896271848807375,6.766065051474954,7.801029283808189,6.976821852360685,5.795195490817005,6.883132015657282,4.464668267003444,4.808385050656093,7.161484638641659,2.3645724322958563,5.707082991771706,2.8439838440483265,2,7.723148957047578,1.1634987322828796,7.126807703142035,5.348728154231077,4.4924941516794314,6.174526265266431,5.563768278452033,5.6570683012015754,3.37851162325373,6.43895814789755,3.127633279725874,3.1953475983222193,6.676803353873854,2.6667565918848033,2.3589588258323295,5.587364990936461,2.454175893185802,3.2265085298086795,6.230164577884252,7.884597920990064,3.7687136570304847,3.127633279725874,6.450551443306483,4.084914659767938,4.135863165368679,2.319039815562536,3.097610796626422,5.624978179569036,3.968090752045256,5.105594275795191,5.185470146066942,5.509379148914689,5.266411766036114,3.1342209397606338,4.707082991771706,6.390598905552461,2.8698714061777126,1.4168397419128291,2.3645724322958563,3.5084286525318573,3.9354597478052895,7.11009175192513,1.4222330006830475,6.929672661579345,7.041001987167506,6.332349900373874,3.423578170981797,5.137503523749935,5.3139713710590195,5.999549087340949,3.2433644256936605,4.7114949066500875,3.7059779016825223,6.291677223176022,0.3895668117627256,5.758089934201813,7.001577085389535,7.931505781869729,2.8257856274647914,9.151270288790165,2.440952198029637,5.74604398299834,7.136683577697236,6.353146825498083,10.212070748654236,6.007419783571914,4.997744026059632,2.4982508675278257,8.25176642434399,2.8579809951275723,3.300123724569014,4.640967910449898,2.625270489374693,5.399854673572431,2.2660368939953175,1.516015147003665,6.199672344836364,5.708187236020708,5.402585758232587,5.269033146455237,4.529820946528695,5.212958362728285,5.648753032989572,2.9467308601403097,2.169925001442312,6.088735246190284,2.4356285940520905,8.627898615871748,5.300489844363109,8.091065077930535,7.119252412157343,10.142962207064079,1.7398481026993275,3.729008870337863,4.45022149589718,6.461724991502711,7.904544309292717,5.490890978894291,6.76593249347046,4.502075956045791,2.782408564927373,4.205548911173033,4.139960569545456,6.708325207143167,4.632268215499513,5.6194130105979365,5.541638734436141,3.581351247168777,3.496973580998276,4.862451391325582,4.784503982929566,5.359310316890443,3.41006969175638,2.5897634869849773,2.769771739249448,10.641220865859923,8.202368742775262],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle RandomForestRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_pred_log_logRF"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle RandomForestRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsRF_log = np.logspace(0, 3, 10, dtype=int)\n","param_gridRF_log = {\n","    'randomforestregressor__n_estimators': n_estimatorsRF_log,\n","    'randomforestregressor__max_features': ['auto', 'sqrt', 'log2'],\n","}\n","\n","GridRF_log, \\\n","BestParametresRF_log, \\\n","ScoresRF_log, \\\n","TotalGHGEmissions_pred_logRF_log, \\\n","figRF_log = reg_modelGrid(model=RandomForestRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBM_train,\n","                         X_test=BEBM_test,\n","                         y_train=TotalGHGEmissions_train_log.ravel(),\n","                         y_test=TotalGHGEmissions_test_log,\n","                         y_test_name='TotalGHGEmissions_test_log',\n","                         y_pred_name='TotalGHGEmissions_pred_log_logRF',\n","                         score=score,\n","                         param_grid=param_gridRF)\n","\n","print(BestParametresRF_log)\n","print(ScoresRF_log)\n","figRF_log.show()\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[1.876719966801776,1.6372972174326375,1.4961743690509706,1.398706462352965,1.3568698355581097,1.339757711358907,1.3353479748348094,1.3325283400432473,1.3304355025174832,1.3307282841012409]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[1.9189925700653334,1.6800494805720274,1.5237741217722924,1.4347544134229648,1.3930388240502434,1.3745286283920093,1.3642823380453821,1.3617723782307507,1.3620329108904488,1.363241137512113]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[1.8344473635382186,1.5945449542932475,1.4685746163296487,1.3626585112829652,1.320700847065976,1.3049867943258047,1.3064136116242366,1.3032843018557438,1.2988380941445177,1.2982154306903688]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.851518199247314,1.595432365117817,1.4717072476003763,1.3731221700938812,1.3213133907966275,1.3087904274530286,1.308292365954088,1.3012479815247076,1.2986697671085872,1.2967488963580007],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.9528313554320906,1.7048986037848428,1.548285945090748,1.440412550397174,1.4096603971774935,1.3728156143529358,1.3582705917376374,1.3586114419085098,1.3585756049193798,1.362207731872288],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.8625832623571805,1.6022351913506823,1.4974294862109494,1.3975056742675716,1.3629015001568712,1.3529291997044863,1.3450891551332962,1.3415698831007719,1.3400611776927696,1.3419736297249962],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.829778103963114,1.6141810267608367,1.4755883213161365,1.3465774015308722,1.3122002590749418,1.289095670743951,1.2950077777021798,1.2950524273802155,1.2881901002108835,1.2877863260129725],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.8868889130091815,1.6697389001490077,1.4878608450366428,1.4359145154753266,1.3782736305846142,1.3751576445401337,1.3700799836468442,1.3661599663020307,1.366680862655796,1.3649248365379472],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle RF pour le paramètre<br>randomforestregressor__max_features=auto<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE RandomForestRegressor\n","# pour le meilleur paramètre max features\n","FigRMSEGRidRF_log = visuRMSEGrid(RandomForestRegressor(), 'RF',\n","                                 n_estimatorsRF_log, 'n estimators',\n","                                 GridRF_log, BestParametresRF_log,\n","                                 'randomforestregressor__max_features')\n","FigRMSEGRidRF_log.show()\n","if write_data is True:\n","    FigRMSEGRidRF_log.write_image('./Figures/EmissionsGraphRMSERF_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.2.7 Modèle AdaboostRegressor"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                         paramètre AdaBoostRegressor()\n","0  adaboostregressor__n_estimators                   4\n","1          adaboostregressor__loss              linear\n","      AdaBoostRegressor()\n","R²               0.417478\n","RMSE             1.606080\n","MAE              1.326605\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predAB_log=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4.55480550564441,5.4778608106496,6.059575726240399,4.3810847887602185,4.55480550564441,4.723049582824984,6.907510396933096,4.616823780779677,5.213442670298336,5.555488028048255,6.059575726240399,4.723049582824984,5.213442670298336,5.555488028048255,6.907510396933096,8.811534607711552,5.96649132732242,4.215264697469774,6.907510396933096,5.96649132732242,4.215264697469774,4.215264697469774,6.059575726240399,5.555488028048255,4.616823780779677,4.3810847887602185,3.4610213160944836,5.96649132732242,6.907510396933096,4.215264697469774,5.213442670298336,5.213442670298336,5.213442670298336,5.96649132732242,5.555488028048255,3.4610213160944836,4.723049582824984,4.55480550564441,4.55480550564441,4.55480550564441,3.4610213160944836,4.3810847887602185,4.55480550564441,4.55480550564441,4.723049582824984,6.059575726240399,4.55480550564441,4.3810847887602185,5.213442670298336,3.4610213160944836,6.955420529824971,4.55480550564441,4.3810847887602185,4.723049582824984,6.907510396933096,4.3919463725839325,6.059575726240399,4.3810847887602185,4.215264697469774,6.955420529824971,5.213442670298336,4.723049582824984,5.555488028048255,4.3810847887602185,5.555488028048255,4.3810847887602185,5.555488028048255,5.555488028048255,4.3810847887602185,3.4610213160944836,4.616823780779677,5.555488028048255,3.4610213160944836,4.3810847887602185,3.4610213160944836,4.55480550564441,3.4610213160944836,4.3810847887602185,4.215264697469774,8.811534607711552,6.907510396933096,4.616823780779677,4.3810847887602185,4.215264697469774,6.907510396933096,4.55480550564441,4.215264697469774,4.215264697469774,4.3810847887602185,4.3810847887602185,6.059575726240399,3.4610213160944836,4.215264697469774,4.723049582824984,4.3810847887602185,8.811534607711552,4.3810847887602185,4.723049582824984,5.96649132732242,3.4610213160944836,4.215264697469774,3.4610213160944836,3.4610213160944836,4.215264697469774,5.213442670298336,6.059575726240399,6.907510396933096,4.3810847887602185,4.723049582824984,4.3810847887602185,6.907510396933096,4.723049582824984,4.616823780779677,6.059575726240399,4.55480550564441,3.4610213160944836,4.723049582824984,8.811534607711552,4.3810847887602185,3.4610213160944836,5.555488028048255,4.3810847887602185,4.3810847887602185,4.3810847887602185,6.406657683113792,4.723049582824984,3.4610213160944836,4.3810847887602185,4.3810847887602185,4.3810847887602185,5.213442670298336,4.723049582824984,4.215264697469774,4.3810847887602185,4.3810847887602185,4.616823780779677,4.616823780779677,4.3810847887602185,6.955420529824971,5.213442670298336,5.555488028048255,4.616823780779677,4.215264697469774,8.20921159078321,3.4610213160944836,4.723049582824984,6.059575726240399,6.059575726240399,3.4610213160944836,4.616823780779677,5.213442670298336,5.213442670298336,4.215264697469774,4.3810847887602185,4.616823780779677,5.213442670298336,5.213442670298336,4.55480550564441,4.723049582824984,3.4610213160944836,5.555488028048255,6.059575726240399,5.4778608106496,4.215264697469774,4.55480550564441,4.215264697469774,5.213442670298336,4.3810847887602185,4.3810847887602185,6.907510396933096,4.3810847887602185,5.213442670298336,5.555488028048255,4.215264697469774,8.811534607711552,6.059575726240399,4.723049582824984,4.616823780779677,6.059575726240399,8.811534607711552,8.811534607711552,7.961621425436382,5.213442670298336,5.213442670298336,4.616823780779677,4.3810847887602185,5.555488028048255,4.723049582824984,4.215264697469774,4.3810847887602185,4.723049582824984,3.4610213160944836,5.213442670298336,4.3810847887602185,3.4610213160944836,5.4778608106496,5.213442670298336,4.3810847887602185,8.20921159078321,3.4610213160944836,4.215264697469774,3.4610213160944836,4.723049582824984,4.215264697469774,4.723049582824984,4.723049582824984,8.20921159078321,8.811534607711552,6.059575726240399,3.4610213160944836,4.723049582824984,6.059575726240399,7.313022196950547,4.3810847887602185,6.059575726240399,3.4610213160944836,5.4778608106496,5.555488028048255,4.723049582824984,4.616823780779677,4.55480550564441,4.215264697469774,4.616823780779677,5.96649132732242,3.4610213160944836,4.616823780779677,3.4610213160944836,6.059575726240399,5.555488028048255,4.3810847887602185,4.215264697469774,3.4610213160944836,5.4778608106496,4.3810847887602185,6.955420529824971,4.55480550564441,4.215264697469774,5.555488028048255,6.059575726240399,4.3810847887602185,5.555488028048255,4.723049582824984,4.55480550564441,5.96649132732242,4.3810847887602185,5.555488028048255,4.215264697469774,3.4610213160944836,4.215264697469774,6.907510396933096,4.55480550564441,6.406657683113792,5.4778608106496,4.3810847887602185,3.4610213160944836,4.55480550564441,4.723049582824984,8.20921159078321,4.215264697469774,4.215264697469774,7.961621425436382,5.555488028048255,5.213442670298336,5.555488028048255,4.723049582824984,4.215264697469774,5.555488028048255,4.55480550564441,3.4610213160944836,5.555488028048255,6.955420529824971,4.3810847887602185,5.555488028048255,4.723049582824984,8.20921159078321,6.059575726240399,5.213442670298336,5.4778608106496,5.213442670298336,4.723049582824984,5.96649132732242,3.4610213160944836,4.55480550564441,4.215264697469774,5.213442670298336,4.723049582824984,5.213442670298336,5.96649132732242,4.3810847887602185,4.616823780779677,4.616823780779677,4.215264697469774,4.3810847887602185,4.215264697469774,4.215264697469774,4.55480550564441,4.215264697469774,4.3810847887602185,5.213442670298336,5.555488028048255,4.723049582824984,5.555488028048255,5.96649132732242,8.20921159078321,5.555488028048255,3.4610213160944836,4.3810847887602185,4.3810847887602185,5.213442670298336,4.723049582824984,5.96649132732242,4.55480550564441,3.4610213160944836,5.555488028048255,4.215264697469774,6.907510396933096,6.907510396933096,4.215264697469774,3.4610213160944836,4.3810847887602185,6.907510396933096,3.4610213160944836,6.059575726240399,8.20921159078321,4.3810847887602185,4.55480550564441,4.3810847887602185,4.3810847887602185,8.20921159078321,4.723049582824984,4.55480550564441,6.059575726240399,4.215264697469774,3.4610213160944836,4.55480550564441,4.55480550564441,4.55480550564441,4.3810847887602185,4.55480550564441,4.723049582824984,4.723049582824984,4.3810847887602185,5.96649132732242,6.059575726240399,5.96649132732242,6.059575726240399,3.4610213160944836,4.723049582824984,4.723049582824984,6.059575726240399,4.215264697469774,4.55480550564441,4.55480550564441,5.96649132732242,5.213442670298336,6.059575726240399,5.555488028048255,5.213442670298336,6.059575726240399,5.4778608106496,4.3810847887602185,3.4610213160944836,3.4610213160944836,6.907510396933096,4.3810847887602185,4.215264697469774,6.907510396933096,4.723049582824984,3.4610213160944836,5.96649132732242,8.20921159078321,4.3810847887602185,4.723049582824984,5.96649132732242,8.20921159078321,3.4610213160944836,6.907510396933096,8.20921159078321,3.4610213160944836,4.215264697469774,4.3810847887602185,8.20921159078321,6.955420529824971,3.4610213160944836,6.907510396933096,3.4610213160944836,5.213442670298336,5.555488028048255,5.4778608106496,3.4610213160944836,4.723049582824984,6.907510396933096,3.4610213160944836,4.215264697469774,4.3810847887602185,4.3810847887602185,5.555488028048255,4.55480550564441,6.059575726240399,8.20921159078321,5.4778608106496,6.059575726240399,5.96649132732242,6.059575726240399,4.723049582824984,4.215264697469774,5.96649132732242,4.3810847887602185,6.059575726240399,4.616823780779677,6.955420529824971,4.723049582824984,5.555488028048255,3.4610213160944836,6.907510396933096,4.3810847887602185,3.4610213160944836,4.616823780779677,4.3810847887602185,4.3810847887602185,8.811534607711552,6.059575726240399,6.907510396933096,4.616823780779677,6.059575726240399,7.961621425436382,6.907510396933096,4.616823780779677,4.723049582824984,5.555488028048255,4.616823780779677,4.616823780779677,4.55480550564441,4.215264697469774,3.4610213160944836,6.059575726240399,4.55480550564441,4.3810847887602185,5.96649132732242,3.4610213160944836,6.907510396933096,4.723049582824984,3.4610213160944836,4.3810847887602185,6.955420529824971,3.4610213160944836,4.723049582824984,4.3810847887602185,3.4610213160944836,6.955420529824971,8.20921159078321,4.723049582824984,6.406657683113792,4.3810847887602185,4.215264697469774,4.215264697469774,4.723049582824984,5.96649132732242,4.55480550564441,4.3810847887602185,4.215264697469774,3.4610213160944836,8.20921159078321,4.616823780779677,3.4610213160944836,4.3810847887602185,4.215264697469774,4.215264697469774,4.215264697469774,8.20921159078321,4.55480550564441,4.723049582824984,5.555488028048255,6.955420529824971,4.215264697469774,5.213442670298336,8.20921159078321,4.723049582824984,4.616823780779677,4.3810847887602185,4.723049582824984,4.55480550564441,4.723049582824984,4.215264697469774,6.907510396933096,4.3810847887602185,6.907510396933096,6.406657683113792,5.96649132732242,3.4610213160944836,4.723049582824984,3.4610213160944836,4.55480550564441,4.215264697469774,6.907510396933096,4.616823780779677,6.955420529824971,5.4778608106496,4.616823780779677,3.4610213160944836,5.213442670298336,6.907510396933096,4.723049582824984,6.059575726240399,4.3810847887602185,5.4778608106496,3.4610213160944836,4.723049582824984,4.55480550564441,4.215264697469774,4.55480550564441,5.213442670298336,5.555488028048255,5.555488028048255,5.555488028048255,3.4610213160944836,5.555488028048255,4.723049582824984,6.406657683113792,4.723049582824984,4.55480550564441,4.3810847887602185,7.313022196950547,5.555488028048255,4.723049582824984,4.55480550564441,3.4610213160944836,6.907510396933096,5.213442670298336,6.059575726240399,4.215264697469774,3.4610213160944836,6.059575726240399,4.723049582824984,6.059575726240399,5.213442670298336,8.20921159078321,7.961621425436382,5.96649132732242,4.55480550564441,5.96649132732242,6.059575726240399,8.811534607711552,8.811534607711552,3.4610213160944836,8.20921159078321,4.616823780779677,5.213442670298336,4.3810847887602185,5.555488028048255,4.3810847887602185,5.555488028048255,5.555488028048255,4.723049582824984,6.059575726240399,3.4610213160944836,3.4610213160944836,7.961621425436382,5.96649132732242,5.555488028048255,4.3810847887602185,4.55480550564441,4.55480550564441,4.215264697469774,5.555488028048255,3.4610213160944836,4.723049582824984,4.3810847887602185,4.3810847887602185,4.215264697469774,5.213442670298336,3.4610213160944836,5.555488028048255,3.4610213160944836,3.4610213160944836,6.059575726240399,4.723049582824984,4.3810847887602185,5.213442670298336,4.723049582824984,3.4610213160944836,4.723049582824984,4.616823780779677,4.3810847887602185,4.723049582824984,4.3810847887602185,5.213442670298336,5.555488028048255,6.955420529824971,5.96649132732242,3.4610213160944836,4.723049582824984,7.313022196950547,3.4610213160944836,4.3810847887602185,4.723049582824984,5.213442670298336,4.55480550564441,3.4610213160944836,4.55480550564441,5.555488028048255,3.4610213160944836,5.213442670298336,4.723049582824984,4.723049582824984,4.3810847887602185,4.55480550564441,4.215264697469774,6.907510396933096,8.811534607711552,4.3810847887602185,4.55480550564441,5.555488028048255,4.616823780779677,8.20921159078321,5.555488028048255,4.215264697469774,5.213442670298336,5.213442670298336,4.723049582824984,4.215264697469774,8.20921159078321,4.215264697469774,5.4778608106496,4.55480550564441,4.723049582824984,4.215264697469774,4.3810847887602185,4.3810847887602185,3.4610213160944836,5.213442670298336,3.4610213160944836,5.96649132732242,5.213442670298336,6.059575726240399,5.213442670298336,6.907510396933096,6.955420529824971,4.3810847887602185,6.059575726240399,4.3810847887602185,3.4610213160944836,6.955420529824971,4.55480550564441,4.616823780779677,4.3810847887602185,5.213442670298336,5.213442670298336,4.723049582824984,8.20921159078321,6.059575726240399,5.555488028048255,3.4610213160944836,4.55480550564441,6.907510396933096,4.723049582824984,4.3810847887602185,5.213442670298336,4.616823780779677,3.4610213160944836,5.555488028048255,4.616823780779677,4.215264697469774,4.55480550564441,5.213442670298336,6.059575726240399,6.059575726240399,5.555488028048255,4.55480550564441,4.215264697469774,6.907510396933096,3.4610213160944836,6.907510396933096,6.907510396933096,4.3810847887602185,8.20921159078321,8.20921159078321,4.723049582824984,4.3810847887602185,4.215264697469774,3.4610213160944836,4.215264697469774,5.555488028048255,6.059575726240399,4.616823780779677,4.3810847887602185,4.3810847887602185,4.215264697469774,4.3810847887602185,3.4610213160944836,6.907510396933096,4.723049582824984,3.4610213160944836,3.4610213160944836,6.059575726240399,4.723049582824984,5.96649132732242,4.616823780779677,5.213442670298336,4.215264697469774,4.616823780779677,4.3810847887602185,8.20921159078321,9.774188016702025,4.723049582824984,6.059575726240399,4.3810847887602185,4.3810847887602185,4.3810847887602185,3.4610213160944836,6.059575726240399,8.20921159078321,6.059575726240399,5.555488028048255,5.96649132732242,4.55480550564441,5.213442670298336,4.215264697469774,4.3810847887602185,3.4610213160944836,4.3810847887602185,3.4610213160944836,5.213442670298336,4.3810847887602185,4.3810847887602185,4.3810847887602185,6.059575726240399,4.55480550564441,4.215264697469774,4.723049582824984,3.4610213160944836,7.313022196950547,8.20921159078321,5.96649132732242,5.213442670298336,4.616823780779677,4.723049582824984,6.955420529824971,4.215264697469774,5.555488028048255,6.059575726240399,5.555488028048255,4.55480550564441,5.213442670298336,8.20921159078321,4.3810847887602185,4.215264697469774,4.3810847887602185,4.3810847887602185,8.20921159078321,6.059575726240399,4.55480550564441,3.4610213160944836,4.723049582824984,4.723049582824984,4.215264697469774,5.96649132732242,4.3810847887602185,4.55480550564441,3.4610213160944836,4.55480550564441,8.20921159078321,6.059575726240399,8.20921159078321,4.616823780779677,6.059575726240399,4.723049582824984,4.3810847887602185,4.723049582824984,5.213442670298336,3.4610213160944836,4.723049582824984,8.20921159078321,3.4610213160944836,5.555488028048255,5.555488028048255,4.3810847887602185,6.907510396933096,6.907510396933096,6.059575726240399,4.3810847887602185,4.723049582824984,5.555488028048255,6.907510396933096,4.616823780779677,8.20921159078321,5.213442670298336,4.616823780779677,5.213442670298336,6.059575726240399,4.215264697469774,7.313022196950547,8.20921159078321,4.3810847887602185,4.3810847887602185,7.961621425436382,6.059575726240399,4.215264697469774,4.723049582824984,6.955420529824971,4.723049582824984,4.215264697469774,3.4610213160944836,6.955420529824971,4.55480550564441,5.555488028048255,7.961621425436382,4.3810847887602185,4.3810847887602185,4.3810847887602185,3.4610213160944836,3.4610213160944836,3.4610213160944836,4.723049582824984,4.723049582824984,4.215264697469774,5.96649132732242,6.907510396933096,3.4610213160944836,6.059575726240399,5.555488028048255,4.215264697469774,4.55480550564441,5.213442670298336,4.723049582824984,4.723049582824984,3.4610213160944836,3.4610213160944836,4.3810847887602185,4.723049582824984,8.20921159078321,4.55480550564441,4.723049582824984,4.215264697469774,5.213442670298336,4.723049582824984,5.555488028048255,6.955420529824971,4.215264697469774,5.96649132732242,6.406657683113792,4.215264697469774,4.55480550564441,3.4610213160944836,4.3810847887602185,5.213442670298336,6.059575726240399,6.059575726240399,4.55480550564441,5.96649132732242,4.3810847887602185,6.059575726240399,4.55480550564441,4.3810847887602185,4.215264697469774,6.907510396933096,4.55480550564441,8.20921159078321,4.3810847887602185,4.215264697469774,5.96649132732242,4.3810847887602185,4.55480550564441,4.215264697469774,4.3810847887602185,4.215264697469774,4.215264697469774,4.723049582824984,4.723049582824984,3.4610213160944836,4.723049582824984,8.20921159078321,4.3810847887602185,4.215264697469774,4.723049582824984,7.961621425436382,4.723049582824984,4.55480550564441,3.4610213160944836,5.96649132732242,4.55480550564441,5.213442670298336,6.059575726240399,6.907510396933096,4.616823780779677,8.20921159078321,4.215264697469774,4.616823780779677,4.55480550564441,4.723049582824984,3.4610213160944836,8.20921159078321,3.4610213160944836,4.616823780779677,5.213442670298336,4.3810847887602185,5.96649132732242,8.20921159078321,5.96649132732242,4.55480550564441,8.20921159078321,5.96649132732242,4.3810847887602185,5.213442670298336,4.55480550564441,8.20921159078321,5.96649132732242,7.313022196950547,6.059575726240399,4.723049582824984,6.059575726240399,4.215264697469774,5.96649132732242,4.616823780779677,3.4610213160944836,8.20921159078321,8.20921159078321,3.4610213160944836,4.616823780779677,4.55480550564441,6.059575726240399,5.555488028048255,8.20921159078321,4.3810847887602185,4.723049582824984,4.616823780779677,4.616823780779677,6.406657683113792,4.616823780779677,4.3810847887602185,4.3810847887602185,5.96649132732242,4.215264697469774,6.059575726240399,5.555488028048255,5.555488028048255,5.213442670298336,4.616823780779677,5.555488028048255,6.059575726240399,4.215264697469774,4.55480550564441,4.3810847887602185,4.616823780779677,4.3810847887602185,6.059575726240399,6.059575726240399,4.55480550564441,4.55480550564441,4.723049582824984,6.059575726240399,3.4610213160944836,3.4610213160944836,4.3810847887602185,5.555488028048255,8.20921159078321,7.313022196950547,4.3810847887602185,5.96649132732242,5.555488028048255,7.313022196950547,3.4610213160944836,3.4610213160944836,8.20921159078321,5.213442670298336,4.55480550564441,3.4610213160944836,5.213442670298336,4.3810847887602185,5.96649132732242,4.215264697469774,4.3810847887602185,4.55480550564441,6.059575726240399,7.961621425436382,6.059575726240399,8.20921159078321,6.059575726240399,6.955420529824971,5.213442670298336,5.555488028048255,4.616823780779677,4.616823780779677,4.55480550564441,4.3810847887602185,4.3810847887602185,4.3810847887602185,4.215264697469774,4.215264697469774,3.4610213160944836,5.555488028048255,4.55480550564441,5.4778608106496,5.213442670298336,5.555488028048255,6.059575726240399,8.20921159078321,4.3810847887602185,4.723049582824984,6.059575726240399,4.616823780779677,5.555488028048255,5.555488028048255,4.723049582824984,4.55480550564441,5.96649132732242,5.96649132732242,5.555488028048255,6.955420529824971,4.723049582824984,5.555488028048255,6.955420529824971,4.215264697469774,4.215264697469774,4.55480550564441,6.907510396933096,5.555488028048255,4.55480550564441,4.55480550564441,5.96649132732242,4.723049582824984,4.215264697469774,5.555488028048255,6.907510396933096,4.3810847887602185,4.723049582824984,4.723049582824984,4.3810847887602185,4.723049582824984,6.907510396933096,4.215264697469774,5.555488028048255,4.616823780779677,4.723049582824984,4.55480550564441,4.215264697469774,3.4610213160944836,5.213442670298336,4.3810847887602185,4.215264697469774,6.907510396933096,4.215264697469774,4.723049582824984,4.723049582824984,8.20921159078321,5.96649132732242,4.55480550564441,4.723049582824984,4.3810847887602185,3.4610213160944836,6.955420529824971,4.55480550564441,6.059575726240399,4.616823780779677,6.059575726240399,4.723049582824984,4.215264697469774,4.215264697469774,8.20921159078321,4.55480550564441,6.059575726240399,3.4610213160944836,4.55480550564441,4.3810847887602185,4.3810847887602185,4.3810847887602185,5.213442670298336,5.213442670298336,4.55480550564441,4.3810847887602185,5.213442670298336,4.3810847887602185,4.55480550564441,4.55480550564441,6.059575726240399,5.213442670298336,5.213442670298336,6.955420529824971,4.723049582824984,4.616823780779677,3.4610213160944836,4.616823780779677,4.616823780779677,4.215264697469774,5.555488028048255,3.4610213160944836,4.723049582824984,4.616823780779677,5.213442670298336,4.723049582824984,5.96649132732242,4.55480550564441,5.213442670298336,6.907510396933096,3.4610213160944836,4.3810847887602185,5.96649132732242,5.213442670298336,6.907510396933096,4.3810847887602185,4.723049582824984,4.55480550564441,4.215264697469774,6.059575726240399,5.213442670298336,4.723049582824984,5.555488028048255,4.215264697469774,4.3810847887602185,6.907510396933096,3.4610213160944836,4.616823780779677,5.213442670298336,4.616823780779677,6.907510396933096,4.723049582824984,6.406657683113792,4.55480550564441,4.55480550564441,5.213442670298336,4.723049582824984,5.96649132732242,8.811534607711552,6.059575726240399,6.059575726240399,5.96649132732242,4.723049582824984,4.3810847887602185,4.215264697469774,6.955420529824971,4.723049582824984,8.20921159078321,7.313022196950547,3.4610213160944836,3.4610213160944836,4.723049582824984,4.55480550564441,4.3810847887602185,4.215264697469774,5.96649132732242,5.96649132732242,5.555488028048255,4.723049582824984,5.96649132732242,7.313022196950547,4.55480550564441,3.4610213160944836,5.555488028048255,4.215264697469774,4.723049582824984,6.907510396933096,4.215264697469774,3.4610213160944836,4.616823780779677,6.059575726240399,4.723049582824984,4.3810847887602185,4.723049582824984,4.215264697469774,4.723049582824984,4.723049582824984,3.4610213160944836,8.20921159078321,4.215264697469774,5.213442670298336,5.4778608106496,8.20921159078321,3.4610213160944836,7.313022196950547,5.4778608106496,5.4778608106496,4.55480550564441,8.20921159078321,4.723049582824984,4.3810847887602185,4.215264697469774,6.907510396933096,5.555488028048255,5.555488028048255,5.4778608106496,6.059575726240399,6.059575726240399,5.96649132732242,5.213442670298336,4.723049582824984,6.059575726240399,5.213442670298336,8.20921159078321,4.215264697469774,4.723049582824984,3.4610213160944836,5.213442670298336,4.723049582824984,4.723049582824984,6.406657683113792,6.059575726240399,6.907510396933096,4.215264697469774,6.059575726240399,4.3810847887602185,5.213442670298336,5.555488028048255,3.4610213160944836,3.4610213160944836,4.723049582824984,4.3810847887602185,3.4610213160944836,4.3810847887602185,4.215264697469774,5.213442670298336,6.907510396933096,7.961621425436382,4.3810847887602185,4.723049582824984,5.213442670298336,4.215264697469774,4.215264697469774,3.4610213160944836,3.4610213160944836,4.55480550564441,5.96649132732242,4.723049582824984,4.55480550564441,4.3810847887602185,5.4778608106496,4.723049582824984,4.616823780779677,5.213442670298336,4.3810847887602185,5.213442670298336,4.3810847887602185,4.723049582824984,4.215264697469774,6.907510396933096,4.616823780779677,4.55480550564441,4.723049582824984,3.4610213160944836,4.723049582824984,4.55480550564441,4.723049582824984,4.616823780779677,4.215264697469774,5.555488028048255,5.213442670298336,4.215264697469774,5.213442670298336,4.616823780779677,5.555488028048255,4.3810847887602185,4.55480550564441,8.811534607711552,4.55480550564441,4.723049582824984,8.811534607711552,5.555488028048255,8.20921159078321,5.4778608106496,5.96649132732242,4.215264697469774,4.616823780779677,4.3810847887602185,4.3810847887602185,5.213442670298336,4.215264697469774,5.213442670298336,4.3810847887602185,4.723049582824984,4.55480550564441,5.555488028048255,3.4610213160944836,5.4778608106496,5.213442670298336,8.20921159078321,5.96649132732242,4.3810847887602185,3.4610213160944836,5.555488028048255,3.4610213160944836,8.20921159078321,6.059575726240399,4.616823780779677,4.616823780779677,8.811534607711552,4.3810847887602185,4.723049582824984,5.555488028048255,4.723049582824984,8.20921159078321,4.3810847887602185,6.059575726240399,3.4610213160944836,3.4610213160944836,4.3810847887602185,4.723049582824984,6.907510396933096,5.555488028048255,3.4610213160944836,4.723049582824984,5.213442670298336,6.907510396933096,6.907510396933096,4.3810847887602185,5.555488028048255,5.213442670298336,3.4610213160944836,4.55480550564441,8.811534607711552,5.555488028048255],"xaxis":"x","y":[6.235152624217932,4.478324711485028,6.139142019145689,4.729552770453971,2.689299160535892,2.4698859762744636,7.592232766722112,6.075532631167357,3.9020735793107426,6.768713657030485,8.05642095591347,6.192983169973992,4.934044647112246,4.088311235888661,7.071462362556624,8.345405246717796,8.140829770773001,5.7554217347342425,5.091276694365222,8.047178455318921,2.475084882948783,5.260778431893426,7.873013392989595,4.4495613746132365,3.4369613378336026,5.575917361118149,2.821710215034674,5.650190123496775,8.870210577263595,2.2868811477881614,5.54936113292434,3.682573297347578,10.919861451442168,7.606886339275915,3.4276061727818994,2.1009776477248208,3.548436624696042,6.819540460505722,4.475084882948782,6.762348815644129,3.929790997718597,4.173127433480656,2.7676547982373463,4.804260115634738,1.9335726382610239,4.846493315964563,3.10936055940423,5.617651119427331,5.739578112048051,5.089159131911238,5.6806056662025926,2.8579809951275723,2.1176950426697547,5.465648025982281,6.027463723408577,6.299391206126829,6.422064766172812,2.4356285940520905,4.153805336079036,8.148324042480084,6.387500406480984,5.944155663440363,6.496494306853713,5.703211467391146,3.192194165283345,5.185866545311334,4.718087583960517,6.035403969942067,1.3785116232537298,6.335211681705785,7.330737650206952,5.452858964713811,2.726831217032493,3.8369340113210915,2.4672794804599825,2.9616233282869446,2.0531113364595623,6.248496885959384,2.817623257511431,8.971342367308225,6.606294160697713,5.901832305785489,5.339137384919585,2.582556003014061,4.779259720372376,6.044831233447812,2.7949356628035362,2.8439838440483265,6.690277401023435,4.017921907997263,6.596487956856693,2.84197311892718,2.3074285251922473,5.976821852360685,5.10936055940423,10.16679075071818,5.2787282129389395,6.339493737901733,7.125981653854716,5.148934104526339,2.321928094887362,2.1210154009613658,2.430285272977781,2.5310694927259543,6.72805655144938,6.989025440631378,5.844988156682612,4.749534267669262,9.159820892013935,4.272769732436511,8.601733652922286,4.124328135002202,3.270528942380718,3.894332742277694,4.943921326553485,1.0565835283663676,3.7322691995014496,10.679004987548526,6.500961352460057,5.787902559391432,3.7782085763980877,4.6661884898727815,7.593353770980297,3.077242998932461,7.602439063741618,5.537296067090842,4.203984165855989,5.299757511839318,5.94251450533924,6.951051533288609,3.542258049766918,3.507160349117523,4.71259578044703,4.6194130105979365,2.550900664647523,6.872951851130105,4.712045448553693,2.867896463992655,8.377687734026882,3.678071905112638,4.0232553523003025,3.1538053360790355,4.635173946674924,7.127117349726852,2.4854268271702415,5.592756010441026,8.25559514727367,4.634593268445757,2.5582676340557358,4.990501111363251,3.727920454563199,3.8308637567517576,5.924337464075348,4.187451054027326,3.940166750482817,4.926473911256073,3.5434958834257717,4.708187236020708,5.568640195419223,4.831370587967541,6.12969515665378,5.751410160064371,6.279471295644468,2.523561956057013,4.552131108253784,2.6016965164809576,3.270528942380718,2.805292455600712,5.533874777258546,5.741197298571427,5.787641414483327,3.565597175854225,4.40735275114004,2.137503523749935,11.52102619171175,7.417936880550136,4.96624587322494,6.385085853547684,7.846744023244868,9.156361220297567,11.010164414182617,7.196430000954568,4.008988783227255,3.2464080872463845,5.763145958149476,7.976535484023556,4.4121040446775694,6.918624755574591,1.9745293124838819,4.546585829008101,0.22650852980867975,4.1715271060388135,6.920055054806967,1.3504972470841332,2.3589588258323295,5.726013748860151,3.2357270598380583,2.6848187375532224,9.236301266827088,2.4854268271702415,5.9618548076361595,4.088311235888661,2.440952198029637,5.456806149230474,6.205157883920094,5.588564737401351,6.620439787651722,8.334139178838237,5.331633567171029,2.211012193485512,3.8063240573900288,8.45889870671479,6.602884408718419,3.8439838440483265,6.132988042627442,2.541019153133559,5.953730849811491,4.18824265626441,1.4489009511451278,2.7070829917717063,3.0107798387532423,3.4019034716079584,8.574290766120185,3.542258049766918,4.313245851787562,3.3854310371935203,6.156032498888866,5.419538891513785,7.3107945270667845,1.3950627995175775,5.8346606579017255,6.025250321561575,6.2268938135713885,5.548128323585586,6.746447139362815,4.583158003874408,5.9576824861651225,5.778208576398088,7.834028534577023,1.570462931026041,7.0083165637349865,10.307269127730322,4.95093492831454,5.099295204337775,4.547203024756931,7.9684364043347875,2.8318772411916733,2.4436066514756147,2.4724877714627436,2.37851162325373,2.6507645591169022,8.202417721575182,4.963011647599427,9.01321099459576,2.176322772640463,2.73768676140986,5.5786368624717415,13.52966114241807,5.72328550477764,5.642412772905056,7.568108130120591,6.034743949324734,5.320845667645722,3.8469946869655733,3.8639384504239715,5.154210530588339,5.777945670695148,3.327687364176047,4.169123281476757,8.135247549809566,6.890446692679906,2.2479275134435857,4.9804823555382605,3.3015876466031866,8.873136468833458,5.8346606579017255,4.189033824390017,8.095608004811197,5.380244590478185,4.571676809970931,5.071247819461442,2.9373443921502322,2.944858445807539,6.706254253573203,6.849624029924543,7.541716163393138,7.907912142840534,6.091065077930535,4.388878338811989,4.078951341394822,2.698218478224414,2.5058909297299574,4.621758856709732,5.3175935046234715,1.996388746447621,2.7070829917717063,5.974529312483882,2.883620816285671,4.853995647176393,4.593951283948411,6.496494306853713,3.9845893503624565,6.212180210044431,6.897240425574799,5.452858964713811,2.232660756790275,3.773996325111173,6.075318692553354,4.161081482277184,3.2094533656289497,4.922673592849446,4.478971805032942,3.0840642647884744,6.054414387917307,3.486714373030702,7.582029046119882,7.461888664432819,5.6604951318885135,2.7311832415722,2.049630767724601,7.850749425896496,2.37851162325373,5.986638471730853,7.210817432089213,2.769771739249448,4.970393537914677,2.889473542531109,7.08533966935737,7.463197379833495,6.09950561709114,4.17951105027151,8.27891401949994,2.4594316186372973,6.221297103163338,4.867402305727543,2.980025300238734,3.4276061727818994,5.933572638261024,2.565597175854225,7.056366761024529,6.199672344836364,4.985955754607509,6.215484464192794,6.7410624357490905,4.59215800212536,5.19377174339668,4.099295204337775,6.714108111595169,2.1634987322828794,8.524737587012376,2.18269229751619,2.765534746362977,4.67581593117227,6.658497136656478,6.557808298936688,4.208673319629471,4.993674361750585,5.422905742612183,5.7824085649273735,5.3847405872923835,5.08278773170927,2.4276061727818994,2.411426245726465,7.603403804166527,5.174326515173915,5.896998342338911,6.425089989874059,0,5.106432077816929,2.9467308601403097,8.843732656611305,3.7311832415722,7.403949364232865,6.247927513443585,8.640931770341906,2.6915341649192004,5.336640446408735,7.390942772802543,2.1984941536390834,3.0125686735030555,2.0214797274104517,8.044284820139424,7.1855692560893685,2.636914580355878,8.491372117815171,2.5185351389821804,6.73226919950145,4.614709844115208,4.15623479785027,2.495695162624069,1.0426443374084937,8.060966074004773,3.5643781685650637,3.679198570566922,4.488000770834068,6.3103401206121505,3.605257262939004,4.821199978056451,7.003039979749236,8.211498981977794,8.84080938879262,7.640895629328568,5.95837871156634,6.804001915179304,3.997292407636508,2.3533232911628965,7.631540867341486,2.7990873060740036,4.706530552538117,3.452858964713811,8.983022086172278,6.187253085582297,4.564378168565064,4.241840183564671,5.187055089968057,5.496654082593496,6.564225720206398,6.793115555274348,8.126549613543878,6.3414521074811265,8.823876711241038,5.094658342654544,8.23429054170594,3.063502942306158,6.9910682753603695,7.485507332459785,6.9092930858238235,6.2992080183872785,5.189033824390017,5.562547724199122,3.2357270598380583,2.851998837112446,2.6507645591169022,6.425425734590699,2.889473542531109,5.012121672712217,4.938285791884053,5.642412772905056,6.199672344836364,2.0738202332916713,7.151270288790165,5.005849075696688,2.3923174227787602,2.1667154449664223,5.727920454563199,2.3103401206121505,6.984019633085816,3.9726926540042644,4.362469888750209,7.173227395028653,9.653203362033977,3.357552004618084,8.556735947914529,3.9420452599160467,4.1026581313637385,6.309067020588001,4.952799477899938,2.3015876466031866,4.580748491763774,5.046578366620329,6.999098033705607,2.752748591407134,6.73348992588162,3.1193561770396756,3.431622959713292,5.15623479785027,2.344828496997441,2.720278465233327,4.763411574470007,7.178117045706399,2.807354922057604,3.685940148445977,4.417515002885087,8.72584338467341,2.4724877714627436,3.619413010597937,7.5130959085510085,3.382667252745041,2.7463127664254587,4.509695841933387,1.6553518286125541,3.0992952043377753,7.846242565109514,2.344828496997441,5.39780296186249,4.658211482751795,4.872828759534886,7.290387539763883,6.802451741275622,5.377123749129487,4.270528942380718,6.325710207515785,3.572889668420581,2.548436624696042,6.845113646617966,3.2265085298086795,6.531849285751203,3.361768359419153,5.599912842187128,2.211012193485512,4.396433531250992,7.406162478777607,5.6519127446457835,5.7114949066500875,3.61706334388818,4.044394119358453,5.772149590784542,5.177917792195843,3.1795110502715103,2.321928094887362,4.854494418154875,8.167066838182594,6.5249725982866265,6.43095427138573,5.669310286202474,4.147306698780294,4.303050084681673,4.3298411765306275,6.912410353808268,4.904002316283692,3.2854022188622483,2.8659188145522125,6.582254908357664,5.198494153639083,5.103077946597573,4.3045110418099535,4.370861740085285,8.654779216959517,8.362951992360674,7.2600256559614555,2.6392321632492775,6.457791260256042,7.31786479839195,5.661920579135087,4.609991295212678,3.460742563789644,9.248900055578497,7.886854944198882,3.7059779016825223,3.0373822220030804,5.982537314399271,5.339850002884624,10.306392126750833,8.18685706717715,5.533874777258546,8.85835405767524,6.166715444966422,6.56727163085093,5.7737326506078555,3.396433531250992,7.532083141492505,4.876271360550127,3.817623257511431,5.643567621908798,4.28318097894557,2.6205864104518777,2.881664619320345,5.006746832440585,5.20163386116965,4.181102550753798,8.571145863602528,7.3433189836610735,7.006298023900369,2.9818526532897405,6.419370342528151,2.414135532984451,8.463115619892362,4.857483428286672,6.972003303808356,4.958842675243241,5.910252962799278,4.4963345134171115,4.707082991771706,2.8094144442358986,3.1651079851445365,6.320484678017694,3.8787253414801053,1.3504972470841332,6.207502459258789,6.152994605492435,2.333423733725192,3.0823619695574735,3.4620523187964327,6.266599165535096,6.2861418728343015,3.6064422281316078,6.75702324650746,5.29020320512607,5.342341397431548,7.833712369021444,2.440952198029637,6.642123916153487,6.722602636851373,4.393690764187454,2.729008870337863,7.036503333738988,7.122569192216447,3.18269229751619,1.0565835283663676,2.992768430768924,4.125981653854716,2.7949356628035362,6.039796407620025,3.4154882710497003,5.535430914871279,4.963011647599427,2.763411574470007,5.062639828286463,7.592232766722112,9.317322159829551,4.955126781261366,1.4489009511451278,6.436128517259249,3.2645364309990255,7.647314509547136,6.15319733085508,2.229587922740652,5.417515002885087,5.0925457415435655,5.34801990922869,2.4195388915137843,7.500961352460057,4.426264754702098,6.247547806890298,4.622344722724236,4.638073837180719,2.763411574470007,8.388663123619393,7.058749412335524,3.496973580998276,3.377123749129487,2.361768359419153,9.114445113289024,4.888986721186558,7.971428593590467,5.424922088210688,7.212958362728285,7.02136867825973,0.8318772411916731,7.720552091605108,3.060047383669939,4.532940288372874,7.506922418047323,2.356143810225275,3.382667252745041,1.372952097911829,9.10522758436473,4.751142324726947,9.036036206482532,7.013685569928653,6.974644026040249,3.521050736900963,2.7070829917717063,6.017253838968937,10.665637927637095,5.325530331567558,7.864928972289788,3.400537929583729,2.851998837112446,2.37851162325373,5.22573765346606,3.8349134297010385,4.126807703142035,2.389566811762726,5.632268215499513,6.783456654360239,5.589164236699772,3.596935142387232,4.365272599764531,2.8399595874895316,6.588864518190148,2.8379432418910273,4.650764559116903,5.37364821133469,4.8875252707415875,7.596264312101167,7.323370069061268,4.88508622529017,5.500164679492167,2.327687364176047,5.592457037268081,5.951401291674311,6.265849421440848,6.862203399053224,3.023255352300303,5.299391206126829,7.7627474419531515,2.7420062108667365,5.932628157022127,2.550900664647523,6.9532652390148435,3.6542063779442917,3.8257856274647914,1.996388746447621,5.713420884868084,6.884964164708779,3.84197311892718,6.009212786805205,7.490730563589641,2.3476656563009706,6.668317719915721,3.8196681834964554,10.872998007770835,13.44366362719111,3.2764966656403565,10.051385697621988,6.023698917266407,3.8787253414801053,6.258895755075436,4.826802684285827,7.317231700222606,7.7475216725404215,4.398487190018967,5.342341397431548,5.767654798237347,2.8155754288625725,5.8011586560936985,2.3757345385831563,2.430285272977781,2.150559676575381,4.505255800932163,3.344828496997441,3.4356285940520905,2.2172307162206693,2.823749360308273,5.130519083027338,4.868390453054454,4.839455765224629,2.2720231890610485,5.525755692829486,3.828834649468056,7.7780114016125745,10.806299887662895,5.515384460636948,3.9699332746978557,6.57334422759282,2.759155833800272,5.033863451866284,4.279471295644468,7.080870811759721,6.138323004056484,5.369815424283912,3.975446765640962,4.708739041359579,9.557597721454812,4.200849574637213,5.854494418154875,5.246788093844365,5.102238193930738,7.479861085016818,3.0373822220030804,6.836681593327116,2.877744249949002,7.240600549955795,3.228049047884462,2.0565835283663674,4.4924941516794314,5.307064162255372,2.9088129077395473,2.682573297347578,2.722466024471091,7.96087076379617,8.903429991567151,9.435816085556171,4.631686366311998,7.457709193357593,6.109569509370688,6.0413306068206225,7.586464525886388,5.687060688339892,2.6915341649192004,2.948600847493356,10.7317008703535,2.6016965164809576,5.20163386116965,8.56017995776418,4.4672794804599825,5.904002316283692,8.44795106244151,4.932155684432056,0.9411063109464314,3.4154882710497003,3.8257856274647914,6.542412837068272,2.5360529002402097,8.739409342229193,4.436961337833602,2.797012977836145,5.624685810525398,6.1705259991768475,5.926948247949772,7.881113960675097,9.500642736062806,7.073606040574941,7.318316841334983,7.971830914780502,6.137913322088407,4.721919445551544,1.922197848396367,8.226941966807852,6.570159301636119,5.3458930856874085,2.73768676140986,10.642557179595622,3.62993940943954,5.175524601089875,9.995102877826515,3.4646682670034443,1.8836208162856714,6.0083165637349865,2.408711861029429,2.950468414150123,4.443606651475615,6.456313341355182,4.8604662585171665,5.661065479806948,6.7997349224788675,4.824258696603374,3.269033146455237,5.9740703670240265,5.925049964727359,5.946496941203293,1.4750848829487828,3.6971065744769747,3.953265239014844,5.464341532761104,3.0908534304511135,3.7676547982373463,2.859969548221026,7.822411496360477,9.401604869714957,2.7092906357233577,2.257010618206024,4.178714641175443,6.423410093269615,6.9070108151861795,3.6484654430273142,8.622783966156124,4.569248029867182,5.366322214245816,5.570462931026041,5.130519083027338,2.773996325111173,2.18269229751619,5.724923070266613,6.9787676510367875,6.751410160064371,4.484138131201669,3.0686708106650986,5.111865963867557,2.0942360698457656,4.315421315945778,4.06177619758669,5.781884234541486,2.974529312483882,4.57349571549561,4.176322772640463,10.888134388352869,4.414812060565847,5.521364878446219,4.582556003014061,3.8369340113210915,3.929790997718597,4.598126959919604,3.61706334388818,2.4489009511451276,4.788163657037767,1.8953026213333068,2.7398481026993275,0.12432813500220177,3.400537929583729,7.515147882149842,5.381629467033664,2.1602748314085933,6.677226328235316,8.025084179448086,3.440952198029637,2.9763636357327616,3.4905701304462013,7.431622959713292,2.7803100990433753,5.659068274843228,8.878265537895738,6.403949364232865,3.452858964713811,8.162995472417697,2.1473066987802936,3.7398481026993275,5.212569338850806,2.8933622107638715,3.9155209007519587,8.562585882154105,4.814550423461808,4.5867647433551415,2.582556003014061,5.077670274232752,7.905507349949873,11.663771557482411,6.559338847606539,2.7949356628035362,8.834123370734224,6.0925457415435655,4.936402377725063,3.454175893185802,2.788685710613534,7.877437521970619,5.3377110921282895,8.303415462685624,7.285032249622911,2.6915341649192004,6.459267666703387,4.398487190018967,8.028624401408491,2.9392265777282085,5.342341397431548,7.217424613813984,7.563234413022304,2.5921580021253603,3.423578170981797,2.763411574470007,5.542258049766918,4.51853513898218,9.535217045453418,5.908092340818271,6.928844036712568,5.612352498752662,6.7786028451432125,6.4202128906107845,2.877744249949002,4.038260575175349,3.4222330006830477,3.777156666004501,4.766065051474954,5.59215800212536,4.534186139090097,6.1147834472498515,5.572889668420581,2.9726926540042644,1.944858445807539,6.412442824775613,3.7990873060740036,4.030336078370959,4.255500733148386,2.5459683691052923,3.7813597135246595,5.135042286233651,5.6061460780657235,7.027021314622255,5.524189078449365,3.145677455195635,7.669948004116052,2.4462562298895643,2.408711861029429,7.221587121264805,4.008092420948722,6.956289026583943,11.260837225974512,5.316869805064333,4.078951341394822,4.509062386361898,8.238500278879492,2.689299160535892,4.22881869049588,5.614415386508126,4.112700132749362,2.9523335663696857,3.693765712217783,7.474030370207135,2.521050736900963,5.727375938594897,2.3673710656485296,2.786596361890807,6.034963989749303,5.578334948681608,8.298429210890312,7.4944956156680504,10.089053172156433,7.896998342338911,8.325215494683038,7.3416300093299105,4.349789870241872,6.438459204423481,8.102815576392821,5.43229133824778,2.4905701304462013,2.873813198359087,6.096978636370538,4.032982416699428,5.604071323668861,2.5582676340557358,4.694323060125386,2.722466024471091,2.3757345385831563,7.995258373404689,7.1078970616646115,7.797726364623276,9.707290101958575,1.0976107966264221,3.1953475983222193,7.672354619892956,3.4208865749755315,6.180903709179382,5.908572758751745,3.372952097911829,7.12525847253797,4.314696525656286,8.915789767665157,5.459103696135399,7.07692245939234,5.79207446192887,4.488000770834068,5.685379551959075,6.442114122419225,5.125981653854716,3.1193561770396756,6.643134661857265,6.657640005207824,6.316507819103109,0,4.053111336459563,5.711219556894335,2.3161457422933562,4.726831217032493,5.8894735425311096,5.981167667105682,6.483976963236257,3.3840498067951597,4.672990993827748,5.239550796613645,6.48767927890162,2.733354340613827,7.754887502163468,5.065227622775619,6.546122758857796,4.556429415449573,3.33771109212829,4.72137265947682,5.533563348214512,7.015582311680932,3.3868109464722167,6.141391916170729,2.097610796626422,6.376255649313362,3.24031432933371,6.5787877956769725,6.052024560482831,6.4571345943180365,6.711081862306696,1.5109619192773793,2.0071955014042038,8.766462652425766,2.584962500721156,7.074676686294496,3.292781749227846,9.20006486151431,7.7525479058471385,4.009884588931844,2.169925001442312,8.76354436429392,6.257010618206024,4.3269687115878925,2.3950627995175777,2.3757345385831563,5.424922088210688,2.250961573533219,2.8011586560936985,3.4892860226258766,5.968551603363945,2.541019153133559,4.888012585402829,6.457791260256042,5.237257770900372,4.161081482277184,3.408711861029429,6.705839705883682,8.251198340973543,5.2671612180222604,7.824386002585445,3.1043366598147357,6.542103245856593,2.2234225499349374,5.316507819103109,3.363171077119244,4.631104282365877,3.9873208659292536,2.140778655782796,7.473705749619415,5.924575003392735,2.871843648509318,6.807870078299782,4.955591791753994,4.214901910160696,3.910732661902913,8.646486708605355,2.2539892662307865,3.816599706534912,6.460578760776026,7.281698250098829,4.901108243014512,2.867896463992655,5.9429835981871015,2.950468414150123,3.8429788317883253,6.045486656359465,4.990047219573794,6.705977901682522,7.860404179081675,2.565597175854225,0,9.146670494470888,6.7014104362328135,3.4019034716079584,4.462706750670158,4.311793717753649,6.649471757181635,2.788685710613534,7.693556651228518,2.8135246892978105,3.343407822297814,3.5360529002402097,3.2675357980687347,6.75127624861109,8.995371450572195,2.6870606883398924,7.605257262939004,5.275752048828529,6.982080909763738,5.305970520984387,2.017921907997262,7.062963556559086,7.2259304111710545,7.0414401300729725,7.778274295338579,2.4646682670034443,2.4620523187964327,3.2141248053528475,4.114367024952,6.111448698487674,2.189033824390017,6.304876050044889,6.0710332444568245,6.157650096861711,4.038260575175349,7.489045125114802,8.475936042446387,0.9411063109464314,2.01435529297707,8.154868729042667,5.955824240806441,2.996388746447621,6.1039172107520665,4.296457407371244,3.657640005207824,3.1259816538547156,8.325890061039784,3.085764553778314,5.309976492370556,3.145677455195635,2.594548549550354,3.631104282365877,5.121015400961366,4.371558862611963,8.492213725013189,2.1043366598147357,5.825022364044702,6.883987308105472,10.959639764364185,2.4620523187964327,5.522306892871389,0,6.593054921669878,3.9354597478052895,8.913068913647477,5.021035479511363,1.7484612330040354,5.982537314399271,7.690137692986635,5.791814071161826,3.578938713093386,4.896271848807375,6.766065051474954,7.801029283808189,6.976821852360685,5.795195490817005,6.883132015657282,4.464668267003444,4.808385050656093,7.161484638641659,2.3645724322958563,5.707082991771706,2.8439838440483265,2,7.723148957047578,1.1634987322828796,7.126807703142035,5.348728154231077,4.4924941516794314,6.174526265266431,5.563768278452033,5.6570683012015754,3.37851162325373,6.43895814789755,3.127633279725874,3.1953475983222193,6.676803353873854,2.6667565918848033,2.3589588258323295,5.587364990936461,2.454175893185802,3.2265085298086795,6.230164577884252,7.884597920990064,3.7687136570304847,3.127633279725874,6.450551443306483,4.084914659767938,4.135863165368679,2.319039815562536,3.097610796626422,5.624978179569036,3.968090752045256,5.105594275795191,5.185470146066942,5.509379148914689,5.266411766036114,3.1342209397606338,4.707082991771706,6.390598905552461,2.8698714061777126,1.4168397419128291,2.3645724322958563,3.5084286525318573,3.9354597478052895,7.11009175192513,1.4222330006830475,6.929672661579345,7.041001987167506,6.332349900373874,3.423578170981797,5.137503523749935,5.3139713710590195,5.999549087340949,3.2433644256936605,4.7114949066500875,3.7059779016825223,6.291677223176022,0.3895668117627256,5.758089934201813,7.001577085389535,7.931505781869729,2.8257856274647914,9.151270288790165,2.440952198029637,5.74604398299834,7.136683577697236,6.353146825498083,10.212070748654236,6.007419783571914,4.997744026059632,2.4982508675278257,8.25176642434399,2.8579809951275723,3.300123724569014,4.640967910449898,2.625270489374693,5.399854673572431,2.2660368939953175,1.516015147003665,6.199672344836364,5.708187236020708,5.402585758232587,5.269033146455237,4.529820946528695,5.212958362728285,5.648753032989572,2.9467308601403097,2.169925001442312,6.088735246190284,2.4356285940520905,8.627898615871748,5.300489844363109,8.091065077930535,7.119252412157343,10.142962207064079,1.7398481026993275,3.729008870337863,4.45022149589718,6.461724991502711,7.904544309292717,5.490890978894291,6.76593249347046,4.502075956045791,2.782408564927373,4.205548911173033,4.139960569545456,6.708325207143167,4.632268215499513,5.6194130105979365,5.541638734436141,3.581351247168777,3.496973580998276,4.862451391325582,4.784503982929566,5.359310316890443,3.41006969175638,2.5897634869849773,2.769771739249448,10.641220865859923,8.202368742775262],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle AdaBoostRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predAB_log"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle AdaBoostRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsAB_log = np.logspace(0, 2, 30, dtype=int)\n","param_gridAB_log = {\n","    'adaboostregressor__n_estimators': n_estimatorsAB_log,\n","    'adaboostregressor__loss': ['linear', 'square', 'exponential']\n","}\n","\n","GridAB_log, \\\n","BestParametresAB_log, \\\n","ScoresAB_log, \\\n","TotalGHGEmissions_pred_logAB, \\\n","figAB_log = reg_modelGrid(model=AdaBoostRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBM_train,\n","                         X_test=BEBM_test,\n","                         y_train=TotalGHGEmissions_train_log.ravel(),\n","                         y_test=TotalGHGEmissions_test_log,\n","                         y_test_name='TotalGHGEmissions_test_log',\n","                         y_pred_name='TotalGHGEmissions_predAB_log',\n","                         score=score,\n","                         param_grid=param_gridAB_log)\n","\n","print(BestParametresAB_log)\n","print(ScoresAB_log)\n","figAB_log.show()\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[1.5746689206952509,1.5667911650776092,1.5760096533927448,1.570602683690121,1.5757867266236274,1.583317247066677,1.580994813595315,1.5570763977953992,1.5603869445950966,1.5509956074085423,1.5607912334819438,1.5634410553604536,1.5619836243718523,1.5674865353717682,1.5665203135056323,1.5680131021586408,1.5775401794275559,1.5805884774166112,1.5829105126737235,1.5876245764541626,1.5933437483448238,1.5940222827941042,1.5941276563506928,1.6014112066655948,1.5960221611976297,1.5895443689551285,1.5971558855023915,1.5970334022128787,1.6019593997095511,1.5978973297806196]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[1.5914712212275097,1.5736504808386436,1.5852345826800713,1.588475954052389,1.5947323631163308,1.6120569337301949,1.5939276460386194,1.5824113921945726,1.5808818008998762,1.5688041953044374,1.585460046504351,1.5719243305836055,1.577356784156548,1.5862006003439073,1.58239727860421,1.5860173188494018,1.5963646987851976,1.595708754785484,1.600318760000656,1.6048772111528755,1.619842279857192,1.615237359309187,1.6172093136834258,1.6211527636095575,1.622001264474751,1.6020199455036093,1.6225691760695349,1.615733281122464,1.6260929822981185,1.6176460843750322]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[1.557866620162992,1.5599318493165748,1.5667847241054182,1.552729413327853,1.556841090130924,1.554577560403159,1.5680619811520105,1.5317414033962258,1.539892088290317,1.5331870195126471,1.5361224204595367,1.5549577801373018,1.5466104645871568,1.548772470399629,1.5506433484070545,1.5500088854678797,1.5587156600699141,1.5654682000477385,1.565502265346791,1.5703719417554496,1.5668452168324556,1.5728072062790213,1.5710459990179597,1.581669649721632,1.5700430579205085,1.5770687924066478,1.5717425949352481,1.5783335233032934,1.5778258171209838,1.578148575186207]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.5467999002073232,1.5657461668523411,1.5613499085917455,1.5356197117478882,1.5411128052576963,1.5530520663289278,1.5618687280931893,1.5134809928044382,1.5438154298300282,1.5206275784552883,1.5185472903787587,1.5595000110942747,1.5383948179166016,1.5447724150158293,1.541801267059248,1.5395072354053145,1.5443525963147207,1.5579284240394597,1.5594981242381418,1.5627691039239768,1.551412627353493,1.5605445716220687,1.574280138303737,1.574541478537994,1.572243823756043,1.5698487764972797,1.56431603066926,1.5764961969297817,1.5658805184045925,1.5665889609115555],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.5816957474037123,1.5705194785948664,1.580155733709372,1.5811857536766358,1.5933242549152373,1.6005578179435562,1.5783203646190687,1.5771075268791088,1.571940207463381,1.5639527175594665,1.5692269967320278,1.5755243590797012,1.5719864840856328,1.5884546066158567,1.5832229501955284,1.5791797117663315,1.588835086953183,1.589129546204785,1.5941020024642933,1.6032517915722366,1.6074944836224532,1.6043990920560436,1.5971424298818655,1.5990615652423363,1.6109292285377363,1.6033733306978224,1.6099285677085158,1.6109953038694946,1.6158358483118807,1.6175234429128365],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.5840949993343885,1.5694104785698915,1.5896573340742808,1.5839501813303378,1.5915326614320646,1.625211899173237,1.597915106329049,1.5860758172345921,1.5941858977525822,1.5719526772074532,1.5778055278399523,1.571393347286405,1.5828499127002802,1.59027772491155,1.5790969014279048,1.5905687025564605,1.5990215961596594,1.596499309928512,1.6098810935521741,1.6057535300173795,1.6308046769363969,1.624967799981152,1.6376913695373647,1.6360936127114145,1.6350323535316134,1.60145679248691,1.6372943952068821,1.6245247204211266,1.6184232049943468,1.6207478943550153],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.595065632362619,1.5541563898278514,1.5754981748795622,1.5794624842888114,1.5807209909659266,1.5495854910701163,1.5743485087365365,1.549271217705278,1.537763457466896,1.5442155987042578,1.549739071601723,1.5537301248152682,1.5527186184263193,1.5517389017509127,1.5541950116854306,1.5564293732801544,1.5841210031504243,1.5674058510344975,1.5772175162395632,1.5718165767748156,1.5813543154542002,1.5860031462885873,1.582455134567609,1.599974017470133,1.5634746191706286,1.5821134193400628,1.5777647382555864,1.577709633599999,1.5810549386446806,1.5925910239875634],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.565688324168211,1.5741233115430957,1.5733871157087638,1.572795287406932,1.5722429205472122,1.5881789608175474,1.5925213601987305,1.5594464343535792,1.5542297304625963,1.5542294651162456,1.5886372808572577,1.5570574345266188,1.5639682887304285,1.5621890285646916,1.5742854371600494,1.5743804877849417,1.5713706145597923,1.591979255875803,1.5738538268744442,1.5945318799824046,1.5956526383575766,1.5941968040226688,1.5790692094628875,1.597385359366095,1.5984307809921274,1.590929525753568,1.5964756956717125,1.5954411562439919,1.628602488192256,1.5920353267361271],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle AB pour le paramètre<br>adaboostregressor__loss=linear<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE AdaBoostRegressor\n","# pour le meilleur paramètre loss\n","FigRMSEGRidAB_log = visuRMSEGrid(AdaBoostRegressor(), 'AB', n_estimatorsAB_log,\n","                             'n estimators', GridAB_log, BestParametresAB_log,\n","                             'adaboostregressor__loss')\n","FigRMSEGRidAB_log.show()\n","if write_data is True:\n","    FigRMSEGRidAB_log.write_image('./Figures/EmissionsGraphRMSEAB_log.pdf')\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["Scores = ScoresLasso.join(\n","    [ScoresRidge, ScoresEN, ScoreskNN, ScoresRF, ScoresAB])\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["ScoresLog = ScoresLasso_log.join(\n","    [ScoresRidge_log, ScoresEN_log, ScoreskNN_log, ScoresRF_log, ScoresAB_log])\n"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["CompareScores = Scores.join(ScoresLog, rsuffix='_log')\n","if write_data is True:\n","    CompareScores.to_latex('./Tableaux/EmmisionsScoresModèles.tex')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"interpreter":{"hash":"117c35e9bc21f93c21d2b781ffd59753bc10bfa2757aefbea5ab108f880d10a5"},"kernelspec":{"display_name":"Python 3.9.9 64-bit ('.env': venv)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
