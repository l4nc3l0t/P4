{"cells":[{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[],"source":["import os\n","import pandas as pd\n","\n","pd.options.plotting.backend = 'plotly'\n","import numpy as np\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","from sklearn import metrics\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n","from sklearn.pipeline import make_pipeline\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, \\\n","                             GradientBoostingRegressor\n","\n","from Pélec_04_fonctions import reg_modelGrid, visuRMSEGrid\n"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 17] File exists: './Figures/'\n","[Errno 17] File exists: './Tableaux/'\n"]}],"source":["write_data = True\n","\n","if write_data is True:\n","    try:\n","        os.mkdir(\"./Figures/\")\n","    except OSError as error:\n","        print(error)\n","    try:\n","        os.mkdir(\"./Tableaux/\")\n","    except OSError as error:\n","        print(error)\n","else:\n","    print(\"\"\"Visualisation uniquement dans le notebook\n","    pas de création de figures ni de tableaux\"\"\")\n"]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[],"source":["BEBNum = pd.read_csv('BEBNum.csv')\n","\n","BEBNumM = BEBNum.drop(columns=['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'])\n","SiteEnergyUse = np.array(BEBNum['SiteEnergyUse(kBtu)']).reshape(-1, 1)\n","TotalGHGEmissions = np.array(BEBNum.TotalGHGEmissions).reshape(-1, 1)\n","\n","BEBNumM_train, BEBNumM_test, TotalGHGEmissions_train, TotalGHGEmissions_test = train_test_split(\n","    BEBNumM, TotalGHGEmissions, test_size=.2)\n","\n","score = 'neg_root_mean_squared_error'\n"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[],"source":["# Scaler moins sensible aux outlier d'après la doc\n","scaler = RobustScaler(quantile_range=(10, 90))\n"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Modèle de prédiction sur les émissions (TotalGHGEmissions)\n","avec les données numériques uniquement\n","## 1.1 Émissions brutes"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.1 Modèle LinearRegression"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["r2 : 0.37202355178140745\n","rmse : 422.88356796671945\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predLR=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[57.43510730696135,43.448974204959896,105.1476923183069,46.096669821352414,29.589923935110505,21.87366237047719,48.13732740617404,5.842957266482898,47.57704865169776,32.333639399776416,119.70200358858209,64.8782430023634,1804.659984681471,23.1167971728527,57.59173751298926,11.966267915740886,-3.3906949886286526,106.89798464835832,119.2357786680105,2472.364274164109,192.08863768467035,-25.103625645085202,54.82715023624403,99.3900550773069,393.32455749676586,253.42358246653876,3.4395009922752138,156.0419807249483,52.19216182926688,67.32020525575916,167.2298226535773,57.91971467509228,27.647784422190142,507.4847004719819,56.54127688349496,64.05965643876242,55.44530214844791,78.32282017114281,64.86746684928424,65.63993331417215,67.45430044770245,55.13197226096074,196.48943537854836,30.10149247012548,277.4557391989422,321.94066158615055,77.60837026441791,-0.7542821605539132,301.523284611018,137.71320911609382,142.19698677663797,73.73058240650604,42.0985498449803,52.442650251491735,40.46050799832793,48.4623635774772,87.87308900538282,1160.0093540045348,519.3171457522143,93.84335340460245,11.06165814785561,154.26848053372277,118.62493355899664,34.739313225684285,39.02132882012346,46.81899602909366,138.6621960690082,48.77007269587643,143.78788873786783,182.5519990950612,39.431180569298746,41.20038050341869,28.75755631465494,62.45287207108019,40.02948042041842,12.410815065782408,31.30214001256047,45.08447747270668,37.80101274362301,79.45813439705393,47.07370741015535,21.94636335042331,88.22453191114208,24.49311418732163,52.74462119684329,129.10935545968275,248.10387990355164,77.82794001674674,50.11186981661304,36.80543449059476,71.0709606185556,46.77050028137674,12.09246148441666,25.5502401919577,231.4031046016821,37.57464594730435,60.64578167794909,34.68874508934243,73.46154069376952,349.710451859705,17.91738294791236,119.09480517613395,13.437870091225314,208.0717369402323,63.566876890613955,79.96030132356283,549.6273869642789,95.03348791604006,-13.270628547728862,47.22727833359541,76.88578734752406,313.91956363019307,198.01856830220373,39.2625359638756,35.22328543882875,299.1512853877541,72.70449752263426,-3.297185336840613,92.93828028242916,66.01871162901529,35.25554204344311,36.54851538122878,75.23672417109148,46.73094526753286,243.3412557715863,25.93908120763836,627.8712027645697,53.64753552207482,119.84275544484132,30.632758936339613,57.775403473956686,26.027727047972178,79.42352867720888,327.3319715779792,-150.39260182077606,-2.723583110664748,192.12304008793808,97.69721379548542,36.023916124334434,25.621802077567388,49.818036243002936,40.5623398078406,605.8241216745184,31.90586413895312,93.89721550362935,69.31619535882542,49.87079579744322,31.04435499973799,228.36917960456242,0.3662103286171856,37.87614610881157,295.5770449180454,-465.7168111807271,85.01989725918372,40.018376190361856,123.21684701996097,290.36678265546436,52.990453875076184,154.1315978019267,50.7875218679387,38.854370004867704,317.12301251642486,93.26853835066592,24.809698418379824,63.27646821304686,0.7906048535574541,288.23123561561823,-6.787206752263813,187.32066643809605,353.76882265490633,67.87022153383563,87.4247718392764,561.4511779522472,-2.4706516698822583,1.7231735651468938,99.45401129112548,40.886165238724914,6.545465621408923,97.13416478989889,98.1828126650524,-34.66061190108931,128.36718549859478,70.34177584425572,92.26691774518,23.93129539968249,287.364313683133,66.57671640832181,41.355510870165354,78.85482764651904,24.956584759666207,54.3383547756406,56.50152651408913,14.686875852934179,54.19125465567292,53.456184382615305,39.96883275716178,56.117318569446645,66.16490740755285,73.05240846955346,14.894021755884069,43.43506305901946,440.8531969596537,111.12395905358431,71.18584194751736,64.9307557230616,141.5048683712807,84.57436473282466,76.76734683404055,102.78819301822377,46.441576502558334,92.39344303625016,61.367456190860494,509.90814136621,43.226915039911574,13.672834923116781,97.63719231922836,41.07231823773653,156.0202864107613,213.13606347111426,233.43355591605368,43.55888947475329,126.76628929172492,44.53129083899538,58.682776876062036,307.60762620744185,144.30394723629365,67.54730558358648,37.77476897519968,4.636182797113662,39.22428420770604,117.32984340958008,109.36672523933052,12.561535299585124,60.3809842314562,16.220789526507176,22.421907508817874,107.71849664209773,33.70809620914512,31.093027714330482,162.74603614961651,207.23624210564853,124.66961898783939,125.47382601941972,46.70477979513859,45.216643448647915,26.049529194591287,64.20346218642204,88.39951902474374,312.0622607725203,50.96289014685381,176.20597282200478,36.46055740536424,73.8912880772932,160.67386758206638,249.66667714929292,97.00108809732575,-13.88806349540954,62.62969155028957,18.362229259868386,44.31370285075231,95.95152743456967,82.31336388173003,54.26463737316943,59.24379100099767,-5.2599065605743505,273.29512003544414,-6.815399904742719,298.5623262401184,25.939161532168434,46.683718808560506,148.85532701240723,39.43499672633723,46.76038865670458,23.73031213820304,946.8544141893926,26.392575557248996,-9.09289709434016,237.0064299607821,21.13173531848711,137.3976901338693,44.5965540307747,58.5640644820465,35.87714915669527,111.61677603117857,17.659400736270058,154.1612208896083,43.315051741024945,37.42215047080966,83.11749581361235,11.025712590429329,108.54042292460919,40.54880872198096,23.534940039124564,54.28945847322598,34.46271114105885,22.112723275460617,298.5436194140154,55.194904861821854,34.26284881453336,59.04857872693032,50.90140940317209,-43.38815756419521,204.5632366340316,66.63350148798361,-39.64619005313908,130.52319460897075,66.35224135350784,52.75539734992246,36.555457749250756,131.07796380412714,69.31767102602639,67.42746586677833,181.45384803937765,119.77395574415655,4.599335862965496,38.55294159617877,52.591690299201815,104.97265635070502,82.47169652995865,38.77835681311731,110.0357459911168,31.280538847684312,46.45783292522304,60.357325971331484,193.75808137772128,28.963402576745878,87.04452553451786,14.802787643752723,359.1794393958541,206.0321621423479,129.95669476407517,95.7986774249569,69.91849201354663,31.89609722941063,54.84723900407438,36.15665989140487,24.296155192407113,110.60948059134213,117.141071338893,61.999563922784674,207.37441256881274,85.42279118031459,-38.86959950700006,160.09893041893787,51.39089148187747,110.64227144405054,35.0910335768918,59.79112584802069,67.61797924144867,193.588598345102,27.105278393998404,272.4303025420011,39.96941764825934,21.386722615008182,110.10708520355031,46.054634295113445,47.525861990155704,998.6791173147226,28.152293020995202,126.71607817928626,396.7258711036138,32.44258934621321,413.03692541523907,36.53163388227436,134.3375168703053,98.98990851060393,115.77970200606555,78.15945403321616,38.33641260414042,11.008864862166334,25.325177973802,53.553273102647694,98.39988419375337,56.754151924422,90.69581753408526,-3.030301435017975,147.870380459888,76.03756902075112,296.2817933634295,76.53645499824458,47.48133083250063,78.94841094485703,39.29684922843582,11.77772599481682,67.01907455469178,-8.233050437563378,34.88661448228167,20.11804314743187,57.46158553110203,323.29691960114434,-166.55400188494824,58.09352861036645,281.90941471884054,70.22492395996511,166.09419720142333,247.76434047366422,18.567568217909916,47.20598482960797,25.551979374423254,60.406405662077695,40.16485999776906,131.88048898654924,73.32764248679948,108.80122459869506,27.378128804966174,222.39132152588036,54.54467024313957,36.20097506001242,17.401907917181433,86.0743687121111,61.05828247733223,39.44315319972331,-17.81935099822087,26.474299914755953,71.70903930782731,8.240511736240016,651.0812837289761,78.18661112965205,54.492670839089904,144.3436292996227,51.61077617246421,41.80349402994915,519.4584318397775,42.25862742923832,3.3581009674104294,36.40795468052322,41.17630085527504,-9.016811877820189,32.77782127766652,63.39745935813078,76.91103565567857,68.51959030317636,190.97734919037666,62.73179551655865,67.85111859861112,64.37657162027622,46.23492345398376,64.09421328136528,169.99288955983894,44.78857206829814,104.05810838986424,9.51686920746986,40.38704973071097,8.604083105614315,23.68591264909287,43.20005851696915,43.03250119814326,-8.883354579527193,45.98880494824084,37.67462033739607,55.63435254327036,77.34842634824754,57.61028037434112,40.32782581934951,166.07929383461345,147.50314952845002,697.8153356657499,30.33077291993189,133.30130356632796,53.232529967769764,43.27309891459145,81.24249559074734,29.509225802286622,110.9180212613254,-104.75063259063347,74.62074969424363,85.23061742704121,147.81246133826338,58.711583270012845,20.302017975271895,25.406897333983913,49.56375604748695,55.6230089914549,86.25340808266259,290.70270607065373,113.51914255392305,36.706938991790956,98.4239988242358,142.75792606926103,5.0359992868693055,80.79422067410829,-318.28332109707634,24.919435897792994,46.42159309205501,54.46336715289536,26.812457280048285,57.06106941760936,156.83594510374303,37.44453114114066,108.21798233099635,37.71712175280379,22.780232794021806,71.39980197162735,30.780537325052343,181.49531223319374,23.31126718494157,811.3727517807381,55.35179009546198,21.19967944242037,17.61733790061392,200.80633131678576,38.85879479710338,160.97892153717578,235.95615532229172,24.242594947907364,143.16125525914623,69.03803379188182,72.99055702982207,7.087813213258656,10.474379002914944,16.966074440712916,55.47107027201882,128.740427844666,38.72799089320035,67.03037773362384,145.7452033650084,294.5656412703328,3.3013790022596368,24.598855161068364,15.513069023612772,20.697279655844028,48.05947387754381,38.06261802219082,-35.2510765695223,40.463844309036794,44.578234075700976,548.5317859334727,76.08958145425406,82.67541360665606,377.86069085171783,219.13524736160187,13.916756292356816,96.35129202549153,21.518349171217295,401.2376780451437,203.78358478916874,43.694876589769606,31.690634007003013,152.38543247882052,26.644216181447625,41.131599939591766,27.897789442795393,19.971494057652208,34.90610128780025,54.20705728020326,78.74445567150555,224.40911748147943,-4.729861916650762,71.77482423833501,122.3596340301436,72.2764956204222,39.06483346357258,61.1708153325202,163.8558357361837,122.87399483543545,55.898955524221286,51.054064735303086,108.9500020339197,21.704194601986984,34.96113048337249,18.334742581583477,91.48945144072042,8.337514519081317,142.4368296735542,86.93891221683943,197.26831252145166,-1.3368858351636277,46.02663678902749,508.3657940158968,20.6035843031975,396.80570242143557,211.71801410373087,67.11129035102671,140.0583597253185,22.41442701798246,94.28275346968744,36.1320363266203,31.485927113990158,100.43032129326836,5.8593228045766494,23.926702012718046,239.71306063953756,19.85322707152787,98.88222879690997,129.68730220653043,148.28115185271628,101.14742346295873,41.846440133951354,323.57506626073337,67.38395972652988,10.159850975110977,33.20974826244126,3087.519180168132,62.335784896583185,37.76560305207006,17.274654664656516,85.16062221743883,107.14974600118353,131.42130692633663,53.87179776631062,56.10053351926549,14.746779585022352,164.4885810258861,14.431887785942273,30.81065138644945,94.62419463423956,278.40841576538827,70.97756941995813,299.3339502352912,1364.5036552755669,33.92931953740205,41.245170962438124,58.902846915586125,48.3223403159,51.894232723419876,244.3469745863058,163.7999808246696,177.49392849271106,258.84397798234227,-9.420234056632765,17.69519616669863,94.01657309604083,37.144866011588114,39.16606114833063,24.2849423230725,34.80928820634653,35.30735649130669,38.53607079140868,99.32272322276495,54.0419685832453,123.36380499111823,176.5019066117714,42.421379919632365,62.65975223283196,33.119296730762386,395.98806871455,26.25294967283773,11.15306363195689,40.761149761877164,665.857982175417,69.36862925014933,56.529922828295085,51.427999782210016,71.85868370690633,40.889774433281005,22.52632889378704,15.581432790531089,50.558842956114006,87.5663651908182,23.548243395185267,884.2134247780377,4.794532881743535,39.012447360688945,16.461218986783464,64.48868907540562,37.903519776136235,-7.054090654086451,54.54434341639164,11.705506041530384,70.33362172556764,191.84338655347642,-2.981311004612948,-45.06776115485037,63.45128975582363,61.96067072232802,-14.818803242876115,34.83665865486927,35.83198019874493,-12.861458862691507,83.18043399445472,31.97080643645056,113.69209290866931,81.62042186369985,-358.1575675732174,35.379106778793776,8.03949921563244,49.975542752589504,46.770609142011196,64.44656905023191,-21.01235421000834,110.89972763027016,29.103960198281158,140.70474962407084,376.8645347160562,223.15970662053644,69.06795266325679,74.1839514004455,42.29297610865452,42.86301519909419,1102.7548524750314,40.16014017090971,82.56719137711269,88.53593722709498,325.49792089784967,516.925733930647,50.64589877629518,72.96730574585106,42.98118952495188,144.1038912730807,4.19379947444699,39.89258733381387,104.78351760736335,21.515049478150416,81.76545871209233,0.39181015374232686,54.314022739799064,27.319807260087117,71.87689158167356,137.59091731699556,39.04216696573156,22.53479333548337,36.46310816411953,33.36872169521665,218.63441620863452,35.23739225827913,151.20867239399752,74.51485570554684,52.69152369001512,-4.923125632991216,27.721238323766286,130.05750280415373,41.934353477879135,56.59444715103843,52.46863065451099,201.46267867023585,25.76678988490471,79.2272594365472,1512.4060324907807,57.64729099397794,120.5965027167482,156.91696226482054,163.07359850209542,23.478600699846574,99.21792822069175,25.98317620461234,159.7988860421941,38.88866298230119,24.12536397507796,195.5854783215902,39.337110507149106,101.3713962310523,14.412160734266294,69.82900072796056,179.33661837648427,63.89535433991223,-15.753500501239117,-7.315567011814608,61.87764908044164,413.5668931052748,-13.595162808420753,134.7448119146898,49.75251267311651,20.295706295095982,41.661900678407356,36.436249473180865,243.2993503414162,127.28909599136259,581.3312634870756,84.13476173961959,46.30960238532772,88.51001242793416,40.58726580677511,94.66709350698996,33.448687043168036,72.93244994462823,24.14079629348587,152.32227714397928,44.95728582066453,48.99000488687017,122.01554726715783,65.13878235850135,55.06680875640322,40.92658592146908,196.7977511659277,58.94187302178216,50.9183093262996,48.77475419726471,47.75743653024065,-14.537834424662321,64.47090540752816,304.1022198258076,116.57874819095932,142.11106095389422,19.967882384412306,106.1350058078539,24.154190718163157,576.3056187665632,428.8651187768023,167.85273534644807,84.8423207836265,807.6158464634923,61.20566544674811,75.19492996450597,43.95559444728968,141.47011443333966,41.633051426057406,14.635551687971443,753.7170364524006,60.21763998579413,50.50125153335297,669.3611065287112,34.83444625875143,43.26146307160607,58.36407812775825,-10.80274512522655,16.440955261469902,26.983779759327945,25.20649725950004,290.01867088907,60.78518501877576,51.563731622643246,17.144321292161194,56.01947607186087,15.309289103980518,42.002093053499124,1.0695765499846743,83.77460407234953,25.82068739224443,47.6934571736304,54.60717281051241,40.57412394399332,92.09930762157104,53.80741282868932,50.31980535633723,60.6763602379216,32.70150122443823,35.08030182793688,287.97071541037565,75.47035411930136,67.15495313391172,19.044572726229845,68.16419377401924,57.81321310481218,25.604526537892475,1.28625364736358,115.94509827133625,32.884474570102924,17.397534066869866,72.09989706511008,-2.8303293423089997,315.40216565732777,37.34994064473614,-37.87549598695543,45.6320666366059,173.8963048535193,91.4894668799484,51.49012929148583,38.069297460011704,15.67505006674476,80.1539967007698,43.02004782556649,42.7009276079772,-24.246138211855353,49.263670313202624,83.51488311292218,56.746491279533394,36.01128506039548,1.0289505144557154,11.299234652181411,152.2931680469492,156.74260913450996,53.047300131588884,63.0444743285909,93.69082018166344,27.30414622879288,80.74028167199754,70.22723202917187,47.58890987810218,227.615421992893,55.596069289724554,63.6705605495325,46.848299715288206,34.506469096648416,37.24310512091945,35.36295222176273,39.90726161173914,93.3379218761342,58.802746553620686,88.91124532131266,506.1512360489641,35.56232458152097,67.69898888045479,238.27003352019503,127.71721064639786,51.28415393155851,41.65441588471427,76.05623023865425,10.721828410438988,19.721695382600736,48.82509677131242,24.88963810480702,107.43660899193719,307.32108412388686,392.2318804000439,-2.167113409967058,75.00633572068483,27.453182427877714,28.169763179518817,440.4811694883487,58.13293306657762,317.6312310902563,325.69756690339636,42.786278274607085,95.1775503728523,58.695966753834355,-83.413248627305,65.59949902714462,158.5150868938881,105.64148698912814,203.86446369767597,143.6818661512358,221.4789884563436,36.785978488398214,45.258676806890634,205.67660594060598,132.69978853142794,67.42893663027152,110.74868007772406,48.13772730191546,162.71487713456776,-101.35965804116213,263.4572664430531,103.14840305777115,146.1937349908,29.362391413516555,58.26283619558364,90.22510115003374,61.921640293645865,-9.996463272396909,45.83673440223983,10.130603562189194,64.62744299523223,207.9017672195746,273.40748597369327,60.894682835528286,103.25149518244062,218.07001554493002,37.4193955380292,25.882292320153375,-12.157518346234667,36.5598495368773,52.181940899477134,47.471110347194006,53.75874011409683,9.512103158502441,70.71040499650111,50.19953729715402,145.54231635557468,84.15497847953603,69.35640434439046,150.0122660250937,22.009884567658872,-6.898568983404488,60.58382745278011,162.52329450205124,173.7497948770848,46.95632734271891,60.1602322086086,189.92025667086253,22.53475641794381,932.0890946054676,510.0041049530485,72.37225078720392,117.17957305562392,13.141650974603841,164.45351045217905,7.548881748333741,39.00424668876202,142.2538593548171,87.91188379678186,43.094015824394944,70.68644655744129,314.0919760865237,35.008968842908246,108.62002962226953,64.07597157575994,527.5547593956776,190.51525120562968,41.135636229871324,40.31455144264246,40.71278274385375,-44.91341448923603,64.08855428480564,58.42142635903189,49.67943641004261,112.37182672650738,1130.8793542316282,532.2331913518221,40.5943984268156,15.723173232986724,18.635643520049946,33.34303456617432,50.23855766733314,14.110177231782437,83.27982658776847,67.56825997250492,96.72406739343553,259.7942381667466,119.51035348932072,47.35747889936893,39.84549946442306,67.22066410271229,-42.849222477918545,75.48283446304781,63.06255054839798,45.86324090618655,89.43500827072592,79.75413778957065,282.1653210826341,63.977983322340975,117.4521739609083,59.31602466107374,1070.0345567477827,32.571749816330545,12.42201241976285,73.17739134018422,210.6950164836485,113.63335878787811,95.56581790551911,1918.8471096257633,13.40157116445959,93.78486814140535,-12.441039782435581,111.52406675558103,18.215006404554657,162.8356785231569,46.90139722211638,120.93283338950594,47.711134201246026,24.775990573962034,7.662416138577953,180.22422017668364,68.1017783896605,60.13844902266639,118.97605630880938,35.65498850931769,677.854145781866,58.91447599957628,214.61767726857988,520.313864278894,36.359735376595154,329.2792327908747,42.86978153520889,759.331435099266,58.333975285791084,30.435660239821814,35.4612893307768,39.11978368178282,85.52236402933816,92.19462211732383,47.93554513309614,23.691260909676416,138.53243347932224,998.7155518904881,59.215879841526885,19.02127124418776,841.7168509676874,111.48028770737531,78.48280926762754,40.11046846313852,261.9084621748885,13.846338264652317,44.058253751006056,23.24314824243322,177.7384069159479,54.97914127586224,82.14318335903646,39.54575289582078,50.81121672429728,47.06073199206827,11.918144749498921,1637.4289879357343,-100.29523928305247,45.467633076146655,134.4197307719548,98.11992341563553,99.40586920856798,44.211123542822286,25.043681737769113,28.01717412500856,81.93593025264062,28.71858414322496,21.67592280042659,37.07297333596373,518.9465298666787,79.36059049636654,30.322903651586657,688.7077753594447,35.601834366110346,131.6514962734209,87.45160642020052,95.26613951909864,44.56898917724983,64.34809629825432,117.27604787116888,46.015532746484155,603.5532772966085,-212.46090999419238,-27.144297242652712,38.3760898038795,36.843185723674864,43.18873982722736,35.327635212524065,53.84168384378218,26.663606589091785,149.3648628031586,43.091394398042375,49.8602414453092,230.7110428299419,29.69598652488419,51.054770856046176,14.794380761837957,15.841774754027035,44.05766896487607,-177.87088322065375,227.2654400140811,92.11141344347419,36.1306536746534,21.7998538366745,31.981325137019248,323.8956035718704,191.0049300583675,69.86157238987647,64.08988298264772,41.544645489915794,10.92997053568834,53.19364096296884,109.20483287667948,40.673287690710026,114.98433357006348,36.73541096098815,158.34008544455588,58.4487631747662,15.587176084467508,25.999549314398926,239.86985819189448,68.94759220538842,-16.419833546453333,59.15737649578473,157.9927293147537,18.80429460251886,72.3965075474482,-63.70269228446858,35.63046730983237,-15.914423663480505,139.61352639967964,162.32238851736764,47.237181144832896,46.52065063016258,55.07787073699242,27.93721400377801,-412.8117471672153,31.032750482486165,85.00220357910999,268.6733972247552,129.8528032471123,59.98249622109231,130.60433877743097,26.08280511474498,140.1516811929775,24.123281944684585,22.207680345768907,165.36381557290338,190.4895308163882,168.15109588127206,159.18832939038333,24.924324801128435,109.53441581711694,1079.463418913457,27.853592235653906,3069.1547179708286,112.73291356791702,58.73054019073679,69.83817838940932,47.36023383214939,4.857618194385196,14.633129824190277,58.18526110890882,33.3253485708112,874.2595617473723,73.74211350355942,51.320965419746585,45.66842389843957,-10.55647859608328,47.35305410713325,121.85650486934938,138.10786348569127,333.4783617718778,26.08037664642299,28.64881854944248,30.63339347100389,522.7034351839245,28.685398201457353,122.11401564422354,89.44523203152744,185.47058874163983,34.88231972752871,34.83640194571668,36.561330003777506,-14.956623943200242,69.1755446273824,30.001896956447837,3418.0925388859823,55.52467452078376,21.98344972553737,115.56812031766336,43.52807792853954,270.24007263480627,1098.9979471577856,42.034125762867184,98.1562699025224,100.68682742343768,24.18030868653217,74.58786477743558,109.71398607684947,11.58218431646472,27.586094040043186,58.25407231639623,84.22752775542335,46.468378689630114,27.109812635501974,63.181443257327025,131.8719887591434,260.6741410172386,66.28600316001805,40.30215270881249,30.062358927805256,1.7040403980425225,89.17969649756043,51.29634323494031,180.05712768510608,111.52884606037033,13.66501178820728,144.63318285641643,43.69667765279233,49.549420915486294,145.89706097183915,55.349995459486806,29.839710431990817,167.8497955150159,43.0393426913089,64.67207177376947,111.54001403487835,-8.684134465189743,8.110583304656558,30.50177541420443,20.460908122012555,51.008582331511576,76.21397592404838,103.51305969906025,107.78310871763252,267.8310019962728,685.7096853521687,246.3270691117731,93.36421392039568,9.626077889813402,153.58584833266102,43.07978988079056,2.398742004673238,127.51153190218496,37.05780915225728,52.78485169422587,-2.311834646363394,58.4149924468686,62.52909784277222,23.231690872610116,87.16258177457863,90.9661683333652,54.17016645854167,50.87902873284109,61.70093541871572,70.63739592461238,37.30453145007846,112.51221649279557,59.89041707942559,3.8858415880706616,38.28628246528143,46.80821987601448,108.4508211587113,87.60117874257367,102.37810049060806,79.02232910739336,72.28647252768616,129.64839399773058,81.93280542898097,33.85839778737132,276.6729560573961,146.25536070784784,99.7272322862614,71.32516528975076,53.783028844604516,-2.0528649192033157,66.15421077622742,75.27583803471887,69.09147357490446,111.76006062408818,126.89918494639977,101.33990192521085,70.39040680522945,3.8387986166021406,29.103541067729218,5.869678421448967,22.45033366869349,71.91216063829754,63.36303748915267,41.32757215546084,51.9390231824393,31.050424252577344,43.00380769004698],"xaxis":"x","y":[7.8,4.52,27.02,82.38,50.55,18.65,6.35,10.65,5.74,3.59,49.58,6.87,429.27,3.3,41.51,4.03,4.26,69.59,12.61,834.96,404.51,6.53,111.96,2.15,3768.66,531.96,20.79,19.06,25.15,10.77,166.13,111.62,3.79,292.8,25.38,5.37,167.98,11.32,34.2,40.06,22.78,72.5,355.05,238.77,13.86,184.83,39.88,8.93,37.62,26.44,97.97,8.1,69.79,64.06,7.85,50.4,68.05,3995.45,202.97,13.71,27.41,63.45,129.65,59.18,61.21,15.26,17.83,9.63,47.24,62.23,6.52,12.63,144.41,1.14,5.07,44.63,49.46,3.88,126.92,106.37,27.45,9.64,26.41,4.86,60.49,196.67,115.34,9.57,5.1,65.08,9.51,40.94,22.29,65.89,298.61,72.71,45.22,8.47,103.21,293.56,66.43,83.55,9.06,96.43,46.79,18.34,202.49,7.56,9,23.14,26.71,271.58,329.29,7.79,5.53,141.95,23.5,60.67,109.16,358.88,153.73,60.59,7.4,3.75,546.2,4.46,131.4,65.65,147.69,21.09,28.27,5.11,54.09,165.86,277.98,37.82,67.56,71.03,3.82,11.46,22.39,63.14,224.36,107.56,29.7,7.08,43.86,4.04,1597.56,4.67,4.41,26.35,866.23,284.42,73.48,78.15,325.94,129.89,47.38,38.97,7.05,60.53,161.06,20.76,6.19,147.84,69.42,134.71,54.11,67.48,11.17,0,122.99,28.52,33.75,94.89,64.57,152.04,14.05,549.74,21.75,31.04,115.04,272.54,3.96,198.45,7.84,14.37,16.66,14.19,2.57,68.14,9.29,7.74,8.45,16.88,10.63,189.18,37.97,43.57,12.36,139.27,46.72,7.74,18.85,19.76,86.02,14.05,12.97,85.88,57.03,64.22,36.09,4.68,187.92,29.39,10.81,74.45,234.75,3.52,40.12,30.87,5,70.85,698.11,64.49,1936.34,5.47,4.55,5.72,48.65,288.43,2.9,5.87,36.71,5.49,17.87,38.87,3.34,47.64,8.55,3.72,13.74,21.52,40.01,30.12,25.44,20.18,352.87,135.81,161.23,39.5,6.26,94.85,548.37,16.5,141.15,22.92,23.74,7.4,64.54,79.98,35.11,9.25,27.04,234.6,48.6,26.85,3.22,10.55,42.23,66.6,27.93,5.27,324.25,9.9,47.34,107.84,4.36,164.06,6.93,5.28,60.43,10.73,4.53,238.09,174.97,4.23,406.02,7.98,7.1,4.65,2.43,6.51,3.47,3.95,317.82,47.62,49.89,253.53,1.12,33.09,15.48,61.36,31.46,35.8,7.37,134.8,18.99,10.55,66.79,31.94,691.26,87.68,3.15,15.9,45.07,62.2,3.18,16.27,139.62,4.21,11.94,7.03,47.14,40.28,82.66,4.01,8.07,309.6,4.39,3,28.19,77.76,5.41,5.47,5.94,96.39,570.98,18.1,46.03,116.06,48.62,30.27,4.76,67.57,6.61,18.54,7.96,134.01,13.35,80.08,59.84,4.81,105.32,5.91,44.51,674.09,3.98,38.08,177.51,67.24,52.73,13.99,74.33,160.28,466.95,10.01,6.86,5.07,3.72,5.81,49.55,13.87,53.61,143.34,64.23,1.68,54.51,25.37,39.58,46.26,5.18,85.03,192.12,4.73,101.27,21.2,73.14,143.82,184.4,78.06,185.82,32.77,14.49,23.42,6.53,143.45,11.14,21.87,4.67,278.97,6.85,64.39,3.77,95.86,112.06,6.74,290.57,29.92,69.57,24.53,112.82,20.67,12.03,61.18,627.87,9.21,16.67,25.12,10.8,14.83,320.35,3.49,8.07,4.6,6.63,26.64,49.53,52.38,24.88,115.24,402.03,27.14,67.96,25.37,88.11,48.15,25.24,4.87,20.3,12.19,10.01,7.11,96.18,24.76,4.76,24.96,6.31,3.42,41.06,24.1,54.98,20.76,42.74,99.45,145.88,36.48,14.17,1.61,22.02,147.16,129.42,147.36,100.02,9.47,7.82,19.73,8.98,160.3,35.14,8.22,5.51,8.37,639.39,46.23,57.71,31.72,14.86,8.09,8.7,173.23,11.73,23.17,13.29,10.92,5.91,42.93,4.38,377.36,25.21,75.67,0,4.5,6.09,7.28,272.85,171.64,15.34,30.22,15.23,3.63,43.08,9.47,4.32,31.97,11.18,11.24,13.76,6.12,32.87,46.7,30.1,104.17,91.49,65.27,211.49,6.73,49.04,41.27,13.33,19.11,0.76,12.06,56.63,18.74,443.73,44.19,24.56,290.88,119.2,26.34,47.78,157.07,95.26,1623.34,24.51,6.16,38.55,7.52,4.6,5.47,6.69,5.02,23.79,7.07,49.25,43.91,8.1,0,103.19,18.1,61.22,36.24,14.65,28.03,10.01,39.54,7.05,3.09,3.5,9.24,27.13,9.68,49.32,66.39,8.2,123.99,111.69,19.12,142.39,47.71,67.27,10.89,39.57,17.74,78.39,25.94,180.38,14.49,14.73,5.32,4.39,52.09,205.45,19.7,8.29,89.02,177.59,20.65,26.94,20.19,10780.64,13.84,8.45,2.73,8.81,332.63,16.67,29.39,6.18,6.16,67.65,31.52,22.97,8.04,509.69,13.18,26.39,277.98,61.33,4.48,72.93,5.38,6.45,109.79,139.83,54.92,160.8,35.72,6.22,51.93,93.87,6.04,3.72,11.79,18.95,3.53,50.57,0.75,150.69,398.56,113.3,5.67,4.52,61.46,5.69,4.32,154.4,173.47,91.53,13.91,25.39,44.26,42.1,0,12.28,25.48,28.94,22.62,172.46,23.79,156.67,16.3,243.38,33.49,57.41,16.89,62.83,7.56,18.21,79.98,19.69,27.7,48.74,6.17,26.31,18.12,119.81,63.68,23.17,105.97,9.68,13.16,18.42,25.2,6.38,6.83,33.04,4.05,269.71,2.62,121.9,282.34,142.36,310.19,79.42,3.41,3.96,723.47,24.54,151.12,7.61,237.39,200.5,6.7,29.84,5.55,65.46,5.91,17.36,6.71,15.68,9.87,5.53,61.19,4.65,11.54,32.99,34.03,2.52,5.09,50.05,81.92,47.84,66.48,24.91,174.98,36.05,7.36,60.02,111.95,0.89,28.41,67.53,16.13,9.67,913.07,38.68,17.46,85.59,19.84,31.58,36.43,52.41,16.27,12.97,21.69,66.05,12.29,68.64,303.14,208.53,54.05,66.53,191.97,119.92,20.46,28.88,12.89,119.03,8.12,4.52,65.77,5.65,100.42,34.92,359.11,154.39,6,328.17,4.01,27.37,4,17.87,17.8,44.61,12.59,38.15,184.51,19.1,59.05,4.94,23.83,126.55,4.9,4.13,6.1,42.4,6.61,177.1,321.71,104.91,3.28,11.29,4.75,139.72,479.7,21.55,124.4,271.32,6.98,12.46,43.28,25.59,4.8,5.63,208.81,15.43,21.85,122.54,3.01,3.5,64.81,150.88,128.69,32.04,48.79,124.92,56.3,71.01,26.11,7.51,6.16,4.27,4.03,7.53,5.32,241.49,61.39,4.47,61.98,2.45,8.33,7.02,5.6,17.41,58.82,14.97,39.05,87.27,60.34,192.2,36.61,26.67,51.65,85.54,1.55,16.42,81.24,116.95,3.07,3.92,11.44,243.66,36.96,14.12,14.24,16.72,35,45.6,24.87,11.97,51.24,9.29,31.79,18.35,9,61.86,4.19,1154.79,6.18,36.86,9.99,3.18,8.1,26.27,18.72,62.41,32.33,22.53,4.8,4.46,30.45,75.02,19.35,5.24,6.25,196.34,35.69,40.76,8.44,21.08,14.51,75.58,7.79,226.73,1.6,29.57,21.69,16.16,28.08,346.25,229.98,67.17,145.02,62.67,6.91,142.16,5.48,295.12,175.27,27.81,8.14,2.18,445.62,120.68,0,13.05,50.29,935.7,87.03,13.22,34.69,45.44,10.43,8.35,50.98,7.2,119.63,39.47,515.85,190.17,9.13,4.26,148.25,220.26,3.67,6.18,6.73,56.03,54.83,23.56,190.58,29.88,41.7,31.27,6.66,4.25,37.31,138.03,42.8,4.72,5.91,31.92,82.92,64.46,71.38,68.16,127.74,66.37,21.18,726.96,70.16,16.99,25.11,7.24,38.55,121.98,4.55,2089.28,138.85,11.08,12.99,3.42,130.32,15.25,3.06,384.27,73.48,11.87,188.58,99.59,17.51,712.39,62.64,180.14,47.02,3.99,6.49,2.94,44.54,47.26,20.08,9.79,2340.63,242.19,262.45,32.17,4.16,35.98,45.3,23.23,6.48,67.52,6.82,29.26,250.05,17.24,7.63,15.6,14.4,51.6,18.41,12.22,22.72,83.45,7.25,563.25,110.42,9.5,28.46,1280.81,4.74,2.69,92.52,48.53,59.34,51.49,452.16,5.29,21.05,22.01,39.57,103.23,256.53,55.23,54.55,2.01,4.68,16.8,25.12,48.1,53.25,68.34,44.71,467.9,28.47,9.76,128.22,9.4,19.98,22.33,391.7,41.13,16.11,8,6.37,9.48,60.79,59.11,3.65,50.55,313.3,30.19,49.15,13.43,11.72,59.74,1.39,276.9,78.36,23.12,3.5,5.83,38.67,51.67,149.39,12.18,9.58,16.61,318.97,333.3,101.21,229.55,280.16,176.88,3.04,24.31,6.46,122.7,10.14,76.48,5.86,399.54,485.76,40.56,440.31,10.75,56.8,135.56,13.56,82.87,7.12,60.7,3.16,376.57,25.14,7.86,37.34,4.27,44.15,326.65,38.59,43.8,355.8,4.33,5.85,25.76,4.59,92.3,384.85,19.55,38.25,92.92,135.11,33.78,127.84,16.04,103.07,169.75,39.75,103.37,42.57,22.46,9.88,10.82,101.43,21.3,70.79,9.58,22.17,7.12,74.52,2.64,128.87,17.4,5.75,5.08,138.96,4.84,41.65,1019.53,28.07,37.13,78.85,38.62,45.08,53.31,58.85,55.71,131.35,4.71,60.57,56.96,9.65,6.76,49.72,2.35,28.57,4.52,4.47,85.41,218.12,103.39,320.96,25.49,53.87,3321.02,216.18,12307.16,93.23,79.58,8.36,6.34,103.31,17.68,8.44,69.36,4829.86,46.6,56.68,45.32,39.1,5.95,16.76,313.83,211.52,477.87,6.01,26.52,351.53,46.94,7.34,85.22,145.35,4.13,12.28,6.46,3.92,17.84,4.21,3243.48,50.42,36.08,125.13,15.5,67.23,765.56,4.72,83.59,52.46,54.12,5.09,19.57,34.87,9.71,14.44,249.01,30.49,5.6,5.68,22.08,203.73,9.63,4.62,10.43,188.67,114.33,6.13,168.62,279.35,8.04,282.72,5.31,9.2,112.01,21.37,2.28,57.59,5.6,53.73,51.14,26.1,18.62,64.67,141.2,8.29,25.77,47.96,51.98,218.49,256.76,92.7,3.13,3.16,100.65,23.72,235.32,379.34,3.98,377.27,137.33,63.07,9.83,85.55,10.38,14.35,4.84,2.17,29.77,63.17,30.14,101.83,47.92,47.84,5.19,11.22,44.97,58.36,11.58,9.99,7.98,22.94,107.71,267,97.01,53.48,8.16,6.85,37.84,181.35,31.61,303.81,225.35,39.35,51.56,157.13,62.94,124.59,4.5,49.79,13.65,65.35,11.39,3.93,4.17,19.23,92.45],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données d'émissions prédites par le modèle de régression linéaire<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predLR"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle régression linéaire\n","pipeLR = make_pipeline(scaler, LinearRegression())\n","\n","pipeLR.fit(BEBNumM_train, TotalGHGEmissions_train)\n","\n","TotalGHGEmissions_predLR = pipeLR.predict(BEBNumM_test)\n","\n","LRr2 = metrics.r2_score(TotalGHGEmissions_test, TotalGHGEmissions_predLR)\n","print(\"r2 :\", LRr2)\n","LRrmse = metrics.mean_squared_error(TotalGHGEmissions_test,\n","                                    TotalGHGEmissions_predLR,\n","                                    squared=False)\n","print(\"rmse :\", LRrmse)\n","\n","fig = px.scatter(\n","    x=TotalGHGEmissions_predLR.squeeze(),\n","    y=TotalGHGEmissions_test.squeeze(),\n","    labels={\n","        'x': f'{TotalGHGEmissions_predLR=}'.partition('=')[0],\n","        'y': f'{TotalGHGEmissions_test=}'.partition('=')[0]\n","    },\n","    title=\n","    \"Visualisation des données d'émissions prédites par le modèle de régression linéaire<br>vs les données test\"\n",")\n","fig.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.2 Modèle Ridge"]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre      Ridge()\n","0  ridge__alpha  7028.244264\n","               R²        RMSE         MAE\n","Ridge()  0.261748  458.512778  104.953918\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predRidge=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[62.7707436821015,58.86142683817894,80.92708575795811,64.21858915219264,55.07674867536284,49.44377289566198,58.112911625179294,82.82247877765008,61.95517431082395,54.75678918016006,107.19087303920091,68.98095635568409,1283.4367789859857,50.40432583410117,73.75374016541168,50.436111987113044,50.716615322805346,211.77829166885317,102.47461981842436,1750.196681835549,253.18789379213246,52.982596903016365,70.0482255766442,70.45475321984915,799.6943005800085,104.99594567604123,47.92300317885012,113.17549788352757,73.88625793578842,82.832265216809,116.62846846779077,69.80177785150582,52.54399096437688,286.20639832297815,61.89566070169223,68.59439576085889,61.461087008674376,56.68234083692752,66.01337437671489,59.79891725171301,63.12811035285371,128.1321025956323,294.8739875791123,118.87931144200479,157.04575938445916,254.53163346225125,74.99249598439816,54.09835435711633,172.27117236391854,121.43057835064693,92.6140914667493,69.05444801978227,96.67824746750766,60.14601140228697,73.79058328170716,68.09350984446837,85.09098971170376,590.2408437650475,275.94179906734604,62.13007741451005,46.51620356323338,123.51417772446835,91.18414709918991,50.64162660572997,72.00757199920184,56.34600727531267,99.71706125633591,58.41171251739553,114.76688203273844,123.87015981836664,74.77541742413939,63.050767256480896,66.10366724702293,55.51509677443773,66.60467663355325,96.94929957867879,48.90677913593925,55.526916717629,53.23180054673782,202.97730691951463,53.503796300070576,71.82222079965163,71.79207351308118,54.71889371478287,52.07492543617244,92.24343645174214,143.1849501640647,70.98934051063706,50.83166298254544,52.76165928276123,76.29949323634574,53.41359089767945,46.44820794831581,49.118257996283845,200.22446039377965,129.48302290413056,67.13490335238184,55.61801451274012,78.28543372605142,164.51099551129732,47.53672160072516,86.38033200985105,48.5401732870288,187.00422312523511,80.6822190594716,66.71208930244715,324.70120325160843,80.25865467520578,52.93490472188283,79.28880215594194,84.00930622596687,321.64264179470297,227.4554575735585,44.5639395988762,54.97701523936786,300.34567965800915,69.62695083846658,49.38999647188245,102.09564138784091,154.5683260021012,74.81431712961154,59.69862852361062,70.69244831157931,60.28126486002457,137.0495729092317,56.15199872668075,521.8009245624806,51.356958484155825,96.08116152533852,54.16053469163812,71.15748040335582,57.91668111675009,96.30827727020326,247.51386851465946,131.58654048274298,102.92758202232685,124.13331964192001,111.44918969359415,56.49944613196665,51.292093005562215,95.09836254745764,53.39143062081086,375.48002467375295,64.1938565273154,73.42387532485567,69.03552938547418,67.40307339285854,50.04110850188332,260.17310976068717,51.346386660854975,57.58146073364381,221.00492002521617,931.9791044476174,78.49243650511283,50.17206187602309,85.23994712155103,183.67694671659757,51.04666524993128,120.4870445187288,79.45420846524861,52.5848771634999,189.31295081027912,75.31835094858467,57.03296290898557,68.22455129986396,223.5649072225786,170.34795048108663,50.72346914121947,152.2980825601568,217.02471060965948,69.81244622101389,79.62808884720741,536.7211555528979,39.37626047778223,53.98537177537635,198.50938238644565,60.96890103119987,84.21612513974877,80.71013863621921,73.53217272406431,70.89811017295806,139.23336280960683,80.8711613760756,82.94436548927376,52.47307518814812,166.75221848620052,78.1276748830292,50.82909706266007,79.79756284041275,49.428027888015116,57.122471350978614,52.70469677422207,56.00506486905461,52.76358917041722,59.48028236848998,65.99282161635423,69.92332591356002,113.14491124077239,66.44709936810712,105.48647150023618,66.67858410909261,201.3607383573097,90.8195404465792,63.97604764367982,56.685226142557255,102.20380411926891,68.92899582965289,116.10803343843665,82.77630437771253,63.600531151185066,82.66846718796323,64.36056664424606,271.1958106192068,55.99267514327583,82.8316184783472,84.45069345724289,61.84594290665741,119.72396938109397,232.34000193575088,148.9677459645544,63.909446258532526,106.4308633854979,60.9872006963667,87.73383881761814,172.28194382061503,119.27657784010589,96.02677213557457,58.27247244219303,54.114828278424014,59.17197146247784,82.56517347244595,100.74491239728947,51.49649695061025,62.75037604903831,110.58514419181995,66.8383461718564,76.890852909907,49.10022835299977,50.06409318589995,128.75405304206805,125.03040202169632,86.04001986257057,127.95451122200797,49.22273510138109,51.01038803153832,269.213405892606,68.66230505454439,97.24747071668335,241.77085923349495,57.01658493274201,104.4439751736036,51.45444861322728,74.3815302969076,106.00477567184032,104.3661743379916,71.65683081372379,161.85641561753147,89.59768773969424,74.64002893588031,51.995741964955734,69.63127901116546,69.83919301098571,52.66633710743035,86.15117692558805,52.90714099584017,209.27911439509086,63.31730107684675,180.47778695932396,49.83370243157896,85.87633323032546,121.25957743503484,84.17416798214582,64.53201666151037,57.7068295606634,836.1368682489236,74.13529010736366,54.782508530059914,145.23865983044152,53.573764133077105,172.46801826060846,65.16762434019401,65.99921598370807,57.66681555970213,84.47664507513251,60.147711363017265,180.61261355010583,69.86093880609762,57.15973341826267,132.040619227929,58.47137307381766,85.49267612513287,54.52939043531323,50.60178516497134,65.1249711883906,55.41741818805676,54.017773578429924,292.006725021425,72.36938062703781,55.667818473910266,54.914051469194746,54.16699990578077,125.2723721065781,130.94484569203158,62.74050499966411,86.73510612817716,107.06063472354754,65.5701788745335,55.04250741514163,57.194407656232386,88.62298995592198,67.49550249866948,66.07793143962822,170.47785818311354,98.90024154456452,46.278290467062746,56.549376577778474,76.2604253589645,76.73853614438471,59.16937349152648,51.79222219240118,87.3431611441509,54.25948419870926,60.282298213752654,71.50327051400205,138.05548321867303,51.60393136090896,76.48603222138469,52.84537440340979,195.6380886067514,138.7777387387804,93.56170660342725,72.915653420495,71.95905763147262,62.79404983185563,64.24402193152878,56.56213163383019,54.07329845120557,81.30264970439818,71.35994535546507,55.34284280174734,174.7625776574085,84.24575302466276,86.37583114987913,102.77078030995105,50.29130495247554,74.4450851454754,56.12139211967664,56.63379188897079,86.00439254690959,198.7180659068988,51.89647720748955,162.88628624259584,53.11143537915368,53.362299252247745,83.2700068599755,64.19873874326917,70.46505009592212,689.8854346403783,57.343482313108375,92.58046200129075,319.9890118759945,50.70139578817934,229.28485933319277,58.75547629893217,96.25893357009049,73.90855576509107,163.96761145601397,84.18309598473022,53.484632070920796,48.263129623500205,51.4127420367128,60.67048010121198,72.49033067908604,52.68145491865457,102.39438547579985,51.3532404792691,174.82356526594427,74.25071754567954,184.74832613639637,149.50879529274863,49.589445287282835,89.26265161412687,52.79382883637838,153.67916631862863,59.47878592710701,53.52507158433494,57.259942397455404,77.33882222867429,138.09768355220336,304.08814812206435,118.99029619999337,142.41897307790515,233.0354771310167,64.34620538097737,116.56991917754901,167.66566620369275,52.362889188746266,56.386813594692384,48.39227744512505,54.54869528737473,59.03094838284804,122.91082810244097,77.96082552568747,114.50966514761458,51.272302079232276,172.14472614426285,74.94859365139563,69.4596133975145,86.90265837588254,67.81421019574451,63.88268838049663,49.90042470128106,69.93064769055913,57.183443891294075,89.94539726432674,76.12813879559779,817.5306640091111,85.007859575633,51.75605617935374,102.40000073609569,50.404643554503366,53.97754006323501,272.47088143821463,52.67264938713579,49.23072081094736,58.40117704479841,61.89504654978386,52.27736965779054,64.23758573558747,65.46779543708038,70.92315169355172,71.84485698816276,181.51717684365988,61.07071560341062,58.06430718355528,64.63720965513104,56.070191067113065,57.43453584550564,129.00783542288343,60.62887124773651,83.3759956788738,79.12729918209925,106.19994805459166,51.09560233617146,51.940669524036394,50.53019603933069,62.771598817509116,61.69801531355101,60.06080944050146,56.13460750711468,95.39351655096901,61.21320520256454,56.93429654341887,53.28068623418526,104.4505338042151,109.92070444245081,377.1801290994122,78.83774641314491,98.32984010410176,56.48647389370015,86.36180895961564,65.53245792791142,64.92807394306391,162.27191605805467,179.69491331792477,82.62361361779278,74.48510199789416,112.59006304577791,54.26647873056638,128.26290270375102,66.47173834862969,55.07587265325607,60.410395057621486,79.77143004799741,304.01605950250087,80.77443770603269,57.488302363576864,100.80214002632951,113.97171340269855,46.98097531315687,76.49694802912327,353.8261449083532,66.70793301533367,49.08900603073886,55.84906087245259,67.22421159034573,74.27891639406597,116.51292432349169,56.02595263721786,74.60522841093972,69.55393177763344,50.21172267947467,59.74009960004074,54.02336880835657,128.1578010530098,68.86028552868824,413.2838820151219,105.69329304652143,48.79825654996549,52.09827811637216,130.31507435398194,52.586966680228684,107.2931806908152,138.59183750082502,52.574110886729244,120.81399667534868,58.65503108973372,76.9186718297471,42.85641677136469,59.98147296898303,70.24085255358001,91.04883989705391,87.96237525305266,103.06559646699648,87.56180726513408,111.2755506427924,165.13149540622777,59.185717944457664,97.46475539289388,42.83899019361047,60.887952054664744,61.95148742429494,49.24849548190018,167.27543867756776,58.11807370162649,90.59861565285948,285.0558227400069,82.48896485009686,71.94684051548211,354.5107626385986,181.71493716531472,66.61083107146814,97.1639385110492,47.160115934884786,174.0443673090149,175.90162678993954,51.22230617786269,52.41382932996663,107.34192575535093,73.21871771877504,60.22256632865039,55.76631315385305,45.493639546653554,52.21733887801699,69.19290168074404,83.31684383469047,131.99558808142172,57.25628117467025,68.13088162565936,85.35737170030265,72.4746283262124,49.72177102096995,58.57755069898687,103.40055164800069,96.36825211986744,57.67133741563201,54.23908823292385,82.72359823539826,49.243783325049314,49.24348128735372,54.28859671479752,76.7489178528598,52.924919224927294,98.53706083884218,101.76461413775021,128.5369386937803,60.748069229536014,154.18825435328716,520.2436569864672,76.09373529919202,229.83368507568176,131.25366901979135,64.95235473212233,92.48166740258245,51.36571312539835,121.66647962621603,75.89322479568655,58.36847191945545,77.55600540943425,55.52626521396629,60.00078835835032,139.22160883887466,51.82566543376955,89.1454789682318,235.89479241811944,108.36625717655018,74.93214893235017,71.39202306697096,246.88409717660983,97.11247837605438,65.9810597334185,55.17051349245946,1331.9661986675883,128.21710018820914,73.2229733577842,50.29950158023898,75.82105161271345,75.65419397121161,137.75450958970586,65.71676039870687,68.34897527675375,48.73844446764342,103.69935254021692,80.75517689320445,60.35984934877873,80.06537437779318,273.06507107254845,75.49693168944565,221.6346913632658,1085.5822993814336,51.40347340905106,58.9650758719328,63.196705826312886,65.26961606677143,58.74268296322892,142.55517882601507,186.1525312478603,239.1697161824756,156.47042512686187,71.22751899779831,61.52867201814395,83.72599793142268,94.2463013823886,53.876416457567956,50.95595825050037,67.97397197742279,55.016716057214765,49.47207377188016,81.13030525512102,53.315456088010876,105.95758994512317,238.82563909128805,65.13698598279073,63.826477164362515,53.98345003483065,352.3510355873791,49.5903753146176,54.43945528987224,187.6795274604745,430.2856175817489,166.67696526779127,52.41859388171933,58.627042678367246,59.97675247842547,53.54605485874094,55.9600291937708,49.85003843690144,61.91715007930266,79.69495338252852,82.87441417690904,468.2232009894122,95.89751099505652,75.44408846766117,49.609149244289874,102.79608917965612,77.02924527334,48.76022513383282,93.62928911226302,109.59937373107786,78.75744622107884,151.82527778649373,55.38462757501844,70.09687742238944,84.73447817635916,120.4277840479083,51.43797220855822,50.68759597376323,52.30196560242855,151.19440308075542,96.9380486082529,54.585448808399704,128.99387339089117,81.10281863511139,221.18329621828886,56.556760611193596,48.7112616349955,61.94346401313663,53.360664404148814,69.92145683931629,50.08577985383985,86.91089220769764,48.9937858029113,72.8080021215895,209.2407018882292,135.30069499727094,84.31075874347057,103.26763378058784,50.1018451099298,54.58335589885367,570.471898333012,57.48853110269532,92.70262727363806,76.04597099429647,179.73268726724092,429.6452316492015,62.26002579754754,68.69400638406688,59.80174280052747,107.53798174926598,58.30675614468919,64.02754448998154,84.86290478708325,138.94153987716163,74.33804631466288,61.03454280367302,66.38610688019328,56.49595327828275,93.68957786516476,200.88691834526963,53.817909989161976,47.64943270099999,57.79483703926091,55.15163421132944,171.5149548062132,117.50499614236452,175.38774120922946,61.2111193771053,52.04985123542703,55.52682434798169,63.85459235202406,117.57409913305736,67.50421358529513,52.74857662552655,52.22563488374584,138.64166543415752,73.15073392435225,63.43645469326118,791.8545990182932,54.39010997166608,118.43295382121251,105.37500433379068,134.10945404109856,74.25940209779198,76.9834778257472,47.97143348796072,113.8052692215772,63.7159563986111,53.843093835411345,122.49106266195297,77.59151377457732,79.5168988999655,54.8023313463374,58.99832116132211,139.5509341165578,56.196279228021595,175.74337341348868,156.61618063379188,55.2434595996957,210.0514609050229,53.70651049307823,124.56339833681366,68.40847636163853,52.034617106648035,53.9106755279139,55.77805541974063,141.1308690676806,87.27701376601121,579.3594464487173,178.31890714954235,60.21229940333836,82.31459415122413,61.75414159056305,100.17236868827989,55.28334739581385,64.57070369840065,51.296263374066555,112.49060123464162,63.68053859453053,89.55626187695437,116.84921970246752,117.39109527316845,60.24086645776768,58.814630667460285,143.66557247228434,115.28541838945833,59.42617288922058,57.26957366943559,62.77580387127625,54.399264874489795,69.9329491813246,176.73338918232054,88.50904363215712,85.98114450508399,48.09243420671294,88.46361033487425,52.202346744941345,513.8385572391267,298.3620379830531,182.60006792998846,66.0880514220892,412.65411067707225,67.24665747079264,104.75584011964128,86.18916635577628,122.06034968869655,58.211232071693445,57.86115605668091,440.59046685270425,81.59623797840501,51.01554045467851,341.3985972265695,50.686551215398836,54.66603582536963,56.13094486654044,69.73215375974772,71.89285391810928,55.19292164220637,50.97416002285932,166.23375422922848,71.35431978225496,59.63463878734218,59.09171955804809,68.94066783839092,52.94901535119745,54.97557951240583,48.61926023440645,74.94187935881267,56.23864819061268,85.7830259761753,55.50117352849081,60.508796690147285,81.06773179673746,63.75298550026433,83.70888260443435,71.1035467735388,54.85286811837359,55.72404752119396,166.11808244225236,62.806683355211554,57.32694566098849,50.38337349839092,66.42583597497091,98.94657429112486,92.49610959742773,40.00603181583186,106.56110170115129,60.90680124691325,46.562564944252706,109.61850912541078,46.08086649562601,351.51066677117785,51.87444147571304,78.2374184704739,63.99918989567023,123.62129550752859,68.9203315559845,54.10523490959247,65.67902072270155,57.63505575231076,65.01843681263034,69.711563072191,59.65252687570302,69.51150589147451,77.65017389343313,65.46119640345373,52.20555816034921,73.13597231275259,81.14873040566027,133.38650833116577,90.4387883362734,129.23806969796607,64.08911313041722,85.42409012776652,78.48025798333084,52.61429195056849,65.29529777919434,59.186377666912755,56.709583186121236,129.40285639959038,71.3699633759839,65.4481601015786,52.253002582213824,54.63851352930471,64.66164961741674,52.08047682917736,48.605056602040065,70.09992354135615,67.25627825175118,81.47439483276786,270.56603928115715,51.030276717283954,80.87442066044265,149.41268183597722,118.85188476931907,54.34774310282066,62.451753600177575,59.69068670431889,48.19085847521913,73.62963075974234,61.40018966365256,51.711835937290644,200.0633477304463,251.11837255953031,232.92498445300726,59.56717114879632,74.90808851256139,53.8567589372104,51.86208071434207,389.73054642005474,62.833129915504315,148.52013439391908,255.1614048003009,63.78629673949398,79.18233560420256,54.8853254363881,169.12387477822918,97.39823212245827,240.6488482266962,96.21427887559574,210.1324896379801,103.23184634983107,154.04554129916966,51.444857884773604,54.475935924886635,125.81163297040635,96.90143746429828,63.34627630563019,83.5729867856493,65.25715449731327,245.10827015833382,137.91049067385333,246.6289972844539,74.50014359639978,99.16683217689182,53.35367869678102,55.05899192315674,238.38859872420605,56.40858313167227,47.32883116567755,72.30952552723062,85.59585001188513,74.11371357703241,145.51752966384743,196.08053685158916,63.32103592774929,78.48533629727747,149.8558488002832,61.26527521173423,51.71027378959172,45.100227796322166,71.55394985546224,102.287065191105,60.76079754464439,63.730000816247696,54.34219388693715,68.77258121707533,57.94239805610432,107.90384310150395,67.1539351255357,98.0280034605079,117.91852920902696,72.52096258630262,120.51083389568726,63.188855978487226,124.04748150939389,129.63760676093307,51.33380102602757,77.45521761269629,136.85031856770303,53.72329604194994,813.8031618519718,335.05945237838495,64.09633437020346,90.71668590277407,48.931304627406625,229.7572038218222,54.53524520632638,51.77779376389016,92.4525602496468,67.78370646358533,78.22128511262696,82.14508038115184,198.98752196817802,51.91331549087457,77.98081491603469,61.53276433042646,472.50106858692016,132.41728090463775,50.69966485004127,53.274417683998905,53.46247418958954,121.0328037067548,80.46882522681594,84.53178043035899,73.45287775073325,304.8621014589421,417.27071119729635,338.8502865442255,45.19288413424045,51.06588332516267,84.4713800719444,51.12661244248706,121.16986755197539,51.083589919264085,69.45703856866409,71.39561089147402,79.32580294066281,307.6629336403696,89.79272820809852,65.95832978458505,96.68644962350332,74.86716035139601,65.2499186176087,101.10067726388229,76.10731078706553,55.89467166189513,76.94566867896498,77.15013996254756,273.6948424105981,146.1286232750158,109.24865601454695,81.6801055295381,497.69182308453776,61.93856782249762,46.5238928218274,540.4399273514673,121.41254442871703,87.89770987996427,80.50053391707138,1189.8744367176082,51.07495148109104,86.73835569667162,117.47644506941018,105.94222606648404,52.828863775488074,249.2964367145961,52.27807678295924,108.10013601439502,52.66045834432687,57.248783964017726,51.82898015575745,156.7239603366039,59.34698114037583,55.566507889971966,102.6633041307077,52.21838493327716,370.7666922415825,70.21469833558727,88.11808501455754,377.51308124968136,84.64769893416845,185.5818817413398,51.632373682807184,413.5818596691192,65.89056111381126,52.71615813882262,65.79313928438111,61.80097706224761,74.96781765271251,69.00150767311582,56.455758002313985,58.20789755417948,71.64200172612854,790.0026941891155,55.13084365202032,57.4724398918368,193.08649525218868,87.47546507539101,71.29858898649722,57.28488646631075,149.709817189431,46.18536105408997,68.50718161511188,52.781439791198636,127.52802971496016,53.1301313842088,113.05705178308628,98.07080697491574,99.39501844480924,68.78069035090368,57.00528441426113,1284.5848715271304,190.44980648181973,61.32085875282311,148.0160616608924,107.56078743180481,106.88671533738292,51.00765061185804,68.8963019871412,46.695035152485886,281.836814897734,53.04965401274282,54.08880764413523,53.11202366098611,328.3358540060445,131.4108478898794,59.9649449989913,825.1535843882407,52.075215103459605,94.41715668107744,76.6782677604329,83.8562681587803,55.28348801872556,61.66126960924672,81.40420904408232,53.30242072518543,327.1544879378607,177.4088050976789,49.20258330272728,53.04356754138999,61.591779713482104,51.66920061219746,49.06495940668435,55.55548377205832,48.42275769674194,137.01431036206333,51.62323124416419,63.03336494287694,148.3803546621972,56.78177006473038,66.85186033054643,71.598562751349,61.82426000716673,56.18637720123964,189.46141286969595,162.1455412234949,70.16334262641523,52.443007981621534,64.27514561201524,59.08915887613143,183.08286876325676,154.61246808273245,169.1561841184452,80.39967522095746,66.28534670444776,55.37529353100497,93.6232632173532,128.06191276021406,66.90870131759145,93.78687215713272,69.16065836934777,99.99383314661861,67.0891169134484,61.806133585782604,51.765645982904516,358.171715910902,68.99717950041695,52.26929652178415,51.571814162543774,101.77617034704946,52.474678333736264,68.42445872605363,255.55815869555147,52.19154508285227,44.47045645827254,100.16630735302465,103.82076246616546,72.97085757189103,69.66999767468066,60.24609024958964,94.2171092342804,271.36319044645506,54.142471261897306,70.09632061594569,162.2565149045462,107.54910699069526,63.7065479081364,103.98945757949431,47.39991804091298,96.31359708373607,62.75953026031757,60.29162488815445,143.9736811104281,125.33579906528186,130.0193136331833,129.35248613595508,51.25789341780979,114.70996801032553,613.3583140968444,40.940934261208106,1917.1596574539258,87.47249322565655,64.60664775955982,72.46753297334065,61.852787991113495,160.01524976088047,65.17947212163463,89.89272652667535,55.216551010868834,708.2837970004866,87.99898190033618,59.616318911540006,67.70568509268398,50.37880932782674,65.95624026785627,108.8797170392964,222.33893670785898,196.96616346616543,131.81736120048305,58.30828886350451,57.59368375475106,328.9656253440941,53.033982637276935,87.79568213884453,76.47536668206176,150.7872750357243,55.960350815940146,51.83182433845196,69.78010523460212,67.7445131554426,63.94093236035102,61.7140850672111,2100.717454797917,104.96124412028975,57.356404365015194,97.02504655247955,63.005624691133015,178.27936642755552,569.8421269949624,49.979608381295876,77.94173084317056,149.78735083999055,93.58733789623076,61.24559640313025,80.45043403012495,120.84768342538985,51.37050936548516,66.99717817738185,82.0090377080402,51.76778962641072,59.61434202307231,73.86413252999733,108.45676158880849,203.43614669275541,80.91677484345699,49.58197142070768,49.77693572419037,44.048145486938836,84.87552436271238,62.56718475667891,181.1404683351472,136.7870690066443,82.97888219538743,202.0970629550441,73.57783180031858,71.1002718755339,105.06400044291424,60.374595528409905,48.029689378962615,117.60714490960129,60.969933575221745,67.92401433185053,123.83878030534201,118.10621640745981,50.420137855113616,53.89172925444313,74.3298397442103,75.50235737893247,62.013493800958685,88.36980003775794,123.20900896729238,210.2852672302949,372.7086105757571,143.4902375621463,71.25668900641759,44.202924533335704,186.9836777785573,55.72459400417818,241.43097971154475,69.8739851206556,56.72276768828108,79.84317679685223,170.54338776715375,54.752641124110255,80.28700350540736,49.59121172397096,71.2905894981728,133.5776778813939,60.66509423678497,55.30078068682557,65.8550610190839,68.73810419105037,88.93042817800348,83.26143083738135,74.83924410601543,62.215671198330554,63.68248779084861,53.37842529634349,88.98705034759578,67.39086510248656,87.83383899682462,79.79782109522668,80.69302522170625,115.99469319734527,82.28581350331007,119.50908278005443,157.8670185421395,117.28875787097732,84.12559937087822,72.02538222952364,87.91060919697613,43.41837414888921,57.26297751806628,72.38150865295269,63.90123154250411,80.08343580993592,90.04777741353587,75.02304291005233,63.37027633771375,51.67747144185634,53.2314419681471,48.943617350585804,138.20544471908556,117.92352997187193,65.30293868892804,54.89714537957937,54.65699157868082,53.00949911673064,66.21468598404351],"xaxis":"x","y":[7.8,4.52,27.02,82.38,50.55,18.65,6.35,10.65,5.74,3.59,49.58,6.87,429.27,3.3,41.51,4.03,4.26,69.59,12.61,834.96,404.51,6.53,111.96,2.15,3768.66,531.96,20.79,19.06,25.15,10.77,166.13,111.62,3.79,292.8,25.38,5.37,167.98,11.32,34.2,40.06,22.78,72.5,355.05,238.77,13.86,184.83,39.88,8.93,37.62,26.44,97.97,8.1,69.79,64.06,7.85,50.4,68.05,3995.45,202.97,13.71,27.41,63.45,129.65,59.18,61.21,15.26,17.83,9.63,47.24,62.23,6.52,12.63,144.41,1.14,5.07,44.63,49.46,3.88,126.92,106.37,27.45,9.64,26.41,4.86,60.49,196.67,115.34,9.57,5.1,65.08,9.51,40.94,22.29,65.89,298.61,72.71,45.22,8.47,103.21,293.56,66.43,83.55,9.06,96.43,46.79,18.34,202.49,7.56,9,23.14,26.71,271.58,329.29,7.79,5.53,141.95,23.5,60.67,109.16,358.88,153.73,60.59,7.4,3.75,546.2,4.46,131.4,65.65,147.69,21.09,28.27,5.11,54.09,165.86,277.98,37.82,67.56,71.03,3.82,11.46,22.39,63.14,224.36,107.56,29.7,7.08,43.86,4.04,1597.56,4.67,4.41,26.35,866.23,284.42,73.48,78.15,325.94,129.89,47.38,38.97,7.05,60.53,161.06,20.76,6.19,147.84,69.42,134.71,54.11,67.48,11.17,0,122.99,28.52,33.75,94.89,64.57,152.04,14.05,549.74,21.75,31.04,115.04,272.54,3.96,198.45,7.84,14.37,16.66,14.19,2.57,68.14,9.29,7.74,8.45,16.88,10.63,189.18,37.97,43.57,12.36,139.27,46.72,7.74,18.85,19.76,86.02,14.05,12.97,85.88,57.03,64.22,36.09,4.68,187.92,29.39,10.81,74.45,234.75,3.52,40.12,30.87,5,70.85,698.11,64.49,1936.34,5.47,4.55,5.72,48.65,288.43,2.9,5.87,36.71,5.49,17.87,38.87,3.34,47.64,8.55,3.72,13.74,21.52,40.01,30.12,25.44,20.18,352.87,135.81,161.23,39.5,6.26,94.85,548.37,16.5,141.15,22.92,23.74,7.4,64.54,79.98,35.11,9.25,27.04,234.6,48.6,26.85,3.22,10.55,42.23,66.6,27.93,5.27,324.25,9.9,47.34,107.84,4.36,164.06,6.93,5.28,60.43,10.73,4.53,238.09,174.97,4.23,406.02,7.98,7.1,4.65,2.43,6.51,3.47,3.95,317.82,47.62,49.89,253.53,1.12,33.09,15.48,61.36,31.46,35.8,7.37,134.8,18.99,10.55,66.79,31.94,691.26,87.68,3.15,15.9,45.07,62.2,3.18,16.27,139.62,4.21,11.94,7.03,47.14,40.28,82.66,4.01,8.07,309.6,4.39,3,28.19,77.76,5.41,5.47,5.94,96.39,570.98,18.1,46.03,116.06,48.62,30.27,4.76,67.57,6.61,18.54,7.96,134.01,13.35,80.08,59.84,4.81,105.32,5.91,44.51,674.09,3.98,38.08,177.51,67.24,52.73,13.99,74.33,160.28,466.95,10.01,6.86,5.07,3.72,5.81,49.55,13.87,53.61,143.34,64.23,1.68,54.51,25.37,39.58,46.26,5.18,85.03,192.12,4.73,101.27,21.2,73.14,143.82,184.4,78.06,185.82,32.77,14.49,23.42,6.53,143.45,11.14,21.87,4.67,278.97,6.85,64.39,3.77,95.86,112.06,6.74,290.57,29.92,69.57,24.53,112.82,20.67,12.03,61.18,627.87,9.21,16.67,25.12,10.8,14.83,320.35,3.49,8.07,4.6,6.63,26.64,49.53,52.38,24.88,115.24,402.03,27.14,67.96,25.37,88.11,48.15,25.24,4.87,20.3,12.19,10.01,7.11,96.18,24.76,4.76,24.96,6.31,3.42,41.06,24.1,54.98,20.76,42.74,99.45,145.88,36.48,14.17,1.61,22.02,147.16,129.42,147.36,100.02,9.47,7.82,19.73,8.98,160.3,35.14,8.22,5.51,8.37,639.39,46.23,57.71,31.72,14.86,8.09,8.7,173.23,11.73,23.17,13.29,10.92,5.91,42.93,4.38,377.36,25.21,75.67,0,4.5,6.09,7.28,272.85,171.64,15.34,30.22,15.23,3.63,43.08,9.47,4.32,31.97,11.18,11.24,13.76,6.12,32.87,46.7,30.1,104.17,91.49,65.27,211.49,6.73,49.04,41.27,13.33,19.11,0.76,12.06,56.63,18.74,443.73,44.19,24.56,290.88,119.2,26.34,47.78,157.07,95.26,1623.34,24.51,6.16,38.55,7.52,4.6,5.47,6.69,5.02,23.79,7.07,49.25,43.91,8.1,0,103.19,18.1,61.22,36.24,14.65,28.03,10.01,39.54,7.05,3.09,3.5,9.24,27.13,9.68,49.32,66.39,8.2,123.99,111.69,19.12,142.39,47.71,67.27,10.89,39.57,17.74,78.39,25.94,180.38,14.49,14.73,5.32,4.39,52.09,205.45,19.7,8.29,89.02,177.59,20.65,26.94,20.19,10780.64,13.84,8.45,2.73,8.81,332.63,16.67,29.39,6.18,6.16,67.65,31.52,22.97,8.04,509.69,13.18,26.39,277.98,61.33,4.48,72.93,5.38,6.45,109.79,139.83,54.92,160.8,35.72,6.22,51.93,93.87,6.04,3.72,11.79,18.95,3.53,50.57,0.75,150.69,398.56,113.3,5.67,4.52,61.46,5.69,4.32,154.4,173.47,91.53,13.91,25.39,44.26,42.1,0,12.28,25.48,28.94,22.62,172.46,23.79,156.67,16.3,243.38,33.49,57.41,16.89,62.83,7.56,18.21,79.98,19.69,27.7,48.74,6.17,26.31,18.12,119.81,63.68,23.17,105.97,9.68,13.16,18.42,25.2,6.38,6.83,33.04,4.05,269.71,2.62,121.9,282.34,142.36,310.19,79.42,3.41,3.96,723.47,24.54,151.12,7.61,237.39,200.5,6.7,29.84,5.55,65.46,5.91,17.36,6.71,15.68,9.87,5.53,61.19,4.65,11.54,32.99,34.03,2.52,5.09,50.05,81.92,47.84,66.48,24.91,174.98,36.05,7.36,60.02,111.95,0.89,28.41,67.53,16.13,9.67,913.07,38.68,17.46,85.59,19.84,31.58,36.43,52.41,16.27,12.97,21.69,66.05,12.29,68.64,303.14,208.53,54.05,66.53,191.97,119.92,20.46,28.88,12.89,119.03,8.12,4.52,65.77,5.65,100.42,34.92,359.11,154.39,6,328.17,4.01,27.37,4,17.87,17.8,44.61,12.59,38.15,184.51,19.1,59.05,4.94,23.83,126.55,4.9,4.13,6.1,42.4,6.61,177.1,321.71,104.91,3.28,11.29,4.75,139.72,479.7,21.55,124.4,271.32,6.98,12.46,43.28,25.59,4.8,5.63,208.81,15.43,21.85,122.54,3.01,3.5,64.81,150.88,128.69,32.04,48.79,124.92,56.3,71.01,26.11,7.51,6.16,4.27,4.03,7.53,5.32,241.49,61.39,4.47,61.98,2.45,8.33,7.02,5.6,17.41,58.82,14.97,39.05,87.27,60.34,192.2,36.61,26.67,51.65,85.54,1.55,16.42,81.24,116.95,3.07,3.92,11.44,243.66,36.96,14.12,14.24,16.72,35,45.6,24.87,11.97,51.24,9.29,31.79,18.35,9,61.86,4.19,1154.79,6.18,36.86,9.99,3.18,8.1,26.27,18.72,62.41,32.33,22.53,4.8,4.46,30.45,75.02,19.35,5.24,6.25,196.34,35.69,40.76,8.44,21.08,14.51,75.58,7.79,226.73,1.6,29.57,21.69,16.16,28.08,346.25,229.98,67.17,145.02,62.67,6.91,142.16,5.48,295.12,175.27,27.81,8.14,2.18,445.62,120.68,0,13.05,50.29,935.7,87.03,13.22,34.69,45.44,10.43,8.35,50.98,7.2,119.63,39.47,515.85,190.17,9.13,4.26,148.25,220.26,3.67,6.18,6.73,56.03,54.83,23.56,190.58,29.88,41.7,31.27,6.66,4.25,37.31,138.03,42.8,4.72,5.91,31.92,82.92,64.46,71.38,68.16,127.74,66.37,21.18,726.96,70.16,16.99,25.11,7.24,38.55,121.98,4.55,2089.28,138.85,11.08,12.99,3.42,130.32,15.25,3.06,384.27,73.48,11.87,188.58,99.59,17.51,712.39,62.64,180.14,47.02,3.99,6.49,2.94,44.54,47.26,20.08,9.79,2340.63,242.19,262.45,32.17,4.16,35.98,45.3,23.23,6.48,67.52,6.82,29.26,250.05,17.24,7.63,15.6,14.4,51.6,18.41,12.22,22.72,83.45,7.25,563.25,110.42,9.5,28.46,1280.81,4.74,2.69,92.52,48.53,59.34,51.49,452.16,5.29,21.05,22.01,39.57,103.23,256.53,55.23,54.55,2.01,4.68,16.8,25.12,48.1,53.25,68.34,44.71,467.9,28.47,9.76,128.22,9.4,19.98,22.33,391.7,41.13,16.11,8,6.37,9.48,60.79,59.11,3.65,50.55,313.3,30.19,49.15,13.43,11.72,59.74,1.39,276.9,78.36,23.12,3.5,5.83,38.67,51.67,149.39,12.18,9.58,16.61,318.97,333.3,101.21,229.55,280.16,176.88,3.04,24.31,6.46,122.7,10.14,76.48,5.86,399.54,485.76,40.56,440.31,10.75,56.8,135.56,13.56,82.87,7.12,60.7,3.16,376.57,25.14,7.86,37.34,4.27,44.15,326.65,38.59,43.8,355.8,4.33,5.85,25.76,4.59,92.3,384.85,19.55,38.25,92.92,135.11,33.78,127.84,16.04,103.07,169.75,39.75,103.37,42.57,22.46,9.88,10.82,101.43,21.3,70.79,9.58,22.17,7.12,74.52,2.64,128.87,17.4,5.75,5.08,138.96,4.84,41.65,1019.53,28.07,37.13,78.85,38.62,45.08,53.31,58.85,55.71,131.35,4.71,60.57,56.96,9.65,6.76,49.72,2.35,28.57,4.52,4.47,85.41,218.12,103.39,320.96,25.49,53.87,3321.02,216.18,12307.16,93.23,79.58,8.36,6.34,103.31,17.68,8.44,69.36,4829.86,46.6,56.68,45.32,39.1,5.95,16.76,313.83,211.52,477.87,6.01,26.52,351.53,46.94,7.34,85.22,145.35,4.13,12.28,6.46,3.92,17.84,4.21,3243.48,50.42,36.08,125.13,15.5,67.23,765.56,4.72,83.59,52.46,54.12,5.09,19.57,34.87,9.71,14.44,249.01,30.49,5.6,5.68,22.08,203.73,9.63,4.62,10.43,188.67,114.33,6.13,168.62,279.35,8.04,282.72,5.31,9.2,112.01,21.37,2.28,57.59,5.6,53.73,51.14,26.1,18.62,64.67,141.2,8.29,25.77,47.96,51.98,218.49,256.76,92.7,3.13,3.16,100.65,23.72,235.32,379.34,3.98,377.27,137.33,63.07,9.83,85.55,10.38,14.35,4.84,2.17,29.77,63.17,30.14,101.83,47.92,47.84,5.19,11.22,44.97,58.36,11.58,9.99,7.98,22.94,107.71,267,97.01,53.48,8.16,6.85,37.84,181.35,31.61,303.81,225.35,39.35,51.56,157.13,62.94,124.59,4.5,49.79,13.65,65.35,11.39,3.93,4.17,19.23,92.45],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Ridge()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predRidge"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression ridge\n","# réglage des paramètre pour la gridsearch\n","alphasridge = np.logspace(-3, 5, 1000)\n","param_gridRidge = {'ridge__alpha': alphasridge}\n","\n","GridRidge, \\\n","BestParametresRidge, \\\n","ScoresRidge, \\\n","TotalGHGEmissions_predRidge, \\\n","figRidge = reg_modelGrid(model=Ridge(),\n","                            scaler=scaler,\n","                            X_train=BEBNumM_train,\n","                            X_test=BEBNumM_test,\n","                            y_train=TotalGHGEmissions_train,\n","                            y_test=TotalGHGEmissions_test,\n","                            y_test_name='TotalGHGEmissions_test',\n","                            y_pred_name='TotalGHGEmissions_predRidge',\n","                            score=score,\n","                            param_grid=param_gridRidge)\n","\n","print(BestParametresRidge)\n","print(ScoresRidge)\n","figRidge.show()"]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[454.5305231291819,454.5305209283968,454.53051868665517,454.53051640319444,454.5305140772387,454.5305117079968,454.53050929466355,454.53050683641794,454.5305043324245,454.5305017818317,454.53049918377235,454.53049653736326,454.53049384170436,454.53049109587937,454.5304882989547,454.53048544997944,454.53048254798466,454.5304795919839,454.53047658097205,454.53047351392553,454.5304703898013,454.53046720753747,454.5304639660517,454.53046066424224,454.5304573009861,454.5304538751401,454.5304503855394,454.53044683099745,454.53044321030586,454.5304395222332,454.53043576552625,454.530431938907,454.5304280410751,454.5304240707047,454.53042002644605,454.5304159069245,454.5304117107389,454.53040743646307,454.5304030826436,454.53039864779987,454.53039413042444,454.5303895289814,454.530384841906,454.5303800676053,454.53037520445514,454.530370250803,454.53036520496414,454.53036006522314,454.5303548298327,454.5303494970126,454.5303440649499,454.53033853179767,454.5303328956746,454.5303271546647,454.53032130681584,454.53031535013986,454.5303092826119,454.5303031021687,454.53029680670886,454.5302903940927,454.53028386213936,454.5302772086284,454.5302704312973,454.5302635278425,454.5302564959164,454.53024933312855,454.53024203704365,454.53023460518114,454.53022703501426,454.53021932396933,454.5302114694249,454.53020346871045,454.5301953191057,454.53018701784015,454.53017856209146,454.5301699489852,454.53016117559275,454.5301522389312,454.5301431359627,454.53013386359225,454.5301244186674,454.53011479797726,454.53010499825115,454.53009501615713,454.5300848483019,454.53007449122816,454.5300639414154,454.53005319527654,454.5300422491585,454.5300310993398,454.5300197420298,454.5300081733676,454.52999638942003,454.5299843861811,454.5299721595699,454.5299597054301,454.52994701952775,454.5299340975499,454.52992093510375,454.5299075277143,454.52989387082397,454.5298799597898,454.5298657898825,454.52985135628535,454.5298366540911,454.5298216783016,454.52980642382647,454.5297908854791,454.5297750579777,454.52975893594146,454.52974251388986,454.5297257862403,454.5297087473059,454.52969139129493,454.5296737123067,454.5296557043319,454.5296373612485,454.5296186768211,454.5295996446981,454.5295802584096,454.5295605113656,454.52954039685335,454.5295199080354,454.5294990379469,454.5294777794933,454.52945612544835,454.52943406845105,454.52941160100374,454.5293887154692,454.5293654040682,454.52934165887666,454.5293174718228,454.5292928346853,454.5292677390895,454.52924217650514,454.529216138243,454.5291896154522,454.5291625991179,454.5291350800568,454.52910704891474,454.52907849616423,454.5290494120999,454.52901978683605,454.5289896103033,454.5289588722447,454.52892756221246,454.52889566956475,454.52886318346134,454.5288300928611,454.5287963865168,454.52876205297196,454.5287270805579,454.5286914573876,454.52865517135416,454.52861821012476,454.5285805611376,454.5285422115966,454.52850314846853,454.52846335847715,454.52842282809917,454.52838154356084,454.5283394908308,454.5282966556177,454.5282530233641,454.5282085792418,454.5281633081465,454.528117194694,454.5280702232129,454.52802237774114,454.5279736420189,454.5279239994851,454.5278734332701,454.5278219261903,454.5277694607431,454.52771601910007,454.5276615831014,454.5276061342497,454.5275496537032,454.52749212227025,454.52743352040227,454.52737382818685,454.52731302534164,454.52725109120746,454.52718800474,454.52712374450505,454.5270582886686,454.52699161499174,454.5269237008216,454.5268545230839,454.5267840582761,454.5267122824578,454.52663917124494,454.5265646997988,454.5264888428193,454.52641157453616,454.5263328687004,454.5262526985749,454.5261710369252,454.52608785601086,454.526003127576,454.5259168228393,454.5258289124848,454.52573936665146,454.52564815492315,454.5255552463188,454.52546060928137,454.52536421166724,454.5252660207358,454.52516600313766,454.52506412490413,454.52496035143486,454.52485464748713,454.52474697716286,454.5246373038972,454.5245255904456,454.52441179887245,454.52429589053656,454.524177826079,454.52405756540975,454.52393506769374,454.5238102913381,454.5236831939768,454.523553732457,454.52342186282397,454.5232875403073,454.52315071930434,454.5230113533659,454.52286939518046,454.5227247965571,454.522577508411,454.5224274807447,454.5222746626334,454.5221190022059,454.52196044662895,454.5217989420873,454.5216344337675,454.52146686583785,454.52129618143033,454.521122322622,454.52094523041376,454.5207648447122,454.5205811043082,454.5203939468564,454.52020330885523,454.52000912562414,454.5198113312825,454.519609858727,454.51940463961,454.51919560431486,454.518982681934,454.51876580024464,454.5185448856834,454.51831986332354,454.51809065684756,454.5178571885235,454.5176193791769,454.5173771481659,454.51713041335296,454.5168790910773,454.5166230961278,454.5163623417128,454.5160967394324,454.51582619924767,454.515550629451,454.5152699366352,454.51498402566165,454.5146927996292,454.51439615984134,454.51409400577234,454.51378623503444,454.5134727433434,454.513153424483,454.5128281702699,454.5124968705177,454.5121594129984,454.51181568340706,454.5114655653222,454.51110894016676,454.51074568716894,454.5103756833213,454.50999880334047,454.50961491962437,454.5092239022094,454.50882561872805,454.5084199343638,454.5080067118068,454.50758581120715,454.5071570901288,454.50672040350247,454.5062756035759,454.50582253986613,454.50536105910817,454.5048910052048,454.5044122191742,454.50392453909683,454.50342780006196,454.5029218341125,454.50240647018944,454.5018815340751,454.50134684833466,454.50080223225734,454.5002475017976,454.49968246951266,454.49910694450074,454.49852073233797,454.4979236350149,454.49731545086814,454.4966959745172,454.4960649967942,454.49542230467506,454.49476768120996,454.4941009054504,454.4934217523778,454.492729992828,454.4920253934166,454.4913077164612,454.49057671990386,454.4898321572311,454.4890737773927,454.48830132471994,454.4875145388402,454.48671315459285,454.48589690194166,454.48506550588644,454.4842186863727,454.4833561582001,454.4824776309294,454.4815828087868,454.4806713905682,454.47974306954046,454.478797533341,454.4778344638771,454.47685353722045,454.4758544235041,454.47483678681283,454.473800285076,454.472744569955,454.47166928673175,454.47057407419254,454.4694585645114,454.46832238313175,454.4671651486445,454.46598647266546,454.4647859597104,454.46356320706684,454.4623178046648,454.4610493349455,454.4597573727265,454.45844148506495,454.4571012311209,454.4557361620133,454.4543458206792,454.4529297417258,454.45148745128307,454.45001846685227,454.44852229715286,454.4469984419661,454.445446391976,454.44386562860893,454.44225562386794,454.4406158401672,454.4389457301615,454.4372447365742,454.4355122920216,454.43374781883404,454.43195072887613,454.43012042336034,454.4282562926616,454.42635771612595,454.4244240618767,454.4224546866182,454.4204489354362,454.418406141593,454.41632562632304,454.414206698622,454.41204865503335,454.4098507794312,454.40761234280205,454.40533260301845,454.4030108046139,454.40064617855006,454.39823794198367,454.3957852980276,454.3932874355082,454.3907435287206,454.3881527371779,454.3855142053579,454.3828270624461,454.38009042207284,454.3773033820488,454.3744650240953,454.37157441356914,454.3686305991866,454.3656326127395,454.3625794688099,454.35947016447807,454.3563036790294,454.3530789736522,454.34979499113604,454.34645065556197,454.34304487199023,454.3395765261422,454.33604448407885,454.332447591873,454.328784675279,454.32505453939586,454.32125596832594,454.3173877248302,454.31344854997695,454.30943716278597,454.30535225986904,454.30119251506403,454.29695657906484,454.29264307904566,454.28825061828167,454.28377777576225,454.2792231058021,454.27458513764395,454.26986237505963,454.26505329594255,454.26015635189833,454.2551699678273,454.2500925415038,454.2449224431496,454.2396580150015,454.23429757087536,454.22883939572296,454.2232817451848,454.2176228451375,454.21186089123614,454.2059940484504,454.20002045059726,454.1939381998662,454.187745366342,454.1814399875199,454.1750200678172,454.1684835780787,454.1618284550781,454.15505260101406,454.14815388300076,454.14113013255394,454.1339791450725,454.1266986793142,454.11928645686794,454.11174016161993,454.10405743921683,454.0962358965229,454.08827310107347,454.0801665805242,454.07191382209555,454.0635122720132,454.0549593349451,454.04625237343396,454.0373887073253,454.028365613194,454.01918032376454,454.0098300273303,454.0003118671678,453.9906229409497,453.98076030015284,453.97072094946486,453.96050184618855,453.95009989964274,453.9395119705625,453.92873487049576,453.9177653611997,453.906600154035,453.8952359093593,453.8836692359188,453.8718966902403,453.8599147760224,453.8477199435266,453.8353085889677,453.82267705390643,453.8098216246418,453.7967385316043,453.7834239487511,453.76987399296314,453.75608472344373,453.7420521411209,453.72777218805095,453.71324074682707,453.6984536399903,453.6834066294459,453.668095415883,453.6525156382004,453.63666287293637,453.62053263370706,453.60412037064725,453.5874214698624,453.57043125288493,453.5531449761388,453.53555783041463,453.51766494035047,453.499461363925,453.48094209195915,453.4621020476272,453.4429360859822,453.42343899348924,453.40360548757445,453.3834302161837,453.3629077573566,453.3420326188144,453.3207992375611,453.29920197950094,453.2772351390713,453.25489293889166,453.23216952943005,453.2090589886854,453.18555532189066,453.1616524612326,453.1373442655914,453.1126245202997,453.0874869369227,453.0619251530594,453.03593273216393,453.00950316339157,452.9826298614631,452.95530616655714,452.92752534422215,452.8992805853142,452.87056500595907,452.84137164753855,452.8116934767024,452.7815233854053,452.7508541909707,452.71967863618,452.68798938938755,452.6557790446635,452.62304012196273,452.58976506731995,452.55594625307265,452.5215759781104,452.48664646815087,452.4511498760424,452.4150782820934,452.3784236944275,452.3411780493642,452.3033332118265,452.2648809757713,452.2258130646451,452.18612113186435,452.1457967613172,452.1048314678874,452.0632166980007,452.02094383018976,451.9780041756786,451.9343889789852,451.89008941853825,451.84509660731163,451.79940159346995,451.752995361025,451.7058688305041,451.658012859623,451.6094182439659,451.5600757176682,451.5099759540993,451.459109566544,451.4074671088791,451.35503907624206,451.3018159056899,451.24778797684405,451.19294561251763,451.1372790793238,451.08077858825925,451.02343429525945,450.9652363017238,450.90617465500355,450.84623934885184,450.7854203238272,450.72370746765154,450.66109061551225,450.5975595503084,450.5331040028339,450.4677136518925,450.4013781243416,450.3340869950565,450.2658297868138,450.19659597008496,450.1263749627372,450.055156129635,449.982928782137,449.90968217748343,449.83540551806766,449.76008795058516,449.68371856505956,449.60628639373124,449.52778040981474,449.4481895261062,449.36750259344836,449.2857083990384,449.202795664579,449.11875304426496,449.0335691226014,448.94723241205094,448.8597313505003,448.7710542985491,448.6811895366113,448.59012526182886,448.49784958479285,448.40435052607165,448.3096160125395,448.21363387350704,448.11639183665227,448.017877523747,447.91807844618234,447.816982000291,447.7145754624674,447.610845984086,447.50578058621994,447.3993661541628,447.2915894317547,447.1824370155176,447.07189534860527,446.95995071456855,446.84658923094776,446.7317968426929,446.61555931542426,446.4978622285359,446.37869096815785,446.2580307199789,446.13586646194824,446.01218295686124,445.8869647448458,445.76019613576074,445.6318612015199,445.5019437683579,445.370427409053,445.23729543512263,445.10253088901146,444.9661165362869,444.8280348578634,444.68826804227604,444.54679797802083,444.40360624598554,444.25867411199295,444.1119825194786,443.9635120823247,443.813243077878,443.66115544017237,443.50722875338204,443.35144224553216,443.19377478248987,443.0342048622656,442.8727106096462,442.7092697711916,442.54385971061663,442.3764574045901,442.20703943897297,442.0355820055268,441.86206089911593,441.68645151543205,441.50872884926713,441.3288674933591,441.1468416378399,440.96262507030553,440.77619117653893,440.5875129419046,440.3965629534429,440.2033134026832,440.0077360891998,439.80980242493285,439.6094834392919,439.40674978506377,439.2015717451419,438.9939192400955,438.7837618365932,438.5710687566981,438.3558088880462,438.13795079492337,437.917462730248,437.69431264847464,437.4684682194226,437.2398968430404,437.00856566510856,436.774441593889,436.5374913177205,436.29768132356503,436.05497791650293,435.8093472401764,435.56075529817946,435.3091679763887,435.0545510662312,434.7968702888811,434.53609132037656,434.2721798176496,434.00510144545495,433.73482190418633,433.4613069585689,433.184522467209,432.90443441298777,432.6210089342814,432.33421235698745,432.04401122733987,431.7503723454908,431.4532627998377,431.15265000207347,430.84850172293534,430.5407861286294,430.22947181790335,429.91452785974315,429.59592383166654,429.2736298585852,428.9476166522075,428.61785555095446,428.28431856035843,427.94697839391426,427.60580851435606,427.26078317532455,426.9118774633988,426.55906734045845,426.20232968634554,425.84164234179696,425.4769841516143,425.1083350080389,424.7356758943036,424.35898892832785,423.97825740652553,423.593465847696,423.20460003696496,422.8116470697484,422.41459539570724,422.01343486266444,421.60815676045496,421.198753864681,420.785220480345,420.3675524853308,419.94574737371187,419.51980429885515,419.0897241163004,418.65550942638964,418.21716461662345,417.77469590372385,417.3281113753834,416.87742103167886,416.4226368261354,415.9637727064206,415.5008446546569,415.0338707273351,414.56287109481957,414.08786808043317,413.60888619911293,413.1259521956308,412.6390950823712,412.14834617666537,411.65373913767854,411.15531000285165,410.6530972238992,410.14714170236914,409.637486824771,409.1241784972808,408.6072651800357,408.08679792103084,407.562830389635,407.03541890974327,406.5046224925881,405.97050286923286,405.43312452277115,404.89255472026406,404.3488635444422,403.80212392520946,403.2524116709808,402.69980549989543,402.1443870709421,401.58624101504256,401.02545496613527,400.46211959230743,399.89632862702337,399.3281789004994,398.75777037127745,398.1852061580496,397.6105925717881,397.0340391482356,396.45565868080934,395.8755672539763,395.29388427715224,394.7107325191768,394.1262381434208,393.54053074356887,392.95374338013113,392.3660126177195,391.7774785631336,391.18828490428393,390.59857894998015,390.0085116706024,389.4182377396646,388.8279155762663,388.2377073884203,387.64777921722595,387.0583009818453,386.469446525217,385.88139366042617,385.2943242176235,384.7084240913607,384.1238832881809,383.54089597427253,382.9596605229564,382.3803795617396,381.8032600186237,381.2285131673126,380.6563546709073,380.08700462362293,379.52068759000235,378.95763264103056,378.3980733864879,377.8422480027995,377.29039925555855,376.7427745158117,376.19962576910604,375.66120961619436,375.1277872641998,374.59962450692893,374.0769916929167,373.5601636796745,373.0494197724981,372.54504364607953,372.04732324705805,371.5565506755333,371.07302204346865,370.5970373078149,370.12890007610815,369.6689173822301,369.21739942997294,368.7746593020335,368.3410126320659,367.9167772374656,367.5022727106384,367.0978199666318,366.7037407451843,366.32035706547634,365.9479906321584,365.5869621915896,365.2375908376427,364.90019326692965,364.5750829838695,364.2625694566621,363.96295722594357,363.67654496867215,363.4036245206319,363.14447986181824,362.8993860698887,362.66860824779525,362.4524004326454,362.2510044937546,362.06464902870977,361.8935482670585,361.73790099192564,361.59788949042314,361.4736785441278,361.3654144711299,361.27322423119,361.1972146053501,361.1374714609372,361.094059112242,361.06701978628223,361.056373201952,361.06211626955667,361.08422291623856,361.1226440411673,361.177307602616,361.2481188372291,361.33496060994196,361.4376938911957,361.55615835633347,361.69017310042716,361.83953746029067,362.0040319341309,362.18341918819505,362.377445138911,362.58584009840007,362.80831997087296,363.0445874872918,363.2943334657886,363.5572380856546,363.83297216322546,364.12119841867536,364.4215727235516,364.7337453198079,365.0573620021099,365.39206525623666,365.73749534748566,366.09329135406426,366.4590921415015,366.8345372751238,367.2192678685863,367.61292736733105,368.0151622666457,368.4256227647083,368.84396335163694,369.26984333610244,369.7029273115207,370.1428855642156,370.589394426239,371.0421365757657,371.5008012881363,371.96508464072747,372.4346896748806,372.9093265181221,373.3887124698801,373.87257205383736,374.3606370399665,374.8526464391862,375.34834647344485,375.8474905239006,376.3498390597156,376.8551595498317,377.3632263599367,377.873820636674,378.3867301809978,378.9017493124219,379.4186787257675,379.9373253418741,380.4575021536063,380.9790280683598,381.50172774815525,382.02543144829144,382.54997485543265,383.0751989258991,383.6009497248472,384.1270782669406,384.65344035903433,385.17989644533145,385.70631145539886,386.2325546553798,386.75849950267957,387.28402350436073,387.80900807943397,388.3333384251974,388.85690338773696,389.37959533667555,389.90131004422193,390.42194656855406,390.9414071415433,391.4595970608081,391.9764245860687,392.4918008397591,393.00563971183766,393.51785776872805,394.0283741663094,394.5371105668691,395.04399105992184,395.5489420867912,396.0518923688495,396.5527728392998,397.0515165783887,397.5480587519288,398.0423365530122,398.53428914679125,399.02385761820636,399.5109849225345,399.9956158386382,400.4776969247899,400.9571764769499,401.4340044893778,401.9081326174568,402.3795141426127,402.8481039392135,403.31385844333124,403.7767356232595,404.2366949516734,404.69369737932664,405.1477053101826,405.5986825778766,406.0465944234134,406.4914074740021,406.9330897229395,407.37161051045257,407.80694050541206,408.23905168783966,408.6679173321262,409.093511990888,409.5158114793884,409.9347928604564,410.35043442983715,410.7627157019136,411.17161739573885,411.5771214213278,411.9792108661524,412.37786998179615,412.7730841707204,413.1648399731008,413.5531250536971,413.93792818871816,414.3192392526513,414.69704920502556,415.0713500770835,415.442134958334,415.80939798297015,416.1731343161274,416.533340139971,416.89001263959636,417.2431499887298,417.5927513352241,417.9388167863376,418.2813473937951,418.6203451386247,418.95581291577156,419.2877545184867,419.61617462249524,419.9410787699453]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[640.0507806224591,640.0507768508259,640.0507730090023,640.050769095682,640.0507651095346,640.0507610492048,640.0507569133124,640.0507527004502,640.050748409187,640.0507440380627,640.0507395855918,640.0507350502601,640.0507304305256,640.050725724818,640.0507209315367,640.0507160490523,640.0507110757048,640.050706009803,640.0507008496248,640.0506955934154,640.0506902393881,640.0506847857223,640.0506792305632,640.050673572023,640.050667808177,640.0506619370658,640.0506559566934,640.050649865026,640.0506436599928,640.050637339484,640.0506309013507,640.0506243434033,640.0506176634132,640.0506108591084,640.0506039281753,640.050596868258,640.0505896769555,640.0505823518238,640.0505748903712,640.0505672900614,640.05055954831,640.0505516624851,640.0505436299054,640.0505354478398,640.0505271135064,640.0505186240715,640.0505099766491,640.0505011682985,640.0504921960255,640.0504830567788,640.050473747452,640.0504642648793,640.0504546058366,640.0504447670405,640.0504347451451,640.0504245367433,640.0504141383645,640.0504035464735,640.0503927574681,640.0503817676811,640.0503705733759,640.0503591707461,640.0503475559148,640.0503357249336,640.0503236737798,640.0503113983558,640.0502988944884,640.0502861579265,640.0502731843395,640.0502599693165,640.0502465083644,640.0502327969073,640.0502188302823,640.0502046037418,640.0501901124487,640.0501753514762,640.0501603158053,640.0501450003245,640.0501293998268,640.0501135090078,640.0500973224649,640.0500808346953,640.0500640400932,640.0500469329485,640.0500295074453,640.0500117576589,640.0499936775548,640.0499752609862,640.0499565016914,640.049937393293,640.0499179292943,640.049898103078,640.0498779079035,640.0498573369048,640.049836383088,640.0498150393295,640.0497932983731,640.0497711528271,640.0497485951623,640.0497256177098,640.0497022126578,640.0496783720495,640.049654087779,640.0496293515912,640.0496041550756,640.0495784896661,640.0495523466383,640.0495257171028,640.0494985920076,640.04947096213,640.0494428180773,640.0494141502813,640.0493849489957,640.0493552042933,640.0493249060617,640.0492940440007,640.0492626076184,640.0492305862273,640.0491979689416,640.0491647446727,640.0491309021252,640.0490964297944,640.0490613159611,640.0490255486882,640.0489891158161,640.0489520049596,640.0489142035021,640.0488756985933,640.0488364771435,640.0487965258188,640.0487558310384,640.0487143789676,640.0486721555149,640.0486291463267,640.0485853367823,640.0485407119887,640.0484952567758,640.0484489556923,640.0484017929979,640.04835375266,640.0483048183482,640.0482549734281,640.0482042009552,640.0481524836703,640.0480998039933,640.0480461440166,640.0479914854992,640.0479358098614,640.0478790981781,640.0478213311711,640.0477624892039,640.0477025522755,640.047641500011,640.0475793116586,640.047515966079,640.0474514417403,640.0473857167094,640.0473187686464,640.0472505747945,640.0471811119737,640.0471103565742,640.0470382845449,640.0469648713893,640.0468900921535,640.0468139214212,640.0467363333007,640.0466573014222,640.0465767989223,640.0464947984395,640.0464112721023,640.0463261915224,640.0462395277817,640.046151251425,640.0460613324498,640.0459697402939,640.0458764438288,640.0457814113453,640.0456846105446,640.045586008528,640.0454855717845,640.0453832661797,640.0452790569442,640.0451729086628,640.0450647852599,640.0449546499913,640.0448424654271,640.0447281934431,640.0446117952052,640.0444932311569,640.0443724610072,640.0442494437135,640.044124137473,640.0439964997039,640.0438664870314,640.0437340552762,640.0435991594367,640.0434617536741,640.0433217912976,640.0431792247477,640.0430340055818,640.0428860844552,640.0427354111062,640.0425819343387,640.0424256020048,640.0422663609863,640.0421041571783,640.041938935469,640.0417706397233,640.0415992127608,640.0414245963394,640.0412467311339,640.0410655567158,640.0408810115339,640.0406930328916,640.040501556928,640.0403065185944,640.040107851634,640.0399054885565,640.0396993606191,640.0394893977995,640.039275528775,640.0390576808976,640.0388357801676,640.0386097512113,640.0383795172537,640.0381450000925,640.0379061200728,640.0376627960591,640.0374149454079,640.0371624839407,640.0369053259135,640.0366433839906,640.0363765692117,640.0361047909655,640.0358279569554,640.0355459731713,640.0352587438556,640.0349661714714,640.0346681566712,640.0343645982607,640.0340553931661,640.0337404363988,640.0334196210199,640.0330928381045,640.0327599767044,640.0324209238104,640.0320755643145,640.0317237809712,640.0313654543577,640.0310004628324,640.0306286824964,640.0302499871487,640.0298642482459,640.0294713348571,640.0290711136215,640.0286634487007,640.0282482017361,640.0278252317996,640.0273943953473,640.0269555461703,640.0265085353464,640.0260532111892,640.0255894191964,640.0251170019991,640.0246357993076,640.0241456478572,640.0236463813543,640.0231378304197,640.022619822531,640.0220921819656,640.0215547297405,640.0210072835522,640.0204496577159,640.019881663103,640.0193031070759,640.0187137934249,640.018113522302,640.0175020901523,640.0168792896471,640.0162449096131,640.0155987349619,640.0149405466185,640.014270121445,640.0135872321687,640.0128916473036,640.0121831310744,640.0114614433355,640.0107263394921,640.0099775704161,640.0092148823649,640.0084380168942,640.0076467107722,640.0068406958914,640.0060196991791,640.0051834425044,640.0043316425866,640.0034640108995,640.002580253575,640.0016800713049,640.00076315924,639.9998292068886,639.9988778980132,639.9979089105238,639.9969219163709,639.9959165814348,639.9948925654162,639.9938495217194,639.9927870973397,639.9917049327439,639.9906026617501,639.9894799114068,639.9883363018671,639.9871714462636,639.985984950579,639.9847764135142,639.9835454263549,639.9822915728371,639.9810144290066,639.9797135630788,639.9783885352962,639.9770388977802,639.9756641943848,639.9742639605431,639.9728377231148,639.9713850002281,639.9699053011211,639.9683981259778,639.9668629657638,639.9652993020572,639.9637066068776,639.9620843425112,639.9604319613331,639.958748905626,639.9570346073976,639.9552884881911,639.953509958896,639.9516984195534,639.9498532591588,639.9479738554605,639.9460595747545,639.9441097716776,639.9421237889937,639.9401009573794,639.9380405952033,639.9359420083033,639.9338044897582,639.9316273196577,639.9294097648649,639.9271510787777,639.9248505010855,639.9225072575186,639.9201205595988,639.9176896043796,639.9152135741856,639.9126916363467,639.9101229429268,639.9075066304486,639.9048418196126,639.9021276150127,639.8993631048447,639.8965473606131,639.8936794368278,639.8907583707019,639.8877831818372,639.8847528719106,639.8816664243509,639.8785228040119,639.8753209568399,639.8720598095338,639.8687382692012,639.8653552230087,639.8619095378241,639.8584000598546,639.8548256142778,639.8511850048678,639.8474770136115,639.8437004003224,639.8398539022465,639.835936233659,639.8319460854597,639.8278821247538,639.8237429944351,639.8195273127528,639.8152336728782,639.8108606424603,639.8064067631749,639.8018705502664,639.7972504920831,639.7925450496011,639.7877526559455,639.7828717158998,639.7779006054074,639.7728376710683,639.7676812296233,639.7624295674326,639.7570809399438,639.7516335711548,639.7460856530628,639.7404353451086,639.7346807736112,639.7288200311909,639.7228511761864,639.7167722320603,639.7105811867955,639.7042759922839,639.6978545637022,639.6913147788807,639.6846544776604,639.6778714612409,639.6709634915178,639.66392829041,639.6567635391764,639.6494668777214,639.6420359038912,639.6344681727583,639.6267611958943,639.6189124406347,639.6109193293273,639.6027792385756,639.5944894984649,639.5860473917819,639.5774501532185,639.5686949685668,639.5597789739006,639.5506992547453,639.5414528452357,639.5320367272623,639.5224478296041,639.5126830270485,639.502739139502,639.4926129310827,639.4823011092064,639.4718003236532,639.4611071656273,639.4502181667989,639.4391297983365,639.4278384699232,639.4163405287609,639.4046322585616,639.3927098785236,639.3805695422959,639.3682073369274,639.3556192818028,639.3428013275652,639.3297493550239,639.316459174049,639.3029265224524,639.2891470648512,639.2751163915237,639.2608300172444,639.2462833801076,639.2314718403389,639.2163906790885,639.2010350972122,639.1854002140383,639.1694810661194,639.153272605971,639.1367697007929,639.1199671311814,639.1028595898215,639.0854416801685,639.0677079151133,639.049652715637,639.031270409446,639.0125552295971,638.9935013131073,638.9741026995491,638.954353329633,638.934247043775,638.9137775806516,638.8929385757393,638.871723559845,638.8501259576155,638.8281390860429,638.8057561529492,638.7829702554632,638.7597743784813,638.7361613931182,638.7121240551439,638.6876550034094,638.6627467582596,638.637391719935,638.6115821669625,638.5853102545352,638.5585680128786,638.5313473456109,638.5036400280878,638.4754377057429,638.4467318924118,638.4175139686535,638.3877751800586,638.3575066355485,638.3266993056706,638.2953440208798,638.2634314698176,638.2309521975811,638.1978966039852,638.1642549418207,638.1300173151036,638.0951736773229,638.0597138296793,638.0236274193209,637.9869039375776,637.9495327181878,637.9115029355244,637.8728036028181,637.8334235703779,637.7933515238128,637.7525759822463,637.7110852965391,637.6688676475056,637.6259110441324,637.5822033217995,637.5377321405018,637.4924849830743,637.446449153417,637.3996117747301,637.3519597877428,637.3034799489568,637.254158828888,637.2039828103174,637.1529380865453,637.1010106596556,637.0481863387839,636.9944507383932,636.9397892765619,636.8841871732751,636.8276294487279,636.7701009216364,636.7115862075609,636.6520697172363,636.5915356549148,636.5299680167201,636.4673505890102,636.4036669467546,636.3389004519208,636.2730342518751,636.2060512777914,636.137934243078,636.0686656418115,635.9982277471852,635.9266026099699,635.8537720569883,635.7797176895965,635.7044208821849,635.6278627806853,635.5500243010918,635.4708861279938,635.3904287131177,635.3086322738818,635.2254767919613,635.140942011859,635.0550074394913,634.9676523407784,634.8788557402427,634.7885964196167,634.6968529164544,634.6036035227512,634.508826283565,634.4124989956438,634.3145992060549,634.2151042108162,634.1139910535268,634.0112365239986,633.9068171568872,633.8007092303164,633.6928887645026,633.5833315203711,633.4720129981688,633.3589084360655,633.2439928087508,633.1272408260176,633.0086269313348,632.8881253004095,632.7657098397326,632.6413541851146,632.5150317001984,632.3867154749639,632.2563783242092,632.1239927860165,631.989531120198,631.8529653067244,631.7142670441327,631.5734077479149,631.4303585488855,631.2850902915341,631.1375735323538,630.9877785381577,630.8356752843722,630.68123345332,630.5244224324862,630.3652113127745,630.2035688867518,630.0394636468884,629.872863783794,629.7037371844518,629.5320514304617,629.3577737962869,629.180871247519,629.0013104391608,628.8190577139374,628.6340791006373,628.4463403124975,628.2558067456364,628.0624434775445,627.8662152656435,627.6670865459238,627.4650214316708,627.259983712294,627.0519368522694,626.8408439902113,626.6266679380872,626.4093711805893,626.1889158746854,625.9652638493567,625.738376605553,625.5082153163717,625.274740827495,625.0379136578923,624.7976940008249,624.5540417251615,624.3069163770442,624.0562771819174,623.8020830469561,623.5442925639156,623.2828640124342,623.0177553638168,622.748924285333,622.4763281450586,622.199924017297,621.9196686886114,621.6355186645019,621.347430176772,621.0553591916073,620.7592614184172,620.4590923194711,620.1548071203722,619.8463608214045,619.5337082098017,619.2168038729731,618.8956022127306,618.570057460565,618.2401236940057,617.9057548541192,617.5669047641823,617.2235271495795,616.875575658968,616.5230038867593,616.165765396958,615.8038137484099,615.4371025214987,615.0655853463438,614.6892159325414,614.3079481004925,613.9217358143698,613.5305332167596,613.1342946650317,612.732974769472,612.3265284332306,611.9149108941187,611.4980777682988,611.0759850959121,610.6485893886748,610.2158476794865,609.7777175740841,609.3341573047776,608.8851257862988,608.4305826737959,607.9704884230014,607.5048043526026,607.0334927088361,606.5565167323323,606.0738407272271,605.5854301325597,605.091251595968,604.591273049695,604.0854637889138,603.5737945523738,603.0562376053698,602.5327668250304,602.0033577879198,601.467987859937,600.9266362885026,600.3792842970063,599.8259151814918,599.2665144095495,598.7010697213751,598.1295712329571,597.5520115413418,596.9683858319199,596.3786919876749,595.7829307003242,595.181105583276,594.57322328632,593.9592936119581,593.3393296332785,592.7133478132607,592.081368125399,591.4434141755124,590.7995133246087,590.1496968126532,589.493999883087,588.8324619079253,588.1651265132577,587.4920417049583,586.8132599944048,586.128838523987,585.4388391921838,584.7433287779584,584.042379064226,583.3360669601182,582.624474621765,581.9076895712981,581.1858048137619,580.4589189516039,579.727136296408,578.9905669775092,578.2493270471173,577.503538581567,576.7533297782827,575.9988350480494,575.2401951021479,574.4775570339126,573.7110743942437,572.9409072606015,572.1672222989905,571.3901928184299,570.609998817399,569.8268270217271,569.0408709133972,568.2523307497165,567.4614135723037,566.668333205338,565.8733102425088,565.0765720221048,564.2783525896843,563.4788926477657,562.6784394919928,561.8772469332262,561.0755752050355,560.2736908560769,559.4718666268568,558.6703813104139,557.8695195964675,557.0695718986224,556.2708341642457,555.4736076666785,554.6781987794848,553.8849187324918,553.0940833494307,552.3060127670419,551.5210311355775,550.7394663006993,549.9616494668451,549.1879148422136,548.4185992655994,547.6540418154017,546.8945834012069,546.140566338458,545.3923339067992,544.6502298927979,543.9145981178397,543.1857819520909,542.4641238155286,541.749964667137,541.043643483465,540.34549672784,539.6558578116245,538.9750565489883,538.3034186067571,537.6412649509709,536.9889112918564,536.3466675289809,535.7148371984035,535.0937169236844,534.4835958726436,533.8847552217771,533.2974676302516,532.7219967253868,532.1585966015234,531.6075113341348,531.0689745110069,530.543208782243,530.0304254307854,529.5308239650612,529.0445917352688,528.5719035747093,528.1129214674579,527.6677942435456,527.2366573026848,526.8196323674388,526.4168272665927,526.0283357493306,525.6542373306821,525.2945971685476,524.9494659724686,524.6188799441591,524.3028607496823,524.0014155230133,523.714536900613,523.4422030865098,523.1843779472894,522.941011136289,522.7120382462105,522.4973809893,522.2969474041794,522.1106320883725,521.9383164555444,521.7798690164509,521.6351456826009,521.5039900916433,521.3862339535202,521.2816974164627,521.1901894519641,521.1115082579154,521.0454416791689,520.9917676448673,520.9502546219568,520.920662084396,520.9027409976541,520.896234318183,520.9008775076279,520.9163990616212,520.9425210530711,520.9789596899108,521.0254258873167,521.0816258544227,521.1472616955604,521.2220320260263,521.3056326023288,521.3977569667759,521.4980971061564,521.606344124109,521.7221889265873,521.8453229196,521.9754387181497,522.1122308649877,522.2553965574778,522.4046363804997,522.5596550429404,522.7201621149208,522.8858727624996,523.0565084761835,523.2317977891843,523.4114769809858,523.5952907614544,523.7829929304384,523.9743470075872,524.1691268269724,524.3671170910393,524.5681138784638,524.7719251006329,524.9783709017354,525.1872839978197,525.3985099506624,525.6119073728902,525.8273480614837,526.0447170575711,526.2639126312652,526.4848461911932,526.707442119294,526.9316375323846,527.1573819729206,527.3846370322411,527.6133759104127,527.8435829175131,528.0752529218416,528.3083907510663,528.5430105527314,528.7791351208311,529.016795195319,529.256028741454,529.4968802158073,529.7393998255623,529.9836427874516,530.229668592309,530.4775402807712,530.7273237351662,530.9790869921047,531.2328995797161,531.48883188292,531.7469545395438,532.0073378695556,532.2700513391405,532.5351630608586,532.802739330645,533.0728442020022,533.3455390973403,533.6208824560892,533.8989294189131,534.179731547106,534.4633365760405,534.7497882013687,535.0391258965503,535.3313847601756,535.6265953914844,535.9247837924482,536.2259712947489,536.5301745100021,536.8374053015893,537.147670776485,537.4609732955263,537.7773105006092,538.0966753573683,538.4190562119497,538.7444368605677,539.0727966305942,539.404110472009,539.7383490581036,540.0754788944066,540.4154624348598,540.7582582043445,541.1038209267192,541.4521016575874,541.803047921076,542.156603849952,542.5127103284636,542.8713051373317,543.2323231003674,543.595696232232,543.9613538868896,544.329222906347,544.6992277692997,545.0712907393392,545.4453320124048,545.8212698631874,546.1990207902181,546.5784996593999,546.9596198457537,547.3422933731823,547.7264310520611,548.1119426144874,548.4987368470368,548.886721720887,549.2758045191822,549.6658919615292,550.0568903255198,550.4487055651957,550.8412434263703,551.234409558745,551.6281096247567,552.0222494051045,552.4167349009177,552.8114724325247,553.2063687348011,553.6013310490725,553.9962672115632,554.3910857383783,554.7856959070248,555.1800078344721,555.5739325517644,555.9673820752005,556.3602694741012,556.7525089351913,557.1440158236227,557.5347067406768,557.9244995781813,558.3133135696829,558.7010693384227,559.087688942162,559.4730959149076,559.8572153055956,560.2399737137843,560.6212993224216,561.001121927742,561.379372966362,561.7559855396355,562.1308944353361,562.5040361467375,562.8753488891559,563.244772614031,563.612249020612,563.9777215653228,564.3411354688795,564.7024377212335,565.0615770844099,565.4185040933216,565.7731710546262,566.1255320437031,566.4755428998232,566.8231612195837,567.1683463486819,567.5110593720981,567.8512631027612,568.1889220687658,568.5240024992141,568.8564723087459,569.1863010808311,569.5134600498882,569.8379220822948,570.1596616563576,570.4786548413014,570.7948792753449,571.1083141429193,571.4189401510926,571.7267395052577,572.0316958841399,572.3337944141817,572.6330216433573,572.9293655144701]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[269.01026563590466,269.01026500596777,269.010264364308,269.01026371070685,269.0102630449428,269.0102623667888,269.0102616760147,269.0102609723857,269.0102602556621,269.01025952560065,269.0102587819529,269.0102580244664,269.0102572528831,269.01025646694075,269.0102556663727,269.01025485090656,269.0102540202645,269.0102531741648,269.01025231231927,269.0102514344356,269.0102505402144,269.01024962935264,269.01024870154015,269.01024775646147,269.0102467937952,269.0102458132144,269.01024481438543,269.0102437969689,269.0102427606189,269.01024170498255,269.0102406297018,269.0102395344107,269.010238418737,269.0102372823011,269.0102361247168,269.0102349455909,269.0102337445223,269.0102325211023,269.010231274916,269.01023000553835,269.01022871253883,269.01022739547767,269.0102260539067,269.01022468737074,269.01022329540393,269.0102218775345,269.0102204332792,269.0102189621478,269.0102174636399,269.0102159372463,269.01021438244777,269.010212798716,269.01021118551245,269.01020954228886,269.0102078684865,269.01020616353645,269.0102044268592,269.01020265786394,269.0102008559496,269.0101990205043,269.01019715090285,269.0101952465107,269.01019330667987,269.01019133075147,269.010189318053,269.0101872679013,269.0101851795989,269.01018305243576,269.010180885689,269.0101786786222,269.01017643048544,269.0101741405137,269.01017180792905,269.01016943193844,269.01016701173427,269.01016454649414,269.0101620353802,269.01015947753785,269.01015687209866,269.0101542181767,269.0101515148699,269.0101487612592,269.01014595640913,269.0101430993658,269.01014018915856,269.01013722479746,269.01013420527596,269.0101311295669,269.01012799662556,269.0101248053865,269.0101215547653,269.0101182436572,269.0101148709366,269.0101114354575,269.01010793605184,269.0101043715307,269.0101007406824,269.0100970422727,269.0100932750451,269.01008943771876,269.0100855289901,269.01008154753015,269.01007749198595,269.01007336097945,269.01006915310666,269.010064866937,269.01006050101466,269.01005605385535,269.0100515239478,269.0100469097529,269.0100422097025,269.0100374221993,269.0100325456161,269.01002757829656,269.01002251855175,269.0100173646631,269.0100121148786,269.0100067674149,269.01000132045465,269.00999577214657,269.009990120606,269.0099843639123,269.00997850010975,269.0099725272056,269.00996644317047,269.0099602459371,269.00995393339997,269.00994750341425,269.00994095379497,269.00993428231754,269.0099274867149,269.00992056467805,269.0099135138558,269.0099063318524,269.009899016228,269.00989156449725,269.00988397412857,269.00987624254356,269.0098683671157,269.0098603451695,269.0098521739802,269.0098438507717,269.0098353727169,269.00982673693636,269.0098179404961,269.0098089804084,269.0097998536303,269.0097905570612,269.00978108754407,269.0097714418624,269.0097616167399,269.00975160884036,269.0097414147642,269.0097310310497,269.0097204541705,269.00970968053474,269.0096987064837,269.00968752829067,269.00967614215983,269.0096645442246,269.00965273054754,269.00964069711665,269.00962843984615,269.00961595457466,269.0096032370624,269.0095902829922,269.0095770879658,269.00956364750357,269.00954995704274,269.00953601193544,269.00952180744775,269.00950733875845,269.0094926009555,269.00947758903646,269.00946229790617,269.00944672237404,269.00943085715414,269.00941469686177,269.00939823601254,269.00938146902,269.00936439019404,269.0093469937391,269.00932927375214,269.00931122422014,269.0092928390188,269.00927411191014,269.00925503654037,269.009235606438,269.00921581501086,269.009195655545,269.00917512120213,269.0091542050169,269.00913289989376,269.0091111986071,269.00908909379604,269.0090665779642,269.00904364347576,269.0090202825528,269.0089964872739,269.0089722495702,269.00894756122347,269.0089224138635,269.0088967989642,269.00887070784154,269.0088441316513,269.00881706138443,269.00878948786544,269.0087614017483,269.00873279351447,269.00870365346884,269.0086739717358,269.00864373825846,269.0086129427918,269.0085815749028,269.0085496239632,269.0085170791504,269.00848392943914,269.00845016360137,269.0084157702004,269.008380737588,269.0083450539013,269.008308707056,269.00827168474643,269.0082339744366,269.0081955633609,269.0081564385162,269.00811658665907,269.00807599430186,269.00803464770627,269.0079925328813,269.00794963557587,269.0079059412762,269.0078614352001,269.00781610229245,269.0077699272192,269.00772289436367,269.0076749878201,269.00762619138925,269.00757648857274,269.0075258625668,269.00747429625824,269.00742177221747,269.007368272693,269.00731377960597,269.0072582745439,269.0072017387546,269.0071441531395,269.0070854982488,269.00702575427204,269.00696490103564,269.0069029179929,269.0068397842181,269.0067754784012,269.00670997883793,269.00664326342553,269.00657530965304,269.0065060945957,269.00643559490624,269.00636378680736,269.0062906460853,269.00621614807926,269.0061402676756,269.00606297929886,269.00598425690293,269.0059040739628,269.00582240346614,269.0057392179041,269.00565448926295,269.0055681890136,269.0054802881033,269.0053907569463,269.0052995654138,269.005206682824,269.0051120779323,269.00501571892096,269.0049175733892,269.0048176083425,269.0047157901812,269.00461208469073,269.0045064570295,269.00439887171893,269.00428929263023,269.00417768297376,269.0040640052874,269.003948221424,269.00383029253913,269.0037101790788,269.00358784076565,269.0034632365888,269.0033363247869,269.0032070628381,269.00307540744416,269.00294131451824,269.0028047391693,269.00266563568925,269.0025239575373,269.0023796573255,269.00223268680384,269.0020829968452,269.0019305374293,269.00177525762604,269.001617105582,269.0014560285015,269.0012919726306,269.0011248832411,269.00095470461355,269.00078138001686,269.0006048516946,269.00042506084446,269.0002419476,269.00005545101317,268.9998655090337,268.999672058492,268.99947503507696,268.99927437331905,268.99907000656754,268.9988618669706,268.9986498854556,268.99843399170663,268.9982141141437,268.9979901799001,268.99776211480093,268.99752984334026,268.99729328865806,268.9970523725172,268.99680701527905,268.99655713588095,268.9963026518098,268.9960434790791,268.9957795322033,268.99551072417074,268.9952369664211,268.99495816881495,268.9946742396105,268.9943850854346,268.99409061125596,268.99379072035657,268.9934853143047,268.99317429292455,268.9928575542683,268.99253499458587,268.99220650829517,268.9918719879515,268.9915313242175,268.99118440583044,268.99083111957134,268.9904713502334,268.99010498058806,268.9897318913522,268.9893519611563,268.988965066508,268.9885710817596,268.9881698790721,268.98776132838054,268.9873452973578,268.9869216513789,268.9864902534837,268.9860509643395,268.98560364220526,268.9851481428911,268.98468431972134,268.98421202349516,268.98373110244654,268.983241402206,268.9827427657575,268.98223503340137,268.98171804270874,268.9811916284833,268.9806556227182,268.9801098545522,268.97955415022767,268.9789883330483,268.9784122233313,268.97782563836824,268.9772283923762,268.9766202964551,268.9760011585401,268.97537078335756,268.9747289723779,268.9740755237682,268.9734102323463,268.97273288953227,268.97204328330236,268.97134119813825,268.97062641498087,268.9698987111809,268.96915786044934,268.9684036328091,268.96763579454455,268.9668541081521,268.9660583322908,268.9652482217309,268.9644235273049,268.96358399585563,268.96272937018716,268.9618593890124,268.960973786904,268.9600722942416,268.9591546371634,268.9582205375127,268.9572697127896,268.956301876098,268.9553167360974,268.9543139969506,268.9532933582741,268.9522545150895,268.9511971577712,268.95012097200004,268.949025638713,268.94791083405414,268.9467762293281,268.9456214909517,268.9444462804083,268.9432502542001,268.94203306380507,268.94079435563015,268.93953377096943,268.93825094596065,268.9369455115436,268.9356170934202,268.9342653120148,268.9328897824362,268.93149011444086,268.93006591239856,268.92861677525775,268.9271422965151,268.9256420641837,268.92411566076555,268.9225626632265,268.9209826429704,268.91937516581805,268.9177397919881,268.9160760760792,268.91438356705686,268.91266180824095,268.9109103372979,268.90912868623417,268.9073163813953,268.9054729434665,268.9035978874779,268.90169072281196,268.89975095321756,268.89777807682566,268.89577158617067,268.893730968216,268.8916557043846,268.88954527059343,268.88739913729563,268.8852167695246,268.8829976269467,268.88074116391886,268.8784468295513,268.87611406777944,268.8737423174384,268.87133101234974,268.8688795814097,268.8663874486896,268.8638540335427,268.86127875071804,268.8586610104842,268.85600021876127,268.85329577726384,268.8505470836485,268.84775353167896,268.84491451139445,268.8420294092921,268.83909760852094,268.8361184890856,268.8330914280625,268.830015799829,268.82689097630544,268.8237163272082,268.82049122031987,268.8172150217699,268.8138870963344,268.8105068077454,268.8070735190209,268.8035865928081,268.80004539174354,268.7964492788323,268.7927976178424,268.7890897737192,268.7853251130181,268.78150300435664,268.7776228188874,268.77368393078984,268.76968571778497,268.7656275616712,268.7615088488827,268.7573289710714,268.75308732571125,268.74878331672915,268.7444163551587,268.73998585982116,268.7354912580323,268.73093198633717,268.72630749126915,268.7216172301437,268.7168606718749,268.712037297826,268.7071466026881,268.70218809539233,268.6971613000512,268.6920657569345,268.68690102347756,268.68166667532455,268.6763623074054,268.6709875350473,268.66554199512456,268.66002534724225,268.6544372749596,268.6487774870505,268.64304571879984,268.63724173334356,268.6313653230445,268.62541631091085,268.619394552053,268.6132999351834,268.60713238415747,268.6008918595563,268.59457836031106,268.5881919253728,268.58173263542153,268.5752006146208,268.56859603241736,268.5619191053795,268.5551700990849,268.54834933004724,268.5414571676906,268.5344940363646,268.52746041740363,268.52035685123053,268.51318393950066,268.5059423472916,268.49863280533015,268.491256112266,268.48381313698,268.476304820937,268.46873218057493,268.46109630973103,268.453398382105,268.4456396537588,268.43782146564604,268.4299452461782,268.4220125138165,268.41402487969424,268.4059840502655,268.39789182997663,268.3897501239589,268.38156094074145,268.37332639497856,268.365048710191,268.35673022151684,268.3483733784676,268.3399807476891,268.3315550157197,268.3230989917441,268.3146156103368,268.3061079341931,268.29757915684036,268.2890326053249,268.28047174287184,268.2719001715085,268.263321634649,268.2547400196313,268.24615936020245,268.237583838945,268.229017789636,268.2204656995349,268.211932211589,268.20342212655225,268.19494040500695,268.186492169282,268.17808270525836,268.1697174640524,268.16140206356863,268.1531422899146,268.14494409866364,268.1368136159607,268.12875713945795,268.12078113907205,268.11289225754956,268.10509731083414,268.0974032882204,268.08981735228485,268.08234683858507,268.07499925510865,268.06778228147175,268.0607037678402,268.0537717335767,268.04699436559065,268.04038001638355,268.03393720177814,268.02767459831443,268.0216010403079,268.01572551654874,268.01005716663644,268.00460527693576,267.9993792761387,267.99438873042493,267.989643338206,267.98515292444165,267.9809274345166,267.97697692766815,267.9733115699495,267.96994162672115,267.9668774546582,267.96412949326395,267.96170825587797,267.9596243201705,267.9578883181143,267.9565109254222,267.9555028504459,267.9548748225251,267.9546375797804,267.95480185634256,267.9553783690142,267.9563778033536,267.95781079917947,267.9596879354908,267.96201971479627,267.96481654685226,267.9680887318051,267.97184644273545,267.97609970760584,267.9808583906056,267.9861321728989,267.991930532773,267.9982627251867,268.00513776072586,268.01256438396246,268.02055105122486,268.02910590778004,268.0382367644344,268.0479510735539,268.0582559045148,268.06915791858506,268.0806633432449,268.09277794595425,268.1055070073717,268.11885529403344,268.13282703049936,268.14742587097413,268.162654870412,268.1785164551102,268.19501239280373,268.21214376226527,268.2299109224209,268.2483134809879,268.26735026264373,268.2870192767332,268.3073176845203,268.32824176599286,268.34978688622573,268.37194746131,268.39471692385143,268.41808768804617,268.44205111433723,268.4665974736552,268.49171591124764,268.51739441010085,268.5436197539536,268.570377489909,268.59765189064103,268.62542591619973,268.65368117541345,268.68239788688766,268.71155483960024,268.741129353091,268.7710972372441,268.80143275165995,268.832108564617,268.86309571161814,268.8943635535211,268.9258797342492,268.957610138083,268.9895188465272,269.0215680947563,269.05371822763607,269.0859276553224,269.11815280843916,269.15034809284043,269.1824658439599,269.21445628075594,269.2462674592612,269.27784522574973,269.3091331695348,269.34007257541555,269.370602375796,269.400659102498,269.43017683830067,269.4590871682386,269.4873191306989,269.5147991683598,269.54145107902355,269.567195966397,269.5919521908862,269.6156353204717,269.6381580817464,269.659430311198,269.679358906833,269.6978477802461,269.71479780924506,269.7301067911573,269.7436693969506,269.75537712631206,269.7651182638415,269.77277783652835,269.77823757269067,269.7813758625717,269.78206772079875,269.7801847509261,269.77559511229583,269.76816348946676,269.757751064474,269.74421549219915,269.72741087914653,269.7071877659337,269.68339311382505,269.65587029564716,269.6244590914474,269.5889956892665,269.5493126914163,269.50523912666614,269.45660046876054,269.4032186616995,269.3449121522317,269.2814959300248,269.2127815759832,269.13857731920353,269.05868810306276,268.97291566094316,268.88105860210976,268.7829125082581,268.678270041253,268.56692106258646,268.4486527650755,268.3232498173209,268.19049452144407,268.0501669846036,267.90204530478945,267.74590577136905,267.5815230808477,267.40867056827494,267.22712045470837,267.0366441111114,266.8370123390248,266.62799566831364,266.4093646722465,266.1808903001113,265.9423442275197,265.6934992244939,265.4341295413624,265.16401131242486,264.88292297727304,264.5906457195768,264.28696392306404,263.9716656443354,263.6445431020701,263.3053931820853,262.954017957621,262.5902252241219,262.21382904769996,261.8246503263556,261.4225173629481,261.0072664488056,260.5787424567749,260.1367994424222,259.6813012520105,259.21212213579776,258.72914736512826,258.2322738517198,257.7214107674903,257.19648016321514,256.6574175842683,256.1041726816609,255.53670981657655,254.95500865658994,254.3590647617509,253.74889015873882,253.12451390130423,252.48598261526035,251.83336102632478,251.1667324691749,250.48619937614356,249.7918837440604,249.0839275778234,248.36249330938264,247.62776419090625,246.87994466100585,246.11926068299522,245.3459600542596,244.56031268591644,243.7626108520393,242.9531694078141,242.1323259760765,241.30044110175376,240.45789837379687,239.60510451423403,238.74248943401224,237.87050625530486,236.98963129995647,236.10036404371573,235.2032270358507,234.2987657836758,233.38754860141964,232.4701664227445,231.54723257607898,230.6193825217612,229.68727354978785,228.7515844367563,227.81301506033768,226.87228596937058,225.93013790738482,224.98733128708085,224.0446456129902,223.10287884924878,222.1628467291098,221.22538200254135,220.29133361797568,219.36156583403329,218.4369572568323,217.51839979832465,216.6067975509959,215.70306557422097,214.8081285876145,213.92291956685412,213.04837823770322,212.1854494643422,211.3350815286238,210.4982242975409,209.67582727702285,208.8688375511764,208.0781976072721,207.304843048139,206.54970019517447,205.8136835868994,205.09769337986648,204.4026126607641,203.7293046806961,203.0786100248566,202.4513437330909,201.84829238910734,201.27021119832492,200.71782107643372,200.19180577266263,199.6928090534129,199.2214319732591,198.77823026128323,198.36371185122056,197.9783345839162,197.62250411006735,197.29657202013897,197.00083422666432,196.73552962190206,196.5008390310138,196.29688447762962,196.12372877490603,195.98137545106948,195.86976901403878,195.7887955551641,195.73828368749932,195.71800580947087,195.72767968042584,195.76697029044163,195.83549200306825,195.9328109464202,196.0584476253238,196.2118797250906,196.392545075969,196.59984474642695,196.83314623312958,197.09178671576987,197.37507634574692,197.68230153899938,198.0127282450417,198.365605166332,198.74016690444952,199.13563701211513,199.55123093275716,199.98615881205134,200.43962816858468,200.91084641344747,201.39902321110705,201.90337267631404,202.42311540401715,202.95748033128916,203.50570643207627,204.06704424718464,204.6407572532918,205.22612307593536,205.82243455239063,206.42900065110925,207.04514725498103,207.670217816097,208.30357388997047,208.9445955573129,209.59268174149537,210.24725042975803,210.90773880608538,211.5736033034479,212.24431958284603,212.9193824462804,213.59830569043288,214.2806219074814,214.96588223909563,215.65365608927908,216.343530801339,217.03511130389202,217.7280197304372,218.42189501667514,219.11639247940363,219.81118338049342,220.50595447913227,221.2004075752345,221.89425904663085,222.58723938240163,223.27909271446651,223.96957634932707,224.65846030164926,225.34552683117914,226.03056998431586,226.71339514149804,227.3938185714203,228.07166699295436,228.74677714553414,229.41899536864975,230.08817719099486,230.7541869297203,231.4168973001688,232.07618903638271,232.73195052262074,233.38407743604986,234.03247240072932,234.67704465295517,235.31770971798903,235.95438909815547,236.5870099722604,237.21550490624853,237.83981157499312,238.45987249508693,239.07563476847795,239.6870498367813,240.29407324607482,240.8966644219763,241.49478645478504,242.0884058944612,242.6774925552043,243.2620193293879,243.84196201059686,244.41729912551196,244.98801177437934,245.55408347979852,246.11550004356434,246.67224941129084,247.22432154454862,247.77170830024565,248.31440331697954,248.85240190809628,249.38570096118485,249.91429884374563,250.4381953147696,250.95739144196898,251.47188952440527,251.9816930202621,252.48680647951699,252.9872354812697,253.48298657548793,253.97406722894183,254.46048577509643,254.9422513677449,255.4193739381648,255.8918641555901,256.35973339079476,256.8229936825936,257.2816577070678,257.7357387493339,258.1852506776786,258.63020791988924,259.0706254416176,259.5065187266179,259.9379037587123,260.36479700533823,260.7872154025414,261.20517634128527,261.6186976549528,262.0277976079222,262.43249488510924,262.8328085823666,263.22875819764727,263.62036362283516,264.0076451361582,264.39062339510326,264.7693194297559,265.1437546364976,265.51395077199174,265.8799299474032,266.2417146227916,266.59932760163315,266.95279202542054]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[228.9463385969024,228.94633394280694,228.946329202099,228.94632437316642,228.94631945436834,228.94631444403117,228.94630934045267,228.94630414189703,228.9462988465964,228.94629345275092,228.94628795852643,228.94628236205523,228.94627666143387,228.94627085472447,228.94626493995344,228.94625891510978,228.94625277814367,228.94624652696967,228.94624015946218,228.9462336734567,228.94622706674693,228.94622033708828,228.9462134821917,228.9462064997262,228.94619938731842,228.94619214255042,228.94618476295815,228.94617724603367,228.94616958922086,228.9461617899158,228.94615384546788,228.94614575317541,228.94613751028754,228.94612911400128,228.94612056146224,228.94611184976287,228.94610297594124,228.9460939369797,228.9460847298068,228.94607535128998,228.94606579824207,228.9460560674154,228.94604615550028,228.9460360591289,228.9460257748659,228.94601529921806,228.94600462862095,228.94599375944858,228.9459826880046,228.94597141052577,228.94595992317755,228.94594822205414,228.9459363031776,228.94592416249665,228.94591179588247,228.94589919913125,228.94588636796053,228.94587329800754,228.9458599848288,228.9458464238994,228.94583261060737,228.94581854025782,228.94580420806588,228.94578960916087,228.94577473857717,228.9457595912614,228.94574416206322,228.9457284457365,228.945712436939,228.9456961302276,228.9456795200599,228.94566260078733,228.94564536665857,228.94562781181483,228.9456099302879,228.94559171599917,228.945573162757,228.94555426425163,228.94553501406043,228.94551540563853,228.94549543232048,228.9454750873147,228.94545436370686,228.94543325445042,228.94541175236978,228.94538985015416,228.94536754035997,228.9453448154008,228.9453216675532,228.94529808894686,228.94527407156667,228.9452496072481,228.9452246876747,228.94519930437505,228.94517344871997,228.94514711192102,228.94512028502479,228.9450929589111,228.94506512429228,228.9450367717048,228.94500789151223,228.94497847389584,228.94494850885627,228.94491798620763,228.94488689557457,228.94485522638786,228.94482296788325,228.94479010909475,228.9447566388523,228.94472254578042,228.94468781828897,228.94465244457385,228.94461641261083,228.9445797101526,228.944542324723,228.94450424361514,228.94446545388428,228.94442594234613,228.94438569557033,228.94434469987613,228.9443029413306,228.9442604057389,228.9442170786442,228.94417294531982,228.94412799076474,228.94408219969998,228.94403555656095,228.94398804549547,228.94393965035414,228.9438903546896,228.94384014174707,228.94378899445996,228.94373689544545,228.94368382699625,228.94362977107656,228.94357470931408,228.94351862299558,228.9434614930602,228.9434033000917,228.9433440243128,228.94328364557919,228.94322214337055,228.94315949678628,228.94309568453673,228.94303068493562,228.94296447589358,228.9428970349118,228.94282833907073,228.94275836502692,228.9426870890014,228.94261448677258,228.94254053367038,228.94246520456406,228.94238847385637,228.94231031547443,228.94223070285997,228.94214960896124,228.9420670062227,228.9419828665774,228.94189716143555,228.9418098616779,228.94172093764138,228.94163035911203,228.94153809531508,228.94144411490214,228.94134838594368,228.94125087591522,228.94115155168845,228.9410503795186,228.9409473250337,228.94084235322302,228.94073542842548,228.94062651431628,228.94051557389494,228.94040256947486,228.94028746266704,228.94017021437068,228.940050784757,228.939929133258,228.93980521855048,228.9396789985442,228.93955043036513,228.93941947034497,228.9392860740021,228.939150196028,228.93901179027355,228.9388708097307,228.93872720651916,228.93858093186753,228.9384319360992,228.93828016861468,228.9381255778743,228.93796811137898,228.9378077156569,228.93764433624082,228.9374779176529,228.93730840338466,228.93713573587664,228.93695985650166,228.93678070554262,228.93659822217234,228.93641234443677,228.93622300922806,228.93603015226742,228.93583370808315,228.9356336099863,228.93542979005036,228.93522217908676,228.93501070662316,228.93479530087805,228.934575888736,228.9343523957272,228.93412474599506,228.93389286227844,228.93365666587914,228.9334160766406,228.93317101291657,228.93292139154576,228.93266712782267,228.9324081354709,228.93214432661327,228.93187561173963,228.9316018996813,228.9313230975752,228.9310391108378,228.9307498431288,228.9304551963211,228.93015507046835,228.92984936376794,228.92953797253145,228.9292207911444,228.9288977120354,228.92856862563707,228.92823342035115,228.927891982509,228.92754419633476,228.92718994390535,228.92682910511232,228.92646155762014,228.9260871768252,228.92570583581445,228.92531740532343,228.92492175369023,228.9245187468144,228.9241082481104,228.92369011846185,228.9232642161743,228.92283039693046,228.92238851373642,228.92193841687765,228.92147995386722,228.92101296939228,228.9205373052666,228.92005280037208,228.91955929061052,228.91905660884385,228.91854458484136,228.9180230452211,228.9174918133921,228.91695070949694,228.91639955034873,228.91583814937263,228.91526631654193,228.91468385831553,228.9140905775722,228.91348627354594,228.91287074175827,228.9122437739516,228.9116051580163,228.91095467792343,228.91029211365273,228.90961724111648,228.9089298320881,228.908229654124,228.90751647048677,228.90679004006836,228.90605011730713,228.90529645210813,228.90452878975952,228.90374687084807,228.90295043117388,228.9021392016608,228.901312908269,228.90047127190402,228.89961400832462,228.89874082804695,228.8978514362519,228.89694553268336,228.89602281155427,228.89508296144092,228.8941256651834,228.89315059977926,228.89215743627776,228.89114583967176,228.89011546878675,228.8890659761683,228.88799700796932,228.88690820383226,228.88579919677193,228.88466961305468,228.8835190720742,228.88234718623045,228.88115356079933,228.87993779380412,228.878699475886,228.8774381901699,228.87615351212324,228.87484500942634,228.87351224182424,228.87215476098595,228.87077211035924,228.86936382501975,228.86792943152292,228.86646844774774,228.8649803827426,228.86346473656425,228.86192100011587,228.86034865498465,228.85874717327172,228.8571160174236,228.85545464005645,228.85376248378293,228.85203898102998,228.85028355385785,228.84849561377362,228.84667456154267,228.84481978699733,228.84293066883956,228.84100657444483,228.83904685965902,228.83705086859044,228.83501793340665,228.8329473741147,228.8308384983514,228.82869060115908,228.82650296476484,228.82427485835237,228.8220055378308,228.81969424559918,228.8173402103094,228.81494264662183,228.81250075495893,228.81001372125354,228.80748071669757,228.80490089747641,228.80227340451043,228.79959736318588,228.7968718830806,228.7940960576893,228.7912689641432,228.7883896629213,228.78545719756266,228.78247059436933,228.77942886210838,228.7763309917047,228.77317595593524,228.76996270911033,228.76669018675665,228.76335730529354,228.7599629617005,228.75650603318462,228.75298537684017,228.74939982930206,228.74574820639748,228.74202930278614,228.7382418916037,228.7343847240887,228.73045652921417,228.72645601330862,228.72238185966947,228.7182327281747,228.71400725488968,228.70970405165744,228.70532170570183,228.70085877920448,228.69631380889166,228.69168530560302,228.6869717538637,228.68217161144418,228.67728330891646,228.67230524920114,228.66723580711255,228.66207332889533,228.65681613175138,228.65146250336505,228.64601070142064,228.64045895311037,228.6348054546406,228.62904837072568,228.62318583408015,228.61721594490186,228.6111367703481,228.60494634400587,228.59864266535405,228.5922236992212,228.58568737523382,228.57903158725915,228.57225419283975,228.56535301262582,228.55832582979176,228.55117038945656,228.5438843980881,228.53646552290715,228.5289113912803,228.52121959010793,228.5133876652067,228.50541312068106,228.49729341829342,228.48902597682488,228.48060817142695,228.4720373329723,228.4633107473939,228.45442565502225,228.44537924991135,228.43616867916353,228.426791042244,228.4172433902917,228.40752272542446,228.3976260000356,228.38755011608902,228.37729192440582,228.3668482239456,228.35621576108474,228.34539122888782,228.33437126637438,228.32315245778332,228.3117313318284,228.30010436095327,228.28826796058402,228.27621848837208,228.26395224344017,228.25146546562016,228.23875433469237,228.2258149696197,228.21264342777968,228.19923570419695,228.18558773077066,228.1716953755046,228.15755444173553,228.1431606673605,228.12850972406437,228.11359721654935,228.09841868176554,228.08296958814256,228.06724533482299,228.051241250902,228.03495259466482,228.01837455283462,228.00150223982047,227.98433069697194,227.96685489184247,227.94906971745348,227.93096999157478,227.9125504560041,227.89380577586408,227.87473053890253,227.8553192548076,227.83556635453516,227.81546618964654,227.79501303166043,227.77420107142186,227.75302441848638,227.7314771005162,227.70955306270514,227.68724616721104,227.6645501926159,227.64145883340692,227.61796569947913,227.5940643156616,227.56974812127234,227.54501046969924,227.519844628007,227.4942437765794,227.4682010087873,227.4417093306974,227.41476166080815,227.3873508298281,227.3594695804901,227.33111056740623,227.30226635696675,227.2729294272794,227.24309216815817,227.2127468811584,227.1818857796629,227.1505009890188,227.11858454672992,227.08612840270573,227.05312441956886,227.0195643730232,226.98543995228715,226.95074276058983,226.9154643157389,226.87959605075784,226.84312931459527,226.8060553729138,226.76836540895468,226.7300505244828,226.69110174082158,226.65150999996752,226.61126616579995,226.5703610253806,226.5287852903506,226.48652959842445,226.4435845149855,226.3999405347867,226.35558808375993,226.31051752093396,226.2647191404655,226.21818317379032,226.1708997918899,226.12285910768662,226.07405117856013,226.02446600899387,225.974093553357,225.92292371881857,225.8709463684031,225.81815132418444,225.7645283706276,225.71006725807766,225.65475770639907,225.59858940876612,225.5415520356192,225.4836352387662,225.4248286556588,225.36512191382974,225.30450463549315,225.24296644232217,225.18049696039225,225.11708582530522,225.05272268748632,224.98739721765946,224.92109911250688,224.85381810050663,224.78554394795987,224.71626646519476,224.64597551296717,224.574661009038,224.5023129349461,224.42892134296898,224.3544763632654,224.27896821121402,224.20238719493813,224.1247237230122,224.04596831236648,223.96611159636407,223.8851443330686,223.80305741369386,223.71984187122928,223.6354888892408,223.54998981085035,223.4633361478797,223.37551959016002,223.28653201500555,223.1963654968383,223.10501231696765,223.01246497351036,222.91871619145167,222.82375893283304,222.7275864070676,222.63019208136677,222.53156969127437,222.4317132512979,222.3306170656264,222.22827573892434,222.12468418719146,222.01983764867447,221.91373169482313,221.8063622412699,221.6977255588283,221.58781828448815,221.47663743239838,221.36418040481684,221.25044500301695,221.13542943812791,221.01913234189576,220.901552777344,220.78269024931893,220.66254471489555,220.54111659362943,220.41840677762798,220.2944166414296,220.1691480516562,220.04260337643078,219.91478549452884,219.78569780424255,219.65534423193932,219.52372924028148,219.3908578360988,219.25673557786976,219.12136858280627,218.98476353350765,218.84692768415812,218.70786886625066,218.56759549380575,218.426116568067,218.28344168164122,218.1395810220686,217.99454537479295,217.84834612550992,217.70099526187138,217.5525053745243,217.40288965746055,217.25216190765747,217.1003365239899,216.94742850539416,216.79345344826356,216.6384275430608,216.48236757013143,216.32529089469878,216.16721546103346,216.00815978577995,215.84814295043145,215.6871845929446,215.52530489848473,215.36252458929854,215.19886491370406,215.034347634207,214.86899501473,214.7028298069671,214.5358752358624,214.36815498421876,214.19969317644788,214.03051436146936,213.86064349477311,213.6901059196585,213.51892734767034,213.34713383824942,213.1747517776191,213.00180785693473,212.8283290497219,212.65434258863078,212.4798759415408,212.3049567870476,212.12961298937,211.95387257271113,211.77776369512077,211.60131462189406,211.42455369855557,211.24750932347402,211.07020992015345,210.8926839092536,210.71495968038775,210.53706556375337,210.3590298016466,210.18088051991822,210.00264569942425,209.82435314753192,209.6460304697329,209.46770504142813,209.2894039799378,209.11115411679623,208.93298197039195,208.75491371900756,208.57697517432075,208.39919175542195,208.22158846340423,208.04418985658336,207.86702002639893,207.6901025740537,207.51346058793808,207.33711662189356,207.16109267435937,206.98541016845144,206.8100899330153,206.63515218469416,206.4606165110527,206.28650185479054,206.11282649908108,205.93960805406408,205.76686344452202,205.5946088987622,205.4228599387285,205.25163137135968,205.08093728120843,204.91079102433588,204.74120522348582,204.5721917645472,204.40376179430413,204.23592571947287,204.0686932070208,203.90207318575793,203.73607384919057,203.5707026596219,203.40596635348118,203.24187094786123,203.0784217482401,202.91562335735975,202.7534796852331,202.5919939602462,202.4311687413231,202.27100593111274,202.11150679016396,201.95267195204184,201.79450143934784,201.63699468059417,201.48015052788938,201.32396727538637,201.16844267844348,201.0135739734495,200.85935789826127,200.70579071320284,200.55286822257364,200.4005857966133,200.2489383938714,200.09792058392802,199.94752657041457,199.79775021428063,199.64858505725724,199.50002434546352,199.35206105310792,199.2046879062332,199.0578974064583,198.9116818546677,198.76603337460392,198.62094393631727,198.47640537943,198.33240943617275,198.18894775415285,198.04601191881693,197.90359347556924,197.76168395151242,197.62027487677634,197.47935780540408,197.33892433576565,197.19896613047194,197.0594749357631,196.92044260034788,196.78186109367275,196.6437225235995,196.50601915347582,196.36874341858066,196.23188794193172,196.09544554944205,195.9594092844162,195.82377242137588,195.6885284792093,195.55367123363828,195.41919472899946,195.2850932893371,195.15136152880712,195.01799436139248,194.8849870099321,194.75233501446658,194.62003423990453,194.48808088301578,194.35647147875738,194.2252029059401,194.09427239224388,193.96367751859142,193.83341622289012,193.70348680315274,193.57388792000867,193.44461859861738,193.31567822999682,193.18706657177998,193.0587837484122,192.9308302508047,192.8032069354567,192.6759150230621,192.54895609661435,192.42233209902602,192.29604533027688,192.17009844410686,192.04449444426947,191.91923668036128,191.79432884324387,191.6697749600745,191.5455793889612,191.42174681325992,191.29828223552983,191.1751909711637,191.05247864171085,190.9301511679098,190.8082147624482,190.686675922468,190.56554142183427,190.4448183031848,190.32451386978138,190.20463567717928,190.08519152473627,189.9661894469798,189.84763770485281,189.7295447768587,189.61191935012553,189.49477031141214,189.37810673807638,189.2619378890281,189.14627319568962,189.03112225298563,188.9164948103864,188.80240076302746,188.68885014292948,188.57585311034262,188.46341994523937,188.35156103898063,188.2402868861796,188.12960807678806,188.0195352884298,187.91007927900642,187.8012508795989,187.69306098769076,187.585520560736,187.47864061009562,187.37243219536637,187.26690641912398,187.16207442210347,187.05794737883747,186.95453649377265,186.8518529978846,186.7499081458085,186.64871321350373,186.54827949646725,186.44861830851093,186.34974098111556,186.25165886337246,186.154383322523,186.05792574510258,185.9622975386963,185.86751013430876,185.77357498935027,185.6805035912393,185.5883074616175,185.49699816117268,185.40658729506274,185.3170865189293,185.22850754549015,185.1408621516945,185.05416218642398,184.96841957871945,184.8836463465111,184.79985460582623,184.71705658044777,184.63526461199302,184.5544911703797,184.47474886464497,184.39605045407916,184.31840885963516,184.2418371755708,184.16634868128068,184.09195685327091,184.01867537722876,183.94651816013692,183.8754993423812,183.80563330979797,183.73693470560647,183.66941844217055,183.60309971253218,183.53799400165823,183.47411709734263,183.4114851007029,183.35011443621212,183.29002186120488,183.23122447479759,183.17373972616218,183.1175854220932,183.0627797338085,183.009341202925,182.95728874655055,182.90664166143608,182.857419627131,182.80964270808855,182.7633313546681,182.71850640298402,182.67518907355208,182.63340096868797,182.59316406861305,182.55450072622736,182.51743366051107,182.48198594851917,182.4481810159378,182.41604262617383,182.38559486795253,182.35686214140225,182.32986914260903,182.30464084662822,182.28120248894334,182.25957954536867,182.23979771039447,182.22188287397978,182.20586109680153,182.19175858397398,182.17960165725708,182.1694167257768,182.16123025528677,182.15506873600353,182.15095864905473,182.1489264315829,182.1489984405535,182.15120091532035,182.15555993900676,182.16210139876478,182.17085094498105,182.1818339495011,182.19507546294886,182.2106001712227,182.22843235125387,182.24859582611592,182.27111391957942,182.29600941020766,182.3233044850951,182.35302069335071,182.38517889943344,182.41979923644806,182.45690105951263,182.49650289931154,182.53862241594712,182.58327635320785,182.63048049336828,182.68024961263916,182.73259743738515,182.7875366012273,182.8450786031469,182.90523376670671,182.96801120050398,183.0334187599667,183.10146301060448,183.1721491928205,183.24548118838987,183.32146148870447,183.40009116488218,183.48136983983298,183.56529566237043,183.65186528345217,183.74107383462712,183.83291490876258,183.9273805431175,184.02446120482287,184.1241457788234,184.22642155832762,184.33127423780823,184.43868790858517,184.5486450570193,184.66112656533545,184.7761117150866,184.89357819326497,185.0135021010563,185.13585796522707,185.26061875212864,185.3877558842908,185.51723925957577,185.64903727285065,185.7831168401343,185.91944342516467,186.05798106832773,186.19869241788163,186.3415387634054,186.48648007139397,186.63347502291649,186.7824810532503,186.93345439339765,187.0863501133876,187.24112216726203,187.39772343964088,187.55610579375804,187.71622012085683,187.8780163908324,188.04144370400402,188.2064503439009,188.37298383094347,188.54099097689985,188.71041793999896,188.88121028058006,189.05331301715964,189.22667068279756,189.40122738164456,189.57692684555659,189.7537124906604,189.9315274737607,190.11031474847783,190.29001712101112,190.4705773054237,190.65193797834908,190.83404183302338,191.01683163255004,191.2002502623087,191.38424078142413,191.5687464732139,191.75371089454043,191.93907792399506,192.12479180884816,192.31079721070327,192.49703924979804,192.6834635478997,192.8700162697476,193.0566441629997,193.2432945966456,193.42991559785202,193.61645588721288,193.80286491237933,193.9890928800509,194.17509078631164,194.36081044530144,194.5462045162153,194.731226528628,194.9158309061459,195.09997298839,195.2836090513203,195.46669632591212,195.6491930152008,195.83105830971292],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[299.937617484219,299.93761778772677,299.93761809688283,299.93761841179236,299.9376187325625,299.9376190593022,299.93761939212266,299.937619731137,299.9376200764604,299.93762042821044,299.93762078650667,299.93762115147075,299.93762152322705,299.9376219019017,299.93762228762375,299.93762268052404,299.9376230807364,299.9376234883968,299.93762390364384,299.9376243266188,299.9376247574654,299.9376251963302,299.93762564336237,299.937626098714,299.93762656253983,299.9376270349976,299.9376275162479,299.9376280064545,299.9376285057839,299.937629014406,299.93762953249376,299.9376300602233,299.937630597774,299.9376311453287,299.9376317030736,299.9376322711982,299.9376328498958,299.93763343936325,299.9376340398009,299.9376346514128,299.937635274407,299.9376359089954,299.9376365553936,299.93763721382163,299.9376378845031,299.9376385676663,299.9376392635433,299.9376399723709,299.93764069439,299.93764142984617,299.93764217898934,299.9376429420745,299.93764371936084,299.9376445111128,299.9376453175996,299.9376461390954,299.9376469758796,299.93764782823666,299.9376486964564,299.93764958083403,299.93765048167035,299.93765139927154,299.9376523339497,299.9376532860227,299.93765425581404,299.9376552436537,299.9376562498774,299.9376572748275,299.9376583188523,299.93765938230683,299.93766046555277,299.93766156895845,299.9376626928989,299.9376638377566,299.9376650039204,299.9376661917873,299.93766740176085,299.93766863425265,299.93766988968173,299.93767116847494,299.9376724710672,299.93767379790137,299.9376751494286,299.9376765261084,299.937677928409,299.93767935680717,299.93768081178865,299.9376822938482,299.9376838034896,299.93768534122637,299.9376869075812,299.937688503087,299.93769012828596,299.93769178373094,299.9376934699847,299.93769518762065,299.93769693722294,299.93769871938645,299.9377005347171,299.93770238383235,299.9377042673608,299.93770618594294,299.9377081402313,299.93771013089025,299.93771215859675,299.9377142240403,299.9377163279233,299.937718470961,299.93772065388225,299.9377228774294,299.9377251423583,299.9377274494394,299.93772979945703,299.93773219321037,299.93773463151337,299.93773711519526,299.9377396451005,299.93774222208935,299.93774484703823,299.93774752083965,299.9377502444028,299.93775301865406,299.9377558445365,299.9377587230114,299.9377616550573,299.9377646416713,299.93776768386914,299.9377707826852,299.9377739391734,299.937777154407,299.9377804294794,299.9377837655042,299.93778716361606,299.9377906249704,299.93779415074437,299.93779774213687,299.93780140036915,299.9378051266854,299.93780892235264,299.9378127886618,299.93781672692757,299.9378207384893,299.9378248247111,299.93782898698265,299.9378332267194,299.9378375453631,299.9378419443825,299.93784642527356,299.93785098956,299.9378556387942,299.9378603745572,299.9378651984595,299.93787011214164,299.9378751172746,299.93788021556065,299.93788540873356,299.93789069855944,299.9378960868374,299.93790157539985,299.93790716611323,299.93791286087924,299.9379186616343,299.9379245703514,299.93793058904,299.93793671974703,299.9379429645575,299.9379493255954,299.937955805024,299.93796240504713,299.93796912790924,299.937975975897,299.9379829513395,299.9379900566091,299.93799729412234,299.9380046663409,299.93801217577203,299.93801982497,299.93802761653643,299.9380355531214,299.9380436374242,299.9380518721946,299.9380602602336,299.938068804394,299.93807750758214,299.93808637275816,299.9380954029375,299.9381046011917,299.93811397064945,299.9381235144977,299.9381332359828,299.9381431384115,299.938153225152,299.93816349963555,299.93817396535707,299.93818462587643,299.93819548482014,299.9382065458819,299.9382178128244,299.93822928948026,299.93824097975363,299.93825288762133,299.93826501713403,299.9382773724181,299.9382899576766,299.9383027771909,299.9383158353222,299.9383291365129,299.9383426852878,299.9383564862565,299.93837054411415,299.9383848636433,299.9383994497157,299.93841430729384,299.9384294414327,299.93844485728107,299.9384605600841,299.93847655518437,299.93849284802405,299.93850944414663,299.9385263491989,299.93854356893286,299.9385611092076,299.93857897599145,299.93859717536384,299.93861571351744,299.93863459676027,299.938653831518,299.9386734243359,299.9386933818813,299.9387137109455,299.9387344184466,299.9387555114314,299.93877699707815,299.93879888269873,299.9388211757412,299.9388438837928,299.93886701458166,299.9388905759801,299.93891457600716,299.9389390228314,299.9389639247734,299.938989290309,299.9390151280718,299.93904144685644,299.9390682556216,299.9390955634926,299.9391233797651,299.93915171390785,299.9391805755662,299.9392099745653,299.93923992091345,299.9392704248054,299.93930149662606,299.93933314695397,299.9393653865646,299.9393982264347,299.93943167774546,299.9394657518866,299.93950046046007,299.93953581528444,299.9395718283983,299.939608512065,299.9396458787764,299.93968394125744,299.9397227124703,299.9397622056189,299.9398024341535,299.9398434117751,299.9398851524404,299.93992767036644,299.9399709800352,299.94001509619915,299.94006003388563,299.94010580840273,299.94015243534386,299.9401999305934,299.94024831033227,299.9402975910432,299.94034778951635,299.94039892285554,299.9404510084835,299.94050406414834,299.9405581079293,299.9406131582431,299.9406692338502,299.9407263538613,299.940784537744,299.940843805329,299.9409041768177,299.94096567278825,299.9410283142034,299.9410921224172,299.94115711918244,299.94122332665825,299.94129076741757,299.94135946445516,299.94142944119506,299.941500721499,299.94157332967455,299.94164729048344,299.9417226291499,299.9417993713697,299.94187754331875,299.9419571716622,299.94203828356336,299.9421209066937,299.9422050692415,299.94229079992226,299.94237812798855,299.94246708323936,299.9425576960315,299.9426499972891,299.94274401851476,299.9428397918003,299.94293734983796,299.9430367259315,299.94313795400785,299.94324106862854,299.943346105002,299.9434530989957,299.94356208714817,299.9436731066823,299.94378619551736,299.9439013922828,299.9440187363313,299.9441382677526,299.9442600273873,299.9443840568407,299.9445103984977,299.9446390955373,299.9447701919475,299.9449037325405,299.94503976296863,299.9451783297396,299.9453194802332,299.94546326271745,299.94560972636543,299.9457589212725,299.9459108984734,299.94606570996046,299.94622340870126,299.9463840486573,299.94654768480297,299.9467143731441,299.94688417073814,299.94705713571346,299.94723332729046,299.9474128058013,299.9475956327115,299.9477818706413,299.9479715833876,299.94816483594593,299.9483616945339,299.9485622266134,299.94876650091516,299.9489745874622,299.94918655759454,299.9494024839943,299.9496224407111,299.94984650318804,299.95007474828833,299.95030725432196,299.9505441010737,299.9507853698308,299.9510311434115,299.9512815061946,299.9515365441489,299.9517963448635,299.9520609975787,299.9523305932177,299.9526052244182,299.9528849855656,299.95316997282634,299.95346028418135,299.95375601946154,299.95405728038236,299.9543641705806,299.9546767956503,299.9549952631809,299.95531968279494,299.9556501661875,299.9559868271654,299.9563297816879,299.95667914790783,299.95703504621383,299.9573975992733,299.95776693207586,299.95814317197807,299.958526448749,299.9589168946165,299.9593146443145,299.9597198351311,299.96013260695804,299.96055310234027,299.9609814665276,299.96141784752666,299.9618623961538,299.9623152660895,299.9627766139337,299.9632465992618,299.9637253846827,299.96421313589656,299.9647100217554,299.9652162143237,299.96573188894024,299.96625722428195,299.96679240242815,299.9673376089269,299.9678930328618,299.9684588669205,299.9690353074651,299.96962255460306,299.9702208122597,299.97083028825284,299.9714511943681,299.9720837464362,299.9727281644114,299.97338467245174,299.9740534990012,299.9747348768725,299.97542904333284,299.9761362401902,299.976856713882,299.97759071556544,299.9783385012094,299.97910033168847,299.97987647287846,299.9806671957542,299.9814727764893,299.98229349655736,299.9831296428356,299.98398150771084,299.9848493891864,299.98573359099316,299.98663442270055,299.98755219983104,299.9884872439769,299.98943988291893,299.9904104507473,299.9913992879855,299.9924067417159,299.9934331657086,299.9944789205521,299.99554437378697,299.99662990004197,299.99773588117296,299.9988627064044,300.0000107724737,300.00118048377806,300.0023722525251,300.0035864988851,300.0048236511473,300.00608414587856,300.0073684280854,300.00867695137913,300.0100101781439,300.0113685797085,300.01275263652093,300.01416283832634,300.0155996843487,300.01706368347567,300.01855535444713,300.0200752260467,300.02162383729774,300.02320173766236,300.0248094872443,300.0264476569955,300.02811682892684,300.0298175963223,300.0315505639569,300.03331634831966,300.03511557783895,300.0369488931134,300.0388169471459,300.0407204055825,300.04265994695476,300.0446362629269,300.04665005854747,300.0487020525041,300.05079297738445,300.0529235799399,300.0550946213546,300.05730687751895,300.05956113930705,300.06185821285936,300.06419891986957,300.06658409787576,300.0690146005567,300.0714912980327,300.07401507717026,300.0765868418927,300.0792075134943,300.0818780309593,300.0845993512858,300.0873724498143,300.09019832056026,300.09307797655134,300.0960124501701,300.09900279349955,300.1020500786743,300.10515539823587,300.10831986549186,300.11154461487956,300.1148308023341,300.1181796056598,300.12159222490624,300.12506988274765,300.12861382486625,300.13222532033893,300.1359056620276,300.1396561669723,300.14347817678816,300.1473730580643,300.15134220276593,300.1553870286389,300.15950897961636,300.16370952622685,300.16799016600527,300.17235242390353,300.1767978527041,300.181328033433,300.18594457577285,300.19064911847795,300.1954433297865,300.20032890783347,300.20530758106196,300.2103811086326,300.2155512808308,300.2208199194715,300.22618887830004,300.2316600433902,300.23723533353655,300.2429167006428,300.24870613010364,300.25460564118055,300.2606172873703,300.2667431567657,300.27298537240745,300.2793460926265,300.2858275113763,300.29243185855347,300.2991614003067,300.306018439332,300.31300531515427,300.3201244043938,300.3273781210162,300.33476891656517,300.34229928037723,300.3499717397763,300.35778886024815,300.36575324559203,300.3738675380498,300.38213441841015,300.39055660608676,300.39913685916946,300.40787797444654,300.416782787397,300.42585417215116,300.43509504141855,300.4445083463806,300.45409707654784,300.4638642595793,300.47381296106227,300.4839462842522,300.4942673697689,300.5047793952499,300.5154855749572,300.5263891593379,300.5374934345344,300.5488017218454,300.5603173771342,300.5720437901831,300.5839843839932,300.59614261402606,300.60852196738847,300.62112596195595,300.6339581454361,300.64702209436797,300.66032141305806,300.6738597324511,300.6876407089332,300.70166802306835,300.71594537826513,300.7304764993739,300.74526513121356,300.76031503702654,300.775629996861,300.7912138058818,300.80707027260684,300.8232032170713,300.8396164689175,300.85631386541195,300.8732992493894,300.89057646712354,300.9081493661261,300.9260217928748,300.94419759047065,300.96268059622656,300.9814746391884,301.0005835375896,301.02001109624376,301.0397611038732,301.0598373303803,301.08024352406176,301.1009834087692,301.1220606810211,301.1434790070665,301.1652420199082,301.18735331628613,301.20981645362826,301.2326349469722,301.25581226586297,301.279351831233,301.3032570122697,301.3275311232761,301.3521774205331,301.3771990991669,301.4025992900313,301.42838105661116,301.4545473919549,301.48110121564287,301.5080453708019,301.5353826211725,301.5631156482382,301.5912470484265,301.6197793303892,301.6487149123725,301.6780561196871,301.7078051822852,301.737964232458,301.76853530266044,301.7995203234749,301.8309211217237,301.86273941874,301.894976828808,301.9276348577818,301.9607149018934,301.9942182467597,302.028146066598,302.0624994236604,302.0972792678957,302.132486436848,302.1681216558022,302.204185538183,302.24067858621765,302.2776011918685,302.3149536380441,302.35273610009534,302.39094864760256,302.4295912464603,302.46866376126457,302.50816595800734,302.5480975070818,302.5884579866029,302.62924688604403,302.67046361019317,302.7121074834284,302.7541777543141,302.79667360051553,302.83959413403164,302.88293840674345,302.92670541627365,302.9708941121546,303.0155034022981,303.0605321597618,303.10597922980435,303.15184343722154,303.1981235939539,303.24481850695713,303.291926986321,303.3394478536296,303.3873799505449,303.4357221476035,303.4844733532102,303.5336325228124,303.5831986682402,303.6331708671928,303.68354827285555,303.7343301236263,303.78551575293284,303.83710459912083,303.8890962153906,303.94149027976255,303.99428660504753,304.0474851488006,304.1010860232348,304.15508950507166,304.2094960453046,304.26430627885117,304.31952103407014,304.37514134211926,304.4311684461291,304.4876038101684,304.54444912797743,304.6017063314444,304.6593775988017,304.7174653625178,304.7759723168618,304.8349014251176,304.8942559264259,304.95403934223117,305.01425548231305,305.07490845038194,305.1360026492181,305.1975427853363,305.25953387315695,305.32198123866783,305.38489052255784,305.4482676828115,305.5121189967458,305.5764510624799,305.64127079982376,305.7065854505767,305.77240257822547,305.83873006703493,305.90557612052345,305.97294925931993,306.0408583183958,306.1093124436718,306.1783210879978,306.24789400650616,306.3180412513404,306.3887731657625,306.4601003776441,306.53203379234833,306.6045845850081,306.6777641922129,306.75158430311257,306.8260568499511,306.90119399804433,306.9770081352155,307.05351186070556,307.13071797357605,307.2086394606228,307.28728948382053,307.36668136732095,307.44682858402473,307.52774474175254,307.6094435690397,307.69193890057846,307.77524466233683,307.8593748563793,307.9443435454187,308.03016483712787,308.1168528682413,308.2044217884755,308.2928857443006,308.38225886259306,308.472555234202,308.56378889745974,308.6559738216704,308.74912389060665,308.84325288604896,308.93837447139776,309.03450217539245,309.1316493759671,309.22982928427695,309.32905492892513,309.42933914042186,309.53069453590564,309.63313350415757,309.7366681909371,309.8413104846688,309.9470720025065,310.0539640768047,310.1619977420203,310.27118372207184,310.381532418181,310.4930538972172,310.6057578805707,310.7196537335723,310.83475045548187,310.9510566700623,311.0685806167581,311.1873301424935,311.30731269410546,311.42853531142464,311.55100462101683,311.67472683059503,311.79970772411264,311.9259526575433,312.0534665553572,312.18225390769555,312.31231876824893,312.4436647528417,312.57629503872147,312.7102123645552,312.84541903112836,312.9819169027443,313.11970740931827,313.25879154916146,313.3991698924451,313.5408425853381,313.6838093548059,313.82806951406064,313.97362196864884,314.1204652231626,314.2685973885599,314.41801619007646,314.56871897571216,314.7207027252737,314.87396405995275,315.02849925241964,315.18430423741006,315.3413746227834,315.49970570102676,315.65929246118213,315.82012960117,315.98221154048446,316.14553243323087,316.3100861814807,316.47586644891317,316.6428666747164,316.81108008771724,316.9804997207104,317.15111842495685,317.3229288848192,317.49592363250423,317.6700950628806,317.8454354483386,318.02193695366327,318.19959165088403,318.3783915340734,318.558328534059,318.73939453301847,318.9215813789247,319.1048808998095,319.28928491781335,319.47478526299165,319.6613737868431,319.8490423755334,320.0377829627797,320.227587542371,320.4184481802898,320.6103570264112,320.8033063257482,320.99728842921854,321.1922958039043,321.388321042781,321.5853568738902,321.78339616893265,321.98243195125883,322.1824574032362,322.3834658729716,322.58545088037084,322.7884061225167,322.9923254783478,323.19720301262555,323.4030329791712,323.6098098233642,323.8175281838871,324.02618289371003,324.23576898030365,324.44628166507647,324.65771636202845,324.8700686756193,325.0833343978481,325.2975095045443,325.51259015087,325.7285726660378,325.9454535472472,326.16322945284674,326.38189719472774,326.6014537299608,326.82189615168517,327.0432216792632,327.26542764771517,327.4885114964494,327.7124707573059,327.93730304193224,328.163006028513,328.3895774478744,328.6170150689884,328.8453166839019,329.0744800921178,329.3045030844556,329.5353834264224,329.76711884112547,329.9997069917573,330.233145463689,330.4674317462061,330.7025632139228,330.9385371079121,331.1753505165909,331.41300035639887,331.65148335231095,331.8907960182262,332.1309346372737,332.3718952420781,332.6136735950296,332.85626516860026,333.099665125753,333.34386830048606,333.5888691785591,333.83466187844596,334.0812401325587,334.3285972687886,334.5767261924091,334.8256193683852,335.07526880413417,335.32566603278104,335.5768020969532,335.8286675331561,336.0812523567717,336.33454604772237,336.5885375368381,336.843215192968,337.0985668108723,337.3545795999321,337.61124017371094,337.8685345404025,338.12644809419555,338.3849656075855,338.64407122466184,338.9037484553962,339.163980170956,339.42474860006485,339.6860353264303,339.9478212872541,340.21008677284345,340.4728114273322,340.7359742505237,340.99955360086375,341.26352719954593,341.5278721357544,341.79256487304167,342.05758125683974,342.3228965230979,342.5884853080396,342.85432165902654,343.1203790465174,343.3866303771033,343.6530480076025,343.91960376019176,344.18626893855117,344.4530143449956,344.719810298563,344.9866266540306,345.253432821823,345.5201977887786,345.7868901397358,346.0534780798994,346.31992945794696,346.58621178983003,346.85229228322834,347.1181378626082,347.38371519483985,347.6489907153238,347.9139306545775,348.17850106523116,348.44266784938253,348.70639678625713,348.96965356012396,349.232403788412,349.4946130499762,349.75624691345996,350.01727096570147,350.27765084013146,350.5373522451117,350.7963409921615,351.0545830240232,351.31204444251665,351.56869153613326,351.82449080732397,352.0794089994328,352.3334131232332,352.5864704830217,352.8385487022293,353.0896157485077,353.3396399582545,353.5885900605388,353.8364352003934,354.08314496143987,354.3286893878167,354.5730390053807,354.8161648421554,355.0580384480017,355.2986319134881,355.53791788794035,355.775869596652,356.0124608572408,356.247666095135,356.4814603581803,356.71381933035605,356.94471934459347,357.17413739469157,357.40205114632573,357.62843894714825,357.8532798359819,358.07655355110757,358.29824053765174,358.518321954078,358.7367796777921,358.95359630986803,359.1687551789081,359.3822403440467,359.59403659711364,359.8041294639723,360.01250520504584,360.2191508150536],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[737.9575441999006,737.9575348867749,737.9575254003312,737.957515737344,737.9575058945278,737.9574958685361,737.9574856559609,737.9574752533276,737.9574646571019,737.957453863679,737.9574428693902,737.957431670497,737.9574202631917,737.957408643596,737.9573968077588,737.9573847516563,737.9573724711892,737.9573599621815,737.9573472203814,737.9573342414554,737.9573210209917,737.9573075544945,737.9572938373849,737.9572798650001,737.9572656325882,737.9572511353114,737.9572363682396,737.9572213263524,737.9572060045356,737.9571903975794,737.9571745001783,737.9571583069252,737.9571418123165,737.957125010743,737.957107896492,737.957090463745,737.9570727065743,737.9570546189441,737.9570361947025,737.9570174275864,737.9569983112144,737.9569788390875,737.9569590045845,737.9569388009625,737.9569182213514,737.956897258755,737.9568759060459,737.9568541559634,737.9568320011133,737.9568094339625,737.9567864468388,737.9567630319261,737.9567391812632,737.9567148867419,737.9566901401012,737.9566649329269,737.9566392566499,737.9566131025407,737.9565864617047,737.9565593250861,737.9565316834584,737.9565035274228,737.9564748474063,737.956445633659,737.9564158762475,737.9563855650543,737.9563546897742,737.95632323991,737.956291204769,737.9562585734583,737.9562253348842,737.9561914777468,737.9561569905333,737.9561218615196,737.9560860787615,737.9560496300937,737.9560125031232,737.9559746852278,737.9559361635498,737.955896924992,737.9558569562136,737.955816243626,737.9557747733877,737.9557325313986,737.9556895032972,737.9556456744547,737.9556010299697,737.9555555546638,737.9555092330756,737.9554620494573,737.9554139877664,737.9553650316633,737.9553151645035,737.955264369333,737.9552126288819,737.9551599255597,737.955106241449,737.9550515582973,737.9549958575136,737.9549391201602,737.9548813269486,737.9548224582301,737.9547624939896,737.9547014138424,737.954639197021,737.9545758223737,737.9545112683556,737.9544455130184,737.9543785340082,737.9543103085532,737.9542408134585,737.9541700250983,737.9540979194061,737.9540244718681,737.9539496575146,737.9538734509106,737.953795826149,737.953716756839,737.9536362161002,737.9535541765515,737.9534706103019,737.953385488942,737.9532987835333,737.9532104646004,737.9531205021169,737.9530288655,737.9529355235958,737.9528404446728,737.9527435964083,737.9526449458772,737.9525444595437,737.952442103246,737.9523378421876,737.9522316409254,737.9521234633561,737.952013272704,737.9519010315096,737.9517867016178,737.9516702441618,737.9515516195519,737.9514307874628,737.9513077068184,737.9511823357781,737.951054631723,737.9509245512419,737.9507920501148,737.9506570832998,737.9505196049166,737.9503795682323,737.9502369256434,737.950091628661,737.9499436278946,737.9497928730334,737.9496393128333,737.9494828950938,737.9493235666458,737.9491612733293,737.948995959978,737.9488275703983,737.9486560473512,737.9484813325345,737.9483033665589,737.9481220889328,737.9479374380368,737.9477493511079,737.9475577642121,737.9473626122304,737.9471638288288,737.9469613464412,737.9467550962432,737.9465450081326,737.9463310107001,737.9461130312103,737.9458909955746,737.9456648283247,737.9454344525909,737.9451997900719,737.9449607610096,737.9447172841636,737.9444692767828,737.9442166545754,737.9439593316822,737.9436972206479,737.9434302323893,737.9431582761679,737.9428812595557,737.9425990884073,737.9423116668252,737.9420188971284,737.9417206798208,737.941416913552,737.9411074950921,737.9407923192878,737.9404712790295,737.9401442652184,737.939811166725,737.9394718703539,737.9391262608044,737.9387742206312,737.9384156302065,737.9380503676758,737.9376783089207,737.9372993275131,737.9369132946755,737.9365200792346,737.9361195475785,737.9357115636109,737.9352959887055,737.9348726816575,737.934441498637,737.9340022931407,737.9335549159404,737.9330992150346,737.9326350355934,737.9321622199117,737.9316806073494,737.9311900342835,737.9306903340453,737.9301813368703,737.9296628698369,737.9291347568097,737.9285968183804,737.9280488718019,737.9274907309348,737.9269222061768,737.9263431044017,737.925753228895,737.9251523792856,737.9245403514773,737.9239169375819,737.9232819258466,737.9226351005844,737.9219762420986,737.9213051266113,737.9206215261847,737.9199252086469,737.9192159375098,737.9184934718918,737.9177575664366,737.9170079712281,737.9162444317067,737.9154666885835,737.9146744777514,737.9138675301986,737.9130455719145,737.912208323798,737.9113555015635,737.9104868156452,737.9096019710986,737.9087006674986,737.9077825988438,737.9068474534465,737.9058949138309,737.904924656626,737.9039363524545,737.9029296658227,737.9019042550074,737.9008597719394,737.8997958620874,737.8987121643356,737.8976083108663,737.8964839270313,737.895338631227,737.8941720347665,737.8929837417476,737.8917733489187,737.8905404455436,737.8892846132634,737.8880054259535,737.8867024495819,737.8853752420624,737.8840233531032,737.8826463240592,737.881243687775,737.8798149684263,737.8783596813631,737.8768773329432,737.8753674203673,737.8738294315099,737.8722628447462,737.8706671287773,737.8690417424511,737.8673861345792,737.8656997437531,737.8639819981537,737.8622323153612,737.8604501021574,737.8586347543268,737.8567856564531,737.8549021817141,737.8529836916694,737.8510295360459,737.8490390525202,737.8470115664985,737.8449463908843,737.8428428258538,737.8407001586179,737.8385176631842,737.8362946001137,737.8340302162718,737.8317237445773,737.8293744037447,737.826981398023,737.8245439169287,737.8220611349749,737.819532211395,737.8169562898601,737.8143324981958,737.811659948088,737.8089377347862,737.8061649368019,737.8033406156002,737.8004638152876,737.7975335622922,737.7945488650378,737.7915087136145,737.7884120794425,737.7852579149275,737.7820451531119,737.7787727073219,737.7754394708016,737.7720443163489,737.7685860959369,737.7650636403351,737.7614757587186,737.7578212382734,737.7540988437939,737.7503073172712,737.7464453774779,737.7425117195414,737.7385050145132,737.7344239089261,737.7302670243469,737.7260329569222,737.7217202769097,737.7173275282091,737.7128532278794,737.7082958656487,737.7036539034158,737.6989257747422,737.6941098843375,737.6892046075297,737.6842082897338,737.679119245905,737.6739357599843,737.6686560843353,737.6632784391684,737.6578010119575,737.6522219568434,737.6465393940306,737.6407514091691,737.6348560527297,737.6288513393625,737.6227352472507,737.6165057174475,737.6101606532048,737.603697919289,737.5971153412831,737.5904107048802,737.5835817551608,737.57662619586,737.5695416886201,737.5623258522338,737.5549762618671,737.5474904482762,737.5398658970067,737.5321000475798,737.524190292664,737.5161339772324,737.5079283977069,737.4995708010862,737.4910583840572,737.4823882920948,737.4735576185442,737.4645634036874,737.4554026337916,737.4460722401457,737.436569098079,737.4268900259575,737.4170317841753,737.4069910741119,737.3967645370903,737.3863487533015,737.3757402407219,737.3649354540055,737.3539307833609,737.3427225534082,737.3313070220181,737.319680379128,737.307838745543,737.2957781717141,737.2834946364942,737.270984045879,737.2582422317216,737.2452649504279,737.232047881631,737.2185866268444,737.2048767080886,737.1909135665013,737.1766925609211,737.1622089664479,737.1474579729835,737.1324346837436,737.1171341137481,737.1015511882896,737.0856807413709,737.0695175141246,737.0530561532016,737.0362912091381,737.0192171346929,737.0018282831625,736.9841189066664,736.9660831544047,736.9477150708935,736.929008594166,736.9099575539511,736.8905556698207,736.8707965493081,736.8506736860006,736.830180457598,736.8093101239468,736.7880558250401,736.7664105789893,736.7443672799646,736.721918696105,736.6990574673965,736.6757761035198,736.6520669816645,736.62792234431,736.6033342969804,736.5782948059579,736.5527956959681,736.5268286478305,736.5003851960773,736.4734567265334,736.4460344738677,736.4181095191058,736.3896727871087,736.3607150440179,736.3312268946623,736.3011987799335,736.2706209741206,736.2394835822103,736.207776537154,736.1754895970914,736.1426123425454,736.1091341735729,736.0750443068808,736.0403317729075,736.0049854128624,735.9689938757281,735.9323456152285,735.8950288867547,735.8570317442538,735.8183420370802,735.7789474068081,735.7388352840061,735.6979928849687,735.6564072084194,735.6140650321626,735.570952909706,735.5270571668384,735.4823638981775,735.4368589636666,735.3905279850425,735.3433563422624,735.2953291698899,735.2464313534466,735.1966475257228,735.145962063051,735.0943590815417,735.0418224332846,734.9883357025051,734.9338822016945,734.8784449676924,734.8220067577422,734.7645500455038,734.7060570170357,734.6465095667376,734.5858892932634,734.5241774953946,734.4613551678838,734.3974029972633,734.3323013576241,734.2660303063559,734.1985695798656,734.1298985892563,734.0599964159843,733.9888418074775,733.9164131727367,733.8426885779013,733.7676457417878,733.6912620314122,733.6135144574743,733.5343796698298,733.4538339529323,733.3718532212562,733.2884130146995,733.2034884939649,733.1170544359264,733.0290852289746,732.939554868346,732.848436951444,732.7557046731383,732.6613308210568,732.5652877708671,732.4675474815455,732.3680814906436,732.2668609095406,732.1638564187001,732.059038262918,731.952376246566,731.8438397288431,731.7333976190206,731.6210183716948,731.506669982041,731.3903199810803,731.2719354309426,731.151482920151,731.0289285589115,730.9042379744158,730.7773763061602,730.6483082012799,730.5169978099025,730.3834087805159,730.2475042553674,730.1092468658754,729.9685987280715,729.8255214380653,729.6799760675392,729.5319231592719,729.3813227226889,729.2281342294503,729.0723166090661,728.9138282445506,728.752626968108,728.5886700568598,728.4219142286033,728.2523156376154,728.0798298704916,727.9044119420257,727.7260162911323,727.5445967768098,727.3601066741421,727.1724986703471,726.9817248608654,726.7877367454879,726.5904852245312,726.3899205950468,726.1859925470789,725.9786501599588,725.7678418986344,725.5535156100455,725.335618519531,725.1140972272711,724.8888977047642,724.6599652913345,724.4272446906698,724.1906799673847,723.9502145436053,723.7057911955816,723.4573520503102,723.2048385821735,722.9481916095908,722.6873512916758,722.4222571248914,722.1528479397067,721.8790618972444,721.6008364859144,721.3181085180315,721.0308141264064,720.7388887609093,720.4422671849941,720.1408834721833,719.8346710024986,719.5235624588435,719.2074898233083,718.8863843734177,718.5601766782904,718.2287965947143,717.892173263126,717.550235103488,717.2029098110534,716.8501243520083,716.4918049589866,716.1278771264493,715.7582656059105,715.382894401016,715.0016867624519,714.6145651826808,714.2214513904994,713.8222663454037,713.4169302317526,713.0053624527321,712.5874816240976,712.1632055676963,711.7324513047613,711.2951350489659,710.8511721992377,710.4004773323206,709.9429641950863,709.478545696581,709.0071338998116,708.5286400132669,708.0429743821634,707.5500464794255,707.0497648963899,706.5420373332391,706.0267705891634,705.5038705522552,704.9732421891379,704.4347895343341,703.8884156793788,703.3340227616892,702.7715119531913,702.2007834487254,701.6217364542293,701.0342691747285,700.438278802129,699.8336615028553,699.2203124053228,698.5981255872948,697.9669940631225,697.3268097709114,696.6774635596279,696.0188451761835,695.3508432525209,694.6733452927409,693.9862376603018,693.2894055653315,692.582733052089,691.8661029866163,691.1393970446354,690.4024956997177,689.6552782117955,688.8976226160491,688.1294057122351,687.3505030545024,686.5607889417585,685.7601364086448,684.9484172171802,684.1255018491394,683.29125949923,682.4455580691358,681.5882641624993,680.7192430809076,679.8383588209597,678.9454740724909,678.040450218025,677.1231473335367,676.1934241905981,675.2511382599939,674.2961457168848,673.3283014475954,672.347459058121,671.3534708844217,670.3461880046021,669.3254602530482,668.2911362366166,667.2430633529527,666.1810878110226,665.1050546539525,664.0148077842423,662.9101899914535,661.7910429824437,660.6572074142291,659.5085229295618,658.3448281952939,657.1659609436097,655.9717580162018,654.7620554114624,653.5366883347666,652.2954912519123,651.0382979457894,649.7649415763395,648.4752547438711,647.1690695557891,645.8462176967942,644.5065305026088,643.1498390372753,641.7759741740791,640.3847666801308,638.9760473046582,637.549646871035,636.1053963725783,634.6431270721483,633.1626706055656,631.6638590888676,630.1465252294222,628.610502440899,627.0556249621102,625.4817279797184,623.8886477548039,622.2762217532874,620.6442887801819,618.9926891176664,617.3212646669441,615.6298590938626,613.9183179782548,612.1864889669589,610.4342219304754,608.6613691232004,606.867785347189,605.053328119373,603.2178578421733,601.3612379774336,599.4833352235889,597.5840196959953,595.6631651103231,593.720648968926,591.7563527500832,589.7701621000125,587.7619670275471,585.7316621013655,583.679146649648,581.6043249620554,579.5071064938891,577.3874060723127,575.245144104507,573.0802467876093,570.8926463203162,568.6822811159977,566.4490960171859,564.1930425112909,561.9140789473972,559.6121707539987,557.2872906575153,554.9394189014505,552.5685434660375,550.1746602882213,547.7577734818415,545.3178955578536,542.8550476444572,540.3692597069826,537.8605707673959,535.3290291232892,532.7746925662211,530.197628599282,527.5979146537545,524.975638304758,522.3308974857613,519.6638007018581,516.9744672417075,514.2630273880441,511.529622626683,508.77440585393526,505.997541582376,503.19920614490457,500.379587897052,497.53888741750416,494.6773177068112,491.7951043842743,488.892485883009,485.96971364319404,483.0270523035339,480.06477989096953,477.0831880086894,474.0825820225044,471.0632812456624,468.0256191221968,464.9699434089115,461.8966163561224,458.8060148872874,455.69853077766726,452.5745708321817,449.4345570626265,446.2789268644376,443.10813319319794,439.9226447410851,436.72294611348656,433.5095380059947,430.28293738201927,427.0436776512517,423.7923088492186,420.52939781817327,417.2555283895617,413.97130156830355,410.6773357191273,407.37426675517185,404.06274832908395,400.74345202679115,397.4170675641366,394.0843029865203,390.745884871663,387.40255853557596,384.0550882417725,380.70425741370326,377.35086885034275,373.9957449447809,370.6397279055981,367.28367998071406,363.92848368329436,360.5750420191948,357.2242787152832,353.8771384478513,350.53458707016125,347.1976118379975,343.86722163190666,340.5444471745848,337.23034124165054,333.9259788637752,330.63245751786513,327.35089730469235,324.08244111003097,320.8282547460138,317.58952706903347,314.3674700701097,311.16331893320387,307.9783320565135,304.8137910312809,301.6710005721538,298.55128839260146,295.45600501834605,292.3865235312167,289.34423923526697,286.33056923643016,283.34695192644466,280.3948463612308,277.47573152341437,274.5911054582215,271.74248427157374,268.93140097889864,266.1594041929397,263.42805663875725,260.7389334841487,258.09362047392614,255.49371185690086,252.94080809504362,250.43651334518069,247.98243270472722,245.58016921442726,243.23132061284846,240.93747583950065,238.70021128594092,236.5210867970611,234.4016414279689,232.34338896542863,230.3478132267009,228.41636315280476,226.55044771761789,224.75143067881763,223.02062520133543,221.3592883886535,219.76861576183427,218.24973573047785,216.8037041037562,215.43149869312134,214.1340140610919,212.9120564725692,211.76633910627694,210.69747758407377,209.70598587495107,208.7922726284536,207.95663798901583,207.19927093830324,206.52024720713032,205.91952779198746,205.39695810376057,204.95226776804014,204.58507108766418,204.29486816904645,204.08104670461967,203.94288439461585,203.8795519826367,203.89011687125176,203.97354727640658,204.12871687288603,204.35440987760785,204.64932651320666,205.0120887912752,205.44124655277437,205.93528370248492,206.49262457490073,207.11164037056514,207.7906556044228,208.5279545111626,209.32178735662796,210.17037660900658,211.07192292854944,212.02461093984675,213.02661475607613,214.0761032300198,215.1712449118898,216.31021269904244,217.49118816739963,218.71236557878333,219.97195556236844,221.26818847202753,222.59931742448555,223.9636210259041,225.3594057967928,226.7850083070174,228.23879703415176,229.71917395955438,231.2245759173471,232.75347571198319,234.3043830203495,235.87584509436581,237.46644727989013,239.07481336740972,240.69960578954274,242.33952567981905,243.99331280656077,245.65974539498865,247.33763984993496,249.0258503907781,250.72326860943835,252.4288229614951,254.141478199724,255.86023475860398,257.58412809762063,259.31222801050427,261.0436379068752,262.7774940721488,264.51296491096593,266.2492501788586,267.9855802063592,269.72121511927315,271.45544405840894,273.1875844016482,274.9169809908708,276.64300536591674,278.36505500745153,280.0825525903321,281.7949452488144,283.5017038547173,285.2023223094598,286.89631685069946,288.5832253741452,290.26260677097224,291.9340402811388,293.59712486279665,295.2514785778871,296.8967379939304,298.53255760194384,300.15860925036276,301.77458159477834,303.38017956326917,304.975123837061,306.5591503462163,308.1320097800365,309.6934671118321,311.24330113770515,312.7813040289774,314.3072808978855,315.82104937616793,317.3224392061558,318.8112918439909,320.2874600745901,321.75080763798053,323.20120886664057,324.6385483334796,326.0627205101075,327.47362943504635,328.87118839154834,330.25531959469794,331.6259538874788,332.9830304455035,334.32649649011205,335.6563070095553,336.972424487993,338.27481864204293,339.5634661646319,340.83835047590856,342.0994614809863,343.3467953342988,344.5803542103557,345.8001460806982,347.0061844968636,348.19848837917345,349.3770818111745,350.5419938395609,351.6932582794231,352.83091352466926,353.9550023634752,355.06557179862676,356.16267287261974,357.2463604973945,358.31669328858493,359.37373340416536,360.4175463873892,361.44820101391116,362.46576914299436,363.47032557270705,364.4619478990137,365.4407163786766,366.40671379587684,367.3600253324787,368.30073844185335,369.2289427261893,370.1447298172143,371.04819326025665,371.9394284015793,372.81853227891645,373.68560351515146,374.5407422150682,375.38404986511864,376.21562923614295,377.0355842889852,377.844020082947],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[581.2971219796735,581.2971236337933,581.2971253186959,581.2971270349545,581.2971287831529,581.2971305638854,581.2971323777573,581.2971342253855,581.2971361073979,581.2971380244346,581.2971399771475,581.2971419662002,581.2971439922691,581.2971460560434,581.2971481582244,581.2971502995268,581.2971524806792,581.2971547024225,581.2971569655123,581.2971592707185,581.2971616188245,581.2971640106285,581.2971664469438,581.2971689285989,581.2971714564378,581.2971740313193,581.2971766541192,581.2971793257294,581.2971820470582,581.2971848190306,581.2971876425893,581.297190518694,581.2971934483232,581.2971964324723,581.2971994721562,581.2972025684085,581.2972057222814,581.2972089348477,581.2972122071995,581.2972155404497,581.2972189357309,581.2972223941982,581.2972259170272,581.2972295054151,581.2972331605828,581.2972368837726,581.2972406762501,581.2972445393053,581.2972484742513,581.2972524824261,581.2972565651921,581.2972607239378,581.2972649600769,581.2972692750495,581.2972736703232,581.297278147392,581.2972827077782,581.297287353032,581.2972920847327,581.2972969044895,581.297301813941,581.2973068147559,581.2973119086345,581.2973170973091,581.2973223825435,581.2973277661343,581.2973332499124,581.297338835742,581.297344525522,581.2973503211872,581.2973562247078,581.2973622380908,581.2973683633809,581.2973746026602,581.2973809580506,581.2973874317122,581.2973940258461,581.2974007426942,581.2974075845402,581.2974145537104,581.2974216525735,581.2974288835435,581.2974362490781,581.2974437516826,581.2974513939068,581.297459178349,581.2974671076556,581.2974751845226,581.297483411696,581.2974917919727,581.2975003282017,581.2975090232853,581.297517880179,581.2975269018947,581.2975360914987,581.2975454521159,581.2975549869279,581.2975646991767,581.297574592164,581.2975846692532,581.29759493387,581.2976053895038,581.2976160397095,581.2976268881077,581.2976379383862,581.2976491943016,581.2976606596809,581.2976723384215,581.2976842344937,581.2976963519419,581.297708694885,581.2977212675193,581.2977340741187,581.2977471190374,581.2977604067094,581.2977739416523,581.2977877284669,581.2978017718402,581.297816076546,581.2978306474474,581.2978454894975,581.2978606077417,581.2978760073194,581.2978916934653,581.2979076715121,581.2979239468914,581.2979405251353,581.2979574118793,581.2979746128644,581.2979921339371,581.2980099810536,581.2980281602808,581.2980466777979,581.2980655398995,581.2980847529979,581.2981043236235,581.2981242584289,581.2981445641902,581.29816524781,581.2981863163186,581.2982077768775,581.2982296367817,581.2982519034613,581.2982745844851,581.2982976875622,581.2983212205459,581.2983451914347,581.2983696083764,581.2983944796706,581.2984198137706,581.2984456192875,581.2984719049921,581.2984986798189,581.2985259528681,581.2985537334098,581.2985820308858,581.2986108549134,581.2986402152901,581.2986701219944,581.2987005851905,581.2987316152324,581.2987632226659,581.2987954182332,581.2988282128766,581.2988616177417,581.298895644181,581.2989303037588,581.2989656082538,581.2990015696648,581.2990382002123,581.2990755123456,581.2991135187443,581.2991522323247,581.2991916662426,581.2992318338988,581.2992727489435,581.2993144252803,581.2993568770718,581.2994001187434,581.2994441649894,581.299489030777,581.2995347313514,581.2995812822422,581.2996286992667,581.299676998537,581.2997261964648,581.2997763097663,581.2998273554692,581.2998793509178,581.2999323137781,581.2999862620455,581.3000412140492,581.3000971884601,581.3001542042947,581.3002122809248,581.3002714380812,581.3003316958617,581.3003930747382,581.300455595562,581.3005192795736,581.300584148407,581.300650224099,581.300717529096,581.3007860862614,581.3008559188837,581.3009270506841,581.3009995058248,581.3010733089171,581.3011484850298,581.3012250596973,581.301303058929,581.3013825092169,581.3014634375461,581.3015458714028,581.3016298387838,581.3017153682067,581.3018024887181,581.3018912299056,581.3019816219049,581.3020736954131,581.3021674816969,581.3022630126037,581.3023603205733,581.3024594386472,581.3025604004818,581.3026632403576,581.3027679931926,581.302874694553,581.3029833806662,581.3030940884319,581.303206855435,581.3033217199592,581.3034387209983,581.3035578982707,581.3036792922327,581.3038029440906,581.3039288958179,581.3040571901652,581.3041878706784,581.3043209817114,581.3044565684412,581.3045946768839,581.3047353539098,581.3048786472591,581.3050246055581,581.3051732783362,581.3053247160408,581.305478970057,581.3056360927227,581.3057961373471,581.3059591582288,581.3061252106737,581.3062943510142,581.3064666366274,581.3066421259555,581.3068208785239,581.3070029549627,581.3071884170275,581.3073773276175,581.3075697507999,581.3077657518295,581.307965397171,581.3081687545217,581.3083758928345,581.3085868823401,581.3088017945714,581.3090207023878,581.3092436799986,581.3094708029889,581.3097021483445,581.3099377944774,581.3101778212542,581.3104223100195,581.3106713436266,581.3109250064625,581.3111833844778,581.3114465652153,581.3117146378396,581.3119876931657,581.3122658236908,581.312549123625,581.3128376889223,581.313131617313,581.3134310083366,581.3137359633753,581.3140465856868,581.3143629804403,581.314685254751,581.3150135177153,581.3153478804483,581.3156884561198,581.3160353599935,581.3163887094643,581.3167486240974,581.317115225669,581.3174886382063,581.3178689880295,581.3182564037928,581.3186510165278,581.3190529596875,581.31946236919,581.3198793834641,581.3203041434958,581.3207367928738,581.3211774778383,581.3216263473296,581.3220835530364,581.3225492494477,581.3230235939027,581.3235067466434,581.3239988708682,581.324500132785,581.3250107016668,581.3255307499077,581.3260604530792,581.3265999899895,581.3271495427409,581.3277092967917,581.3282794410156,581.3288601677654,581.3294516729351,581.3300541560258,581.3306678202099,581.3312928723992,581.3319295233122,581.3325779875433,581.3332384836335,581.3339112341415,581.3345964657176,581.3352944091775,581.3360052995771,581.3367293762907,581.3374668830883,581.3382180682162,581.3389831844758,581.339762489309,581.3405562448798,581.341364718161,581.3421881810206,581.3430269103094,581.343881187952,581.3447513010383,581.3456375419152,581.3465402082832,581.3474596032916,581.3483960356355,581.3493498196583,581.3503212754501,581.3513107289532,581.3523185120661,581.353344962749,581.354390425135,581.3554552496386,581.3565397930688,581.3576444187431,581.3587694966039,581.3599154033366,581.3610825224898,581.3622712445981,581.3634819673044,581.3647150954899,581.3659710413992,581.3672502247731,581.3685530729798,581.3698800211529,581.3712315123256,581.3726079975722,581.3740099361498,581.3754377956437,581.3768920521114,581.378373190235,581.379881703471,581.3814180942056,581.3829828739098,581.3845765633008,581.3861996925025,581.3878528012104,581.3895364388592,581.3912511647916,581.3929975484325,581.3947761694632,581.396587618,581.3984324947764,581.4003114113248,581.4022249901659,581.4041738649971,581.4061586808856,581.408180094465,581.4102387741333,581.4123354002559,581.41447066537,581.4166452743942,581.418859944839,581.4211154070221,581.4234124042865,581.4257516932223,581.4281340438918,581.4305602400563,581.4330310794094,581.4355473738116,581.4381099495286,581.4407196474741,581.4433773234545,581.4460838484192,581.448840108713,581.451647006333,581.454505459189,581.4574164013668,581.4603807833962,581.4633995725228,581.4664737529838,581.4696043262854,581.4727923114876,581.47603874549,581.4793446833235,581.482711198443,581.4861393830289,581.4896303482866,581.4931852247558,581.496805162619,581.5004913320168,581.5042449233666,581.508067147685,581.5119592369141,581.5159224442526,581.5199580444901,581.5240673343461,581.5282516328123,581.5325122815004,581.5368506449913,581.5412681111914,581.5457660916911,581.5503460221269,581.5550093625491,581.5597575977926,581.5645922378508,581.5695148182547,581.5745269004555,581.5796300722104,581.5848259479742,581.5901161692901,581.5955024051907,581.6009863525965,581.6065697367226,581.6122543114845,581.6180418599124,581.6239341945654,581.629933157948,581.6360406229347,581.6422584931919,581.6485887036065,581.655033220718,581.661594043148,581.6682732020405,581.6750727614966,581.6819948190175,581.6890415059469,581.6962149879157,581.7035174652901,581.7109511736197,581.7185183840874,581.7262214039619,581.7340625770502,581.7420442841517,581.7501689435129,581.7584390112821,581.7668569819643,581.7754253888787,581.7841468046106,581.793023841469,581.8020591519377,581.811255429131,581.8206154072411,581.8301418619901,581.8398376110765,581.8497055146196,581.859748475601,581.8699694403041,581.8803713987477,581.8909573851181,581.901730478193,581.9126938017655,581.9238505250571,581.9352038631287,581.9467570772832,581.9585134754626,581.9704764126342,581.9826492911733,581.9950355612334,582.0076387211086,582.020462317586,582.0335099462867,582.0467852519969,582.0602919289845,582.0740337213057,582.088014423097,582.1022378788518,582.1167079836838,582.1314286835725,582.1464039755942,582.161637908135,582.1771345810822,582.1928981460019,582.2089328062904,582.2252428173094,582.2418324864949,582.2587061734463,582.2758682899878,582.2933233002068,582.3110757204661,582.329130119386,582.3474911177999,582.3661633886802,582.3851516570304,582.4044606997485,582.4240953454548,582.4440604742848,582.4643610176465,582.4850019579417,582.505988328245,582.5273252119475,582.5490177423541,582.571071102243,582.5934905233762,582.6162812859677,582.639448718104,582.6629981951138,582.6869351388915,582.7112650171649,582.7359933427149,582.7611256725354,582.7866676069402,582.8126247886128,582.8390029015957,582.8658076702196,582.8930448579699,582.9207202662919,582.9488397333272,582.977409132588,583.006434371558,583.0359213902292,583.0658761595637,583.0963046798856,583.127212979197,583.1586071114209,583.1904931545657,583.2228772088142,583.2557653945311,583.2891638501918,583.3230787302299,583.3575162028019,583.3924824474676,583.4279836527883,583.4640260138376,583.5006157296264,583.5377590004439,583.5754620251058,583.6137309981212,583.652572106765,583.691991528066,583.7319954257042,583.7725899468195,583.8137812187327,583.8555753455756,583.8979784048357,583.9409964438099,583.9846354759754,584.0289014772693,584.0738003822875,584.1193380803956,584.165520411761,584.2123531633005,584.2598420645497,584.3079927834559,584.3568109220946,584.4063020123117,584.4564715112997,584.5073247971019,584.5588671640545,584.6111038181679,584.6640398724493,584.7176803421727,584.7720301400983,584.8270940716486,584.882876830041,584.9393829913868,584.9966170097592,585.0545832122359,585.1132857939202,585.1727288129532,585.2329161855118,585.293851680812,585.3555389161138,585.4179813517402,585.4811822861167,585.5451448508355,585.6098720057593,585.6753665341624,585.7416310379283,585.8086679328031,585.8764794437202,585.9450676001987,586.0144342318305,586.0845809638607,586.1555092128746,586.2272201825951,586.2997148598083,586.3729940104178,586.4470581756439,586.521907668376,586.5975425696867,586.6739627255174,586.751167743549,586.829156990262,586.9079295881997,586.9874844134432,587.0678200933065,587.1489350042643,587.2308272701174,587.3134947604092,587.3969350890985,587.4811456135004,587.5661234335006,587.6518653910539,587.738368069972,587.8256277960103,587.9136406372596,588.002402404847,588.0919086539561,588.18215468517,588.2731355461399,588.3648460335875,588.4572806956423,588.5504338345198,588.6442995095383,588.7388715404823,588.834143511309,588.9301087742009,589.0267604539646,589.1240914527716,589.2220944552447,589.3207619338835,589.4200861548265,589.5200591839489,589.6206728932859,589.721918967783,589.8237889123587,589.9262740592817,590.0293655758458,590.1330544723419,590.2373316103107,590.3421877110716,590.4476133645127,590.5535990381335,590.6601350863255,590.767211759879,590.8748192157026,590.982947526739,591.0915866920667,591.2007266471654,591.3103572743369,591.4204684132576,591.5310498716512,591.642091436061,591.7535828827047,591.8655139883947,591.9778745415042,592.0906543529603,592.2038432672472,592.3174311733967,592.4314080159519,592.5457638058796,592.6604886314159,592.7755726688256,592.891006193053,593.0067795882492,593.1228833581551,593.2393081363218,593.3560446961494,593.4730839607284,593.590417012464,593.7080351024673,593.8259296596957,593.9440922998286,594.0625148338587,594.181189276389,594.3001078536162,594.419263010991,594.5386474205377,594.6582539878252,594.7780758585744,594.8981064248931,595.0183393311269,595.1387684793182,595.2593880342637,595.3801924281642,595.5011763648566,595.6223348236285,595.7436630626028,595.8651566216939,595.9868113251314,596.1086232835453,596.2305888956157,596.3527048492854,596.4749681225329,596.5973759837116,596.719925991453,596.8426159941409,596.9654441289564,597.0884088205008,597.211508779003,597.3347429981135,597.4581107522968,597.581611593826,597.7052453493908,597.829012116327,597.952912258478,598.0769464016968,598.2011154290034,598.3254204754058,598.4498629223989,598.574444392153,598.6991667414072,598.8240320550786,598.949042639604,599.0742010160282,599.1995099128524,599.3249722586606,599.4505911745376,599.5763699662955,599.7023121165242,599.8284212764829,599.9547012578488,600.0811560243396,600.207789683226,600.3346064767527,600.4616107734807,600.5888070595748,600.7161999300431,600.8437940799553,600.9715942956485,601.0996054459417,601.2278324733725,601.3562803854721,601.484954246098,601.6138591668333,601.7430002984743,601.872382822617,602.0020119433593,602.1318928791317,602.2620308546709,602.3924310931504,602.5230988084787,602.6540391977815,602.785257434075,602.9167586591462,603.0485479766498,603.1806304454291,603.3130110730781,603.4456948097464,603.5786865422012,603.7119910881516,603.8456131908465,603.979557513949,604.1138286366973,604.248431049358,604.3833691489752,604.5186472354217,604.6542695077574,604.7902400608984,604.9265628825988,605.0632418507482,605.2002807309892,605.3376831746536,605.4754527170201,605.6135927758929,605.7521066505021,605.8909975207239,606.0302684466214,606.1699223683032,606.3099621060959,606.4503903610321,606.5912097156465,606.7324226350792,606.8740314684802,607.0160384507119,607.1584457043448,607.3012552419377,607.4444689686026,607.5880886848413,607.7321160896508,607.8765527838908,608.0214002739015,608.1666599753707,608.3123332174334,608.458421247003,608.6049252333192,608.7518462727069,608.8991853935335,609.0469435613577,609.1951216842544,609.3437206183105,609.4927411732759,609.6421841183602,609.792050188163,609.9423400887265,610.0930545036952,610.2441941005729,610.3957595370633,610.5477514674795,610.7001705492095,610.8530174492257,611.0062928506211,611.1599974591608,611.3141320098317,611.4686972733784,611.6236940628077,611.7791232398483,611.9349857213504,612.0912824856081,612.2480145785917,612.4051831200713,612.5627893096187,612.720834432469,612.8793198652294,613.0382470814156,613.1976176568025,613.3574332745732,613.5176957302476,613.6784069363807,613.8395689270076,614.0011838618256,614.1632540300956,614.3257818542484,614.4887698931798,614.6522208452211,614.8161375507707,614.9805229945723,615.1453803076257,615.3107127687184,615.4765238055631,615.6428169955311,615.8095960659675,615.9768648940792,616.1446275063823,616.312888077702,616.4816509297117,616.6509205290057,616.8207014846964,616.9909985455276,617.1618165964984,617.3331606549908,617.5050358663977,617.6774474992466,617.8504009398121,618.0239016862222,618.1979553420499,618.3725676093934,618.5477442814441,618.7234912345455,618.8998144197429,619.0767198538298,619.2542136098949,619.4323018073745,619.61099060162,619.7902861729837,619.9701947154376,620.1507224247302,620.3318754860967,620.5136600615316,620.6960822766409,620.8791482070856,621.0628638646331,621.2472351828357,621.4322680023503,621.6179680559228,621.8043409530549,621.9913921643736,622.1791270057322,622.3675506220568,622.5566679709714,622.746483806222,622.9370026609298,623.1282288306965,623.3201663565958,623.5128190080757,623.7061902658028,623.9002833044817,624.095100975678,624.2906457906786,624.4869199034226,624.6839250935357,624.8816627494997,625.080133851997,625.2793389574599,625.4792781818587,625.679951184771,625.8813571537587,626.0834947890932,626.2863622888646,626.4899573345057,626.694277076771,626.8993181222012,627.1050765201107,627.3115477501303,627.5187267103368,627.7266077060071,627.9351844390239,628.1444499979641,628.354396848905,628.5650168269708,628.7763011286513,628.9882403049189,629.2008242551678,629.4140422220012,629.627882786886,629.8423338667006,630.0573827111895,630.2730159013458,630.4892193487364,630.7059782957809,630.9232773170023,631.1411003212517,631.359430554919,631.5782506061347,631.7975424099669,632.0172872546113,632.2374657885767,632.4580580288614,632.6790433701146,632.9004005947775,633.122107884188,633.344142830645,633.56648245041,633.7891031976352,634.0119809791948,634.2350911704004,634.4584086315784,634.6819077254795,634.9055623354998,635.1293458846772,635.3532313554381,635.5771913100579,635.8011979117999,636.0252229467006,636.2492378459609,636.4732137089036,636.6971213264582,636.920931205133,637.1446135914265,637.3681384966435,637.5914757220611,637.8145948844094,638.0374654416145,638.2600567187612,638.4823379342295,638.7042782259551,638.9258466777735,639.147012345794,639.3677442847646,639.5880115743769,639.8077833454687,640.0270288060786,640.2457172673099,640.4638181689573,640.6813011048594,640.8981358479319,641.1142923748436,641.3297408902946,641.5444518508632,641.7583959883799,641.9715443328,642.183868234537,642.3953393862296,642.6059298439086,642.8156120475404,643.024358840918,643.2321434908755,643.4389397058047,643.644721653453,643.849463977985,644.0531418162882,644.255730813511,644.45720713782,644.6575474943609,644.8567291384236,645.054729887793,645.2515281342875,645.44710285448,645.6414336195984,645.8345006046061,646.0262845964639,646.2167670015783,646.4059298524379,646.5937558134455,646.7802281859559,646.9653309125251,647.149048580384,647.3313664241459,647.5122703277649],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[424.5139933852138,424.5139943908825,424.51399541526683,424.5139964587149,424.5139975215818,424.51399860422885,424.51399970702397,424.5140008303423,424.51400197456576,424.5140031400831,424.514004327291,424.51400553659283,424.5140067684002,424.5140080231312,424.5140093012132,424.5140106030803,424.51401192917524,424.5140132799491,424.5140146558608,424.51401605737834,424.51401748497824,424.51401893914607,424.514020420376,424.5140219291717,424.51402346604624,424.5140250315221,424.51402662613174,424.514028250417,424.5140299049305,424.51403159023454,424.51403330690215,424.51403505551707,424.51403683667405,424.51403865097825,424.51404049904676,424.51404238150803,424.514044299002,424.51404625218083,424.51404824170817,424.5140502682608,424.51405233252774,424.51405443521065,424.51405657702475,424.51405875869807,424.5140609809722,424.5140632446033,424.5140655503605,424.51406789902785,424.5140702914041,424.51407272830255,424.5140752105519,424.51407773899564,424.514080314494,424.51408293792235,424.5140856101728,424.5140883321538,424.5140911047909,424.51409392902684,424.5140968058219,424.51409973615415,424.5141027210197,424.51410576143365,424.51410885842984,424.5141120130609,424.5141152263998,424.5141184995388,424.5141218335908,424.5141252296895,424.51412868898944,424.51413221266694,424.5141358019199,424.5141394579687,424.51414318205656,424.5141469754494,424.51415083943704,424.51415477533345,424.5141587844766,424.51416286822956,424.51416702798105,424.5141712651451,424.51417558116253,424.51417997750065,424.5141844556544,424.51418901714595,424.5141936635266,424.5141983963759,424.514203217303,424.51420812794714,424.5142131299777,424.5142182250954,424.5142234150328,424.51422870155403,424.51423408645684,424.51423957157203,424.5142451587643,424.5142508499333,424.5142566470142,424.5142625519778,424.5142685668316,424.5142746936209,424.51428093442854,424.51428729137643,424.51429376662594,424.51430036237855,424.51430708087656,424.51431392440446,424.5143208952889,424.51432799589986,424.5143352286516,424.5143425960029,424.51435010045867,424.51435774457053,424.51436553093714,424.51437346220587,424.5143815410733,424.51438977028596,424.51439815264183,424.5144066909907,424.5144153882355,424.5144242473332,424.51443327129545,424.5144424631905,424.5144518261436,424.51446136333766,424.5144710780154,424.51448097347924,424.51449105309393,424.5145013202859,424.5145117785461,424.51452243143,424.5145332825592,424.5145443356231,424.51455559437926,424.5145670626555,424.5145787443507,424.51459064343635,424.5146027639579,424.51461511003583,424.51462768586754,424.51464049572866,424.51465354397374,424.51466683503924,424.5146803734434,424.5146941637891,424.51470821076424,424.5147225191448,424.514737093795,424.51475193966934,424.5147670618156,424.51478246537414,424.51479815558173,424.51481413777276,424.51483041738004,424.5148469999384,424.5148638910852,424.5148810965625,424.5148986222196,424.51491647401457,424.5149346580159,424.5149531804054,424.5149720474801,424.5149912656536,424.5150108414592,424.51503078155173,424.51505109270994,424.5150717818383,424.5150928559702,424.51511432226954,424.51513618803375,424.51515846069594,424.51518114782755,424.51520425714085,424.51522779649133,424.51525177388123,424.5152761974613,424.5153010755336,424.51532641655564,424.515352229141,424.5153785220648,424.51540530426433,424.5154325848436,424.5154603730761,424.5154886784078,424.51551751046014,424.51554687903393,424.51557679411184,424.51560726586297,424.51563830464494,424.5156699210079,424.51570212569953,424.5157349296656,424.515768344057,424.51580238023143,424.51583704975786,424.5158723644205,424.5159083362231,424.5159449773923,424.5159823003823,424.516020317879,424.51605904280416,424.5160984883201,424.51613866783407,424.5161795950021,424.51622128373504,424.51626374820194,424.5163070028355,424.51635106233715,424.5163959416814,424.51644165612134,424.51648822119415,424.51653565272545,424.51658396683575,424.5166331799449,424.51668330877834,424.5167343703726,424.51678638208136,424.5168393615803,424.5168933268746,424.516948296304,424.51700428854895,424.51706132263797,424.51711941795315,424.51717859423684,424.51723887159886,424.51730027052304,424.5173628118735,424.51742651690313,424.5174914072596,424.51755750499314,424.51762483256425,424.517693412851,424.5177632691565,424.51783442521787,424.5179069052129,424.51798073376904,424.51805593597237,424.51813253737447,424.51821056400235,424.5182900423673,424.51837099947284,424.518453462825,424.51853746044105,424.51862302085897,424.5187101731477,424.51879894691626,424.51888937232405,424.5189814800912,424.51907530150925,424.51917086845083,424.5192682133814,424.5193673693697,424.51946837009933,424.5195712498799,424.5196760436586,424.5197827870324,424.5198915162595,424.52000226827255,424.5201150806902,424.52022999182986,424.5203470407219,424.5204662671215,424.5205877115224,424.5207114151713,424.52083742008125,424.5209657690457,424.5210965056534,424.52122967430296,424.52136532021797,424.5215034894626,424.5216442289558,424.52178758648927,424.5219336107419,424.5220823512973,424.5222338586599,424.52238818427253,424.52254538053296,424.52270550081323,424.5228685994757,424.5230347318931,424.5232039544661,424.523376324643,424.52355190093925,424.5237307429564,424.52391291140316,424.52409846811537,424.5242874760773,424.52447999944195,424.52467610355427,424.5248758549718,424.5250793214879,424.52528657215447,424.5254976773051,424.525712708579,424.5259317389451,424.52615484272644,424.52638209562593,424.5266135747505,424.5268493586392,424.52708952728744,424.5273341621757,424.5275833462955,424.5278371641791,424.52809570192636,424.5283590472345,424.5286272894274,424.5289005194862,424.52917883007956,424.52946231559406,424.5297510721676,424.53004519772054,424.53034479198885,424.5306499565581,424.53096079489694,424.53127741239246,424.5315999163856,424.5319284162064,424.53226302321207,424.53260385082314,424.5329510145624,424.5333046320931,424.5336648232591,424.5340317101243,424.53440541701394,424.5347860705571,424.5351737997276,424.5355687358884,424.53597101283503,424.536380766841,424.5367981367027,424.537223263787,424.53765629207703,424.53809736822154,424.5385466415838,424.53900426429124,424.53947039128644,424.539945180379,424.5404287922979,424.54092139074595,424.5414231424536,424.54193421723545,424.5424547880461,424.5429850310383,424.5435251256218,424.54407525452217,424.5446356038438,424.5452063631294,424.5457877254248,424.54637988734254,424.5469830491275,424.54759741472293,424.54822319183944,424.54886059202283,424.54950983072473,424.5501711273742,424.55084470545086,424.5515307925582,424.5522296205001,424.5529414253562,424.55366644756185,424.55440493198586,424.55515712801304,424.5559232896258,424.5567036754885,424.5574985490324,424.5583081785433,424.5591328372496,424.559972803412,424.5608283604166,424.56169979686655,424.5625874066781,424.5634914891773,424.56441234919737,424.5653502971806,424.56630564927826,424.5672787274559,424.56826985959833,424.5692793796166,424.57030762755824,424.5713549497178,424.5724216987504,424.57350823378664,424.57461492055,424.5757421314765,424.57689024583533,424.57805964985266,424.5792507368376,424.58046390730954,424.5816995691287,424.58295813762817,424.5842400357487,424.58554569417566,424.58687555147816,424.5882300542513,424.58960965725987,424.591014823586,424.5924460247766,424.5939037409978,424.5953884611867,424.59690068321004,424.5984409140238,424.6000096698355,424.6016074762689,424.60323486853343,424.60489239159386,424.6065806003445,424.6083000597866,424.610051345207,424.6118350423625,424.613651747664,424.61550206836654,424.61738662276144,424.61930604037155,424.6212609621498,424.62325204068077,424.6252799403871,424.6273453377371,424.62944892145754,424.6315913927488,424.6337734655047,424.6359958665342,424.63825933578937,424.64056462659414,424.6429125058782,424.6453037544159,424.64773916706605,424.65021955301836,424.652745736042,424.6553185547397,424.65793886280403,424.6606075292795,424.6633254388274,424.6660934919962,424.6689126054948,424.67178371247076,424.67470776279316,424.6776857233392,424.6807185782851,424.6838073294028,424.68695299635925,424.6901566170212,424.6934192477658,424.6967419637933,424.7001258594464,424.7035720485339,424.7070816646589,424.7106558615514,424.7142958134072,424.71800271523017,424.72177778318024,424.72562225492624,424.72953739000377,424.7335244701785,424.73758479981353,424.74171970624275,424.7459305401497,424.75021867595024,424.75458551218213,424.7590324718983,424.7635610030661,424.7681725789725,424.77286869863343,424.7776508872087,424.78252069642286,424.78747970499103,424.79252951904976,424.79767177259373,424.80290812791804,424.808240276065,424.8136699372763,424.8191988614523,424.8248288286137,424.8305616493712,424.8363991653992,424.84234324991417,424.8483958081601,424.85455877789684,424.8608341298957,424.8672238684386,424.8737300318231,424.88035469287195,424.8870999594478,424.8939679749731,424.9009609189534,424.9080810075068,424.9153304938978,424.92271166907426,424.93022686221,424.9378784412518,424.9456688134693,424.9536004260092,424.96167576645473,424.9698973633861,424.97826778694696,424.98678964941263,424.99546560576186,425.0042983542516,425.01329063699376,425.022445240536,425.0317649964419,425.04125278187695,425.0509115201922,425.0607441815121,425.070753783322,425.0809433910573,425.09131611869213,425.1018751293282,425.1126236357852,425.1235649011871,425.1347022395501,425.14603901636934,425.1575786492005,425.16932460824285,425.1812804169169,425.1934496524396,425.20583594639584,425.2184429853046,425.2312745111814,425.24433432209383,425.25762627271183,425.2711542748512,425.2849222980085,425.2989343698891,425.3131945769258,425.32770706478794,425.34247603887945,425.3575057648272,425.3728005689563,425.388364838752,425.40420302331097,425.4203196337731,425.43671924374286,425.45340648969085,425.47038607133925,425.48766275202894,425.5052413590666,425.52312678405167,425.5413239831822,425.559837977537,425.5786738533339,425.59783676216495,425.6173319212029,425.63716461338123,425.6573401875461,425.677864058577,425.6987417074775,425.71997868143274,425.7415805938319,425.7635531242574,425.78590201843576,425.8086330881508,425.8317522111173,425.8552653308135,425.8791784562712,425.9034976618211,425.92822908679466,425.9533789351758,425.97895347520716,426.0049590389444,426.0314020217595,426.0582888817915,426.08562613934157,426.1134203762131,426.1416782349932,426.1704064182759,426.1996116878245,426.2293008636714,426.2594808231562,426.29015849989645,426.32134088269447,426.3530350143756,426.38524799055773,426.41798695835143,426.451259114988,426.48507170637623,426.5194320255853,426.5543474112529,426.5898252459198,426.62587295428597,426.66249800139275,426.6997078907247,426.73751016223673,426.7759123902998,426.8149221815699,426.85454717277855,426.89479502844216,426.9356734384956,426.9771901158444,427.019352793841,427.06216922368236,427.10564717173,427.1497944167555,427.19461874710953,427.2401279578181,427.2863298476055,427.3332322158473,427.38084285945354,427.4291695696857,427.47822012890873,427.52800230727996,427.57852385937946,427.62979252078355,427.6818160045835,427.7346019978556,427.78815815808247,427.84249210953396,427.8976114396064,427.9535236951292,428.0102363786406,428.06775694463795,428.12609279580704,428.1852512792377,428.24523968262747,428.30606523048283,428.3677350803204,428.43025631887707,428.4936359583329,428.55788093255495,428.62299809336724,428.6889942068548,428.75587594970744,428.8236499056115,428.89232256169635,428.96190030504107,429.0323894192525,429.10379608111884,429.1761263573459,429.24938620138704,429.32358145037097,429.3987178221354,429.4748009123756,429.551836191914,429.6298290040972,429.70878456233135,429.7887079477564,429.8696041070744,429.951477850531,430.03433385006133,430.11817663760473,430.20301060359515,430.28883999563175,430.3756689173362,430.46350132740184,430.5523410388375,430.64219171841347,430.73305688631,430.8249399159747,430.91784403418905,431.01177232134984,431.1067277119633,431.2027129953572,431.2997308166098,431.39778367769543,431.4968739388485,431.5970038201432,431.69817540328694,431.80039063362943,431.90365132237804,432.00795914902284,432.1133156639623,432.2197222913283,432.32718033200246,432.43569096682126,432.5452552599601,432.65587416249076,432.76754851610525,432.88027905699664,432.99406641988804,433.108911142201,433.2248136683533,433.3417743541752,433.4597934714338,433.57887121245335,433.6990076948219,433.8202029661685,433.9424570090022,434.0657697455984,434.190141042918,434.3155707175495,434.4420585406574,434.5696042429246,434.69820751947555,434.82786803476455,434.95858542741786,435.09035931501313,435.22318929878367,435.35707496823585,435.4920159056612,435.6280116905354,435.76506190378734,435.9031661319275,436.0423239710223,436.1825350305028,436.32379893679615,436.46611533676867,436.6094839009698,436.75390432666705,436.89937634066223,437.0458997018802,437.19347420372236,437.3420996761763,437.4917759876749,437.6425030467004,437.7942808031257,437.9471092492897,438.10098842080214,438.2559183970754,438.41189930158106,438.5689313018288,438.72701460906825,438.8861494777139,439.0463362044929,439.2075751273194,439.3698666238963,439.5332111100507,439.6976090378034,439.86306089318174,440.02956719377937,440.1971284860696,440.36574534248086,440.5354183582426,440.7061481480083,440.87793534226864,441.0507805835622,441.2246845224953,441.3996478135839,441.5756711109268,441.75275506372566,441.93090031166275,442.11010748014996,442.290377175465,442.47170997978634,442.65410644614326,442.83756709329526,443.0220924005557,443.20768280257613,443.39433868410504,443.5820603747382,443.7708481436756,443.9607021945011,444.15162265999896,444.3436095970263,444.5366629814532,444.7307827031888,444.92596856130723,445.1222202592886,445.31953740039114,445.51791948316696,445.71736589713726,445.91787591864113,446.11944870686926,446.32208330009865,446.52577861213854,446.7305334290005,446.936346405805,447.14321606393486,447.3511407884455,447.56011882574325,447.7701482815404,447.9812271190957,448.193353157748,448.406524071751,448.62073738941604,448.83599049256765,449.0522806163184,449.26960484916793,449.4879601334277,449.7073432659777,449.927750899355,450.1491795431773,450.3716255659017,450.5950851969185,450.81955452898046,451.04502952096493,451.27150600096815,451.49897966972696,451.7274461043669,451.9569007624691,452.1873389864546,452.418756008277,452.6511469544195,452.88450685118744,453.11883063028955,453.3541131346988,453.590349124784,453.82753328470204,454.0656602290408,454.3047245097003,454.5447206230028,454.7856430170164,455.0274860990826,455.2702442435312,455.51391179957096,455.75848309934065,456.0039524661044,456.25031422257837,456.49756269937,456.74569224351495,456.99469722709455,457.24457205591716,457.49531117824375,457.7469090935423,457.9993603612503,458.2526596095284,458.50680154398543,458.7617809563553,459.01759273310734,459.2742318639683,459.5316934503392,459.7899727135842,460.049065003172,460.3089658046514,460.5696707474371,460.83117561238896,461.0934763391624,461.356569033309,461.62044997310943,461.8851156161161,462.1505626053868,462.4167877753891,462.68378815755585,462.951560985473,463.22010369967927,463.4894139520605,463.75948960981964,464.0303287590036,464.30192970757145,464.5742909879845,464.8474113593047,465.12128980878214,465.3959255529196,465.6713180379976,465.9474669400465,466.2243721642532,466.5020338437895,466.78045233805057,467.05962823029324,467.3395623246648,467.6202556426126,467.9017094186674,468.1839250955938,468.4669043189016,468.7506489307139,469.03516096298847,469.3204426300899,469.60649632071136,469.8933245891472,470.1809301459152,470.46931584773444,470.75848468685945,471.04843977977794,471.3391843552785,471.630721741894,471.9230553547335,472.21618868170924,472.51012526917407,472.8048687069797,473.1004226129729,473.39679061694426,473.69397634404623,473.99198339770163,474.29081534202015,474.59047568374547,474.8909678537561,475.1922951881432,475.494460908891,475.79746810418686,476.1013197083885,476.4060184816768,476.7115669894266,477.0179675813234,477.3252223702628,477.63333321106137,477.94230167901884,478.2521290483614,478.56281627060787,478.87436395289257,479.18677233628426,479.50004127413945,479.81417021053096,480.12915815878944,480.44500368020084,480.76170486289953,481.07925930099924,481.39766407400276,481.71691572653367,482.03701024843053,482.35794305524814,482.67970896920525,483.0023022006227,483.325716329893,483.649944290023,483.97497834979066,484.30081009755645,484.62743042576926,484.95482951620585,485.28299682598276,485.61192107437796,485.94159023049735,486.27199150182435,486.6031113236834,486.9349353496524,487.2674484429542,487.60063466885833,487.9344772881192,488.2689587514788,488.60406069525857,488.93976393806184,489.2760484786103,489.6128934947296,489.9502773435046,490.2881775626162,490.6265708728725,490.9654331819451,491.3047395893169,491.6444643924463,491.98458109415225,492.32506241121814,492.66588028421245,493.00700588852266,493.34840964659276,493.690061241355,494.0319296308439,494.3739830639754,494.71618909747497,495.058514613932,495.4009258409602,495.74338837143625,496.08586718478927,496.4283266693113,496.7707306454562,497.1130423900908,497.4552246616631,497.7972397262468,498.1390493844219,498.48061499894806,498.82189752318567,499.16285753021924,499.5034552426332,499.8436505628924,500.1834031042752,500.5226722223089,500.8614170466521,501.1995965133726,501.5371693975638,501.87409434624703,502.2103299115015,502.5458345837678,502.880566825267,503.21448510348097,503.54754792463626,503.8797138671369,504.2109416148896,504.5411899904677,504.8704179880584,505.19858480614107,505.5256498798439,505.8515729129278,506.1763139093466,506.49983320433637,506.82209149498556,507.14304987024093,507.4626698403045,507.7809133653797,508.0977428837271,508.41312133898913,508.7270122067496,509.0393795202928,509.35018789552845,509.65940255505456,509.9669893513288,510.27291478892306,510.57714604583856,510.8796509938582,511.18039821792036,511.47935703449474,511.7764975089478,512.0717904718855,512.3652075344633,512.6567211026561,512.9463043904835,513.2339314321858,513.5195770933534,513.8032170810053,514.0848279526258,514.3643871241602,514.6418728769797,514.9172643638227,515.1905416137233,515.4616855359421,515.730677922909,515.9975014521979,516.2621396875487,516.5245770789551,516.7847989618375,517.0427915553246,517.298541959663,517.5520381527798,517.8032689860221,518.0522241790986,518.2988943142484],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Ridge en fonction de alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Ridge pour les paramètres de GridSearchCV\n","FigRMSEGRidRidge = visuRMSEGrid(Ridge(), 'Ridge', alphasridge, 'alpha',\n","                                GridRidge)\n","FigRMSEGRidRidge.show()\n","if write_data is True:\n","    FigRMSEGRidRidge.write_image('./Figures/EmissionsGraphRMSERidge.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.3 Modèle Lasso"]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre  Lasso()\n","0  lasso__alpha      1.0\n","               R²        RMSE         MAE\n","Lasso()  0.370266  423.474821  112.829957\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predLasso=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[55.75513400068231,41.00573377120466,105.12594455809977,43.78769241762673,31.77154780147211,20.34000476167435,49.14947674929415,2.4817103968568546,48.77182882301144,33.7582500051246,116.63161835794166,62.111905676872574,1794.7225676784096,24.68037346589142,55.283628087667,14.047033288240435,-0.7679209999928531,111.6131948940697,116.86655321594124,2458.745527493478,207.44268720413402,-20.251456725442566,53.11637070700425,94.7271648143393,408.42259428951024,239.56974831461852,7.287672657339684,155.60139560468343,52.24024375953055,66.02715782808063,163.2780566443507,55.43247691730557,29.14304536995997,498.3933909608319,56.00455836338068,61.30566124107894,52.80895361479094,73.38297011885454,65.62736978570348,66.29397518650015,68.00101745032936,56.85306479319442,215.34258251256554,42.158972318552756,270.9617436233641,324.6174807352229,74.65009617297139,2.320258619364779,298.32453329646955,139.2496668045155,141.17780653933588,70.65659183384322,45.18026283905765,53.38988667376553,42.60809513018545,49.00832674018888,88.63449439889364,1140.9055092944614,512.9624520203188,88.37692395178655,10.494138002975482,150.67692393858957,115.90969147991277,32.253208221878396,40.4074216395819,44.15076124737364,134.60920790440358,49.77268190777248,141.37437753936436,176.97690471312748,38.88929040542344,42.66548732456261,29.205190903953664,59.2005530426586,41.68642462995816,21.23073733056485,32.83722519708924,42.44239465909739,38.9690064681378,88.34230696779338,47.927710038083546,25.410754061606198,84.75782748074296,26.271912318422622,53.33893809053733,128.72647673869952,242.0524330026232,74.6921720908427,50.74588166190375,37.988438911091485,72.0923336672368,44.114994720112534,11.887554197524118,24.569400301641146,229.63771473888107,47.25147082333601,62.4181517190722,32.98093123247658,74.44023425089438,334.7237122791202,16.90176554956554,116.1915720995384,16.03265623896222,211.91281093198216,61.34286141363939,76.43512139666656,545.8498609733336,95.33856676420669,-12.63066589853078,47.626032608561616,77.98703559181257,322.63327952523605,200.9513028104796,36.35986608052655,32.90406671098648,299.3106670655258,70.73918647256629,-2.611388487090892,83.4543120545114,76.67740160097276,40.46927555321273,35.31062227643101,75.18766830597974,44.682426060505136,240.27962064927567,28.146611623559707,634.0774251143855,50.527977760337905,119.9479592304796,32.14885057376029,56.440586919567295,28.501815170676537,81.2837849560944,327.2401564072985,-140.66499570807298,2.722725258832625,190.19291249038088,101.22331934345351,37.392887083242925,28.0305516694487,51.516993751556356,37.988438911091485,601.9108239829566,33.90693551527771,90.64659539671945,66.16555056393113,52.17411582372857,32.314221314316846,239.46924369110081,-0.7679209999928531,39.713850873457204,299.55371390130085,-421.6354018775121,81.94987687542726,40.978754617148695,122.73089693828672,284.54903908115693,49.880803172687344,154.06818282352796,50.43246690189024,36.306220791003156,311.20677820832265,93.42604468503235,25.125118437703,60.534281429535845,9.865212592288174,281.92314055799426,-1.1340062312024344,188.24371112071518,352.440572808551,63.31497847913002,84.31849228544802,572.1407873445069,-0.299615078273078,5.737095985502698,108.50349578770619,42.35707344144217,11.335765116203504,97.78794206745562,98.09204139413093,-26.77775723304609,130.9056488802887,71.39811463281171,90.64100336069879,21.774976268860172,283.16729833503064,69.0690798664604,39.24181217678205,79.80327469412642,24.07645642874502,52.054176482130444,53.33893809053733,15.273989071553643,54.30019848158485,50.687878294349034,39.877482484540046,56.842674404349474,66.25603334485395,70.09742574572152,21.768996350650298,45.7309193247486,425.85927511503473,107.66035822118151,68.4762429154598,61.64107674019609,141.10928128811278,80.98552740966483,79.34802952076694,99.27609026550154,44.47038588528749,89.25086463112453,62.18013006293179,499.19707106722035,44.343982918325786,20.650215757451278,94.37693638172749,38.83909481306634,150.6966576219997,216.2586469688128,229.56370168863788,46.342032637684326,127.40363804889711,42.78722942362278,60.58102844402698,303.91907798967947,144.92213162281388,69.48608672444934,35.74215911984065,7.483571019434365,40.72556839233407,113.4267010352657,107.24176613191125,11.706846791517876,57.50827041336004,22.003568227849385,22.615696873135306,103.78369130303085,32.00241880258141,32.36216017266132,162.74125445500897,205.5012325539681,124.1794964895941,125.58570171420818,47.39016157778971,42.789394652346395,32.312792470226555,61.44729877709674,86.02784286528689,303.97642586278715,48.96100176361942,167.24298946601877,33.94850057606069,74.68931887525325,156.11481812651078,239.56974831461852,93.20098017162232,4.052230590481017,64.4684340479617,17.725659807027625,44.59537498450946,91.92934574643886,83.05291392398738,54.81638395515911,59.646071641289176,-5.544391958067209,272.9694228739937,-1.5234964382585758,295.97290995438624,24.10630854069327,45.45207210180086,149.3790300695316,45.26982836295972,44.44140412232426,25.674829615018872,987.1747436468144,29.141082274385177,-5.3933467220706035,235.31443811472104,22.899424413524656,140.86415966407614,43.032341253151856,55.89292832618331,33.92731162661077,107.58767333779065,19.827810087314276,155.7424558167162,47.509638317742905,38.77003974113908,83.15988684980837,11.855951758118628,104.94159170621354,41.675372925585606,25.092211839850876,55.38303319651923,35.89035880216527,20.34031191899335,301.61723324907325,52.433678657774024,35.658372026778785,59.526422926915444,47.99743928658462,-26.40062938990465,199.53333168979057,67.19259397552005,-36.99525348566432,127.40363804889711,63.38949671662224,49.82347398170643,38.449867991237234,126.43556011042261,70.37858338687232,64.44850967823226,179.13389020392444,116.28950096838206,6.777077434750112,36.18352042955247,54.26640532971705,104.77951213318676,81.53810350750624,40.56483752744432,110.11463032938676,32.721027433671175,43.969226832500176,58.38333844174717,189.39694238439867,27.138146668173327,87.47005687866395,16.769599725117644,351.45326582277227,198.41125580908368,130.16822764819216,95.82305357783629,69.13762646765097,25.998413740409575,52.23214277987708,37.52362942418243,23.267967326157503,110.1627211479437,114.5701502590484,58.760581156826184,204.46207401174598,86.73405508459611,-35.3992642878616,159.0746272863488,48.305357964366266,109.59069631362283,32.96146782190075,56.948346345178656,67.25625424568447,199.41113173417634,29.012288787842834,266.36044390816147,37.40445645489501,19.576643420044192,110.01071063320894,43.74629067632921,46.52611861507516,981.0430448105262,30.349209532539646,126.59742978695206,406.213009657204,33.69137397221299,404.8470895093197,37.742054577723295,130.12956665406534,98.88622954680051,126.80068544732246,76.11709915392144,39.496333909927145,9.37172674115427,23.327783758910428,54.483764259626085,94.60557896907738,53.565590131043784,89.44686481367462,-1.1340062312024344,157.27474094356273,73.10297847185387,295.48712014880493,82.85231488437796,48.15500427228584,76.82360530958566,36.74202859413485,16.538937609546394,63.97895438422619,-5.362414759266429,36.39925086966919,27.52853767128257,64.24147236431614,325.4234859888532,-152.20502423364678,69.47617073649945,281.50641734702674,70.7158297811271,161.1774040923495,242.7631127403164,20.373918194376493,48.96100176361942,24.34231637343582,57.18494195317452,37.86685712426281,129.15533805346627,71.2098138075362,107.8336966988729,25.17719436146156,221.72554676945458,57.14320466356355,35.5828096467236,17.011612557366483,86.16616086143213,58.30169412045841,40.41220447307751,-16.926789146492055,25.092498322861175,75.1203572930039,8.54463909517196,658.1050984803285,80.25683906895576,51.36037066431943,140.20498009661452,48.523405003007966,39.210879798875894,513.0159345997536,43.873981176215835,6.487983597384883,37.86685712426281,38.94150964680229,-5.544391958067209,33.508300459042914,65.13147360711702,73.84612873465102,69.3986121452345,194.41418667404753,59.85425382615677,64.51740824086527,61.44361487563923,43.5754949472398,64.51740824086527,171.24384783777643,46.02391748719026,100.5268586604895,9.495746702571168,39.909992641468016,10.977342384163443,22.38011772025544,40.41220447307751,40.769723380939745,-4.589056022155049,43.50727056118058,35.31844194033606,62.70883473318678,76.72167314913229,55.00725042170444,37.75746077543169,161.26456149708554,147.77662580648,677.2387682598476,33.123749719011556,133.0294046180512,54.469647521582104,46.00384232432038,81.40713965123405,31.948255589761956,115.53147184218355,-90.42759106816007,76.63584012142141,81.98323663723593,144.02248150857247,59.11855831013894,24.74753504954205,30.195063335316277,47.38392620720876,52.807552128847696,85.52181837620637,289.3566567334694,113.19713985067543,38.633641829308544,96.03246121207886,139.1657349954397,3.629783654278498,77.78791235551958,-254.68145237645706,23.943055301553954,47.11124458378543,51.50569210179475,27.14206992927901,55.81043209836754,152.85731130739563,35.09182188270758,106.72292506136479,36.515011639624376,25.736908801929104,68.01258682198144,32.22856461613235,177.14488017592492,23.276327479591806,797.0028782169007,55.02206744489341,24.07645642874502,16.48301313437951,199.53333168979057,36.31057886903446,160.11553375925973,233.78797741422775,25.934665350289116,140.59732378709904,65.22670807029672,70.27609069197105,10.289659799359882,13.476218253807389,20.76530598009559,59.56231749345845,128.18892827840568,46.634126135323726,65.20888361916565,141.93378872189976,291.336914525816,7.057197086855069,31.491352474233068,14.44418721876147,20.582861271993707,50.13982263682611,39.05248412730662,-29.474904788244118,38.633641829308544,45.05120398941056,537.9506780559334,73.50257329758745,81.38855244575788,390.86327059385394,223.76812137857826,15.292360080244713,95.27792534354434,20.58987708161802,378.08624548744706,204.42008808405473,44.95758644724353,29.74993386617821,151.8257951671211,29.22267596099121,39.10511889887969,26.349668095337663,18.920797947254606,32.09710772419445,55.47605769761874,75.95992353243452,218.71492514492104,-2.6940750404244227,68.73032134400114,117.98107538537273,69.3986121452345,40.0395888013999,61.101649406381355,159.0746272863488,119.23323443334363,56.61989667154518,48.14779297866506,108.87107322801955,21.809028887255415,36.22472275747838,20.312188271635797,88.0401324933138,10.005105605832071,141.85300776902614,89.44686481367462,196.03195967778444,-0.4517002391445004,57.24948837608099,527.6935479044001,20.372017254411716,389.2089571798069,210.08961454437105,68.4364518610001,137.8146126473664,20.667344115244603,96.86848718518517,40.97154046115425,33.54717421719563,96.77958573013072,5.087724654087481,26.81704062815559,233.78797741422775,17.93993161388599,95.77738429222208,134.1216239434479,144.25731848916547,101.01195367511328,44.0869892331454,327.2401564072985,73.68580406691541,15.292360080244713,34.62114945532535,3015.281101063244,72.52786008155036,37.12972845874905,19.073984037864562,84.74533514908197,106.59529994260306,133.45949885764176,52.965303170078165,54.01282059233286,17.720967823692376,159.6978324448271,17.989004470711066,33.70573985742807,94.93544454630987,281.6050336536114,68.68461142572876,299.55371390130085,1372.22649857282,35.155688190735475,42.53541916019691,59.75268059948825,45.97980566737914,49.14947674929415,242.0524330026232,164.7782872955815,178.8546956177076,252.97896531300285,-2.6369126212133267,20.04760490728051,95.41523991474165,37.97532414804307,40.313473540799066,25.830906066159095,34.547704684666265,32.9868701935815,39.51879847665752,95.86139299088887,51.192041737063235,123.76428723276564,192.85989058082976,44.00639442061368,59.75268059948825,30.831800607095275,394.5110581032696,24.3544872423942,13.381559917185463,57.657844567280705,659.8516822364357,77.2165535477805,52.85591270380465,48.70652729793413,68.46765285240865,38.31093668540894,21.948166233108594,18.65459769605009,51.94821852435459,84.45795078245015,19.299259715019232,869.6174044639201,11.802904456335739,40.46927555321273,18.31009598388544,66.8219526941671,40.40954621273379,-2.611388487090892,55.76179467200612,14.477329958443548,69.0690798664604,192.85077804383752,2.5954159852419707,-44.46711786809459,61.40320032950403,64.43075936088795,-11.26617681741088,32.34908593856737,37.02966174420176,-7.232709234429933,81.2837849560944,33.400887606556616,111.75742109850218,80.18602956575111,-340.9617901479404,33.29174297263083,7.616369451911353,47.43389886739714,47.629181692938346,65.38700131740725,-17.26171569407512,107.3126084636457,27.789233235585876,133.21553058573733,372.7461609072129,218.09003249309816,67.72226102007325,76.1968527161036,39.51879847665752,40.27082787032309,1087.5021387875472,36.86566519706598,80.85792177145896,85.23872092662968,318.2795758483877,532.82550519149,48.09414768914166,69.90482337344105,40.72556839233407,143.84330107619036,8.196596180110909,41.16091025540018,104.94159170621354,33.00096333182785,82.3242224254021,3.3779373163065785,52.543629141866795,28.99419654032139,72.33324606684083,137.5682997013731,40.19144735592219,21.128025350655008,34.93673960407266,32.1696394267861,221.72554676945458,39.979295244345536,156.88297819878755,71.08067375602857,53.28664115416152,-0.08126194446026602,26.25847896615835,130.4202077711949,43.5625772224401,53.430457729194984,49.58849211580939,198.15365393006977,28.03935147408319,75.7220268593811,1489.282967394908,58.167688549236495,121.9141913430221,156.11481812651078,160.81467847502506,25.661659792330155,95.58547234954989,24.785689846556387,155.60139560468343,40.19738990375574,22.147655953122488,190.5000537936014,42.76375967077716,100.53575945524537,16.398581201198425,66.46546912086393,180.47116803854567,60.62128648086793,5.315348384261384,-16.0111242523692,58.634002898587404,404.00404422323624,-13.406562707906417,133.97324381621505,51.7582191011921,18.375739417017677,39.07142130187374,34.59865378186426,237.35373751254951,126.75947868413371,584.5937360323329,95.7370009359523,43.823231218451056,90.64100336069879,42.36943626640952,96.03246121207886,34.856485669016465,69.69630980451164,22.230116973509134,148.58209749269048,42.66548732456261,51.72589151634119,120.7066634725699,68.74612272444915,52.274218697748395,42.22163754194209,193.03646055267126,63.24846130941697,51.88852879197685,46.07703173721572,46.00595920783981,-10.124417817734667,65.4109707465795,301.83349594813234,117.18865775287976,136.08134186991532,18.39277150712782,102.74662524087167,22.5521824793596,567.7572128792528,423.0493849535862,172.24656952128458,81.25242788112229,797.0028782169007,58.494700910879516,76.14954706824943,42.82384690341563,139.2496668045155,39.713850873457204,17.01906580718307,746.1811031153018,58.68422789811663,51.129392528659636,656.6945247171715,32.34690689955171,40.64686651019481,55.56941207661641,-6.753914834695863,19.15028546273875,24.962974361353226,27.138146668173327,283.3770471023073,63.20132513740264,52.23532770536676,17.993898237264204,56.630954957622215,17.49516332741902,42.386642320525084,3.8425107741656603,84.24943721352074,27.65139507849331,50.72542312260772,55.56941207661641,42.16380964417574,88.80315591462936,51.2079944425176,50.20331765357753,58.147559530816025,34.1705672357854,33.354320001538746,281.4923650094677,75.7220268593811,63.768201042264366,20.75805786173362,65.17412967044652,60.53426419142728,29.970765385071182,-0.299615078273078,116.63161835794166,35.68106310521846,16.74086865257579,74.9325917088053,-2.8886948477220358,329.75094423307695,34.82447426035539,-35.30938772932788,43.330094224338445,166.81706285484472,91.2739795767121,52.51118639523869,39.755796062084755,14.570480949043606,80.33505245553008,44.67120028378035,43.969226832500176,-22.23194031204814,48.298545245114404,79.9450044717272,53.484606795743424,39.03947054930011,4.41257112173146,15.144485694409553,146.05540509182768,153.53027857054107,52.81211690998855,63.915094262812616,90.3158818331139,25.31843578131752,80.91249779467958,66.85769614368246,44.90906682482279,221.69870373393127,53.55834109182434,64.44850967823226,44.00543980989831,32.19805806991314,41.16663033680222,36.56770547288217,41.09822941598633,89.6199377012508,59.8282727884625,89.48281475255858,499.19707106722035,33.06381073570335,70.05471419509303,233.12851471428485,125.127162122544,48.37441303629353,42.98619195246283,72.24920125272381,10.909210510310317,25.661659792330155,46.30079857925474,23.441721042606908,115.52041222440636,308.90341619117044,388.57854072770783,1.20140459770694,75.7875545391451,29.63421934645244,29.51278964830729,437.16105892669987,58.99437502203911,306.2556781204861,324.6174807352229,44.187350032373345,91.78019605163641,59.2005530426586,-68.65799400542411,68.15240493265813,165.77466203094434,102.57315535067664,213.8033572015865,143.25345567952073,222.99516389616977,34.708048163136844,46.42448901668379,199.28553764955137,128.736697757204,68.4762429154598,110.64263194774989,50.26075257352728,163.53674081339412,-94.92323842161761,269.34568635669575,102.76865523789583,141.85300776902614,30.831800607095275,58.8327626109934,111.55782000275198,62.37759192748865,-6.998898169149015,43.88004274215243,14.820842211589465,65.73933146137011,205.9438109548106,262.81416765132076,61.587556498246784,99.49557225647543,214.56881815388024,38.94150964680229,27.404172235464507,-11.818980942991601,35.306744554087885,58.31181677118381,44.967226701671756,51.160055584173115,12.427709267527575,71.3822118260575,47.480332863299765,145.67572494779404,84.31398058206022,68.15240493265813,147.32056918094978,28.03935147408319,-0.4072823408690738,62.212500891659715,163.6640480370259,171.24384783777643,43.960972679939935,63.11593924824356,188.03349585889904,21.608041625022587,933.1687805227025,508.75075240043236,69.5763495849011,117.15074731094793,15.359054596399105,166.26200414213992,6.403802151292965,36.78175297741213,137.6822100827433,84.31398058206022,42.76375967077716,68.98860502343526,308.09070242975724,36.219059230376814,104.77491785338347,64.67362487341887,530.1182016169558,186.02882224276448,42.079169320056224,37.74438654133774,38.136613564156264,-37.37362865077523,62.24960423943899,57.144501825918674,45.65145705878952,138.1724408795635,1090.5152304228995,530.9951164491881,37.67164756795294,14.047033288240435,22.560709414846713,34.57824285158598,54.40069881870406,15.562188608707125,79.8876752807463,68.46162536850134,97.19888086584805,280.87748247332695,113.10597168383757,48.729752905140124,43.46107187231358,68.68461142572876,-38.36318195813257,79.83033920774432,59.982404621008925,43.20941639260917,86.19811783026472,80.46377226674818,281.6050336536114,65.45694078243312,114.17188261255387,59.01667199929552,1045.4318906150581,30.640875859585393,14.266255834839043,142.05832712765596,205.03341334217532,109.95773952025934,95.86139299088887,1887.776855509886,15.404207139737487,90.75687840014498,-4.93311795978228,108.98333140655618,21.518279132289962,168.33147652783157,44.05773674627412,123.73931655810767,44.85526502600512,27.72143100936765,9.890011547698663,176.26241425812808,68.46765285240865,60.62128648086793,121.11031620568514,36.85533862294909,666.3682973843223,56.867822311075926,202.03842510421987,514.3466100754524,38.553646669966064,326.158064434986,40.282545361051575,750.5427994396848,55.66630826855483,28.188626281101563,37.37062733462199,40.71003238427648,82.3242224254021,91.92934574643886,49.636710961305326,22.430690108135934,128.57297864720192,1011.9461560119539,59.712627211338344,21.147476524448948,793.9434802223979,109.88825132709913,75.33716763947761,37.71756389657604,259.17211961568137,12.980907030886954,45.65445467747223,21.642858882319885,177.14488017592492,55.53976749635238,83.30922262431949,47.53167323321437,53.09038890499905,44.91140237313269,14.570480949043606,1664.5328356530229,-87.33638667022035,43.228160884033436,136.8593433479103,101.00397934869099,100.84876158383099,41.40802530323343,24.340341146615415,26.44118642384074,96.11027207246576,30.19770025353867,24.86879542341883,34.76446155569782,509.58775578907444,83.15988684980837,32.73533485425551,690.5146884978205,33.262308817210005,131.8460462696828,87.87100005754512,93.88507559390047,41.93467856844897,64.94164667234486,113.19713985067543,43.873981176215835,579.7618862769519,-198.8985688523652,-23.67782153942565,36.87043758025429,35.409171971045495,44.10131752658729,36.53102926491125,50.89338213839473,25.011656434300235,147.37771014759744,44.00543980989831,51.02059708717098,229.13592704565755,28.146611623559707,49.18084981258352,14.316065664586539,19.82501761496276,45.13132880441994,-160.60725778565387,222.7557396973944,92.03416402564898,37.32383201131565,23.507809315291915,30.02496834652314,316.87560054572987,188.99761471022063,73.67993800366355,63.633323122543494,39.64754170758948,15.273989071553643,54.82887710409438,107.98682128765648,42.32052498351477,115.16278955209361,35.55903691830843,153.51732803009975,59.479626545957146,18.043844658932734,27.519661303294413,251.5524076349468,70.73918647256629,-12.358059719897284,55.583564290873035,157.00018214344192,20.607075369051955,69.34263130740116,-40.62919933577342,33.7709166775778,-11.818980942991601,135.5461946811367,161.26456149708554,45.259374439064246,48.0797251019001,52.285113892826686,29.5780819317053,-380.33393024045324,32.47697506391742,81.061989181446,266.36044390816147,127.33295787763113,60.81605163912958,126.88098339869678,27.845529179760703,135.90205221726285,23.99666118752272,24.460329764729337,168.42279312073958,189.35538413380692,164.46640722655985,156.06916964301286,26.460648341684397,107.13988091502763,1075.534383276988,25.397334975643894,3052.020759244213,109.07087064088634,55.36370872689373,70.69731939856693,48.55828299947691,17.32194530916314,17.267365963380634,57.86977078020834,34.733676949066634,837.1119295304791,72.21966801547872,52.285113892826686,47.92632184330769,-6.016522230697902,48.72539482710881,120.25130076794945,147.3596011044986,330.3625036995081,28.414635050237877,26.965075778077917,30.520561651236513,509.58775578907444,30.16501466830379,118.13634712259936,86.13430596206531,180.5717773027116,36.26850295116315,36.04909418715545,36.42552616186637,-15.121794208280818,69.69630980451164,31.91003428010132,3435.3991752851825,58.883922778982665,25.192786524427,112.21169654831826,41.25782812044724,264.8999701003907,1087.5021387875472,39.2638509118255,94.76678495215876,108.05213332664482,29.5780819317053,71.1525820435453,109.6783876969865,12.58852976311941,25.38202402893345,59.287871112579204,85.76119849234233,44.04811554370082,29.57857551321301,61.32460724597072,127.58940166836615,261.61538355655296,65.25105702917101,41.14565715564699,31.378061377160805,1.5809658282858408,86.73405508459611,48.73478515974524,183.14091726487308,113.78398115088753,17.879463293308227,158.10059893836325,42.5910362197358,51.06282951433654,143.02868865055416,52.55313569175268,27.845529179760703,163.5309185826646,44.94696302354381,62.22405060002684,111.03509182914958,-4.93311795978228,7.946887884476702,31.954005700159392,20.772105052652954,54.00619697152838,72.75417572005428,103.86447225590446,111.03509182914958,265.7728432255323,669.0716982784018,244.00267292163755,93.34609441802678,11.456800676463018,154.11843649313226,44.1681935594989,17.198974342091788,121.77490857239721,38.450773435702175,54.171181592006896,7.738315227605085,58.92381508766998,65.25105702917101,21.12234247776822,83.7118887532269,95.00662906404176,54.20147595601379,51.67565714501612,62.71648379231277,71.31030353854077,35.34377582895455,108.6793177946416,57.54764962493614,4.536983345840646,37.04964309192923,47.666225356204535,104.93886642213599,83.9696895336484,102.74662524087167,77.15054441141983,69.75680522967053,126.60222671702101,82.78377464185066,42.158972318552756,273.89826426083243,147.32056918094978,99.75375788202483,68.46162536850134,53.084536171680064,1.5809658282858408,62.84608531585523,72.13195596488974,69.61350632191662,111.48630139548123,122.84818658039855,101.20153006947555,67.19259397552005,4.248701621683708,30.576853042263238,5.007619669302542,23.80058967623367,72.27908331182539,64.14562325505572,42.44239465909739,49.01940858492845,29.25901278772068,42.67222107053683],"xaxis":"x","y":[7.8,4.52,27.02,82.38,50.55,18.65,6.35,10.65,5.74,3.59,49.58,6.87,429.27,3.3,41.51,4.03,4.26,69.59,12.61,834.96,404.51,6.53,111.96,2.15,3768.66,531.96,20.79,19.06,25.15,10.77,166.13,111.62,3.79,292.8,25.38,5.37,167.98,11.32,34.2,40.06,22.78,72.5,355.05,238.77,13.86,184.83,39.88,8.93,37.62,26.44,97.97,8.1,69.79,64.06,7.85,50.4,68.05,3995.45,202.97,13.71,27.41,63.45,129.65,59.18,61.21,15.26,17.83,9.63,47.24,62.23,6.52,12.63,144.41,1.14,5.07,44.63,49.46,3.88,126.92,106.37,27.45,9.64,26.41,4.86,60.49,196.67,115.34,9.57,5.1,65.08,9.51,40.94,22.29,65.89,298.61,72.71,45.22,8.47,103.21,293.56,66.43,83.55,9.06,96.43,46.79,18.34,202.49,7.56,9,23.14,26.71,271.58,329.29,7.79,5.53,141.95,23.5,60.67,109.16,358.88,153.73,60.59,7.4,3.75,546.2,4.46,131.4,65.65,147.69,21.09,28.27,5.11,54.09,165.86,277.98,37.82,67.56,71.03,3.82,11.46,22.39,63.14,224.36,107.56,29.7,7.08,43.86,4.04,1597.56,4.67,4.41,26.35,866.23,284.42,73.48,78.15,325.94,129.89,47.38,38.97,7.05,60.53,161.06,20.76,6.19,147.84,69.42,134.71,54.11,67.48,11.17,0,122.99,28.52,33.75,94.89,64.57,152.04,14.05,549.74,21.75,31.04,115.04,272.54,3.96,198.45,7.84,14.37,16.66,14.19,2.57,68.14,9.29,7.74,8.45,16.88,10.63,189.18,37.97,43.57,12.36,139.27,46.72,7.74,18.85,19.76,86.02,14.05,12.97,85.88,57.03,64.22,36.09,4.68,187.92,29.39,10.81,74.45,234.75,3.52,40.12,30.87,5,70.85,698.11,64.49,1936.34,5.47,4.55,5.72,48.65,288.43,2.9,5.87,36.71,5.49,17.87,38.87,3.34,47.64,8.55,3.72,13.74,21.52,40.01,30.12,25.44,20.18,352.87,135.81,161.23,39.5,6.26,94.85,548.37,16.5,141.15,22.92,23.74,7.4,64.54,79.98,35.11,9.25,27.04,234.6,48.6,26.85,3.22,10.55,42.23,66.6,27.93,5.27,324.25,9.9,47.34,107.84,4.36,164.06,6.93,5.28,60.43,10.73,4.53,238.09,174.97,4.23,406.02,7.98,7.1,4.65,2.43,6.51,3.47,3.95,317.82,47.62,49.89,253.53,1.12,33.09,15.48,61.36,31.46,35.8,7.37,134.8,18.99,10.55,66.79,31.94,691.26,87.68,3.15,15.9,45.07,62.2,3.18,16.27,139.62,4.21,11.94,7.03,47.14,40.28,82.66,4.01,8.07,309.6,4.39,3,28.19,77.76,5.41,5.47,5.94,96.39,570.98,18.1,46.03,116.06,48.62,30.27,4.76,67.57,6.61,18.54,7.96,134.01,13.35,80.08,59.84,4.81,105.32,5.91,44.51,674.09,3.98,38.08,177.51,67.24,52.73,13.99,74.33,160.28,466.95,10.01,6.86,5.07,3.72,5.81,49.55,13.87,53.61,143.34,64.23,1.68,54.51,25.37,39.58,46.26,5.18,85.03,192.12,4.73,101.27,21.2,73.14,143.82,184.4,78.06,185.82,32.77,14.49,23.42,6.53,143.45,11.14,21.87,4.67,278.97,6.85,64.39,3.77,95.86,112.06,6.74,290.57,29.92,69.57,24.53,112.82,20.67,12.03,61.18,627.87,9.21,16.67,25.12,10.8,14.83,320.35,3.49,8.07,4.6,6.63,26.64,49.53,52.38,24.88,115.24,402.03,27.14,67.96,25.37,88.11,48.15,25.24,4.87,20.3,12.19,10.01,7.11,96.18,24.76,4.76,24.96,6.31,3.42,41.06,24.1,54.98,20.76,42.74,99.45,145.88,36.48,14.17,1.61,22.02,147.16,129.42,147.36,100.02,9.47,7.82,19.73,8.98,160.3,35.14,8.22,5.51,8.37,639.39,46.23,57.71,31.72,14.86,8.09,8.7,173.23,11.73,23.17,13.29,10.92,5.91,42.93,4.38,377.36,25.21,75.67,0,4.5,6.09,7.28,272.85,171.64,15.34,30.22,15.23,3.63,43.08,9.47,4.32,31.97,11.18,11.24,13.76,6.12,32.87,46.7,30.1,104.17,91.49,65.27,211.49,6.73,49.04,41.27,13.33,19.11,0.76,12.06,56.63,18.74,443.73,44.19,24.56,290.88,119.2,26.34,47.78,157.07,95.26,1623.34,24.51,6.16,38.55,7.52,4.6,5.47,6.69,5.02,23.79,7.07,49.25,43.91,8.1,0,103.19,18.1,61.22,36.24,14.65,28.03,10.01,39.54,7.05,3.09,3.5,9.24,27.13,9.68,49.32,66.39,8.2,123.99,111.69,19.12,142.39,47.71,67.27,10.89,39.57,17.74,78.39,25.94,180.38,14.49,14.73,5.32,4.39,52.09,205.45,19.7,8.29,89.02,177.59,20.65,26.94,20.19,10780.64,13.84,8.45,2.73,8.81,332.63,16.67,29.39,6.18,6.16,67.65,31.52,22.97,8.04,509.69,13.18,26.39,277.98,61.33,4.48,72.93,5.38,6.45,109.79,139.83,54.92,160.8,35.72,6.22,51.93,93.87,6.04,3.72,11.79,18.95,3.53,50.57,0.75,150.69,398.56,113.3,5.67,4.52,61.46,5.69,4.32,154.4,173.47,91.53,13.91,25.39,44.26,42.1,0,12.28,25.48,28.94,22.62,172.46,23.79,156.67,16.3,243.38,33.49,57.41,16.89,62.83,7.56,18.21,79.98,19.69,27.7,48.74,6.17,26.31,18.12,119.81,63.68,23.17,105.97,9.68,13.16,18.42,25.2,6.38,6.83,33.04,4.05,269.71,2.62,121.9,282.34,142.36,310.19,79.42,3.41,3.96,723.47,24.54,151.12,7.61,237.39,200.5,6.7,29.84,5.55,65.46,5.91,17.36,6.71,15.68,9.87,5.53,61.19,4.65,11.54,32.99,34.03,2.52,5.09,50.05,81.92,47.84,66.48,24.91,174.98,36.05,7.36,60.02,111.95,0.89,28.41,67.53,16.13,9.67,913.07,38.68,17.46,85.59,19.84,31.58,36.43,52.41,16.27,12.97,21.69,66.05,12.29,68.64,303.14,208.53,54.05,66.53,191.97,119.92,20.46,28.88,12.89,119.03,8.12,4.52,65.77,5.65,100.42,34.92,359.11,154.39,6,328.17,4.01,27.37,4,17.87,17.8,44.61,12.59,38.15,184.51,19.1,59.05,4.94,23.83,126.55,4.9,4.13,6.1,42.4,6.61,177.1,321.71,104.91,3.28,11.29,4.75,139.72,479.7,21.55,124.4,271.32,6.98,12.46,43.28,25.59,4.8,5.63,208.81,15.43,21.85,122.54,3.01,3.5,64.81,150.88,128.69,32.04,48.79,124.92,56.3,71.01,26.11,7.51,6.16,4.27,4.03,7.53,5.32,241.49,61.39,4.47,61.98,2.45,8.33,7.02,5.6,17.41,58.82,14.97,39.05,87.27,60.34,192.2,36.61,26.67,51.65,85.54,1.55,16.42,81.24,116.95,3.07,3.92,11.44,243.66,36.96,14.12,14.24,16.72,35,45.6,24.87,11.97,51.24,9.29,31.79,18.35,9,61.86,4.19,1154.79,6.18,36.86,9.99,3.18,8.1,26.27,18.72,62.41,32.33,22.53,4.8,4.46,30.45,75.02,19.35,5.24,6.25,196.34,35.69,40.76,8.44,21.08,14.51,75.58,7.79,226.73,1.6,29.57,21.69,16.16,28.08,346.25,229.98,67.17,145.02,62.67,6.91,142.16,5.48,295.12,175.27,27.81,8.14,2.18,445.62,120.68,0,13.05,50.29,935.7,87.03,13.22,34.69,45.44,10.43,8.35,50.98,7.2,119.63,39.47,515.85,190.17,9.13,4.26,148.25,220.26,3.67,6.18,6.73,56.03,54.83,23.56,190.58,29.88,41.7,31.27,6.66,4.25,37.31,138.03,42.8,4.72,5.91,31.92,82.92,64.46,71.38,68.16,127.74,66.37,21.18,726.96,70.16,16.99,25.11,7.24,38.55,121.98,4.55,2089.28,138.85,11.08,12.99,3.42,130.32,15.25,3.06,384.27,73.48,11.87,188.58,99.59,17.51,712.39,62.64,180.14,47.02,3.99,6.49,2.94,44.54,47.26,20.08,9.79,2340.63,242.19,262.45,32.17,4.16,35.98,45.3,23.23,6.48,67.52,6.82,29.26,250.05,17.24,7.63,15.6,14.4,51.6,18.41,12.22,22.72,83.45,7.25,563.25,110.42,9.5,28.46,1280.81,4.74,2.69,92.52,48.53,59.34,51.49,452.16,5.29,21.05,22.01,39.57,103.23,256.53,55.23,54.55,2.01,4.68,16.8,25.12,48.1,53.25,68.34,44.71,467.9,28.47,9.76,128.22,9.4,19.98,22.33,391.7,41.13,16.11,8,6.37,9.48,60.79,59.11,3.65,50.55,313.3,30.19,49.15,13.43,11.72,59.74,1.39,276.9,78.36,23.12,3.5,5.83,38.67,51.67,149.39,12.18,9.58,16.61,318.97,333.3,101.21,229.55,280.16,176.88,3.04,24.31,6.46,122.7,10.14,76.48,5.86,399.54,485.76,40.56,440.31,10.75,56.8,135.56,13.56,82.87,7.12,60.7,3.16,376.57,25.14,7.86,37.34,4.27,44.15,326.65,38.59,43.8,355.8,4.33,5.85,25.76,4.59,92.3,384.85,19.55,38.25,92.92,135.11,33.78,127.84,16.04,103.07,169.75,39.75,103.37,42.57,22.46,9.88,10.82,101.43,21.3,70.79,9.58,22.17,7.12,74.52,2.64,128.87,17.4,5.75,5.08,138.96,4.84,41.65,1019.53,28.07,37.13,78.85,38.62,45.08,53.31,58.85,55.71,131.35,4.71,60.57,56.96,9.65,6.76,49.72,2.35,28.57,4.52,4.47,85.41,218.12,103.39,320.96,25.49,53.87,3321.02,216.18,12307.16,93.23,79.58,8.36,6.34,103.31,17.68,8.44,69.36,4829.86,46.6,56.68,45.32,39.1,5.95,16.76,313.83,211.52,477.87,6.01,26.52,351.53,46.94,7.34,85.22,145.35,4.13,12.28,6.46,3.92,17.84,4.21,3243.48,50.42,36.08,125.13,15.5,67.23,765.56,4.72,83.59,52.46,54.12,5.09,19.57,34.87,9.71,14.44,249.01,30.49,5.6,5.68,22.08,203.73,9.63,4.62,10.43,188.67,114.33,6.13,168.62,279.35,8.04,282.72,5.31,9.2,112.01,21.37,2.28,57.59,5.6,53.73,51.14,26.1,18.62,64.67,141.2,8.29,25.77,47.96,51.98,218.49,256.76,92.7,3.13,3.16,100.65,23.72,235.32,379.34,3.98,377.27,137.33,63.07,9.83,85.55,10.38,14.35,4.84,2.17,29.77,63.17,30.14,101.83,47.92,47.84,5.19,11.22,44.97,58.36,11.58,9.99,7.98,22.94,107.71,267,97.01,53.48,8.16,6.85,37.84,181.35,31.61,303.81,225.35,39.35,51.56,157.13,62.94,124.59,4.5,49.79,13.65,65.35,11.39,3.93,4.17,19.23,92.45],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Lasso()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predLasso"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression lasso\n","# réglage des paramètre pour la gridsearch\n","alphaslasso = np.linspace(0.1, 1, 5)\n","param_gridLasso = {'lasso__alpha': alphaslasso}\n","\n","GridLasso, \\\n","BestParametresLasso, \\\n","ScoresLasso, \\\n","TotalGHGEmissions_predLasso, \\\n","figLasso = reg_modelGrid(model=Lasso(),\n","                            scaler=RobustScaler(quantile_range=(10, 90)),\n","                            X_train=BEBNumM_train,\n","                            X_test=BEBNumM_test,\n","                            y_train=TotalGHGEmissions_train,\n","                            y_test=TotalGHGEmissions_test,\n","                            y_test_name='TotalGHGEmissions_test',\n","                            y_pred_name='TotalGHGEmissions_predLasso',\n","                            score=score,\n","                            param_grid=param_gridLasso)\n","\n","print(BestParametresLasso)\n","print(ScoresLasso)\n","figLasso.show()\n"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[454.2574524493636,453.638414266517,452.98170246826055,452.2870650056858,451.60181378321386]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[639.4517397630896,638.1606246180932,636.7879383142786,635.3514275191591,633.935130557181]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[269.06316513563763,269.1162039149408,269.1754666222425,269.22270249221253,269.26849700924674]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[228.63973934232357,227.78328580571073,226.9918237885197,226.21149417559798,225.44180855283162],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[299.94386000535667,299.97173218022954,300.0019550506649,300.0353246729756,300.07287175267726],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[736.5628802882601,733.4348744764565,730.316971887589,727.2040703888448,724.1174451254481],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[581.5140730163722,582.1128265796037,582.4430490627904,582.5601580246368,582.6815965171398],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[424.6267095945054,424.88935229058455,425.1547125517387,425.4242777663737,425.69534696797257],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Lasso en fonction de alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Lasso pour les paramètres de GridSearchCV\n","FigRMSEGRidLasso = visuRMSEGrid(Lasso(), 'Lasso', alphaslasso, 'alpha',\n","                                GridLasso, None, None)\n","FigRMSEGRidLasso.show()\n","if write_data is True:\n","    FigRMSEGRidLasso.write_image('./Figures/EmissionsGraphRMSELasso.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.4 Modèle ElasticNet"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.361e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.045e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.144e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.708e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.119e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.006e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.341e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.888e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.838e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.686e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.319e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.661e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.981e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.783e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.092e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.701e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.288e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.052e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.626e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.537e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.909e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.046e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.361e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.957e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.708e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.117e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.511e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.144e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.317e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.959e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.340e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.888e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.835e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.974e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.659e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.695e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.904e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.089e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.048e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.778e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.684e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.623e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.505e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.951e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.534e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.285e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.947e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.362e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.046e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.709e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.116e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.144e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.832e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.888e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.966e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.315e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.656e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.993e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.338e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.772e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.688e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.682e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.086e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.045e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.283e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.531e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.620e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.204e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.944e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.362e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.828e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.709e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.113e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.047e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.498e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.943e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.312e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.337e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.680e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.653e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.145e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.956e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.888e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.083e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.201e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.041e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.280e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.886e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.681e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.939e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.527e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.936e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.363e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.491e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.616e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.709e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.047e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.111e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.145e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.823e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.335e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.972e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.309e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.888e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.649e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.943e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.678e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.037e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.673e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.079e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.879e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.276e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.198e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.926e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.758e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.523e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.612e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.483e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.363e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.934e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.709e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.306e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.048e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.108e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.818e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.145e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.928e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.074e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.333e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.956e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.889e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.645e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.675e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.750e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.032e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.664e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.871e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.273e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.519e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.608e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.194e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.914e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.474e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.048e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.364e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.104e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.710e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.302e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.929e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.811e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.936e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.889e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.908e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.330e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.146e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.026e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.069e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.640e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.672e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.741e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.268e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.653e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.190e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.862e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.514e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.603e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.364e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.924e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.710e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.465e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.049e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.146e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.100e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.803e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.327e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.298e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.635e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.884e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.889e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.642e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.020e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.909e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.597e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.668e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.264e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.063e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.730e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.884e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.852e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.185e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.508e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.050e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.710e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.365e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.095e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.794e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.454e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.889e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.292e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.918e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.871e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.323e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.147e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.663e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.056e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.852e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.628e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.013e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.840e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.180e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.629e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.503e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.258e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.717e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.863e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.590e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.443e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.366e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.912e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.051e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.089e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.889e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.711e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.286e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.811e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.783e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.819e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.620e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.147e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.252e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.005e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.319e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.657e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.702e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.048e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.827e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.839e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.175e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.614e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.366e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.496e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.051e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.430e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.711e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.583e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.081e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.147e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.905e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.889e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.770e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.278e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.313e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.745e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.611e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.996e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.685e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.597e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.038e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.649e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.812e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.809e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.245e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.574e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.052e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.489e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.712e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.367e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.416e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.754e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.072e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.890e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.640e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.148e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.305e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.638e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.985e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.683e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.269e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.600e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.794e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.578e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.162e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.771e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.664e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.237e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.027e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.564e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.401e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.888e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.053e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.712e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.368e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.060e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.734e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.258e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.296e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.890e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.582e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.586e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.148e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.628e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.973e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.013e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.227e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.640e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.774e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.155e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.472e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.724e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.552e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.054e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.369e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.712e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.383e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.045e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.878e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.709e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.245e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.149e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.890e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.442e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.284e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.233e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.613e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.996e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.958e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.611e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.570e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.530e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.462e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.663e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.146e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.539e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.216e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.750e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.370e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.055e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.867e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.150e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.025e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.229e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.678e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.364e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.713e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.242e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.890e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.549e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.975e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.942e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.844e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.594e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.269e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.575e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.203e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.722e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.586e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.137e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.500e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.450e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.523e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.371e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.057e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.713e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.855e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.343e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.214e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.150e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.891e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.948e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.524e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.249e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.638e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.532e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.949e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.689e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.188e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.465e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.126e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.922e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.569e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.437e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.319e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.841e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.504e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.483e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.058e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.372e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.714e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.586e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.967e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.223e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.891e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.183e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.151e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.535e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.493e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.507e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.767e+07, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.114e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.424e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.917e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.650e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.478e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.347e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.423e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.825e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.292e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.922e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.373e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.483e+07, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.715e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.059e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.452e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.517e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.151e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.891e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.410e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.187e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.152e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.490e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.147e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.876e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.870e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.602e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.101e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.407e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.375e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.261e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.161e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.374e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.715e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.862e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.061e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.455e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.808e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.153e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.425e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.139e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.110e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.892e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.400e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.339e+06, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.836e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.428e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.824e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.317e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.086e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.905e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.324e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.121e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.545e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.422e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.388e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.788e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.062e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.299e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.375e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.227e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.778e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.056e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.892e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.342e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.333e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.153e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.071e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.795e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.246e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.068e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.089e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.474e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.214e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.367e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.382e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.542e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.765e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.187e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.377e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.659e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.064e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.242e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.122e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.154e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.049e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.070e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.892e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.218e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.744e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.975e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.666e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.160e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.048e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.029e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.387e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.342e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.333e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.739e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.378e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.065e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.718e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.142e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.710e+07, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.487e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.889e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.893e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.155e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.836e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.120e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.035e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.811e+07, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.546e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.681e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.999e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.278e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.025e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.053e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.314e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.091e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.554e+07, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.380e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.708e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.271e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.718e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.230e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.067e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.162e+07, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.156e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.313e+07, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.951e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.627e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.893e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.381e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.759e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.602e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.219e+07, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.937e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.998e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.140e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.281e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.193e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.673e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.069e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.719e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.032e+08, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.834e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.306e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.894e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.894e+06, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.574e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.157e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.152e+07, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.962e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.580e+07, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.712e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.330e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.501e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.151e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.966e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.242e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.094e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.629e+07, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.858e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.632e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.383e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.720e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.071e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.312e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.215e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.785e+06, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.158e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.369e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.822e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.894e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.796e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.889e+06, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.929e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.371e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.548e+07, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.730e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.673e+07, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.197e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.964e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.584e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.832e+07, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.385e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.073e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.721e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.197e+07, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.160e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.895e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.927e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.059e+06, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.716e+07, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.200e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.795e+07, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.175e+06, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.345e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.622e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.126e+07, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.143e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.426e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.886e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.075e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.793e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.387e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.354e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.722e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.890e+05, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.903e+07, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.895e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.528e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.161e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.690e+07, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.572e+06, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.972e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.252e+07, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.947e+05, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.939e+06, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.833e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.025e+08, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.079e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.443e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.460e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.818e+07, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.078e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.562e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.723e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.389e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.896e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.369e+07, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.162e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.771e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.818e+05, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.921e+05, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.663e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.131e+05, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.195e+07, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.378e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.249e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.199e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.001e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.391e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.557e+07, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.080e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.109e+06, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.725e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.164e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.897e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.242e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.149e+05, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.695e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.851e+05, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.828e+06, tolerance: 9.518e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.906e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.863e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.394e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.280e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.110e+07, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.246e+07, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.165e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.083e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.726e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.201e+06, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.897e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.808e+07, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.392e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.091e+05, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.830e+05, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.602e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.934e+07, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.790e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.511e+07, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.396e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.159e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.086e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.727e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.515e+06, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.167e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.898e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.034e+06, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.002e+05, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.651e+05, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.487e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.464e+07, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.645e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.010e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.089e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.331e+06, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.399e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.728e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.772e+05, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.899e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.519e+05, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.464e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.343e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.910e+06, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+05, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.401e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.823e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.092e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.730e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.900e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.022e+05, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.871e+05, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.161e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.234e+08, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.666e+05, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.588e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.404e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.731e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.096e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.900e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.172e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.928e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.602e+05, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.290e+08, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.052e+05, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.099e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.438e+07, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.733e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.407e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.901e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.174e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.626e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.587e+05, tolerance: 6.854e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.846e+07, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.142e+07, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.410e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.103e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.735e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.176e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.902e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.233e+08, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.883e+07, tolerance: 8.420e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.414e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.737e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.107e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.636e+07, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.179e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.903e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.349e+07, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.792e+06, tolerance: 9.949e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.417e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.111e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.739e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.181e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.904e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.856e+07, tolerance: 1.096e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.421e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.741e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.115e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.905e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.184e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.425e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.120e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.743e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.186e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.907e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.429e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.125e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.745e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.908e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.189e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.130e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.433e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.747e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.909e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.192e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.437e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.135e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.749e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.195e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.910e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.140e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.752e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.442e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.912e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.198e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.146e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.447e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.913e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.152e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.201e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.452e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.915e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.205e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.457e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.760e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.158e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.916e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.462e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.164e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.763e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.918e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.212e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.467e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.171e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.766e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.920e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.216e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.473e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.769e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.178e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.220e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.922e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.772e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.479e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.185e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.224e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.924e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.775e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.193e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.485e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.926e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.228e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.200e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.491e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.779e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.928e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.233e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.782e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.497e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.930e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.237e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.504e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.216e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.786e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.242e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.932e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.510e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.225e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.790e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.247e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.935e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.517e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.233e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.793e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.252e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.937e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.524e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.242e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.797e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.257e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.940e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.531e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.801e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.252e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.263e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.942e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.539e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.261e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.805e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.268e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.945e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.271e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.546e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.809e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.948e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.274e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.814e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.281e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.554e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.951e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.280e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.561e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.291e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.818e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.954e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.286e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.569e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.822e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.302e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.292e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.957e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.577e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.312e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.827e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.298e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.961e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.324e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.832e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.586e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.964e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.305e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.594e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.335e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.836e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.311e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.968e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.841e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.602e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.347e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.971e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.318e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.359e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.846e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.975e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.325e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.620e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.851e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.371e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.332e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.979e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.629e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.384e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.856e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.339e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.984e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.396e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.861e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.638e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.988e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.647e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.346e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.410e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.866e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.354e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.993e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.657e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.872e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.423e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.362e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.666e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.997e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.877e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.437e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.369e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.676e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.002e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.452e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.883e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.378e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.007e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.889e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.466e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.686e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.013e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.386e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.895e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.697e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.481e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.394e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.018e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.497e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.901e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.707e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.403e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.024e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.513e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.718e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.907e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.412e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.529e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.030e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.913e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.729e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.036e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.740e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.421e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.920e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.545e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.430e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.042e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.751e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.562e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.927e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.049e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.439e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.763e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.934e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.580e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.449e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.055e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.941e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.598e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.775e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.063e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.616e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.459e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.787e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.070e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.799e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.469e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.948e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.955e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.479e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.653e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.812e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.634e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.085e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.825e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.971e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.490e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.077e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.963e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.501e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.692e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.672e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.839e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.988e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.852e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.102e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.512e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.093e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.523e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.979e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.732e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.866e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.712e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.120e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.535e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.881e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.005e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.111e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.996e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.547e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.774e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.753e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.895e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.910e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.139e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.024e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.129e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.559e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.015e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.572e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.816e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.926e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.159e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.795e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.044e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.034e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.585e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.941e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.149e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.598e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.860e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.958e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.838e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.181e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.974e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.066e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.612e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.170e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.055e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.626e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.904e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.882e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.991e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.009e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.089e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.204e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.640e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.192e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.655e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.948e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.077e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.026e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.926e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.229e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.216e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.045e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.670e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.101e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.113e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.063e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.993e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.685e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.701e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.971e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.083e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.256e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.139e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.242e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.718e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.126e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.038e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.015e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.102e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.284e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.270e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.153e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.734e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.122e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.167e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.752e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.082e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.143e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.060e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.164e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.196e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.314e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.769e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.299e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.787e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.181e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.126e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.104e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.347e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.186e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.330e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.228e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.212e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.208e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.806e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.825e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.169e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.231e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.147e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.381e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.845e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.244e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.254e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.363e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.261e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.865e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.211e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.190e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.302e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.417e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.277e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.296e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.907e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.399e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.886e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.279e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.232e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.252e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.436e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.455e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.351e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.326e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.333e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.950e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.315e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.928e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.377e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.272e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.292e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.475e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.973e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.403e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.495e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.372e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.996e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.311e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.330e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.352e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.516e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.537e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.429e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.456e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.412e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.392e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.019e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.042e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.366e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.348e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.483e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.559e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.510e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.580e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.453e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.066e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.091e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.432e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.400e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.383e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.538e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.496e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.565e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.625e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.603e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.140e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.115e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.474e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.417e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.433e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.593e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.621e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.538e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.671e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.165e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.190e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.517e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.449e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.464e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.649e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.582e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.694e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.677e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.215e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.240e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.492e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.560e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.705e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.763e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.478e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.265e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.733e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.603e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.625e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.740e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.291e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.519e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.809e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.506e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.761e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.646e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.788e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.668e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.315e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.786e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.544e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.340e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.815e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.855e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.532e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.842e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.710e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.689e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.365e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.832e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.869e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.389e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.566e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.895e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.555e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.751e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.878e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.730e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.437e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.587e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.920e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.577e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.943e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.460e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.790e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.945e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.922e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.770e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.606e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.482e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.985e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.993e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.597e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.828e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.504e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.809e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.526e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.964e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.016e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.623e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.615e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.025e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.005e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.547e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.846e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.038e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.863e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.568e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.639e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.060e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.631e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.588e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.063e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.897e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.081e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.044e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.101e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.880e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.652e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.607e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.121e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.625e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.098e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.928e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.646e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.665e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.643e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.913e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.131e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.081e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.140e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.659e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.943e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.115e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.158e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.661e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.957e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.677e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.676e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.671e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.175e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.162e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.192e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.984e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.693e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.147e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.971e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.709e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.208e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.686e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.681e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.190e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.723e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.008e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.223e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.176e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.996e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.737e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.695e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.238e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.216e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.691e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.020e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.751e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.031e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.252e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.203e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.703e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.265e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.764e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.239e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.277e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.699e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.051e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.776e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.041e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.787e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.710e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.228e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.289e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.706e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.261e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.798e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.250e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.301e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.069e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.060e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.809e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.716e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.713e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.311e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.280e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.321e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.085e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.270e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.819e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.078e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.828e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.721e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.331e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.718e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.297e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.837e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.100e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.340e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.093e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.845e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.349e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.289e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.726e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.724e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.853e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.312e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.305e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.113e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.357e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.107e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.364e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.730e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.861e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.868e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.326e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.728e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.372e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.125e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.319e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.874e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.119e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.734e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.378e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.732e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.338e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.881e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.135e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.332e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.385e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.130e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.886e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.737e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.391e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.349e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.144e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.735e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.396e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.892e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.140e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.740e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.402e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.897e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.344e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.359e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.902e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.152e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.738e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.149e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.407e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.354e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.742e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.907e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.411e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.160e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.416e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.741e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.367e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.911e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.363e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.915e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.156e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.744e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.743e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.420e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.166e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.424e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.374e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.371e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.919e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.922e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.163e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.746e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.427e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.745e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.431e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.171e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.378e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.926e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.169e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.929e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.748e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.434e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.747e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.387e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.437e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.176e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.932e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.384e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.174e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.935e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.440e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.749e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.749e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.392e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.937e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.442e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.389e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.180e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.178e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.940e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.750e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.445e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.750e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.396e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.942e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.184e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.447e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.394e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.182e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.449e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.752e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.946e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.944e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.400e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.187e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.451e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.751e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.186e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.948e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.398e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.753e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.453e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.752e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.404e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.189e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.950e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.190e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.455e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.402e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.456e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.753e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.951e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.953e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.193e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.458e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.753e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.407e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.954e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.754e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.405e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.459e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.192e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.754e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.956e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.409e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.408e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.195e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.461e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.755e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.194e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.411e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.957e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.196e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.462e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.463e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.197e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.754e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.958e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.959e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.410e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.755e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.464e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.755e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.465e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.413e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.960e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.199e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.412e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.198e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.961e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.466e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.756e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.756e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.962e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.415e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.200e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.467e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.414e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.963e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.199e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.756e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.756e+08, tolerance: 9.518e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.468e+08, tolerance: 1.096e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.417e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.416e+08, tolerance: 6.854e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.201e+08, tolerance: 8.420e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.963e+08, tolerance: 9.949e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"name":"stdout","output_type":"stream","text":["              paramètre  ElasticNet()\n","0     elasticnet__alpha     88.048836\n","1  elasticnet__l1_ratio      1.000000\n","                    R²        RMSE         MAE\n","ElasticNet()  0.273076  454.981512  105.313051\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predEN=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[56.985684417164805,57.559276161015944,83.53643010749259,62.95034886374507,50.79572140987348,44.465521543364034,57.93170847566077,119.78076132065934,61.65603162210904,53.73609752364344,109.22391447591215,68.6931631969721,1394.8039399896754,48.947353626080634,72.93820189445155,50.91641706739726,50.11522779792985,230.9678999964766,100.35450787159277,1957.8457338907813,203.80792860626684,46.241471932642604,66.01142063503882,75.70500615787778,639.2617681977516,143.485388458331,37.25826656551508,118.01055834364381,62.421586935545626,77.73154372182478,119.35085488338414,69.0931830904795,51.301493689020774,306.6866081146371,68.8793793542945,68.26785468950732,60.23182286332836,63.08828675805797,66.6240947822786,59.70880834739195,63.952697562418805,118.13815089588324,217.50860995889548,78.83619635878057,167.09460855243623,251.72870257302043,75.30728522927558,54.261411004485055,184.9587153484092,120.31871910847966,98.02680590506262,69.2771002828967,99.89701385545499,60.168601328434946,76.99012753989294,77.06829234667026,86.60784722085981,641.6998204797321,298.4758549556618,70.33002620948515,37.578972169792564,127.18342831545158,112.80340283333189,49.0186215381423,55.25456384353793,55.29479572937919,103.0132607844739,58.26046045710652,138.71273731510468,133.97686961036183,67.6965619105614,62.35836540065222,53.901622996818915,55.38675432558779,65.76543139018081,70.37485602513685,46.41504378298634,54.39360148653492,52.56132645707858,169.56714530799493,53.36366520899861,70.4599177266298,72.7922176229704,53.99013314566969,52.29464652807364,95.98647455168431,151.84442485369266,71.40594178512576,50.926762409470726,52.04405935340521,77.93270315103109,51.77737942440028,35.651290096769806,41.03316693987807,210.03582453449417,105.4559109962648,63.57106938815311,51.177349584139165,79.12011852457462,219.5960700928307,40.17220458287506,110.66191702412414,42.880385241218306,160.6747490546234,80.05809620590233,67.98393252371326,340.3802377654678,82.29728802358173,43.924115308185904,67.66897433169882,84.91465956816897,302.97148082780967,232.17485657171443,43.33787925735609,53.2855004022213,340.59863943146325,64.76423217395968,36.558231751877116,163.86111441325136,131.7468736523033,58.974289060175764,53.54298447160538,76.96943685574601,56.986833899617416,149.12359988837073,52.34292479108316,498.42143069201893,50.81181416420998,99.20272645408006,54.239570837885516,65.19873654104532,53.14181509564536,95.52553208818871,237.79467628251246,164.825530190989,115.89666011329864,134.09986423279082,103.42362602005477,55.653434254592725,45.678225530864935,79.83049868028606,52.04405935340521,376.4981259088478,62.16755131351937,73.06119651688054,70.1070266136793,62.53423621590116,49.05080704681531,210.1093914114611,50.11522779792985,54.48556008274352,208.83346588906676,580.8381730615233,79.15805144551067,49.697965667633326,88.85968334551788,193.87984866309594,50.47041787578556,125.04883940095947,71.07718980368001,51.15665889999223,198.06051634322938,77.36485881944299,44.25976418434729,67.86093790128426,247.94000840922615,180.72402199300322,33.05805768368731,159.26663305017922,223.0996926083783,83.49504873919872,80.407538871495,501.9043625234196,40.358420740197474,43.81146602783038,188.9037391257581,62.02961341920647,77.66372425712093,80.93170286988402,75.90271713972626,55.98448520094368,119.27728800641725,82.60534932088053,76.71080330390933,52.306141352599724,184.77479815599202,72.45312029945119,46.73115145745339,82.88237459195894,40.98258971196334,53.54758240141581,52.29464652807364,42.86084403952398,55.26950711542183,58.74324308720166,79.02241251610299,74.0198648823552,120.20951827548194,65.70450882019261,98.239460158795,62.42273641799824,242.07879738338056,92.72079490382643,61.8192581303793,56.67417467250818,106.44216694060204,70.397845674189,97.45206467875886,84.37440281544346,60.56747173948975,83.8582851942227,64.80561354225355,296.44471946190436,55.46721809727032,62.38020556725176,85.71354987273118,60.339874213873465,135.71258811379914,248.48601257421473,175.40766564969357,56.80981360191586,108.11466390914595,56.60635520780434,87.50329405144105,188.0565705581864,121.44176346467717,96.12441244599721,55.524692219900686,49.22207993225382,57.823657125115666,84.39509349959039,128.0926689354641,44.66668097257035,62.3411231638631,116.8886634698989,56.67992208477121,78.90516530593702,43.58042005485626,49.07609566077267,133.58029816421225,132.56300619365464,89.66432106234312,117.88411527385699,49.1565594324552,48.01972128682639,200.58937773896582,68.3425710489268,100.93844495751738,314.0237546096307,52.44407924691262,127.94208673417252,49.91291888627093,75.32797591342252,110.43431949850785,143.485388458331,73.26120646363425,117.44961090677135,89.5539707468928,76.86713291746395,56.593710900825656,72.04735299368075,69.31848165119057,53.02916581528983,74.43137960038868,48.14501487416061,199.72036900479458,49.65658429933946,192.05906845816565,45.90927150383904,90.87702504984402,123.7338314751765,66.59420823851082,63.295193599527316,58.04895568582674,671.2840503624911,78.43272801791535,49.02551843285795,152.93873214857499,51.931410073049676,190.22449446380412,60.67437360758224,65.4125402772303,56.16380446355045,87.88262326080152,58.15815651882445,169.8671602281255,57.11327696940424,56.37990716464066,147.48673687585767,45.0126751908052,87.36305719222293,53.988983663217084,49.16460580962345,65.14356138332016,55.538486009331976,50.98078808474328,320.45970686178003,105.47200375060132,54.738446222317165,55.50974894801679,53.40044864748205,78.01431640516621,137.3000233808501,63.526239572501424,62.13191735748854,108.11466390914595,65.44357630345071,54.363714942767125,53.90277247927152,93.56911295385076,66.44477551967184,66.00222477541796,179.53545713700709,131.78365709078676,44.47356792053229,55.015471493395566,76.95564306631472,79.43047878677865,64.70560856887671,47.79672169102054,90.0919285347131,53.18894387620227,59.12257229656213,68.37935448741024,143.6853984050847,47.630046735392455,78.14650688721608,53.22572731468571,209.55534086930425,168.08201397922605,94.60709560855533,74.88657465162125,85.71354987273118,100.35450787159277,63.48140975684973,55.722403201749174,47.247269078674165,85.41123598769542,83.11227108248043,55.169502142044976,193.6430552778588,83.25020897679333,83.65597628256378,108.07213305839946,49.63934206255034,80.20408047738349,53.74414390081169,55.04190958980554,76.99012753989294,184.91733398011536,48.62779750425575,172.51441831648054,51.7359980561064,50.46696942842774,86.11356976623858,62.92850869714553,63.9067182643145,864.2821541552886,53.55677826103667,95.38414574651799,288.2098271714243,49.77727995686324,245.5686261094969,59.01911887582746,100.14760103012343,76.31997927002278,127.59494303348507,81.71679938501494,52.8395012106096,47.7449949806532,48.62779750425575,60.74564151964391,74.06354521555429,52.36361547523009,98.04289865939911,33.05805768368731,144.6107317794337,74.49115268792427,188.76924967880302,155.83887637650366,49.56002777332043,92.30813070334035,51.38655539051373,205.31604958408786,58.548980552710994,51.72450323158033,57.81561074794742,53.115376999235394,105.4559109962648,351.24859435487167,139.27943216424018,106.773217886953,266.0133210115737,65.35276718969473,126.02475000322323,175.7605567626441,50.599159910477596,52.44407924691262,41.66193384145437,54.323483056925866,58.007574317532864,131.390534091995,75.2636048960765,109.00781177482196,49.20943562527514,163.10820340679345,69.51619263303905,62.1411132171094,94.60709560855533,69.61159967660546,63.04805487221671,49.39910022995538,48.30019500526262,54.55108058254214,81.44552152619957,75.87053163105325,955.5464629625128,81.31677949150753,51.250916461106044,105.96513172276994,49.757738755168916,52.68891900931801,294.78256783543395,49.271507677715945,42.245870927378974,58.007574317532864,60.39389988914601,48.14501487416061,49.90372302665007,61.48360925421791,71.0898341106587,72.53703251849153,180.82517644883268,60.122622030330646,58.19149150995007,64.41708847327223,54.99133236189081,58.19149150995007,127.07767592981169,60.20308580201317,85.03420574324016,83.1226164245539,91.26325115392014,49.45887331749097,46.12422472247664,49.39910022995538,61.3583156668837,52.74294468459057,58.878882016609346,54.5591269597104,72.12896624781587,66.21487902915034,54.49015801255395,51.922214213428816,109.22736292327,111.69300278411305,506.44022028140876,82.91111165327412,102.17988600633348,54.3752097672932,92.53227978159882,67.10113000011071,62.397447804040866,175.87205656054698,96.99112221526326,79.15805144551067,75.25211007155042,115.94608785876076,55.12122387903546,101.15914558841801,52.24406930015891,51.15780838244484,59.82835452246313,91.95523959038985,275.1769951237606,83.87092950120137,54.07864329452047,106.2053735553649,117.18752890757685,47.68867034047543,76.96253996103036,168.2440910050437,61.84684570924188,49.00942567852144,55.25111539618011,56.0028769201854,68.55982323246963,120.48654354656034,54.43958078463922,85.09167986587053,64.87803093676783,42.09528872608739,60.03526136393248,52.929160841912974,133.2986749633234,60.06284894279506,448.51435104715955,115.35640336057313,40.98258971196334,47.478315051648266,137.3000233808501,51.158957864897445,112.5447692814952,147.48443791095247,52.71190865837016,120.30377583659575,61.042207992416635,76.92345755764171,33.966148821247224,58.72600085041255,62.351468505936566,81.28344450038192,91.77936877514091,79.06954129665989,91.8092553189087,114.724188011639,177.43880114345103,55.720104236843966,78.85918600783272,35.14551781762251,51.31873592580989,57.36386414407267,48.68182317952831,115.552964859969,54.07864329452047,84.30773283319222,307.93609554062147,82.54902468070276,81.45701635072565,295.9343492529467,167.7222259715599,51.695766170265145,121.86707197214193,38.914670779722464,273.5792145146362,163.36338851127232,49.08759048529875,48.83815279308293,112.09532164252568,75.01071875650285,57.42823516141869,51.335978162599005,37.70196679222157,54.276354276368956,69.11617273953165,87.63088660368048,139.53346778626644,39.54688612865659,68.26095779479166,89.41143492276947,72.53703251849153,49.202538730559496,62.216979058981494,108.07213305839946,98.82569620962481,57.948950712449886,53.479762936711964,85.51239044352488,59.45707169027091,47.708211542169764,54.155658618845166,78.20168204494124,56.88108151397753,102.91095684619182,98.04289865939911,135.41487215857381,48.79792090724166,104.46390763966455,446.98553938519166,76.69471054957282,245.16630725108428,138.90699984959537,62.421586935545626,103.96158380787509,47.69556723519108,133.0825722622332,61.9411032703557,55.048806484521194,79.13391231400591,53.610803936309225,54.160256548655596,147.48443791095247,49.31518801091504,90.37585070050716,234.37151753864737,112.02635269536921,77.4430236262203,68.42648326796716,237.79467628251246,76.56022110261775,51.695766170265145,54.191292574876,1577.6486652785911,95.23356354522642,66.49650223003917,49.23587372168512,84.64912912161665,79.6385351107006,119.58764826862128,57.156957302603324,65.66772538170918,41.15616156230708,108.4008850398452,86.45496605466302,53.486659831427616,82.08463376984933,285.94304777488236,73.45087106831448,208.83346588906676,1156.3805886347354,50.54973216501547,58.36621284274641,63.52509009004881,64.1067282110682,57.93170847566077,151.84442485369266,207.37017472689746,276.1988850241287,165.45544657501796,58.331728369168175,62.1411132171094,81.27309915830845,84.37210385053824,53.2705571303374,49.55428036105739,58.83520168341026,53.32918073542038,48.9278124243863,82.5697153648497,51.79117321383156,109.06298693254712,183.76670204505524,66.01142063503882,63.52509009004881,52.19234258979158,360.63067013305397,45.90122512667079,53.73264907628561,117.44961090677135,462.8978249766371,132.77795941229223,54.39934889879795,57.73514697626489,60.28240009124309,52.214182756391125,47.51739745503692,41.90332515650194,60.35251852085214,80.48110574846189,123.0108070124864,506.44022028140876,78.63388744712165,58.974289060175764,49.557728808415206,102.56611211040959,77.49475033658763,36.558231751877116,101.67641269209139,109.85612982484628,72.45312029945119,152.77550564030471,38.50775399149942,57.412142407082186,84.0134653253247,145.14983904970666,48.43353496976509,49.06919876605703,51.53828707425792,170.72352465531807,95.52553208818871,53.54758240141581,130.1939228588306,74.79231709050742,94.53237924913584,54.02461761924792,39.812416575208914,60.95024939620804,53.20618611299138,70.42083532324115,45.4736176543008,88.72174545120498,42.09184027872956,87.45271682352632,224.711267006934,140.58639371285489,83.11227108248043,103.58800201077764,48.9278124243863,53.2855004022213,606.2451837115067,60.37091024009386,90.16089748186957,76.96943685574601,192.05562001080784,373.77500197862065,61.29854257934811,68.8805288367471,57.823657125115666,111.8079510293738,48.69446748650699,64.596407735879,87.36305719222293,87.90331394494845,75.55442395658619,61.63763990286732,62.02961341920647,55.14651249299283,81.68231491143672,177.06177089899575,53.20618611299138,41.667681253717404,52.68432107950758,48.33927740865127,163.10820340679345,130.98246782131935,144.45095371852128,61.653732657203825,52.26705894921106,42.403350023386196,64.51594396419647,128.02829791811808,66.75513578187586,52.34292479108316,50.822159506283455,164.7255252176122,53.823458190041606,64.10213028125777,860.1624090451434,54.84189964305184,127.1420469471577,110.43431949850785,132.35035193992223,55.812062833052565,78.50399592997701,40.66763151994889,118.01055834364381,64.14581061445686,51.5348386269001,128.57315260065403,70.34956741117948,86.17909026603722,59.70880834739195,59.21912882258116,139.05298412107652,56.136216884687876,117.93929043158215,293.10547293707964,55.08788888790984,247.66528210305296,50.631345419150605,117.08522496929479,65.15735517275145,49.545084501436534,52.615352132351134,51.79462166118939,149.442006527743,91.02530828623038,649.1875491760172,126.94433596530922,59.04555697223743,76.71080330390933,59.3789068834936,106.2053735553649,54.31543667975761,64.84699491054742,48.2059374441488,115.0943213613786,62.35836540065222,97.17274044277525,111.94818788859192,130.95028231264632,59.58006631269991,58.20068736957092,175.7605567626441,117.89790906328828,59.376607918588384,56.310938217484214,58.39839835141942,47.91511838363911,70.43347963021982,185.4955236537769,88.20907627734204,95.87037682397096,43.2781061698205,90.12871197319654,48.79792090724166,626.4346935091047,314.0237546096307,157.2573377230213,67.01951674597558,448.51435104715955,66.78502232564365,91.99891992358894,93.2840413056041,120.31871910847966,54.48556008274352,60.0237665394064,472.7109566745472,82.06509256815501,51.12907132112964,395.57378720986907,49.068049283604424,53.44642794558635,54.3752097672932,45.317288040746185,69.57136779076421,53.01996995566897,47.630046735392455,177.26522929310727,65.4585195753346,61.90661879677747,44.979340199679584,74.88427568671602,49.83475407949362,60.214580626539245,43.48271404638463,76.4475718222622,57.18799332882372,80.9880275100618,54.3752097672932,58.83060375359983,82.50074641769325,62.94115300412421,75.01071875650285,70.52543822642842,54.9579973707652,51.34632350467247,176.57324085663757,64.10213028125777,57.65123475722454,50.926762409470726,66.38500243213625,104.992669567864,66.6677751154777,40.358420740197474,109.22391447591215,54.3752097672932,37.36286946870236,119.58764826862128,38.36751713228131,292.82729818354863,50.37501083221914,52.310739282410154,62.7089575486975,150.95127698801662,71.79101840674927,52.78892398269487,64.74698993717058,52.76593433364272,66.53558463342783,71.46226642530353,59.12257229656213,67.06089811426945,72.03240972179684,66.32982727441109,52.153260186402925,67.36091303440001,60.67437360758224,146.5705993611295,101.00741390467383,137.55750745023417,74.73484296787706,91.73798740684703,79.64773097032146,49.77268202705281,66.84019748336881,59.42603566405051,55.6948156228866,137.18392565313673,69.05065223973301,66.00222477541796,51.29459679430513,52.91306808757648,52.939506183986445,51.29459679430513,46.75758955386337,71.43352936398834,67.48850558663943,83.13181228417476,296.44471946190436,49.44622901051228,76.22572170890896,155.92738652535445,126.6362746680104,53.59930911178314,62.23881922558103,61.47211442969184,35.98349052557337,55.812062833052565,60.35251852085214,46.40124999355505,185.1713696021416,254.54148613455095,248.7572904330301,53.915416786250205,75.9073150695367,49.59336276444604,51.1670042420657,486.0472520896993,63.12507019654141,175.95252033222954,251.72870257302043,63.48715716911277,80.42018317847369,55.38675432558779,154.1204001098555,97.39114210877067,235.53824222804394,97.80265682680415,177.5778885202165,107.57325767396782,149.70293904448488,47.7898247963049,53.21997990242267,137.6264763973906,99.9154055746967,61.8192581303793,86.44691967749476,61.05370281694271,209.3484340278349,146.98096459671035,215.1809079923653,77.88097644066374,102.91095684619182,52.19234258979158,55.3269812380522,150.65930844505434,57.06269974148951,45.5494834961729,70.84614383070591,85.65377678519559,74.53023509131292,161.21960373715933,288.06384289994315,64.20328473708723,80.42363162583152,177.06177089899575,60.39389988914601,50.38420669183999,34.129375329517494,67.61035072661583,85.1640972603848,59.64903525985636,62.915864390166846,48.62779750425575,69.65987793961499,57.051204916963435,110.1975261132707,68.72190025828728,97.39114210877067,116.74267919841775,53.823458190041606,108.86872439805646,59.60650440910989,122.57055523313772,127.07767592981169,50.926762409470726,70.14955746442578,130.85257630417468,46.542636335225765,895.2135774725035,326.60254108851444,62.24341715539146,93.80360737418269,45.179350146433286,257.1036825214131,55.848846271536004,48.63009646916097,96.5221333745994,68.72190025828728,70.34956741117948,78.48560421073529,210.22204069181663,51.11067960188793,79.66382372465796,62.19743785728716,523.1812827211843,137.98511492260417,50.2784543062001,51.91531731871317,52.12222416018252,146.98096459671035,77.90741453707372,88.85968334551788,88.13780836528038,223.62845453657775,560.6647560182619,328.5589602228524,44.02986769382579,50.91641706739726,86.4997958703147,50.24511931507449,131.1135088209166,54.706260713644156,70.22312434139266,72.04275506387032,80.19833306512045,233.2726123139546,113.11951050779894,65.55737506625886,101.18903213218582,73.45087106831448,49.86119217590359,89.7229446674261,81.0190635362822,54.798219309852755,77.64418305542661,78.37410441283237,285.94304777488236,135.34935165877516,121.65786616576739,72.50024908000809,568.6352673446422,59.93870483791345,45.983987863258534,281.75088527022285,128.39268385559467,90.00916579812537,82.5697153648497,1391.135941483405,50.48536114766946,87.72744312969951,123.15794076642015,106.11686340651411,44.02986769382579,295.9343492529467,51.32218437316771,100.35450787159277,51.74289495082205,50.237072937906234,50.714108155738344,177.82962517733756,60.28240009124309,56.136216884687876,97.36585349481331,51.446328478049324,399.21994554954,67.36436148175783,121.73488149009208,371.89559816860736,66.60685254548949,197.24783224923587,49.77727995686324,427.9029811894547,65.29299410215913,50.798020374778694,67.33332545553743,63.19863707350829,75.55442395658619,72.04735299368075,52.7027127987493,53.30619108636823,104.95243768202273,740.6553163773531,55.65688270195055,58.09378550147843,363.24919116009386,103.37074982723482,71.74618859109758,55.824707140031244,160.47129066051184,38.30659456229311,67.85863893637905,48.5737718289832,133.2986749633234,53.45562380520721,100.02345692524182,72.7014085092144,82.01911327005071,67.4666654200399,52.76593433364272,1187.9200881693798,221.88583913842479,59.266257603138065,129.45250667689876,101.85688143715076,111.94818788859192,49.924413710797005,68.01611803238627,40.74694580917881,283.52108824723837,51.8578431960828,45.664431741433646,50.82905640099909,376.43605385640694,147.48673687585767,55.38215639577736,792.6475571912424,49.91521785117614,95.51173829875742,78.35801165849585,100.53957454646257,54.12577207507738,62.33882419895789,83.87092950120137,49.271507677715945,457.27110837112343,83.54792493201867,48.79792090724166,46.76563593103162,56.68337053212904,51.34517402221986,47.7449949806532,54.928110826997404,43.29879685396743,135.98846390242494,51.29459679430513,62.842297513199966,156.78145198764182,52.34292479108316,63.433131493840214,79.70290612804662,51.658982731781705,55.812062833052565,199.17436483980606,169.1487336952458,72.52898614132327,51.69346720535993,46.48171376523757,58.66852672778217,195.23853692207797,152.2789292207783,158.7114330255698,71.21512769799293,68.12187041802616,42.86084403952398,78.02236278233447,126.16038893263092,66.09993078388959,96.67846298815402,64.39869675403051,104.8558811560037,67.30458839422224,67.34137183270568,50.44512926182819,383.97321029815424,64.76423217395968,47.922015278354756,52.63144488668764,106.97782576351713,50.7221545329066,68.58396236397438,227.2159892711657,47.75419084027406,34.129375329517494,103.50753823909511,109.22736292327,71.57376622320646,69.13801290613118,59.58581372496295,81.47540806996737,336.4398119179294,53.060201841510235,75.08428563346973,172.51441831648054,105.89731225806608,64.08603752692127,137.98511492260417,44.12182629003439,99.77172026812077,53.20963456034921,57.42478671406087,136.609184426833,131.89285792378445,163.57604276500467,131.27558584673423,49.886480789860954,115.98746922705463,623.5839770266381,38.18130097495889,1910.0238154149517,89.54132643991412,67.07699086860596,73.22212406024559,61.5433823417535,152.91804146442806,70.46796410379805,82.39729299695858,54.247617215053765,955.0177010343133,84.22152164924667,59.58581372496295,63.57106938815311,38.389357298880846,65.55507610135365,103.97422811485376,227.80912221671116,210.20020052521704,150.17767529741178,54.90282221304004,46.32883259904078,376.43605385640694,51.84060095929369,90.39998983201193,77.44187414376769,162.19896278678092,55.06030130904726,51.02101997058455,60.173199258245376,50.46237149861731,64.84699491054742,62.6169989524889,2002.3376017014066,109.4124295981398,49.409445572028844,99.04524735807284,61.615799736267775,187.43814899868357,606.2451837115067,48.79332297743123,78.59020711392257,129.9962118769821,81.47540806996737,61.69166557813987,82.53752985617669,91.51498781104118,49.317486975820245,67.20343393839278,79.67761751408926,48.7427457495165,55.63964046516144,70.29554173590692,148.08561723366617,213.21414351595394,74.34976634625355,49.52899174710002,48.62779750425575,33.569577375097644,83.25020897679333,61.63649042041471,192.25218151020374,119.59684412824214,75.76707821031857,157.73552242330604,68.5092460045549,70.71165438375084,125.00286010285518,59.72720006663367,44.12182629003439,122.19352498868247,57.583415292520705,65.9459001352402,121.60269100804221,123.15794076642015,40.581420336003326,52.78432605288444,69.31043527402231,68.6506323462256,62.536535180806375,90.7183964713842,121.60269100804221,235.46467535107706,471.6040050726862,152.87321164877636,73.39914435794715,43.2781061698205,194.254579942646,55.30399158900005,224.73310717353354,102.17988600633348,56.986833899617416,62.989431267133725,165.24853973354857,55.24077005410664,74.34976634625355,50.43363443730212,72.2404660457188,109.53312525566358,67.82645342770604,55.34077502748349,65.56312247852189,69.62194501867894,78.30973339548635,85.41123598769542,74.13251416271073,51.0750456458571,58.04895568582674,53.22572731468571,91.08278240886077,68.45292136437712,90.12871197319654,75.62914031600567,80.57306434467048,118.4071297897934,83.5214868356087,78.83619635878057,168.66020365288762,116.74267919841775,87.67801538423738,72.04275506387032,82.37315386545382,33.569577375097644,57.30983846880012,74.89347154633688,64.80331457734835,83.01801352136663,92.8828719296441,77.54302859959715,63.526239572501424,39.79287537351459,52.05785314283651,43.1102817317398,109.16644035328179,120.61643506370498,65.84244671450551,54.39360148653492,53.93955591775496,48.897925880618516,56.191392042413035],"xaxis":"x","y":[7.8,4.52,27.02,82.38,50.55,18.65,6.35,10.65,5.74,3.59,49.58,6.87,429.27,3.3,41.51,4.03,4.26,69.59,12.61,834.96,404.51,6.53,111.96,2.15,3768.66,531.96,20.79,19.06,25.15,10.77,166.13,111.62,3.79,292.8,25.38,5.37,167.98,11.32,34.2,40.06,22.78,72.5,355.05,238.77,13.86,184.83,39.88,8.93,37.62,26.44,97.97,8.1,69.79,64.06,7.85,50.4,68.05,3995.45,202.97,13.71,27.41,63.45,129.65,59.18,61.21,15.26,17.83,9.63,47.24,62.23,6.52,12.63,144.41,1.14,5.07,44.63,49.46,3.88,126.92,106.37,27.45,9.64,26.41,4.86,60.49,196.67,115.34,9.57,5.1,65.08,9.51,40.94,22.29,65.89,298.61,72.71,45.22,8.47,103.21,293.56,66.43,83.55,9.06,96.43,46.79,18.34,202.49,7.56,9,23.14,26.71,271.58,329.29,7.79,5.53,141.95,23.5,60.67,109.16,358.88,153.73,60.59,7.4,3.75,546.2,4.46,131.4,65.65,147.69,21.09,28.27,5.11,54.09,165.86,277.98,37.82,67.56,71.03,3.82,11.46,22.39,63.14,224.36,107.56,29.7,7.08,43.86,4.04,1597.56,4.67,4.41,26.35,866.23,284.42,73.48,78.15,325.94,129.89,47.38,38.97,7.05,60.53,161.06,20.76,6.19,147.84,69.42,134.71,54.11,67.48,11.17,0,122.99,28.52,33.75,94.89,64.57,152.04,14.05,549.74,21.75,31.04,115.04,272.54,3.96,198.45,7.84,14.37,16.66,14.19,2.57,68.14,9.29,7.74,8.45,16.88,10.63,189.18,37.97,43.57,12.36,139.27,46.72,7.74,18.85,19.76,86.02,14.05,12.97,85.88,57.03,64.22,36.09,4.68,187.92,29.39,10.81,74.45,234.75,3.52,40.12,30.87,5,70.85,698.11,64.49,1936.34,5.47,4.55,5.72,48.65,288.43,2.9,5.87,36.71,5.49,17.87,38.87,3.34,47.64,8.55,3.72,13.74,21.52,40.01,30.12,25.44,20.18,352.87,135.81,161.23,39.5,6.26,94.85,548.37,16.5,141.15,22.92,23.74,7.4,64.54,79.98,35.11,9.25,27.04,234.6,48.6,26.85,3.22,10.55,42.23,66.6,27.93,5.27,324.25,9.9,47.34,107.84,4.36,164.06,6.93,5.28,60.43,10.73,4.53,238.09,174.97,4.23,406.02,7.98,7.1,4.65,2.43,6.51,3.47,3.95,317.82,47.62,49.89,253.53,1.12,33.09,15.48,61.36,31.46,35.8,7.37,134.8,18.99,10.55,66.79,31.94,691.26,87.68,3.15,15.9,45.07,62.2,3.18,16.27,139.62,4.21,11.94,7.03,47.14,40.28,82.66,4.01,8.07,309.6,4.39,3,28.19,77.76,5.41,5.47,5.94,96.39,570.98,18.1,46.03,116.06,48.62,30.27,4.76,67.57,6.61,18.54,7.96,134.01,13.35,80.08,59.84,4.81,105.32,5.91,44.51,674.09,3.98,38.08,177.51,67.24,52.73,13.99,74.33,160.28,466.95,10.01,6.86,5.07,3.72,5.81,49.55,13.87,53.61,143.34,64.23,1.68,54.51,25.37,39.58,46.26,5.18,85.03,192.12,4.73,101.27,21.2,73.14,143.82,184.4,78.06,185.82,32.77,14.49,23.42,6.53,143.45,11.14,21.87,4.67,278.97,6.85,64.39,3.77,95.86,112.06,6.74,290.57,29.92,69.57,24.53,112.82,20.67,12.03,61.18,627.87,9.21,16.67,25.12,10.8,14.83,320.35,3.49,8.07,4.6,6.63,26.64,49.53,52.38,24.88,115.24,402.03,27.14,67.96,25.37,88.11,48.15,25.24,4.87,20.3,12.19,10.01,7.11,96.18,24.76,4.76,24.96,6.31,3.42,41.06,24.1,54.98,20.76,42.74,99.45,145.88,36.48,14.17,1.61,22.02,147.16,129.42,147.36,100.02,9.47,7.82,19.73,8.98,160.3,35.14,8.22,5.51,8.37,639.39,46.23,57.71,31.72,14.86,8.09,8.7,173.23,11.73,23.17,13.29,10.92,5.91,42.93,4.38,377.36,25.21,75.67,0,4.5,6.09,7.28,272.85,171.64,15.34,30.22,15.23,3.63,43.08,9.47,4.32,31.97,11.18,11.24,13.76,6.12,32.87,46.7,30.1,104.17,91.49,65.27,211.49,6.73,49.04,41.27,13.33,19.11,0.76,12.06,56.63,18.74,443.73,44.19,24.56,290.88,119.2,26.34,47.78,157.07,95.26,1623.34,24.51,6.16,38.55,7.52,4.6,5.47,6.69,5.02,23.79,7.07,49.25,43.91,8.1,0,103.19,18.1,61.22,36.24,14.65,28.03,10.01,39.54,7.05,3.09,3.5,9.24,27.13,9.68,49.32,66.39,8.2,123.99,111.69,19.12,142.39,47.71,67.27,10.89,39.57,17.74,78.39,25.94,180.38,14.49,14.73,5.32,4.39,52.09,205.45,19.7,8.29,89.02,177.59,20.65,26.94,20.19,10780.64,13.84,8.45,2.73,8.81,332.63,16.67,29.39,6.18,6.16,67.65,31.52,22.97,8.04,509.69,13.18,26.39,277.98,61.33,4.48,72.93,5.38,6.45,109.79,139.83,54.92,160.8,35.72,6.22,51.93,93.87,6.04,3.72,11.79,18.95,3.53,50.57,0.75,150.69,398.56,113.3,5.67,4.52,61.46,5.69,4.32,154.4,173.47,91.53,13.91,25.39,44.26,42.1,0,12.28,25.48,28.94,22.62,172.46,23.79,156.67,16.3,243.38,33.49,57.41,16.89,62.83,7.56,18.21,79.98,19.69,27.7,48.74,6.17,26.31,18.12,119.81,63.68,23.17,105.97,9.68,13.16,18.42,25.2,6.38,6.83,33.04,4.05,269.71,2.62,121.9,282.34,142.36,310.19,79.42,3.41,3.96,723.47,24.54,151.12,7.61,237.39,200.5,6.7,29.84,5.55,65.46,5.91,17.36,6.71,15.68,9.87,5.53,61.19,4.65,11.54,32.99,34.03,2.52,5.09,50.05,81.92,47.84,66.48,24.91,174.98,36.05,7.36,60.02,111.95,0.89,28.41,67.53,16.13,9.67,913.07,38.68,17.46,85.59,19.84,31.58,36.43,52.41,16.27,12.97,21.69,66.05,12.29,68.64,303.14,208.53,54.05,66.53,191.97,119.92,20.46,28.88,12.89,119.03,8.12,4.52,65.77,5.65,100.42,34.92,359.11,154.39,6,328.17,4.01,27.37,4,17.87,17.8,44.61,12.59,38.15,184.51,19.1,59.05,4.94,23.83,126.55,4.9,4.13,6.1,42.4,6.61,177.1,321.71,104.91,3.28,11.29,4.75,139.72,479.7,21.55,124.4,271.32,6.98,12.46,43.28,25.59,4.8,5.63,208.81,15.43,21.85,122.54,3.01,3.5,64.81,150.88,128.69,32.04,48.79,124.92,56.3,71.01,26.11,7.51,6.16,4.27,4.03,7.53,5.32,241.49,61.39,4.47,61.98,2.45,8.33,7.02,5.6,17.41,58.82,14.97,39.05,87.27,60.34,192.2,36.61,26.67,51.65,85.54,1.55,16.42,81.24,116.95,3.07,3.92,11.44,243.66,36.96,14.12,14.24,16.72,35,45.6,24.87,11.97,51.24,9.29,31.79,18.35,9,61.86,4.19,1154.79,6.18,36.86,9.99,3.18,8.1,26.27,18.72,62.41,32.33,22.53,4.8,4.46,30.45,75.02,19.35,5.24,6.25,196.34,35.69,40.76,8.44,21.08,14.51,75.58,7.79,226.73,1.6,29.57,21.69,16.16,28.08,346.25,229.98,67.17,145.02,62.67,6.91,142.16,5.48,295.12,175.27,27.81,8.14,2.18,445.62,120.68,0,13.05,50.29,935.7,87.03,13.22,34.69,45.44,10.43,8.35,50.98,7.2,119.63,39.47,515.85,190.17,9.13,4.26,148.25,220.26,3.67,6.18,6.73,56.03,54.83,23.56,190.58,29.88,41.7,31.27,6.66,4.25,37.31,138.03,42.8,4.72,5.91,31.92,82.92,64.46,71.38,68.16,127.74,66.37,21.18,726.96,70.16,16.99,25.11,7.24,38.55,121.98,4.55,2089.28,138.85,11.08,12.99,3.42,130.32,15.25,3.06,384.27,73.48,11.87,188.58,99.59,17.51,712.39,62.64,180.14,47.02,3.99,6.49,2.94,44.54,47.26,20.08,9.79,2340.63,242.19,262.45,32.17,4.16,35.98,45.3,23.23,6.48,67.52,6.82,29.26,250.05,17.24,7.63,15.6,14.4,51.6,18.41,12.22,22.72,83.45,7.25,563.25,110.42,9.5,28.46,1280.81,4.74,2.69,92.52,48.53,59.34,51.49,452.16,5.29,21.05,22.01,39.57,103.23,256.53,55.23,54.55,2.01,4.68,16.8,25.12,48.1,53.25,68.34,44.71,467.9,28.47,9.76,128.22,9.4,19.98,22.33,391.7,41.13,16.11,8,6.37,9.48,60.79,59.11,3.65,50.55,313.3,30.19,49.15,13.43,11.72,59.74,1.39,276.9,78.36,23.12,3.5,5.83,38.67,51.67,149.39,12.18,9.58,16.61,318.97,333.3,101.21,229.55,280.16,176.88,3.04,24.31,6.46,122.7,10.14,76.48,5.86,399.54,485.76,40.56,440.31,10.75,56.8,135.56,13.56,82.87,7.12,60.7,3.16,376.57,25.14,7.86,37.34,4.27,44.15,326.65,38.59,43.8,355.8,4.33,5.85,25.76,4.59,92.3,384.85,19.55,38.25,92.92,135.11,33.78,127.84,16.04,103.07,169.75,39.75,103.37,42.57,22.46,9.88,10.82,101.43,21.3,70.79,9.58,22.17,7.12,74.52,2.64,128.87,17.4,5.75,5.08,138.96,4.84,41.65,1019.53,28.07,37.13,78.85,38.62,45.08,53.31,58.85,55.71,131.35,4.71,60.57,56.96,9.65,6.76,49.72,2.35,28.57,4.52,4.47,85.41,218.12,103.39,320.96,25.49,53.87,3321.02,216.18,12307.16,93.23,79.58,8.36,6.34,103.31,17.68,8.44,69.36,4829.86,46.6,56.68,45.32,39.1,5.95,16.76,313.83,211.52,477.87,6.01,26.52,351.53,46.94,7.34,85.22,145.35,4.13,12.28,6.46,3.92,17.84,4.21,3243.48,50.42,36.08,125.13,15.5,67.23,765.56,4.72,83.59,52.46,54.12,5.09,19.57,34.87,9.71,14.44,249.01,30.49,5.6,5.68,22.08,203.73,9.63,4.62,10.43,188.67,114.33,6.13,168.62,279.35,8.04,282.72,5.31,9.2,112.01,21.37,2.28,57.59,5.6,53.73,51.14,26.1,18.62,64.67,141.2,8.29,25.77,47.96,51.98,218.49,256.76,92.7,3.13,3.16,100.65,23.72,235.32,379.34,3.98,377.27,137.33,63.07,9.83,85.55,10.38,14.35,4.84,2.17,29.77,63.17,30.14,101.83,47.92,47.84,5.19,11.22,44.97,58.36,11.58,9.99,7.98,22.94,107.71,267,97.01,53.48,8.16,6.85,37.84,181.35,31.61,303.81,225.35,39.35,51.56,157.13,62.94,124.59,4.5,49.79,13.65,65.35,11.39,3.93,4.17,19.23,92.45],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle ElasticNet()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predEN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression elasticnet\n","# réglage des paramètre pour la gridsearch\n","alphasEN = np.logspace(-3, 3, 200)\n","l1ratioEN = np.linspace(0, 1, 6)\n","param_gridEN = {\n","    'elasticnet__alpha': alphasEN,\n","    'elasticnet__l1_ratio': l1ratioEN\n","}\n","\n","GridEN, \\\n","BestParametresEN, \\\n","ScoresEN, \\\n","TotalGHGEmissions_predEN, \\\n","figEN = reg_modelGrid(model=ElasticNet(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=TotalGHGEmissions_train,\n","                         y_test=TotalGHGEmissions_test,\n","                         y_test_name='TotalGHGEmissions_test',\n","                         y_pred_name='TotalGHGEmissions_predEN',\n","                         score=score,\n","                         param_grid=param_gridEN)\n","\n","print(BestParametresEN)\n","print(ScoresEN)\n","figEN.show()\n"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"y":[454.5278720755776,454.52767763570046,454.5274585486148,454.52725122340587,454.52698614835845,454.52672517598666,454.5264697809347,454.5261440726678,454.525850716151,454.52547672413993,454.52513564602,454.5247032415621,454.5242866291963,454.52381658812635,454.52337170287694,454.522804259802,454.5222865250037,454.5216302639009,454.52099798062017,454.520284672454,454.5196107228705,454.51874829171265,454.51790863365807,454.5169149831114,454.5160076306732,454.51493872237626,454.5138373176913,454.5127263700024,454.51132133720074,454.50996414252484,454.5084353009348,454.50698932306057,454.5051417348909,454.50345900981154,454.5013250084503,454.4992654584709,454.4969478895772,454.4947531971382,454.4919504392022,454.48942980514477,454.4859847665515,454.4830363971593,454.4795578567353,454.47597432672046,454.4719344606757,454.46811303985214,454.4620263286209,454.4544423422637,454.44990754204036,454.4468808462983,454.44116932248215,454.4345068435805,454.42744796898705,454.41998213998795,454.41238930777916,454.40897191529956,454.3967912538913,454.38699580939544,454.3768301635908,454.36557208852093,454.353654244985,454.3410024057777,454.3272530189458,454.31273209687487,454.29717879269793,454.28065397551137,454.26353726206406,454.2447388015553,454.2251457941967,454.2036860739119,454.18592985459344,454.1508764037602,454.11533805288957,454.0862149915317,454.0551768392471,454.02181935631563,453.9860769457317,453.9480324044671,453.9070536941802,453.86373248759674,453.8168300158616,453.76690429755564,453.71311550693974,453.65672063981435,453.5958285494827,453.53007051816365,453.4597032017317,453.3791085218842,453.29190936262165,453.1912616140553,453.08363620734036,452.96843445252114,452.8450176710177,452.71305320870914,452.57172677360893,452.4204527318831,452.25822761186794,452.08460061680427,451.89978521866414,451.7016639207135,451.5000049391074,451.285688326151,451.0563000248696,450.8114016737383,450.5496822391152,450.2701014415543,449.97114789286223,449.651102957435,449.31050344707745,448.9470546321383,448.55900861296413,448.14562316405033,447.70656633201395,447.2404940799253,446.7414231023352,446.2162046501324,445.65773788854267,445.0675813869149,444.441316029872,443.77590373540636,443.07001423871736,442.3220931769403,441.5307538436426,440.691324485074,439.78647045706674,438.78915588061375,437.58885499297577,436.306020984516,434.95046248176084,433.5847984567742,432.1700152838195,430.6649839497853,429.06559297675557,427.36691424980893,425.5238206510721,423.5521685977834,421.41347304720796,419.13532745699996,416.72648846962386,414.05652864028434,411.2242199879611,408.2282016959501,404.96814981329237,401.4473719232807,397.7506797149173,395.3019613086781,392.72066771161565,390.0068310325172,387.17009416388134,384.2207869913007,381.8248116511928,380.03829013179904,378.10061616588155,376.04000235751903,373.88526745016395,371.6743850490396,369.2632746285934,366.80562163919166,364.3659416569463,362.04276009667274,359.9399662842253,358.42951706271606,357.7131827570067,357.18975019128027,356.92721117710096,357.00552238775543,357.51456415020857,358.5497261457366,360.20534651680293,362.5673404758869,365.70721468629233,369.6794812408451,374.52313748331306,380.26623853078274,386.9317267990738,394.54288397314986,403.1275348693199,412.720850123095,417.0296542525343,419.51602817064276,422.3180438507669,425.47466111770007,429.02853345486244,433.02602578714004,437.3075372952705,440.5023414371743,442.2282991828532,442.2282991828532,442.2282991828532,442.2282991828532,442.2282991828532,442.2282991828532,442.2282991828532,442.2282991828532,442.2282991828532,442.2282991828532,442.2282991828532,442.2282991828532,442.2282991828532,442.2282991828532]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"y":[640.0449876483245,640.0445396829607,640.0440946773192,640.0436142910011,640.0430694685282,640.0424833493315,640.0418906185843,640.0412182401499,640.0405368503498,640.0397652163265,640.0389815076144,640.0381027928852,640.0371412498454,640.0361845763433,640.0351523241418,640.0339827488731,640.0327950462491,640.0314638092603,640.0300057563243,640.0285565933465,640.0269934662176,640.0252195201306,640.0233129653157,640.0212700586437,640.0191926601208,640.0170005157659,640.0144632692972,640.0119114173228,640.0090526294761,640.0059211192133,640.0028072239429,639.9994488284291,639.9956398112652,639.9917757809064,639.9874436506925,639.9826956061236,639.9779811333276,639.9728924394033,639.967119974673,639.9612706683454,639.9542643824554,639.9475072818327,639.9403716544086,639.932113814656,639.923907622575,639.9150490387457,639.9008964023732,639.8853023810884,639.8744128445815,639.8696902988087,639.856631407608,639.8428991728264,639.8277825116801,639.8118636843667,639.7944761813127,639.7749541917294,639.7563248327826,639.7348935464179,639.7124998514073,639.6878717629224,639.6618772349622,639.6343579865135,639.6040453128498,639.5723245971585,639.5387625739515,639.5029158336276,639.4646559222302,639.4238036848935,639.3802325749147,639.3333731154185,639.2822586070656,639.2304200197486,639.1752561825815,639.1134853633133,639.0476157825481,638.9769083524654,638.901118082278,638.8203870746077,638.733422515619,638.6413166819304,638.541896393911,638.4356754430369,638.3216113544621,638.2005045927388,638.0697534905582,637.9301683672917,637.7805363026671,637.6114766945989,637.4290016119603,637.2209970701451,636.9985858477794,636.7607448979771,636.5055417873733,636.2326566792027,635.9403274011878,635.6273484425595,635.2919920655072,634.932887309259,634.550178086334,634.1399396569094,633.7265531189471,633.2875921357745,632.8178696699774,632.3157186877279,631.7787571470694,631.2046777766238,630.5904286475882,629.9340852973444,629.2331297708912,628.4844665945978,627.684847043763,626.8315553465427,625.9202714503924,624.9481144602776,623.8872612452772,622.7815597609545,621.6042501080095,620.357995035826,619.0335837916665,617.623630634755,616.125916070865,614.5345338611187,612.848382979221,611.0594264391975,609.160429242844,607.1244189244092,604.6317915345937,601.9660008599326,599.151353631127,596.1516713728987,592.9772783365216,589.6446085701226,586.1537247037493,582.503894336378,578.6161473021111,574.5418127870431,570.267505557478,565.8452788324342,561.3111483448964,556.5427308955964,551.6952466741761,546.8416019479573,541.9663997076393,537.0025788728257,532.1961784283365,529.3674035613315,526.6879724802585,524.0956247369036,521.6383051875919,519.3634716051492,517.7967116439163,516.7388412378168,515.6867947590396,514.6465165247009,513.6915460730971,512.9148885703909,512.3798308224137,512.0393712786781,511.9151951965675,512.027780832609,512.3730760961629,512.9586493011465,513.7397311010873,514.6279306528554,515.6250570591811,516.7334643695591,517.9588751698113,519.3149635410764,520.8297000057879,522.5529028378302,524.5640011017897,526.9790730679554,529.9567933405359,533.7034924572852,538.4774837189802,544.5919443713221,552.4142879410684,562.3588631492457,567.5819682228228,570.8517950138004,574.4084351924313,578.2803357252678,582.4997877561605,587.1036919685039,591.7903362895465,594.0677331298818,593.7109261452875,593.7109261452875,593.7109261452875,593.7109261452875,593.7109261452875,593.7109261452875,593.7109261452875,593.7109261452875,593.7109261452875,593.7109261452875,593.7109261452875,593.7109261452875,593.7109261452875,593.7109261452875]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"y":[269.01075650283065,269.0108155884402,269.0108224199103,269.0108881558107,269.0109028281887,269.0109670026418,269.0110489432851,269.01106990518576,269.0111645819522,269.0111882319534,269.01128978442557,269.01130369023906,269.0114320085472,269.0114485999094,269.0115910816121,269.0116257707309,269.0117780037583,269.0117967185414,269.011990204916,269.01201275156154,269.01222797952335,269.0122770632947,269.0125043020005,269.0125599075791,269.01282260122554,269.01287692898666,269.01321136608544,269.01354132268193,269.0135900449254,269.01400716583635,269.0140633779267,269.01452981769205,269.0146436585167,269.01514223871663,269.0152063662081,269.01583531081826,269.0159146458268,269.01661395487315,269.0167809037314,269.01758894194415,269.01770515064754,269.0185655124859,269.018744059062,269.01983483878496,269.01996129877637,269.0211770409586,269.0231562548686,269.023582303439,269.0254022394992,269.02407139378795,269.0257072373563,269.0261145143346,269.027113426294,269.0281005956092,269.03030243424564,269.04298963886976,269.03725767500003,269.03909807237306,269.0411604757743,269.04327241411943,269.0454312550079,269.04764682504197,269.0504607250417,269.0531395965913,269.0555950114443,269.0583921173951,269.0624186018979,269.06567391821704,269.0700590134787,269.0739990324053,269.08960110212126,269.0713327877718,269.05541992319763,269.05894461975004,269.06273789594616,269.0667303601658,269.07103580918545,269.07567773432646,269.08068487274136,269.086148293263,269.0917636378123,269.09813315207447,269.1046196594174,269.11293668688995,269.1219036084072,269.1299726690356,269.13887010079634,269.1467403491694,269.154817113283,269.1615261579655,269.16868656690133,269.17612400706514,269.184493554662,269.1934497382156,269.20312614603006,269.2135570212066,269.2244631582287,269.2363139243496,269.2493923509943,269.26338818451757,269.2734567592677,269.28378451652753,269.2947303797617,269.3070846597487,269.32060733116106,269.33552510648474,269.3518671381363,269.3681206175257,269.3878771232637,269.40964266967876,269.4331701821652,269.45969098155797,269.49286121363554,269.53287369957314,269.5955849593932,269.65084953931034,269.7112256690758,269.77716773800387,269.84904826807747,269.92817683605773,270.0141124065698,270.1096524927619,270.21312470806413,270.3232225309505,270.4125116712895,270.4538928368183,270.54591845135786,270.64604110909943,270.7495713323947,271.0179255406497,271.3627522311174,271.68535932944803,271.9774612497619,272.2299341632398,272.4314940000331,272.5625244085237,272.5594405369379,272.4253760815657,272.1418285943513,271.57032638497236,270.75319330174597,269.61480144394284,267.9698999189455,265.8921649737357,263.3051810014981,261.2365190560247,258.7533629429728,255.91803732813082,252.70188314017076,249.07810237745218,245.85291165846928,243.33773902578125,240.51443757272355,237.43348819033713,234.07898882723075,230.4338815276883,226.14671843477313,221.5718719997052,216.8166881173251,212.0577393607365,207.50685647228775,203.90038482428562,201.68663441292603,199.75156972970507,198.2293652950208,197.27758040595177,197.07025313060578,197.78448875039686,199.58099302781795,202.58177811394364,206.850428270795,212.37988941373484,219.0894816260902,226.82898460428035,235.38596987916725,244.49382357497754,253.8407817975714,263.0828370969443,266.4773402822459,268.18026132748514,270.22765250910254,272.66898651013236,275.5572791535644,278.9483596057762,282.82473830099445,286.93694974446674,290.7456722204189,290.7456722204189,290.7456722204189,290.7456722204189,290.7456722204189,290.7456722204189,290.7456722204189,290.7456722204189,290.7456722204189,290.7456722204189,290.7456722204189,290.7456722204189,290.7456722204189,290.7456722204189]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[228.94332426707666,228.94314175209047,228.94283215743022,228.94262217477487,228.94226586008446,228.94202427599922,228.94176865839293,228.9413362497336,228.94104216681512,228.94054451053807,228.9402061742216,228.9396334254583,228.93924417682769,228.93858500556018,228.93813718390302,228.93737855103203,228.93686334320202,228.93599024288773,228.9353975100382,228.9343926732509,228.93371075155298,228.93255430500483,228.93176977646053,228.93043885022692,228.9295362790959,228.92800455524326,228.9269661849486,228.92586751670942,228.92400878295132,228.9227448371089,228.92060572760974,228.9191516502233,228.916689891575,228.91501709466326,228.91218404639687,228.91025964809728,228.90699934594403,228.90478552712048,228.90103358324416,228.89848684820052,228.89416918727258,228.89123951444344,228.88627092137574,228.88290078784883,228.8771832395465,228.87330650096285,228.8692048115611,228.86226785534075,228.85754980781996,228.8495677468614,228.84414087418406,228.83495654966677,228.82871456357304,228.81814728123322,228.81096802308875,228.82667203808188,228.79973025793578,228.78901565692192,228.77759802705438,228.7652896139519,228.75185756400666,228.73702228980616,228.72236468821043,228.7057391956352,228.68667217493186,228.66676969281775,228.6479762963399,228.6249217520441,228.60132381455585,228.5753449010051,228.5731029897015,228.49183236911574,228.4131492466372,228.37500365938772,228.3341590173106,228.2904257143788,228.24355324242953,228.19335641044563,228.13951148293512,228.08195259112196,228.0201364424478,227.95426166819826,227.883497492619,227.8077866003948,227.72675951291905,227.64005951097695,227.5472958319039,227.4480524194392,227.34188651935827,227.22832723817004,227.1068565605654,226.97651247892964,226.8376084182714,226.6891416874253,226.5304373708261,226.36084654819462,226.1790626875883,225.98549089933678,225.77874115004605,225.55801627067655,225.32248639010282,225.0712867361824,224.80262598155187,224.51703347561119,224.21269068210495,223.8885939867834,223.54359883922638,223.17564268691424,222.7854104778774,222.37079801478689,221.92964495187113,221.46272703870187,220.97643908241702,220.46910867558094,219.95472433875904,219.38818754456733,218.7900660737189,218.15953783694513,217.49595686032762,216.79890521946118,216.0682554175977,215.3066664228354,214.5111729920863,213.68355380769017,212.82525985538553,211.94229086157455,211.03442938772847,210.1102845531635,209.1802123862781,208.5910271582632,208.168262234215,207.7201121136782,207.25299681232582,206.76990679916702,206.27633347160804,205.7672807937592,205.2488227563059,204.72510827767576,204.20105589816342,203.68402762829265,203.17672441643933,202.61650700678896,201.79285577799862,200.94705849995051,200.08333942671396,199.2038441240758,198.32146936121043,197.44204147581436,196.57659117694067,195.73753957077741,195.10707967010305,195.2073965458572,195.33474990001872,195.49398448395058,195.6907151982066,195.91273681615897,195.31499168822216,194.68828634978436,194.03278211100076,193.34901999118017,192.6380097519787,191.90133570385854,191.14128212566735,190.36098152680287,189.564589380752,188.7574893426441,187.94653329153348,187.14032074901255,186.34952222953603,185.58725074762123,184.8694848704446,184.21554513711018,183.64862309229682,183.19635828887542,182.89145308901112,182.77230770141134,182.88364860627897,183.27711271007044,184.0117382489797,185.1543035087746,186.77944873029543,188.96951882148633,191.81407852549532,195.40908015326607,199.85570660300206,205.258964365893,211.34051666633286,211.34051666633286,211.34051666633286,211.34051666633286,211.34051666633286,211.34051666633286,211.34051666633286,211.34051666633286,211.34051666633286,211.34051666633286,211.34051666633286,211.34051666633286,211.34051666633286,211.34051666633286],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[299.93767691896426,299.93767815234963,299.93767925290837,299.9376936222194,299.9376950978938,299.93769640779277,299.9377140056124,299.9377157701422,299.93773654966077,299.9377388799392,299.93774098866777,299.9377664397905,299.93776923501406,299.937771753722,299.9378029276608,299.93780627948615,299.9378092863957,299.9378474715159,299.93785148968715,299.9378550779008,299.9379018535605,299.9379066696427,299.9379619091025,299.9379682529365,299.93797402494386,299.9380417054783,299.93804933568725,299.93805625388705,299.93813918611977,299.9381483659434,299.93815666023505,299.93825829445933,299.9382693437603,299.9382792932029,299.93840386742204,299.93841717663014,299.9384291219504,299.93858184543024,299.93859789383475,299.9387783688364,299.9387995356144,299.93881891574443,299.9390402901372,299.9390659435658,299.93908939420976,299.9393610361671,299.9393921942173,299.93942064585184,299.93975412060877,299.939792069792,299.9398267080249,299.94023631894123,299.9378801328458,299.93987584488684,299.94010024436125,299.9402681796115,299.9404656274325,299.94070881336404,299.9409150789643,299.9411945974542,299.9414555002721,299.9418256541621,299.9420754615186,299.9424660071844,299.942886330028,299.94319309527555,299.94370933223144,299.94412671187405,299.9448191316095,299.9452376850006,299.94599053539866,299.9466942848916,299.94735065961044,299.9482272127879,299.949103288696,299.95000006447,299.95125582075127,299.95262962365734,299.9541854338263,299.95631260789503,299.95819503356665,299.96039348142483,299.9625656275996,299.9684120277926,299.9745891536115,299.9775905078281,299.9808706113654,299.9844829515355,299.98840318709443,299.9926915283499,299.99740788813966,300.00249204591137,300.0081695548071,300.01432063481894,300.021136282421,300.0285497469638,300.036778034737,300.0458443144953,300.0558646295975,300.06685442110313,300.0790967877073,300.09261135699336,300.10770488037224,300.1243294728251,300.14279929799136,300.1634839526985,300.1863690392426,300.2089161282338,300.2373339876558,300.2691206586793,300.3047153257476,300.3446179508223,300.38939795217493,300.4397041369568,300.5083437636259,300.58871475335786,300.6782872985704,300.77698845628345,300.88700754759117,301.0091665218442,301.1451517072686,301.29725650440037,301.46649204417236,301.6447570003541,301.7606970024075,301.795771233046,301.84213651760797,301.89967580750937,301.96996681302903,302.05522883878666,302.1570459302249,302.28024060801243,302.4273336357764,302.6021481236748,302.8081199817891,303.0528379144993,303.3410135031633,303.67827202433523,304.0752477474818,304.5381016021678,305.0804625963638,305.71076359528,306.3517306387383,307.0125372924955,307.7611388604396,308.4859426138313,308.71773255285746,308.9731801074251,309.25466978307435,309.56537450055623,309.90869957439287,310.28848372823876,310.70905247076234,311.17532011222653,311.6929726828809,312.2677484319281,312.9068830851569,313.41266664214055,313.8567600858424,314.34121676999985,314.87015028485524,315.4481394060936,316.08028403691566,316.7722678423665,317.53042828750796,318.3618348367107,319.27437611074424,320.27685682796624,321.3791053714718,322.592092820295,323.9280642535057,325.4006830741687,327.025188997726,328.8185701979858,330.79974989509265,332.9897873958899,335.41209325205165,338.092657782618,341.06029171757456,344.34687716756315,347.9876265309826,352.0213463446605,356.49070251246724,361.44248286618824,366.92785269584846,373.0025988124933,375.55083524044795,375.55083524044795,375.55083524044795,375.55083524044795,375.55083524044795,375.55083524044795,375.55083524044795,375.55083524044795,375.55083524044795,375.55083524044795,375.55083524044795,375.55083524044795,375.55083524044795,375.55083524044795],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[737.9440731851981,737.9430315456528,737.9419738497938,737.9408425714912,737.9395597404084,737.9382576448107,737.9368649868908,737.9352851165967,737.9336821535085,737.9318669610706,737.9300220420599,737.9280486984724,737.9258131993561,737.923541964907,737.9211126692493,737.918359542328,737.9155634911008,737.9125729040574,737.909182294027,737.9057401685094,737.9020586173101,737.8978829171745,737.8936454474206,737.888847780382,737.8839707315097,737.8787541651269,737.8728456109627,737.8668416132623,737.8604197515793,737.8531430916825,737.8457517719997,737.8378461677338,737.8288846284628,737.8197854678194,737.8100533601893,737.7990168249522,737.7878152695752,737.7758347351779,737.7622427649982,737.7484531321877,737.7328367071376,737.716965734028,737.6999901742473,737.6807579214291,737.661219942411,737.6403225493154,737.6098557325869,737.5795077483414,737.5522955300137,737.5388906415473,737.5087414938384,737.4765961763734,737.4419337740557,737.404854982893,737.3649961544721,737.3224998938452,737.2769545833197,737.227904002422,737.1754556277148,737.1190905292425,737.058949076394,736.9944533856981,736.9249086663019,736.8508938259753,736.7712194889907,736.6860311296692,736.5944461908985,736.4966486162659,736.3916419423605,736.2789430680178,736.1587817788609,736.0292142477173,735.8911336427321,735.7423911445469,735.5830821821584,735.4125230113295,735.2299913935022,735.0346845689868,734.8246663573537,734.6011087514817,734.3608355212691,734.1027840663329,733.8267464349098,733.5314303831057,733.2134804754473,732.8743293357087,732.5094957955982,732.1191650878175,731.7014711079215,731.252613948131,730.7722878568669,730.2581290315528,729.7061062903908,729.1151666295855,728.4817455189057,727.8028629797406,727.0752952418994,726.2962105273493,725.4612578127288,724.5665177483905,723.6592749720461,722.6939758116829,721.6598574577455,720.5518909467141,719.3649158251377,718.0933025564726,716.7304328931896,715.2706415623277,713.7071256158048,712.0326193903576,710.2393656084475,708.3190851616328,706.262945623162,704.0615285680711,701.6567495991352,699.1307047887376,696.4275445809061,693.534166031468,690.4398203530169,687.125846753719,683.5828859741559,679.7911785262204,675.7420325993198,671.4094551658354,666.7762829954659,661.821994859977,655.7918478414136,649.275397671983,642.3102170295411,634.8523758415467,626.8717763845151,618.340838097862,609.2255514113486,599.4750555309655,589.0549847599535,577.9254495065269,566.0443166989219,553.3685724358692,539.8548152628762,525.4599047191329,510.14275060974006,493.868649964824,476.6002294021905,458.3195508889514,439.0212146427399,425.927728314632,412.58676300396723,398.4683084367699,383.6001156076122,368.00931284785406,354.83053877801416,343.90938384806674,332.43089647887507,320.4030635654837,307.8628614906673,294.87725350393623,281.5567864984411,268.06928795449295,254.65731507675585,241.7019356031077,229.70766190241287,220.51737005972066,215.1207503343234,210.49000245118128,206.94041228342442,204.84373343725363,204.61750484832865,206.70243744279355,211.5289953158094,219.4798450957447,230.85914879828283,245.87876479512127,264.66468622397025,287.27885323654294,313.7471684287376,344.08555820272693,378.3197525633674,416.49805468137293,427.01168169649253,427.01168169649253,427.01168169649253,427.01168169649253,427.01168169649253,427.01168169649253,427.01168169649253,427.01168169649253,427.01168169649253,427.01168169649253,427.01168169649253,427.01168169649253,427.01168169649253,427.01168169649253,427.01168169649253,427.01168169649253,427.01168169649253,427.01168169649253,427.01168169649253,427.01168169649253,427.01168169649253,427.01168169649253],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[581.2992135245978,581.2993831437095,581.299565973021,581.2997630425414,581.2999754626858,581.3001047023408,581.3003436373835,581.3006011807669,581.3008787834484,581.3011780096689,581.3015005458122,581.3016967572364,581.3020595550673,581.3024506103736,581.3028721268826,581.3033264804386,581.3038162324714,581.3041141322525,581.3046650244829,581.3052588302446,581.3058988971361,581.3065888343484,581.3070087559913,581.3077848496446,581.3086214047879,581.3095231390516,581.310495138928,581.3115428887028,581.3121805227657,581.313359140032,581.3146296025541,581.3159990806779,581.3174753060094,581.319066615535,581.3200349624752,581.3218251021436,581.3237547978293,581.3258349536469,581.328077328663,581.3304946043346,581.331965479101,581.3346849544503,581.3376165652138,581.3407769070692,581.3441838801912,581.3478567926785,581.3500916491495,581.3474485482782,581.3542496955539,581.3581629713849,581.3627082124051,581.3675769326063,581.3727199131908,581.3780150345714,581.3836201773468,581.389678127854,581.3973318895081,581.4038712582037,581.4124160998896,581.4199170365873,581.4287110559851,581.4391316042878,581.4486593242747,581.4602320508172,581.4742666825625,581.489439257546,581.5075445997636,581.5256019309408,581.5453794792211,581.5670502075799,581.5899552928695,581.6141332016605,581.6410839895941,581.669181558964,581.7000756544777,581.7325262825051,581.766775605596,581.8043848215605,581.8443130862408,581.8879286186718,581.9334810687789,581.9839607641492,582.0364390702638,582.0947310183979,582.1563208600884,582.2216569016597,582.2932832458141,582.3431388863773,582.3914912808593,582.4081414458831,582.4260047584478,582.4452414119265,582.4659339411936,582.4883265866906,582.512118408414,582.5377741156803,582.5646025829152,582.5924564851708,582.627334072405,582.6629142076671,582.7009085709069,582.7416098397954,582.7846353545664,582.8318399742158,582.8826769354107,582.937488981468,582.9958414847275,583.0596347151727,583.1285004195606,583.2028734275084,583.2824328228492,583.3693085004043,583.4624328052832,583.5641252250637,583.6742151600121,583.7926764520982,583.9220030577185,584.0847122269279,584.2635752392084,584.4562546005395,584.6647875406644,584.891679007534,585.1365910747961,585.4021909861542,585.6914917354381,586.0046623961218,586.3450007584304,586.7151429153135,587.1180487436558,587.5569736582707,588.0355376816747,588.5577702351286,589.1281194975048,589.7515799649993,590.2258027046156,590.6617818902874,591.1365774792306,591.6586826751644,592.2285302770026,592.853883539052,593.5407018471734,594.2972918337708,594.9456329189697,595.2663006515579,595.6134346073854,595.9900027274077,596.3981923610419,596.8416273828059,597.3229462889776,597.8461760556314,598.4153838829843,599.0353813483802,599.7102676813823,600.1988266341763,600.5933785069172,601.0194943260547,601.4799119466986,601.9776340014337,602.5159566111155,603.0985014934994,603.7292518865854,604.412592752912,605.1533557852597,605.956869793555,606.829017117263,607.776296777181,608.8058951551078,609.9257650688317,611.1447141925207,612.4725038576734,613.9199593555728,615.4990929463281,617.2232408589053,619.1072156369914,621.1674752418859,623.4223103595074,625.8920513662486,628.5992963786521,631.5691617341521,634.8295561126379,638.4114792987373,642.3493462896676,646.6813370612465,651.4497718055027,656.7015108417311,661.1975276717143,661.1975276717143,661.1975276717143,661.1975276717143,661.1975276717143,661.1975276717143,661.1975276717143,661.1975276717143,661.1975276717143,661.1975276717143,661.1975276717143,661.1975276717143,661.1975276717143,661.1975276717143,661.1975276717143],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[424.5150724820508,424.51515358469976,424.5152415099205,424.5153347060023,424.5154345807197,424.51554284898964,424.515657616394,424.5157820461,424.51591392732246,424.5160552594828,424.5162084793381,424.51637088685294,424.51654697971634,424.516733606069,424.5169336066889,424.51715044572546,424.5173802718486,424.51762656879106,424.51789358486593,424.5181766123643,424.5184834947929,424.5188087323925,424.51915727931544,424.5195351823667,424.51993571302813,424.52037004698104,424.5208303179302,424.52132357745063,424.5218584425876,424.52242527785756,424.5230327422755,424.52369142220857,424.52438950464676,424.5251465778369,424.52594880576794,424.5268085405314,424.52774091258726,424.52872892431554,424.52980062527087,424.5309360721644,424.5321529236319,424.53347286713034,424.5348713327024,424.5363700736891,424.5379958470201,424.5397183201367,424.54158725558955,424.54356691350665,424.54568855620585,424.54799080190617,424.5504293239581,424.5531682403146,424.55599146127,424.55901755635546,424.56226193962686,424.56574133710484,424.5694739112604,424.5734793160656,424.5777659843311,424.58236866536856,424.58729802826707,424.5925790949347,424.5982569544236,424.6043294047625,424.6108492869765,424.6178367022479,424.62400989108704,424.63239499665184,424.6425646032365,424.65185450795593,424.66181867613636,424.6725079154164,424.6839727258736,424.6962713819718,424.709464053593,424.7236217088944,424.73880866637984,424.7551065976851,424.77259211054496,424.79135986881295,424.81150201324607,424.8331215076727,424.8563289093063,424.88124316938104,424.9079927453471,424.93671633464487,424.96757052397703,425.0007032642514,425.0362947178751,425.0745339097422,425.1156239726822,425.15979729428557,425.20727015042524,425.2583105050252,425.31319628747775,425.3722302688363,425.4353995121997,425.50300085766906,425.57572842854336,425.6540169557302,425.7382579747738,425.82895788610136,425.9266764501119,426.0319144993253,426.1453284549313,426.2676377303487,426.3994972079254,426.5406796945265,426.6941467344888,426.8598616693595,427.0388843559053,427.2323771686906,427.4416161970323,427.6680037939545,427.91308265014356,428.1807397119006,428.47078843179935,428.7825023829505,429.12022014921587,429.48934558146766,429.88899055389976,430.3236854237118,430.7974805078383,431.31666546533603,431.87862069663674,432.38106005234886,432.9308604596985,433.5296039746103,434.1738674363001,434.86838678700354,435.6174541884679,436.4259586942457,437.2939635268222,438.23588083023805,439.25386233739437,440.353492883844,441.29663479841776,442.2460018719555,443.2727931625951,443.74672571277625,444.180460470089,444.6477960790868,445.1503003285645,445.69141228344813,446.27427103730747,446.9022887634435,447.57918127900126,448.3089977597706,449.0961479628017,449.94553198168444,450.86235635046944,451.7508051884525,452.3181142983694,452.928816991758,453.58640937214767,454.29469216712,455.0577999244483,455.8802332481069,456.7668944000168,457.7231266255768,458.7547575952942,459.86814739099566,461.07024150286713,462.36862934249575,463.77160881655726,465.2882575449879,466.92851134532896,468.70325064007926,470.62439547467676,472.7050098581002,474.95941615365587,477.4033202514975,480.053948243667,482.9301952935181,486.05278734064143,489.4444562062137,493.13012855865287,497.1371290627613,501.4953978654724,506.23772236774573,511.39998299732684,517.0214124361937,523.1448674786106,529.8171124142508,536.0409346392784,536.0409346392784,536.0409346392784,536.0409346392784,536.0409346392784,536.0409346392784,536.0409346392784,536.0409346392784,536.0409346392784,536.0409346392784,536.0409346392784,536.0409346392784,536.0409346392784,536.0409346392784,536.0409346392784,536.0409346392784],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle EN pour le paramètre<br>elasticnet__l1_ratio=1.0<br>en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE ElasticNet pour tout le meilleur paramètre l1 ratio\n","FigRMSEGRidEN = visuRMSEGrid(ElasticNet(), 'EN', alphasEN, 'alpha', GridEN,\n","                             BestParametresEN, 'elasticnet__l1_ratio')\n","FigRMSEGRidEN.show()\n","if write_data is True:\n","    FigRMSEGRidEN.write_image('./Figures/EmissionsGraphRMSEEN.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.5 Modèle kNeighborsRegressor"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                          paramètre  KNeighborsRegressor()\n","0  kneighborsregressor__n_neighbors                      3\n","                             R²        RMSE        MAE\n","KNeighborsRegressor()  0.501066  376.938808  96.126155\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predkNN=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[175.93000000000004,35.4,106.17,20.116666666666667,9.456666666666667,25.843333333333334,20.91333333333333,81.81333333333333,5.859999999999999,9.646666666666667,103.87666666666667,13.38,474.43,9.513333333333334,24.22666666666667,4.703333333333333,4.579999999999999,465.98,20.69666666666667,635.11,762.7366666666667,12.013333333333334,55.446666666666665,44.53,619.9533333333333,12.783333333333333,15.323333333333332,133.12,63.68333333333334,56.92666666666667,29.46666666666667,21.813333333333333,12.763333333333334,182.23000000000002,49.71333333333333,26.62,12.436666666666667,20.563333333333336,58.85333333333333,30.426666666666666,32.13333333333333,236.78,208.72666666666666,128.43666666666667,80.93333333333334,256.4766666666667,6.726666666666667,14.92,209.17333333333332,55.546666666666674,139.63666666666666,41.71,28.976666666666663,15.600000000000001,16.223333333333333,16.326666666666664,21.123333333333335,2236.2566666666667,287.59666666666664,138.03333333333333,20.790000000000003,52.949999999999996,57.56333333333333,28.453333333333333,130.70666666666668,21.350000000000005,23.766666666666666,20.39333333333333,113.92999999999999,56.473333333333336,12.263333333333334,11.786666666666667,7.463333333333334,101.28333333333335,96.86666666666667,16.423333333333336,25.236666666666668,30.05,48.300000000000004,66.29666666666667,11.766666666666667,21.113333333333333,22.62,6.0633333333333335,11.156666666666666,16.203333333333333,166.61,29.819999999999997,17.583333333333332,32.160000000000004,28.62333333333333,15.48666666666667,50.03666666666666,47.53666666666666,143.25,63.09666666666667,28.473333333333333,25.28,17.653333333333336,133.73,19.586666666666666,66.58999999999999,38.81666666666666,173.55999999999997,71.45,32.526666666666664,741.7333333333332,72.22333333333334,66.24,19.566666666666666,85.13000000000001,734.8733333333333,82.73333333333333,14.923333333333332,14.416666666666666,142.45666666666668,36.96,23.810000000000002,106.94000000000001,123.24000000000001,21.650000000000002,11.386666666666665,164.6866666666667,35.81333333333333,203.33333333333334,8.306666666666667,261.88,6.016666666666667,77.53333333333335,20.796666666666667,30.21666666666667,14.15333333333333,25.786666666666665,212.58666666666667,199.1966666666667,42.43333333333334,82.12,114.48333333333335,36.27333333333333,129.19666666666666,23.86,37.28,390.04333333333335,10.026666666666666,40.15333333333333,12.89,172.95333333333335,45.339999999999996,253.92999999999998,98.37666666666667,25.953333333333333,247.26333333333332,430.8633333333333,136.62,27.463333333333328,69.33333333333333,75.60666666666667,16.136666666666667,56.61333333333334,9.376666666666667,3.893333333333333,44.56999999999999,43.160000000000004,23.113333333333333,33.28666666666667,405.9766666666667,163.63,47.14000000000001,43.78333333333333,289.1433333333333,9.046666666666667,59.086666666666666,423.3633333333333,41.37,43.623333333333335,89.51666666666665,60.16,53.88333333333333,34.733333333333334,165.08666666666667,11.993333333333334,49.29666666666666,40.13333333333333,63.80666666666667,6.23,132.77333333333334,56.21,8.216666666666667,40.32666666666667,47.53666666666667,27.25,88.64,74.31,37.026666666666664,15.336666666666666,27.383333333333336,23.956666666666667,212.58666666666667,30.083333333333332,40.98,6.640000000000001,132.26666666666668,71.07666666666667,68.80666666666667,15.866666666666665,39.306666666666665,97.91333333333334,83.36666666666666,17.423333333333332,4.3566666666666665,50.856666666666676,46.48333333333334,235.06000000000003,18.593333333333334,12.623333333333335,49.126666666666665,27.889999999999997,66.93,332.9666666666667,72.36333333333333,62.29666666666666,38.96666666666666,17.186666666666667,178.59666666666666,106.76,40.84,148.99333333333334,4.760000000000001,14.6,10.946666666666665,41.5,69.87,9.110000000000001,34.01,172.22333333333333,45.49,33.43,68.83666666666666,48.093333333333334,216.24666666666667,73.42999999999999,47.74,155.62,40.306666666666665,16.963333333333335,117.90333333333332,25.096666666666664,34.696666666666665,388.0566666666667,27.25,144.96666666666667,34.973333333333336,49.5,119.24000000000001,30.426666666666666,46.29333333333333,146.08,178.59666666666666,22.503333333333334,28.526666666666667,16.95,13.273333333333332,8.48,68.96333333333332,66.24,161.87666666666667,25.959999999999997,86.89666666666666,3.393333333333333,28.373333333333335,55.546666666666674,25.080000000000002,20.116666666666667,5.676666666666667,1175.3799999999999,39.29666666666667,8.889999999999999,39.21666666666666,35.03,170.56,6.673333333333333,12.949999999999998,41.46333333333333,46.326666666666675,7.696666666666668,435.9066666666667,59.25666666666666,7.6066666666666665,44.31666666666666,57.57333333333333,49.910000000000004,11.766666666666666,9.423333333333334,16.956666666666667,15.053333333333335,7.453333333333333,942.1533333333333,26.320000000000004,5.1866666666666665,56.07333333333333,21.563333333333333,204.24333333333334,105.38,30.52,68.96,75.53333333333333,62.49,34.36333333333334,25.953333333333333,68.95333333333333,35.42333333333334,9.663333333333334,180.49,68.05,3.6933333333333334,35.48666666666667,18.563333333333333,208.92,105.68,55.413333333333334,64.84666666666668,4.683333333333334,11.603333333333333,36.95333333333334,64.75666666666666,28.113333333333333,96.85333333333334,4.216666666666666,296.2266666666667,48.44333333333333,55.73333333333334,62.72,18.94666666666667,33.49333333333333,7.213333333333334,36.27333333333333,29.616666666666664,106.84333333333332,64.47333333333333,101.28333333333335,117.71666666666665,39.60333333333334,75.25999999999999,234.96,48.78,210.67999999999998,21.686666666666667,7.086666666666666,68.96333333333332,181.11,13.08,159.29,54.23666666666667,5.343333333333334,231.15,20.116666666666667,10.623333333333333,652.1966666666666,14.589999999999998,16.203333333333333,200.4333333333333,22.76,242.33,19.643333333333334,23.403333333333336,176.39666666666665,86.50666666666666,59.836666666666666,16.56,25.316666666666666,3.793333333333333,5.783333333333332,24.59,88.64,51.446666666666665,43.703333333333326,640.9633333333334,8.356666666666667,232.27666666666664,84.86,38.77,34.32333333333333,45.71333333333333,491.6266666666667,68.71333333333332,23.733333333333338,34.56333333333333,144.73333333333332,121.19333333333334,149.94666666666666,160.11333333333334,176.26666666666665,280.84,32.13333333333333,45.77,199.19000000000003,26.066666666666663,20.21,68.83666666666666,17.666666666666668,18.563333333333333,37.88999999999999,428.43333333333334,104.99333333333334,9.316666666666666,98.18,382.93000000000006,57.603333333333325,100.63333333333333,63.169999999999995,27.83,18.81,62.419999999999995,8.6,102.54,40.04,621.09,39.4,12.35666666666667,37.77,31.676666666666666,72.23,350.0566666666667,23.74,22.213333333333335,4.783333333333334,27.889999999999997,69.00333333333334,20.099999999999998,56.74333333333333,29.819999999999997,57.903333333333336,315.3633333333333,78.70666666666666,42.413333333333334,16.276666666666667,48.06333333333333,26.36,99.74333333333334,30.936666666666667,17.423333333333332,49.96,91.39,11.87,14.336666666666666,6.763333333333333,26.413333333333338,14.25,18.563333333333333,21.686666666666667,163.06,109.45666666666666,22.39,21.803333333333338,97.20666666666666,45.61333333333334,163.26,14.71,31.653333333333336,21.366666666666664,40.85333333333333,66.38666666666667,21.94666666666667,73.11666666666666,39.163333333333334,39.4,48.54,249.62666666666667,87.54333333333334,168.48,241.56666666666663,19.10333333333333,31.790000000000003,9.136666666666665,164.38666666666666,69.88,25.953333333333333,42.04,99.26333333333334,196.58,65.62,198.35333333333332,33.776666666666664,30.293333333333337,7.819999999999999,19.286666666666665,15.31,168.81333333333333,40.96333333333333,102.12333333333333,63.19333333333333,59.01,67.74,4.06,53.22,31.023333333333337,383.79333333333335,101.56666666666668,30.946666666666662,22.136666666666667,103.58333333333333,3.893333333333333,103.04333333333335,196.36333333333334,4.72,49.196666666666665,123.16333333333334,16.563333333333333,82.48666666666666,15.766666666666666,57.03666666666667,21.599999999999998,63.87,18.080000000000002,40.21333333333333,63.36666666666667,113.42333333333333,51.67666666666667,79.32666666666667,34.56666666666667,19.169999999999998,36.92666666666667,7.023333333333333,12.946666666666667,15.363333333333335,47.076666666666675,214.64,49.48,8.433333333333332,427.20666666666665,122,51.826666666666675,86.76333333333334,100.96333333333332,63.13666666666666,304.54333333333335,12.783333333333333,17.58,48.98333333333333,36.97,35.28333333333334,4.736666666666667,57.586666666666666,5.739999999999999,52.24,32.083333333333336,116.92,49.24666666666667,73.61,269.56,31.709999999999997,15.933333333333332,23.25333333333333,108.56,78.06333333333333,48.26666666666666,21.563333333333333,55.25,11.446666666666667,3.1366666666666667,4.636666666666666,46.50333333333334,5.263333333333333,52.17666666666667,28.66666666666667,86.04,21.706666666666663,138.29666666666665,438.7666666666667,46.21,186.14666666666665,117.39666666666666,37.12,11.526666666666666,9.433333333333334,74.76,35.78333333333333,14.589999999999998,69.13000000000001,7.840000000000001,64.90666666666665,206.13333333333333,9.44,88.96333333333332,253.30333333333337,38.46,198.32333333333335,31.926666666666673,347.1133333333333,163.06,41.93666666666666,35.73,1425.7933333333333,101.14999999999999,12.263333333333334,5.71,96.36000000000001,145.49666666666667,28.3,24.543333333333333,31.276666666666667,27.73,108.56,29.813333333333333,60.31,72.22333333333334,342.91,78.85666666666667,94.32333333333332,280.8333333333333,16.216666666666665,19.09,18.383333333333333,15.590000000000002,20.206666666666667,162.53666666666666,244.00333333333333,480.64333333333326,146.23333333333332,52.876666666666665,80.33666666666667,36.156666666666666,106.22333333333334,37.78333333333333,3.6566666666666667,6.656666666666666,14.416666666666666,4.843333333333333,35.14666666666667,41.82666666666667,27.98,729.6999999999999,42.61666666666667,27.83,7.390000000000001,369.2633333333333,9.93,4.136666666666667,111.42,678.9166666666666,144.20666666666668,14.123333333333333,5.97,65.55,40.53,26.52,16.673333333333336,86.74666666666667,16.103333333333335,91.07,260.57666666666665,39.156666666666666,230.39333333333332,4.426666666666667,75.94666666666667,32.47,52.20000000000001,188.57666666666668,30.006666666666664,38.79,48.576666666666675,28.349999999999998,20.973333333333333,114.50999999999999,29.39,30.14,28.453333333333333,38.64333333333334,700.25,112.71333333333332,4.626666666666666,191.67333333333332,36.63,154.16,35.48666666666667,6.456666666666666,12.626666666666667,16.723333333333333,27.150000000000002,5.336666666666667,65.21333333333334,68.83666666666666,123.56,213.8166666666667,223.04,28.22333333333333,55.663333333333334,4.773333333333333,21.746666666666666,2059.4,34.276666666666664,20.996666666666666,45.046666666666674,327.2233333333333,493.6566666666667,12.626666666666667,41.71,18.563333333333333,21.566666666666666,3.736666666666667,10.026666666666666,146.21333333333334,221.07999999999996,21.186666666666667,25.14,34.60666666666666,18.706666666666667,39.60333333333333,82.00666666666666,37.78333333333334,26.673333333333336,16.81,85.73333333333335,122,37.95666666666667,28.296666666666667,58.02333333333333,11.156666666666666,35.09333333333333,19.51,479.25666666666666,90.09000000000002,88.64,47.846666666666664,195.0666666666667,230.39333333333332,45.21333333333333,599.0733333333334,17.156666666666666,38.666666666666664,103.04333333333335,197.91333333333333,56.92333333333334,69.84666666666666,103.96666666666665,113.88666666666666,10.026666666666666,4.963333333333334,92.80999999999999,24.22333333333333,57.26,14.68,54.129999999999995,31.47,25.256666666666664,135.29,938.1833333333334,40.99333333333333,96.93333333333334,6.296666666666667,53.22,48.60999999999999,9.44,72.23,24.02,166.60999999999999,63.87,267.6666666666667,51.99666666666666,11.603333333333333,142.69333333333333,30.83,35.54333333333333,42.67333333333334,85.05666666666667,3.6966666666666668,71.98333333333333,20.116666666666667,28.73333333333333,57.419999999999995,75.3,31.790000000000003,13.356666666666667,108.01,82.80333333333333,26.539999999999996,17.263333333333332,51.72,9.270000000000001,27.150000000000002,106.76,50.623333333333335,38.983333333333334,9.930000000000001,36.763333333333335,4.916666666666667,423.24,339.73333333333335,161.62,22.78,207.02666666666664,12.57,59.64000000000001,28.650000000000002,49.196666666666665,15.363333333333335,14.92,754.6333333333333,22.413333333333338,5.933333333333334,969.1300000000001,28.453333333333333,4.136666666666667,62.26333333333334,31.08666666666667,86.58999999999999,29.416666666666668,29.443333333333328,185.97333333333333,69.45,55.199999999999996,50.60333333333333,34.36,6.933333333333334,15.92,9.443333333333333,21.186666666666667,22.19666666666667,33.78666666666667,24.143333333333334,37.34333333333334,9.053333333333333,9.326666666666666,32.92333333333333,7.22,15.053333333333335,25.28,185.97333333333336,43.51666666666667,23.723333333333333,4.426666666666667,50.27,13.136666666666665,18.926666666666666,218.14666666666668,102.40666666666665,45.379999999999995,100.96333333333332,91.96,9.47,362.05333333333334,3.876666666666667,87.08,20.116666666666667,92.40666666666665,5.306666666666667,28.49666666666667,50.42000000000001,39.96666666666667,7.7,40.07,9.983333333333333,52.07333333333333,20.343333333333334,21.86333333333333,33.11,17.226666666666667,114.43,54.276666666666664,24.116666666666664,36.17333333333333,15.816666666666665,40.77,51.46666666666667,8.66,39.59666666666667,54.13,32.17333333333334,66.83666666666666,20.736666666666668,29.473333333333333,20.676666666666666,4.636666666666667,37.196666666666665,52.04333333333333,14.713333333333333,13.770000000000001,15.563333333333333,11.323333333333332,164.59,65.48666666666666,9.843333333333334,142.21,43.03666666666667,21.563333333333333,12.626666666666667,73,48.72333333333333,35.78333333333333,5.096666666666667,27.683333333333337,110.15666666666668,403.31666666666666,115.02666666666666,20.43,79.2,9.456666666666665,6.733333333333333,414.25,35.06,155.92666666666665,415.1033333333333,35.78666666666666,15.756666666666666,56.07333333333333,513.3933333333333,40.77,423.6566666666667,33.93,171.95000000000002,39.306666666666665,33.416666666666664,20.296666666666667,41.346666666666664,92.49666666666667,8.596666666666666,41.92666666666667,233.27,23.30666666666667,135.69333333333336,845.6233333333333,140.34333333333333,198.3233333333333,55.833333333333336,7.919999999999999,56.07333333333333,89.33333333333333,34.63333333333333,7.433333333333334,31.44333333333333,42.77,8.020000000000001,60.79666666666666,405.02666666666664,18.383333333333333,69.13,83.3,14.04,4.236666666666667,5.303333333333334,31.44333333333333,31.833333333333332,31.926666666666666,9.326666666666666,6.933333333333334,18.926666666666666,28.99666666666667,36.196666666666665,63.873333333333335,73.77,81.22333333333333,222.8133333333333,182.76333333333332,140.93666666666667,83.90666666666668,97.04,13.493333333333332,34.196666666666665,174.49666666666667,31.083333333333332,284.42,183.2266666666667,62.72333333333333,21.77,32.913333333333334,463.5233333333333,11.699999999999998,20.29666666666667,62.88999999999999,71.15333333333334,11.61,61.68,485.3999999999999,3.9599999999999995,50.586666666666666,34.46333333333333,317.6933333333334,46.10999999999999,13.653333333333334,21.803333333333338,37.28,586.6466666666666,27.263333333333332,60.95333333333334,23.39,734.8733333333333,225.35333333333332,129.57,21.560000000000002,28.323333333333334,12.176666666666668,19.013333333333332,109.69,6.353333333333334,83.32666666666667,71.98666666666666,41.82333333333333,194.99333333333334,44.73,18.60666666666667,42.95666666666667,56.10333333333333,28.51,28.003333333333334,22.709999999999997,28.53333333333333,80.27666666666667,102.13,273.49333333333334,42.99,41.33333333333333,8.113333333333333,361.3633333333333,24.146666666666665,3.3266666666666667,426.1233333333333,57.24333333333334,70.28666666666668,73.35333333333334,1854.1433333333334,4.366666666666666,35.14333333333334,59.083333333333336,103.87666666666667,37.656666666666666,484.06333333333333,20.676666666666666,66.05333333333333,19.316666666666666,41.62,4.136666666666667,304.39666666666665,50.06,22.47666666666667,94.29,5.0566666666666675,182.68999999999997,55.446666666666665,36.53666666666667,798.3066666666667,75.29666666666667,308.3066666666667,36.300000000000004,383.7933333333333,6.47,2.516666666666667,6.396666666666667,55.36000000000001,21.293333333333333,5.306666666666667,20.21,21.74333333333333,117.01,236.03666666666666,59.059999999999995,4.8500000000000005,289.50666666666666,10.163333333333334,29.819999999999997,35.96333333333333,89.64333333333333,39.93666666666667,45.85999999999999,8.936666666666667,23.30666666666666,17.583333333333332,55.54,41.61666666666667,33.24333333333333,93.55,38.196666666666665,954.7533333333334,197.40666666666667,5.096666666666667,68.11,47.01666666666667,49.370000000000005,17.793333333333333,21.49666666666667,107.54666666666667,417.68000000000006,4.4366666666666665,63.38666666666666,20.12,1005.4433333333333,74.05666666666666,3.7266666666666666,1396.68,22.55,55.73333333333334,96.85333333333334,56.693333333333335,13.030000000000001,34.46333333333333,35.02,22.733333333333334,88.78666666666668,104.37333333333333,10.913333333333334,52.21666666666666,37.14333333333334,26.853333333333335,7.023333333333333,57.03333333333333,15.35,167.73999999999998,26.853333333333335,4.626666666666666,58.13,5.19,9.033333333333333,263.03,9.106666666666667,22.25,203.92333333333332,199.18999999999997,25.03,26.439999999999998,70.19333333333333,3.986666666666667,187.08333333333334,65.84333333333333,95.56666666666668,17.856666666666666,13.47,47.42333333333334,65.43333333333334,196.74,96.86666666666667,17.189999999999998,77.66666666666667,308.66333333333336,15.833333333333334,14.476666666666667,6.763333333333333,105.83,13.643333333333333,12.013333333333334,27.186666666666667,234.96,26.066666666666663,76.98666666666666,417.68000000000006,20.816666666666666,21.046666666666667,21.453333333333333,90.81333333333333,51.410000000000004,45.86000000000001,31.790000000000003,106.22333333333334,215.70333333333335,18.38,204.92333333333332,160.47666666666666,38.46,57.92000000000001,68.05,34.29,70.52666666666666,22.143333333333334,20.076666666666668,37.14333333333334,86.04,140.03,152.33333333333334,3.516666666666667,90.19666666666666,3389.6366666666668,24.373333333333335,7519.82,65.21333333333334,20.116666666666667,15.516666666666667,11.616666666666665,97.50666666666667,30.156666666666666,91.13,34.18,372.3833333333334,20.720000000000002,26.539999999999996,41.49666666666666,30.026666666666667,18.60666666666667,27.873333333333335,175.6866666666667,65.47333333333334,54.276666666666664,21.55,37.88,949.0233333333332,11.943333333333333,111.56,46.50333333333333,99.05,6.793333333333334,27.826666666666668,7.793333333333333,45.696666666666665,32.13333333333333,7.556666666666666,4539.596666666666,48.25333333333333,41.62,56.879999999999995,26.413333333333338,175.32000000000002,1422.573333333333,1.8833333333333335,69.13,164.11999999999998,148.99333333333334,58.02333333333333,83.15666666666667,121.30999999999999,7.096666666666667,10.523333333333333,11.753333333333332,17.026666666666667,8.883333333333335,22.76666666666667,37.73,274.15333333333336,8.016666666666667,5.716666666666668,16.66,32.27,35.07333333333333,5.8566666666666665,221.98666666666668,122.28000000000002,48.68333333333334,70.48,43.77333333333333,33.30666666666667,43.19333333333333,31.790000000000003,44.5,81.22333333333334,60.52333333333333,182.94666666666663,40.61666666666667,39.083333333333336,47.5,21.296666666666667,74.76666666666667,382.93000000000006,62.72666666666667,34.36666666666667,56.74666666666667,224.10666666666665,219.89000000000001,162.53666666666666,39.48,3.23,148.58333333333334,16.55,398.52,10.923333333333332,17.383333333333333,89.50666666666666,226.92333333333332,8.173333333333334,23.849999999999998,16.146666666666665,28.01666666666667,60.846666666666664,19.41333333333333,13.270000000000001,58.85333333333333,18.926666666666666,54.21333333333334,55.419999999999995,29.653333333333336,25.810000000000002,7.9433333333333325,16.723333333333333,36.763333333333335,62.24,34.36666666666667,132.73,10.763333333333334,91.68333333333334,53.163333333333334,60.673333333333325,113.42333333333333,28.13,52.86000000000001,79.97,105.88666666666666,24.093333333333334,23.723333333333333,163.03333333333333,32.13333333333333,74.17999999999999,36.74666666666666,185.45000000000002,24.463333333333328,37.60333333333333,7.456666666666667,9.746666666666666,150.30666666666667,97.64999999999999,6.973333333333334,24.66,12.936666666666667,14.360000000000001,130.9366666666667],"xaxis":"x","y":[7.8,4.52,27.02,82.38,50.55,18.65,6.35,10.65,5.74,3.59,49.58,6.87,429.27,3.3,41.51,4.03,4.26,69.59,12.61,834.96,404.51,6.53,111.96,2.15,3768.66,531.96,20.79,19.06,25.15,10.77,166.13,111.62,3.79,292.8,25.38,5.37,167.98,11.32,34.2,40.06,22.78,72.5,355.05,238.77,13.86,184.83,39.88,8.93,37.62,26.44,97.97,8.1,69.79,64.06,7.85,50.4,68.05,3995.45,202.97,13.71,27.41,63.45,129.65,59.18,61.21,15.26,17.83,9.63,47.24,62.23,6.52,12.63,144.41,1.14,5.07,44.63,49.46,3.88,126.92,106.37,27.45,9.64,26.41,4.86,60.49,196.67,115.34,9.57,5.1,65.08,9.51,40.94,22.29,65.89,298.61,72.71,45.22,8.47,103.21,293.56,66.43,83.55,9.06,96.43,46.79,18.34,202.49,7.56,9,23.14,26.71,271.58,329.29,7.79,5.53,141.95,23.5,60.67,109.16,358.88,153.73,60.59,7.4,3.75,546.2,4.46,131.4,65.65,147.69,21.09,28.27,5.11,54.09,165.86,277.98,37.82,67.56,71.03,3.82,11.46,22.39,63.14,224.36,107.56,29.7,7.08,43.86,4.04,1597.56,4.67,4.41,26.35,866.23,284.42,73.48,78.15,325.94,129.89,47.38,38.97,7.05,60.53,161.06,20.76,6.19,147.84,69.42,134.71,54.11,67.48,11.17,0,122.99,28.52,33.75,94.89,64.57,152.04,14.05,549.74,21.75,31.04,115.04,272.54,3.96,198.45,7.84,14.37,16.66,14.19,2.57,68.14,9.29,7.74,8.45,16.88,10.63,189.18,37.97,43.57,12.36,139.27,46.72,7.74,18.85,19.76,86.02,14.05,12.97,85.88,57.03,64.22,36.09,4.68,187.92,29.39,10.81,74.45,234.75,3.52,40.12,30.87,5,70.85,698.11,64.49,1936.34,5.47,4.55,5.72,48.65,288.43,2.9,5.87,36.71,5.49,17.87,38.87,3.34,47.64,8.55,3.72,13.74,21.52,40.01,30.12,25.44,20.18,352.87,135.81,161.23,39.5,6.26,94.85,548.37,16.5,141.15,22.92,23.74,7.4,64.54,79.98,35.11,9.25,27.04,234.6,48.6,26.85,3.22,10.55,42.23,66.6,27.93,5.27,324.25,9.9,47.34,107.84,4.36,164.06,6.93,5.28,60.43,10.73,4.53,238.09,174.97,4.23,406.02,7.98,7.1,4.65,2.43,6.51,3.47,3.95,317.82,47.62,49.89,253.53,1.12,33.09,15.48,61.36,31.46,35.8,7.37,134.8,18.99,10.55,66.79,31.94,691.26,87.68,3.15,15.9,45.07,62.2,3.18,16.27,139.62,4.21,11.94,7.03,47.14,40.28,82.66,4.01,8.07,309.6,4.39,3,28.19,77.76,5.41,5.47,5.94,96.39,570.98,18.1,46.03,116.06,48.62,30.27,4.76,67.57,6.61,18.54,7.96,134.01,13.35,80.08,59.84,4.81,105.32,5.91,44.51,674.09,3.98,38.08,177.51,67.24,52.73,13.99,74.33,160.28,466.95,10.01,6.86,5.07,3.72,5.81,49.55,13.87,53.61,143.34,64.23,1.68,54.51,25.37,39.58,46.26,5.18,85.03,192.12,4.73,101.27,21.2,73.14,143.82,184.4,78.06,185.82,32.77,14.49,23.42,6.53,143.45,11.14,21.87,4.67,278.97,6.85,64.39,3.77,95.86,112.06,6.74,290.57,29.92,69.57,24.53,112.82,20.67,12.03,61.18,627.87,9.21,16.67,25.12,10.8,14.83,320.35,3.49,8.07,4.6,6.63,26.64,49.53,52.38,24.88,115.24,402.03,27.14,67.96,25.37,88.11,48.15,25.24,4.87,20.3,12.19,10.01,7.11,96.18,24.76,4.76,24.96,6.31,3.42,41.06,24.1,54.98,20.76,42.74,99.45,145.88,36.48,14.17,1.61,22.02,147.16,129.42,147.36,100.02,9.47,7.82,19.73,8.98,160.3,35.14,8.22,5.51,8.37,639.39,46.23,57.71,31.72,14.86,8.09,8.7,173.23,11.73,23.17,13.29,10.92,5.91,42.93,4.38,377.36,25.21,75.67,0,4.5,6.09,7.28,272.85,171.64,15.34,30.22,15.23,3.63,43.08,9.47,4.32,31.97,11.18,11.24,13.76,6.12,32.87,46.7,30.1,104.17,91.49,65.27,211.49,6.73,49.04,41.27,13.33,19.11,0.76,12.06,56.63,18.74,443.73,44.19,24.56,290.88,119.2,26.34,47.78,157.07,95.26,1623.34,24.51,6.16,38.55,7.52,4.6,5.47,6.69,5.02,23.79,7.07,49.25,43.91,8.1,0,103.19,18.1,61.22,36.24,14.65,28.03,10.01,39.54,7.05,3.09,3.5,9.24,27.13,9.68,49.32,66.39,8.2,123.99,111.69,19.12,142.39,47.71,67.27,10.89,39.57,17.74,78.39,25.94,180.38,14.49,14.73,5.32,4.39,52.09,205.45,19.7,8.29,89.02,177.59,20.65,26.94,20.19,10780.64,13.84,8.45,2.73,8.81,332.63,16.67,29.39,6.18,6.16,67.65,31.52,22.97,8.04,509.69,13.18,26.39,277.98,61.33,4.48,72.93,5.38,6.45,109.79,139.83,54.92,160.8,35.72,6.22,51.93,93.87,6.04,3.72,11.79,18.95,3.53,50.57,0.75,150.69,398.56,113.3,5.67,4.52,61.46,5.69,4.32,154.4,173.47,91.53,13.91,25.39,44.26,42.1,0,12.28,25.48,28.94,22.62,172.46,23.79,156.67,16.3,243.38,33.49,57.41,16.89,62.83,7.56,18.21,79.98,19.69,27.7,48.74,6.17,26.31,18.12,119.81,63.68,23.17,105.97,9.68,13.16,18.42,25.2,6.38,6.83,33.04,4.05,269.71,2.62,121.9,282.34,142.36,310.19,79.42,3.41,3.96,723.47,24.54,151.12,7.61,237.39,200.5,6.7,29.84,5.55,65.46,5.91,17.36,6.71,15.68,9.87,5.53,61.19,4.65,11.54,32.99,34.03,2.52,5.09,50.05,81.92,47.84,66.48,24.91,174.98,36.05,7.36,60.02,111.95,0.89,28.41,67.53,16.13,9.67,913.07,38.68,17.46,85.59,19.84,31.58,36.43,52.41,16.27,12.97,21.69,66.05,12.29,68.64,303.14,208.53,54.05,66.53,191.97,119.92,20.46,28.88,12.89,119.03,8.12,4.52,65.77,5.65,100.42,34.92,359.11,154.39,6,328.17,4.01,27.37,4,17.87,17.8,44.61,12.59,38.15,184.51,19.1,59.05,4.94,23.83,126.55,4.9,4.13,6.1,42.4,6.61,177.1,321.71,104.91,3.28,11.29,4.75,139.72,479.7,21.55,124.4,271.32,6.98,12.46,43.28,25.59,4.8,5.63,208.81,15.43,21.85,122.54,3.01,3.5,64.81,150.88,128.69,32.04,48.79,124.92,56.3,71.01,26.11,7.51,6.16,4.27,4.03,7.53,5.32,241.49,61.39,4.47,61.98,2.45,8.33,7.02,5.6,17.41,58.82,14.97,39.05,87.27,60.34,192.2,36.61,26.67,51.65,85.54,1.55,16.42,81.24,116.95,3.07,3.92,11.44,243.66,36.96,14.12,14.24,16.72,35,45.6,24.87,11.97,51.24,9.29,31.79,18.35,9,61.86,4.19,1154.79,6.18,36.86,9.99,3.18,8.1,26.27,18.72,62.41,32.33,22.53,4.8,4.46,30.45,75.02,19.35,5.24,6.25,196.34,35.69,40.76,8.44,21.08,14.51,75.58,7.79,226.73,1.6,29.57,21.69,16.16,28.08,346.25,229.98,67.17,145.02,62.67,6.91,142.16,5.48,295.12,175.27,27.81,8.14,2.18,445.62,120.68,0,13.05,50.29,935.7,87.03,13.22,34.69,45.44,10.43,8.35,50.98,7.2,119.63,39.47,515.85,190.17,9.13,4.26,148.25,220.26,3.67,6.18,6.73,56.03,54.83,23.56,190.58,29.88,41.7,31.27,6.66,4.25,37.31,138.03,42.8,4.72,5.91,31.92,82.92,64.46,71.38,68.16,127.74,66.37,21.18,726.96,70.16,16.99,25.11,7.24,38.55,121.98,4.55,2089.28,138.85,11.08,12.99,3.42,130.32,15.25,3.06,384.27,73.48,11.87,188.58,99.59,17.51,712.39,62.64,180.14,47.02,3.99,6.49,2.94,44.54,47.26,20.08,9.79,2340.63,242.19,262.45,32.17,4.16,35.98,45.3,23.23,6.48,67.52,6.82,29.26,250.05,17.24,7.63,15.6,14.4,51.6,18.41,12.22,22.72,83.45,7.25,563.25,110.42,9.5,28.46,1280.81,4.74,2.69,92.52,48.53,59.34,51.49,452.16,5.29,21.05,22.01,39.57,103.23,256.53,55.23,54.55,2.01,4.68,16.8,25.12,48.1,53.25,68.34,44.71,467.9,28.47,9.76,128.22,9.4,19.98,22.33,391.7,41.13,16.11,8,6.37,9.48,60.79,59.11,3.65,50.55,313.3,30.19,49.15,13.43,11.72,59.74,1.39,276.9,78.36,23.12,3.5,5.83,38.67,51.67,149.39,12.18,9.58,16.61,318.97,333.3,101.21,229.55,280.16,176.88,3.04,24.31,6.46,122.7,10.14,76.48,5.86,399.54,485.76,40.56,440.31,10.75,56.8,135.56,13.56,82.87,7.12,60.7,3.16,376.57,25.14,7.86,37.34,4.27,44.15,326.65,38.59,43.8,355.8,4.33,5.85,25.76,4.59,92.3,384.85,19.55,38.25,92.92,135.11,33.78,127.84,16.04,103.07,169.75,39.75,103.37,42.57,22.46,9.88,10.82,101.43,21.3,70.79,9.58,22.17,7.12,74.52,2.64,128.87,17.4,5.75,5.08,138.96,4.84,41.65,1019.53,28.07,37.13,78.85,38.62,45.08,53.31,58.85,55.71,131.35,4.71,60.57,56.96,9.65,6.76,49.72,2.35,28.57,4.52,4.47,85.41,218.12,103.39,320.96,25.49,53.87,3321.02,216.18,12307.16,93.23,79.58,8.36,6.34,103.31,17.68,8.44,69.36,4829.86,46.6,56.68,45.32,39.1,5.95,16.76,313.83,211.52,477.87,6.01,26.52,351.53,46.94,7.34,85.22,145.35,4.13,12.28,6.46,3.92,17.84,4.21,3243.48,50.42,36.08,125.13,15.5,67.23,765.56,4.72,83.59,52.46,54.12,5.09,19.57,34.87,9.71,14.44,249.01,30.49,5.6,5.68,22.08,203.73,9.63,4.62,10.43,188.67,114.33,6.13,168.62,279.35,8.04,282.72,5.31,9.2,112.01,21.37,2.28,57.59,5.6,53.73,51.14,26.1,18.62,64.67,141.2,8.29,25.77,47.96,51.98,218.49,256.76,92.7,3.13,3.16,100.65,23.72,235.32,379.34,3.98,377.27,137.33,63.07,9.83,85.55,10.38,14.35,4.84,2.17,29.77,63.17,30.14,101.83,47.92,47.84,5.19,11.22,44.97,58.36,11.58,9.99,7.98,22.94,107.71,267,97.01,53.48,8.16,6.85,37.84,181.35,31.61,303.81,225.35,39.35,51.56,157.13,62.94,124.59,4.5,49.79,13.65,65.35,11.39,3.93,4.17,19.23,92.45],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle KNeighborsRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predkNN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle kNN\n","# réglage des paramètre pour la gridsearch\n","n_neighbors = np.linspace(1, 100, dtype=int)\n","param_gridkNN = {'kneighborsregressor__n_neighbors': n_neighbors}\n","\n","\n","GridkNN, \\\n","BestParametreskNN, \\\n","ScoreskNN, \\\n","TotalGHGEmissions_predkNN, \\\n","figkNN = reg_modelGrid(model=KNeighborsRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=TotalGHGEmissions_train,\n","                         y_test=TotalGHGEmissions_test,\n","                         y_test_name='TotalGHGEmissions_test',\n","                         y_pred_name='TotalGHGEmissions_predkNN',\n","                         score=score,\n","                         param_grid=param_gridkNN)\n","\n","print(BestParametreskNN)\n","print(ScoreskNN)\n","figkNN.show()\n"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[385.4598821626539,335.1182328273445,351.3270989074432,368.9972494605398,364.21263725030474,364.12293619857365,366.3432795821917,369.00584852374243,370.1141819526173,370.66810943155303,375.1396096543523,373.3158861548481,372.20992182977204,373.6170088751356,374.98816226575985,373.15399358070806,374.5854086098586,375.8464641378836,376.896828681197,376.59541458562796,377.2278070415385,378.2610820583417,379.4880844496268,379.62779674523557,380.1480019131735,381.26311908189484,381.88030452847886,382.193231379316,382.21137603755267,381.4789783814785,381.932373298576,382.0861367874102,382.98679625154466,383.5738676947668,384.1091873889635,384.6461874489413,385.0882311811525,385.64422736177505,386.1109096963547,386.69810396139854,387.50994236672705,387.97021649893713,388.69011920420036,389.14408765126615,389.67028810497106,390.17869488339545,390.4986870999593,390.96973094364733,391.3916029961773,391.81294603368286]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[492.9020509848368,456.14718796775384,468.9497670301249,482.42522672708185,480.21110168251124,479.5402238644041,486.11229301568255,492.16518513679966,499.3682509142096,503.63271938464055,508.3652409937652,506.6008735877658,507.37834468836246,511.26137922602015,513.7189748048901,510.749976814089,513.301595828182,515.2026568544613,517.1297263810286,516.2117681910718,517.5428437998028,519.3078684783484,521.2604981281503,522.4851378969573,523.2100117818619,524.3644870541519,525.6716918728137,526.3259639525563,527.1617139442305,526.9716888259954,527.233536415943,527.8565275073366,529.2385838258966,530.0841797531499,531.0788675943951,532.0156637055557,532.7900546102737,533.5519116519534,534.2615686638853,535.1385425121606,536.1194771487446,536.7492506771507,537.5602953001415,538.1730724010292,538.6881948923361,539.3208568808977,539.6304190808526,540.1416832283703,540.6234220617907,541.237672774196]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[278.017713340471,214.0892776869352,233.70443078476148,255.5692721939978,248.21417281809823,248.7056485327432,246.57426614870087,245.84651191068522,240.86011299102495,237.70349947846552,241.91397831493936,240.03089872193038,237.04149897118162,235.97263852425098,236.2573497266296,235.55801034732715,235.86922139153515,236.49027142130578,236.6639309813654,236.97906098018422,236.91277028327417,237.21429563833493,237.7156707711033,236.7704555935138,237.08599204448512,238.16175110963775,238.08891718414407,238.06049880607566,237.26103813087485,235.98626793696155,236.631210181209,236.31574606748381,236.73500867719272,237.06355563638377,237.13950718353195,237.27671119232699,237.38640775203126,237.73654307159669,237.96025072882415,238.25766541063646,238.90040758470943,239.1911823207235,239.81994310825914,240.1151029015031,240.65238131760597,241.03653288589322,241.36695511906595,241.7977786589244,242.15978393056386,242.38821929316978]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[225.98204275317073,194.84348806415383,186.94393206565465,213.50913323394727,209.57866238470277,222.38011871540863,216.04379076758633,209.47826742788664,201.11921628650833,196.2101133149638,192.40663170546503,191.0360302045508,188.59373575911604,184.30342322403865,183.000714281823,183.9022129056967,181.32225712968668,180.46621040604032,179.84123884313937,178.47191276425892,177.61881655088794,177.11269673416413,176.08086771049503,172.8983668686565,173.86284211195633,175.08213899421463,174.33442745868058,173.9206417964739,173.0479858443868,172.6806071217821,172.57924820671374,172.9858419845591,172.67525889605736,172.54014791700754,172.0903274692947,171.98189418318637,171.8971434429031,171.75307277218343,171.35291027625075,171.3917956051166,171.3856786909283,171.188703982392,171.74929429102147,171.7762084079361,172.30212295715242,172.38763159356824,172.70621233062855,172.78659444148963,172.95259561321924,172.98939207918625],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[336.0216239937603,303.1211977193322,322.310107596805,322.0016543243023,313.14114871435703,285.09757207338805,285.73688104146214,286.6394647798006,282.9965764509401,285.5722614946253,301.9077187923081,298.7130218087012,294.570527569721,295.495584492298,300.65620358837714,302.54798520325926,305.0904614110763,306.90098871963005,306.6973564785553,307.6608420405215,307.940201211225,309.06263429541605,311.2498046972629,312.7156498722966,310.81333230214995,311.3550741913213,312.0309886404751,312.94226277725295,313.41130074255676,313.9476197677492,314.5735920269681,313.6302186701812,314.2906090861356,315.11049238060326,315.6991407487302,316.3530483061113,316.18349679230124,316.5866830127541,317.21207027959133,317.30314983579274,318.1541918702855,318.77807777925335,319.38466845640386,319.5087653316846,319.85367983342564,320.39650370160575,320.8314936107488,321.4493750351561,321.8133609220775,321.76562179748294],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[423.75960001421385,233.309675493842,283.92876532629305,316.7253731176783,310.4216890344507,326.6206591305759,334.64300545420065,345.7144355888776,347.3851558882991,340.5482140084481,343.223191713063,349.07293930106766,349.1302857818901,351.68572251817943,348.8347160652447,348.57403074879994,351.43663963163345,353.7391945957667,355.57856108781164,356.8382939463959,357.24743505198956,358.50415682330987,360.71762589248937,361.58312433765485,362.54540034500207,363.24749313389066,363.3547883736791,362.77064488813016,360.23082476287755,354.3572759241444,355.65345671352577,355.08222018696523,356.24180116509905,356.49278367473636,356.66960779310847,356.5608695420641,357.4345777145315,358.44676835212647,359.3289152298136,360.1890815740697,361.6480247567074,362.39711582636914,363.0144354639387,363.87234506952916,364.53910836432794,365.28750415253626,365.2352584347206,365.941955860664,366.7139783745474,367.5332303761801],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[553.8942792940757,520.1908262610691,518.8826830024898,533.5357489979717,539.7625662690623,545.7533142638955,557.4370354589743,564.4623126088969,572.7464087581081,578.1882942675805,580.4586604485715,584.3392238566335,587.3631660221868,590.5492564491115,593.3098989058386,595.3892309148305,596.9852885004282,598.6341811974851,600.1635694075113,597.2587945433529,598.062821291409,599.9662400621188,601.9718155069885,602.8038472107913,603.1571269855823,603.8764816633409,605.0000549168857,605.6073188101483,606.694715464503,607.050571512208,606.242284578089,607.6063554345426,608.4584741954251,608.8887095518394,609.541927455111,610.4285699435611,611.1800580222719,611.5340539864944,611.9406320253072,612.558865165918,613.0676510596415,613.4198295610302,614.0707809947476,614.3968152502879,614.770998935543,615.2507127979369,615.3371623349195,615.5174130717072,615.9149892297811,616.3117941138728],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[387.64186475804894,424.12597659832545,444.57000654597334,459.2143376287992,448.15911984895087,440.7630168096003,437.85568518873487,438.73476221325046,446.32355237923076,452.82166407214754,457.70184561235374,443.4182156032873,441.3918940159464,446.05105769205034,449.1392784875159,435.35650813095367,438.0923963764684,439.49174577049575,442.203417588967,442.7472296336108,445.2697611021807,446.6596823766995,447.42030844089834,448.1379954367788,450.36130782117704,452.75440742670685,454.68126325267383,455.7252886245746,457.672053373439,459.35881758150884,460.6132849675838,461.1260476608028,463.2678379150063,464.8372049496477,466.544933478573,467.9065552697836,468.7458799337547,469.9005586853171,470.7200206708108,472.0476276260959,473.2941654560724,474.0673553456409,475.23141681489017,476.166304196893,476.8855304344062,477.57112217133005,478.3833087887789,479.15331630921963,479.5630908412613,480.4646918016925],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle kNN en fonction de n neighbors"},"xaxis":{"title":{"text":"n neighbors"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE kNN pour tout les paramètres de GridSearchCV\n","FigRMSEGRidkNN = visuRMSEGrid(KNeighborsRegressor(), 'kNN', n_neighbors,\n","                              'n neighbors', GridkNN)\n","FigRMSEGRidkNN.show()\n","if write_data is True:\n","    FigRMSEGRidkNN.write_image('./Figures/EmissionsGraphRMSEkNN.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.6 Modèle RandomForestRegressor"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                             paramètre RandomForestRegressor()\n","0  randomforestregressor__n_estimators                      21\n","1  randomforestregressor__max_features                    log2\n","                              R²        RMSE       MAE\n","RandomForestRegressor()  0.66703  307.929601  77.67689\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predRF=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[76.84428571428572,33.621904761904766,21.936190476190472,40.30142857142857,16.9047619047619,16.949523809523807,29.148095238095227,45.70714285714287,8.821428571428571,5.5509523809523795,94.11904761904763,17.947142857142854,644.9771428571428,4.765714285714285,38.331904761904745,7.303333333333332,6.122857142857144,149.56666666666655,14.838095238095239,3498.588571428571,494.4766666666666,35.28333333333334,41.36523809523809,31.955238095238094,2325.4704761904763,61.99809523809525,11.888095238095234,72.2904761904762,85.24476190476189,34.00380952380953,73.91333333333333,40.68285714285715,15.546666666666665,267.8528571428572,35.47428571428571,12.424285714285713,51.53952380952381,34.08142857142857,82.22047619047619,57.62190476190477,50.302857142857135,194.52142857142854,293.1833333333333,123.0352380952381,94.24190476190478,426.5771428571428,6.731420634920634,20.205714285714283,100.26904761904764,66.11761904761904,178.9871428571429,84.89,87.88999999999999,30.63619047619047,22.99619047619048,15.56428571428571,27.849999999999994,2085.0799999999995,142.71047619047621,48.279999999999994,28.83809523809524,79.05142857142857,59.67619047619047,18.271904761904764,143.31619047619046,23.24380952380952,28.157142857142855,12.950476190476186,59.39714285714284,60.886190476190485,21.32380952380953,9.072857142857142,10.754285714285713,35.484285714285704,61.551904761904765,43.028571428571425,24.621904761904766,11.86952380952381,49.062857142857126,100.26238095238097,17.332857142857144,27.91142857142857,26.171428571428567,8.805714285714284,21.06333333333334,87.85666666666667,71.01238095238097,25.494285714285713,5.917142857142856,41.730952380952374,72.56571428571428,17.372857142857146,31.630952380952387,50.68285714285714,497.745238095238,182.4357142857143,28.446190476190477,26.178571428571434,119.30714285714282,200.25095238095236,33.139047619047616,107.31523809523809,31.427619047619043,119.97857142857144,61.19523809523809,40.60904761904762,180.93761904761902,61.63714285714286,14.313333333333333,22.536190476190477,73.23999999999998,535.0714285714288,318.3771428571428,12.760476190476188,24.48619047619048,186.20238095238096,130.75190476190477,43.778571428571425,119.6052380952381,222.88095238095244,57.987619047619035,40.415238095238095,124.32333333333334,46.87619047619047,200.4604761904762,9.323333333333334,405.7295238095238,16.168095238095237,50.52095238095238,12.605238095238095,43.06333333333333,17.178571428571427,111.50952380952383,258.35285714285715,333.6528571428571,43.95761904761904,62.85714285714285,52.234761904761896,21.902857142857137,65.87761904761904,49.7095238095238,37.945238095238096,233.2133333333334,44.44857142857143,91.71142857142857,21.113333333333333,39.18619047619047,25.37380952380952,793.530476190476,7.358095238095238,28.643333333333334,207.9814285714286,2196.207142857143,58.071428571428584,39.78619047619046,54.76285714285715,177.81047619047618,40.60666666666667,66.50571428571429,26.182380952380953,6.409523809523811,96.99619047619048,118.67857142857143,31.997142857142848,35.69190476190475,191.02714285714285,101.02809523809522,126.99571428571429,80.25142857142856,111.59761904761903,42.36333333333334,16.755238095238095,502.8471428571429,57.43714285714285,54.40142857142856,161.39380952380952,66.87142857142857,105.0395238095238,40.84952380952381,343.6295238095238,62.41857142857142,62.62285714285714,103.65285714285714,103.78190476190474,4.655714285714285,150.2404761904762,95.11666666666667,28.307619047619045,46.82666666666667,30.50904761904762,42.941428571428574,13.117619047619046,95.11666666666667,35.06619047619047,40.28081632653062,56.61333333333334,31.873809523809523,136.68666666666664,33.27047619047619,67.84761904761905,10.64857142857143,107.50285714285717,28.473809523809532,74.90190476190476,17.060476190476194,21.44047619047619,103.41904761904763,53.45428571428571,26.415238095238106,28.36952380952381,45.77952380952381,70.89952380952379,297.60142857142864,18.264761904761905,101.13761904761907,12.768571428571429,18.180000000000003,55.06190476190477,226.71619047619052,51.06,34.691428571428574,48.18285714285713,43.990476190476194,65.35142857142857,183.3680952380952,75.39857142857142,205.42142857142855,7.061428571428574,13.285714285714283,14.777619047619044,57.46476190476191,128.02000000000004,21.596666666666668,24.197619047619053,45.93571428571428,26.078571428571433,36.05523809523808,31.213333333333335,25.050952380952385,67.92761904761907,118.75190476190475,46.44476190476192,216.02619047619044,33.93619047619048,40.889523809523816,203.06571428571428,11.834285714285715,38.64333333333333,591.092857142857,53.02333333333333,99.90904761904764,34.40714285714285,5.479642857142856,138.3061904761905,71.32523809523809,39.24142857142858,226.06095238095241,79.1090476190476,44.74714285714286,14.904285714285717,35.30238095238095,38.24285714285715,9.862857142857143,20.53142857142857,14.64,146.28714285714284,27.593809523809515,97.40000000000002,22.152380952380952,34.45095238095238,69.77380952380953,80.67095238095237,26.77476190476191,19.209999999999997,1620.945238095238,38.20095238095238,21.728571428571428,73.5495238095238,11.301428571428568,112.3857142857143,18.84,5.249523809523809,61.06619047619048,23.957142857142863,4.836190476190477,299.3995238095239,96.72333333333331,10.22904761904762,120.83809523809524,16.823809523809523,69.69095238095238,18.103333333333335,4.4576190476190485,14.825714285714284,14.473333333333331,6.225238095238095,260.83285714285716,126.41619047619047,6.604761904761905,124.1952380952381,21.971904761904767,72.05047619047618,133.92999999999998,67.58714285714285,41.72238095238096,56.97904761904764,59.44666666666667,29.92238095238095,32.94095238095239,86.3057142857143,38.950476190476174,30.55952380952381,257.6166666666667,83.96476190476191,8.845238095238095,8.55190476190476,44.00380952380952,57.38666666666665,39.8657142857143,38.24380952380952,75.06666666666665,4.955714285714286,13.060476190476193,20.48428571428571,314.5338095238095,18.834285714285713,63.53428571428573,9.519523809523808,167.7347619047619,207.23095238095235,64.66333333333334,46.04285714285715,28.90809523809524,183.8752380952381,22.032857142857143,24.25571428571428,18.763333333333335,61.40285714285714,253.12809523809523,81.31285714285715,117.50666666666666,32.29190476190475,128.47619047619048,142.50142857142856,44.300476190476196,101.06904761904765,12.168095238095239,38.100476190476186,80.89714285714285,237.79095238095238,18.445238095238093,60.425238095238086,34.12190476190477,5.1261904761904775,60.84095238095237,14.041428571428577,29.590952380952373,626.7057142857144,10.927142857142858,28.324761904761907,169.62285714285713,35.91619047619047,150.25238095238097,38.67809523809523,36.33619047619048,144.96714285714287,291.6328571428571,31.149047619047618,24.496190476190474,9.894285714285713,5.400714285714287,11.766190476190479,61.10380952380952,52.85857142857144,31.262380952380948,139.74619047619046,89.85047619047619,5.949999999999998,131.08666666666664,91.21095238095238,47.69380952380951,63.62571428571427,7.996666666666668,166.96714285714287,97.3409523809524,17,46.8804761904762,157.1942857142857,265.24619047619046,123.58952380952381,251.65000000000003,83.42809523809525,203.25476190476192,125.72761904761907,66.09904761904762,106.45904761904764,17.004285714285714,33.985238095238095,40.213333333333324,20.085238095238093,24.696666666666673,148.50809523809525,27.680952380952384,98.90809523809524,4.825238095238094,319.71428571428567,43.605714285714285,37.22428571428571,167.08380952380955,26.077619047619052,54.171428571428564,8.62095238095238,35.554761904761904,34.6752380952381,28.31619047619048,51.98809523809525,609.6466666666668,40.301428571428566,14.161428571428573,37.05428571428572,21.46047619047619,22.57904761904762,224.47380952380956,34.12714285714285,9.45142857142857,19.84952380952381,17.581428571428575,13.05571428571429,31.711428571428563,55.87809523809525,11.32714285714286,34.74190476190477,410.4333333333333,56.06714285714285,36.33761904761905,26.08904761904762,54.767142857142844,27.906666666666677,127.75642857142863,8.753809523809524,40.41333333333334,44.74571428571429,50.41,4.967619047619047,33.36000000000001,7.915714285714284,11.51190476190476,29.884285714285717,11.91047619047619,9.16095238095238,157.80333333333334,58.72523809523808,26.88428571428571,14.400476190476187,97.07952380952379,38.25809523809524,198.73714285714289,37.26095238095238,21.21380952380953,25.997619047619043,34.766190476190474,64.07000000000001,26.422857142857143,116.57714285714287,118.65,35.40857142857143,28.65352380952382,61.97571428571429,55.110476190476184,105.25333333333332,75.2047619047619,16.014285714285712,15.634761904761904,42.95190476190476,448.74047619047616,29.69238095238095,31.38619047619048,54.05,64.13285714285715,204.40190476190477,49.92285714285715,379.4004761904762,32.142857142857146,35.044761904761906,15.64619047619047,6.467619047619047,26.448095238095238,72.92238095238095,17.85761904761905,172.6238095238095,31.45619047619048,60.38095238095238,72.38476190476192,4.082857142857142,82.65333333333334,25.17809523809524,655.7633333333334,105.84904761904761,34.92714285714286,27.62285714285714,199.36952380952377,6.027619047619046,131.7042857142857,212.50047619047615,4.953809523809523,60.25142857142857,73.67999999999999,27.77666666666666,44.873333333333335,12.71714285714286,30.35809523809524,34.52095238095238,86.04000000000002,92.00666666666667,66.4,79.01666666666667,169.74285714285716,17.230952380952385,57.75,51.00761904761905,27.707619047619048,22.629047619047622,3.4385714285714277,83.26714285714286,28.605238095238104,25.266190476190477,266.6919047619048,71.39380952380955,58.339999999999996,699.9542857142858,211.9057142857143,25.529047619047613,107.38714285714289,121.06619047619047,179.30809523809526,726.5380952380954,17.00190476190476,43.506666666666675,52.80476190476191,24.581904761904763,13.701428571428576,9.080952380952382,12.409047619047618,10.434761904761904,66.63857142857144,42.36285714285714,51.44476190476191,48.74428571428571,54.81857142857143,77.09380952380954,28.18238095238096,14.300476190476193,36.760952380952375,82.21238095238095,42.920952380952386,26.267619047619043,20.70571428571429,44.68047619047619,35.81666666666667,11.162857142857145,6.081428571428573,114.43095238095239,37.735238095238095,53.57714285714283,37.71333333333333,175.44333333333338,34.109047619047615,137.24904761904762,1003.2528571428572,50.94761904761904,151.72761904761902,52.87095238095238,29.74190476190477,25.7647619047619,8.591428571428573,115.51619047619046,42.50619047619047,33.87857142857143,50.23904761904761,17.394761904761907,14.487619047619045,205.9719047619048,10.872857142857141,42.513333333333335,93.52000000000001,40.9695238095238,96.57142857142858,51.69238095238095,286.7280952380953,120.9838095238095,29.041428571428572,15.144285714285713,3116.115714285714,63.957142857142856,20.867619047619048,11.167142857142856,57.01380952380952,207.68285714285716,42.607619047619046,47.43380952380953,51.96333333333333,35.40476190476191,81.75380952380952,26.430952380952373,32.121428571428574,43.78857142857144,521.462857142857,43.73428571428571,122.53142857142863,448.05095238095237,24.620634920634924,14.784285714285716,13.523809523809518,36.6204761904762,27.34333333333333,53.25619047619047,363.9066666666667,307.91142857142853,254.2795238095238,63.23714285714285,39.76952380952381,63.81476190476191,92.86380952380951,37.45904761904763,4.014285714285715,13.215238095238094,22.793333333333333,21.892857142857142,49.645714285714284,27.800952380952385,38.83476190476191,413.9780952380953,79.09190476190477,32.676666666666655,4.917142857142857,379.2285714285713,13.417142857142855,4.224285714285715,236.65904761904767,581.7547619047618,351.62952380952385,69.74809523809523,13.251428571428571,50.78619047619048,16.07142857142857,22.144761904761907,22.872857142857146,35.910000000000004,18.261904761904766,99.15190476190476,346.28095238095233,40.56523809523811,79.37380952380951,8.923333333333332,113.02761904761907,30.64095238095238,31.57761904761906,46.56761904761905,51.228571428571435,44.87666666666668,60.536666666666676,82.45190476190476,14.99095238095238,39.33857142857143,86.65523809523808,9.563809523809525,17.40809523809524,14.510476190476194,255.577619047619,122.7257142857143,9.898095238095236,195.9609523809524,68.04476190476191,219.09095238095233,22.64619047619048,31.696666666666662,19.18190476190476,14.528888888888886,10.75095238095238,6.0423809523809515,68.07190476190476,35.9204761904762,167.55095238095242,190.30333333333328,175.2776190476191,110.0352380952381,50.63666666666666,26.474761904761905,8.832380952380955,2085.319523809523,42.461904761904755,85.11285714285715,87.4914285714286,165.14571428571426,236.10619047619053,9.193333333333333,67.5695238095238,16.66190476190476,48.716190476190484,15.74904761904762,21.379047619047622,72.69190476190477,93.49809523809525,57.26523809523809,29.529047619047613,26.98190476190476,10.597142857142856,76.02380952380953,72.12333333333332,37.832380952380944,31.038571428571426,12.794285714285715,54.895714285714284,398.5509523809525,57.00904761904762,107.20428571428573,26.886666666666677,22.377619047619053,37.79714285714286,25.33238095238095,197.57190476190476,76.49847619047621,60.58904761904763,72.39190476190477,139.98190476190473,167.59619047619043,47.534285714285716,503.2295238095238,26.39952380952381,58.75428571428571,180.46619047619046,205.9871428571429,57.67476190476191,93.65761904761904,69.13857142857141,54.30714285714285,21.868571428571432,9.693809523809522,70.46095238095238,20.742380952380948,59.12190476190476,154.03190476190477,127.63714285714282,58.12238095238095,36.388571428571424,143.98904761904762,203.3728571428571,58.56047619047619,84.77666666666669,19.33571428571429,86.90809523809523,29.352380952380962,7.78904761904762,50.53380952380952,14.086666666666666,117.5909523809524,71.14095238095238,340.44047619047615,159.75047619047623,11.127142857142854,183.68095238095233,32.04285714285715,55.2952380952381,36.39714285714285,94.78047619047618,7.797222222222222,53.02,9.135238095238096,39.295238095238105,108.38047619047623,81.10333333333331,15.463333333333331,10.214285714285712,115.29809523809523,69.8452380952381,36.06809523809525,17.6352380952381,37.642380952380954,32.103809523809524,10.75095238095238,184.9442857142857,151.3595238095238,236.40857142857143,38.71936507936508,19.973809523809525,13.246190476190472,346.91904761904766,994.0399999999998,106.13380952380956,49.57380952380952,692.8161904761904,15.97904761904762,97.14761904761905,45.31380952380953,58.95666666666666,21.32238095238095,9.18761904761905,797.6371428571427,35.98904761904762,12.849523809523811,366.39666666666676,17.40809523809524,7.5828571428571445,14.863809523809524,122.36047619047618,103.70476190476191,22.348095238095237,22.82666666666666,104.05714285714286,47.967619047619046,38.438095238095244,67.83904761904762,81.13523809523811,8.846190476190475,11.16952380952381,9.80619047619048,30.3152380952381,38.51142857142858,87.6804761904762,32.103809523809524,68.1690476190476,23.679523809523804,15.003809523809526,46.567619047619054,11.443809523809522,10.978095238095236,22.3004761904762,81.2,58.856190476190456,20.515238095238097,6.649523809523809,60.093333333333334,54.34,38.179047619047616,63.50333333333333,49.740952380952386,40.7852380952381,35.95761904761905,72.72,43.959523809523816,507.93428571428586,4.6499999999999995,43.74047619047618,18.482380952380957,69.83999999999999,38.35142857142857,22.007619047619052,45.27095238095238,29.7147619047619,19.52952380952381,22.634761904761902,11.366666666666665,97.97,272.1771428571429,22.6147619047619,19.511904761904766,21.111428571428576,49.47761904761906,96.44666666666666,168.68095238095233,372.22380952380956,39.6052380952381,67.21619047619048,39.91761904761905,4.448095238095238,29.857619047619043,31.88761904761904,20.0104761904762,125.44857142857141,33.60333333333334,16.54190476190476,31.42761904761905,4.972380952380951,72.39142857142858,29.140476190476193,17.299047619047617,23.243412698412698,22.713809523809523,55.024761904761895,350.38333333333327,56.925238095238086,23.865238095238098,116.13952380952381,40.49476190476191,65.63142857142856,22.63666666666667,96.78666666666666,39.60047619047619,49.43380952380951,6.414285714285714,13.597142857142858,127.3742857142857,448.7809523809524,172.08952380952377,37.90571428571428,103.72190476190478,26.734761904761907,7.079111111111112,347.2209523809523,35.6752380952381,193.7404761904762,328.97476190476186,35.05619047619047,29.11571428571429,48.07619047619047,509.8671428571429,63.046190476190475,336.55380952380955,32.514761904761905,146.6619047619048,463.49571428571437,90.0409523809524,80.82285714285716,28.043333333333333,70.95952380952382,17.99095238095238,41.32285714285714,37.46333333333333,19.37,144.0785714285714,217.80809523809526,303.5571428571428,187.6571428571429,50.52142857142857,5.027619047619046,87.8952380952381,183.0657142857143,34.0247619047619,7.062380952380953,17.94619047619048,24.43047619047619,10.143333333333336,76.58333333333334,401.37761904761896,56.652380952380966,35.275238095238095,58.29619047619047,21.42571428571429,4.746857142857143,27.861428571428572,80.07904761904763,88.00380952380952,9.56428571428571,12.09857142857143,30.264761904761897,25.395238095238092,16.733809523809526,43.84666666666666,100.39809523809527,63.5452380952381,72.95000000000002,171.77333333333328,346.95238095238085,97.63523809523811,115.56476190476191,128.70071428571427,18.219047619047622,39.27285714285715,162.57952380952378,25.789047619047615,2285.276666666667,164.60285714285718,72.66809523809522,25.521904761904764,13.991904761904761,252.21523809523816,15.73190476190476,46.614761904761906,130.43095238095233,119.81761904761905,31.72619047619047,80.24142857142857,180.17571428571424,10.286666666666667,276.9147619047619,72.8995238095238,559.7176190476191,51.77666666666666,18.899047619047618,14.400476190476187,27.616666666666667,224.3171428571429,58.030000000000015,78.24857142857142,43.26523809523809,889.171904761905,1100.9814285714288,219.3357142857143,16.948095238095235,8.26,21.31666666666667,22.584285714285713,115.41476190476189,6.578571428571427,102.80714285714285,75.14850793650794,17.51,307.2914285714285,40.49857142857143,29.355952380952377,47.50761904761905,49.75285714285714,63.2995238095238,24.14571428571429,53.994285714285695,32.353809523809524,107.05952380952381,31.819523809523822,556.515238095238,101.57809523809526,42.60476190476191,16.66333333333333,1276.3738095238095,26.56666666666667,3.3823809523809523,415.54333333333335,64.48619047619049,95.68523809523808,58.235238095238095,784.3638095238096,5.403809523809524,47.02285714285715,109.87809523809526,45.53285714285713,39.919999999999995,184.05476190476193,33.5052380952381,103.26619047619049,9.299999999999997,30.412452380952384,7.213809523809526,111.20428571428572,69.89761904761905,30.832857142857144,70.22047619047622,15.938095238095235,201.1095238095238,41.39,112.70333333333335,544.1228571428571,85.02380952380953,104.5090476190476,32.13142857142857,493.8404761904762,15.378095238095236,5.513333333333334,14.912857142857145,53.51047619047618,50.19428571428573,39.417142857142856,41.28476190476189,10.06761904761905,163.65619047619046,295.37000000000006,62.68095238095237,10.453333333333337,197.65904761904758,78.82238095238095,47.03666666666666,14.618095238095242,116.10142857142857,41.86619047619048,57.03285714285715,27.268095238095242,93.4204761904762,17.84380952380952,56.10809523809524,103.33428571428571,48.663333333333334,45.30238095238095,34.004285714285714,517.0833333333333,296.69428571428574,39.779047619047624,94.51095238095235,120.28857142857143,75.0142857142857,31.496190476190485,37.65571428571428,15.824761904761901,185.4304761904762,10.988571428571431,51.98809523809524,6.561904761904761,623.3657142857143,121.42809523809522,26.809047619047618,1181.0295238095237,9.919047619047621,92.87904761904764,80.12761904761908,34.66047619047619,32.345714285714294,39.12571428571429,40.07238095238094,26.90238095238095,152.73238095238096,230.01571428571424,13.787619047619046,32.94428571428572,16.506666666666668,38.88380952380954,80.13047619047617,21.869523809523816,33.85285714285714,251.2066666666667,32.47952380952382,9.63142857142857,41.753333333333345,14.762857142857142,28.868571428571425,311.5138095238096,30.599999999999994,21.300000000000004,198.93000000000004,188.79285714285714,31.270476190476188,30.973333333333326,44.209999999999994,10.09761904761905,150.13714285714283,109.03428571428573,181.42095238095246,42.269999999999996,32.45857142857143,85.95095238095239,37.23666666666667,171.04190476190473,57.06428571428571,46.059523809523796,31.561428571428575,277.3728571428572,15.29666666666667,68.6952380952381,4.216190476190477,220.0990476190476,89.12761904761906,28.284285714285726,48.641904761904776,213.50428571428571,16.920476190476194,75.83952380952381,227.87238095238098,52.665714285714294,23.641428571428573,28.820476190476192,117.75904761904765,42.17047619047619,46.01285714285715,15.463333333333331,92.66238095238093,233.54142857142853,24.56666666666667,82.49666666666667,66.5547619047619,31.025714285714283,96.78333333333335,104.34333333333333,30.663333333333338,95.03999999999999,38.16047619047619,30.240476190476194,38.107619047619046,156.8961904761905,226.41809523809528,243.12238095238095,4.679682539682542,79.20238095238095,2611.104761904761,79.38857142857144,8311.36523809524,58.833333333333336,55.78666666666667,19.66809523809523,9.135238095238094,143.25095238095238,32.24809523809523,66.28190476190476,55.54571428571429,2379.638571428571,40.56047619047619,41.67190476190478,28.82238095238095,22.573809523809526,29.299761904761898,60.27095238095237,194.73190476190476,161.38095238095235,216.2071428571429,26.044285714285714,29.560952380952376,449.31333333333333,12.06321428571429,68.86238095238097,66.40095238095238,206.49809523809523,7.305238095238094,37.61857142857143,19.89285714285714,42.42190476190475,87.10095238095238,34.84571428571428,5021.145238095238,49.58714285714285,57.99190476190478,69.39904761904761,12.325714285714287,130.6509523809524,2068.6852380952378,3.182380952380952,59.636190476190485,148.30142857142852,79.01714285714284,24.817142857142862,52.48523809523811,75.09142857142858,10.001428571428571,11.575238095238095,110.92238095238096,27.32761904761905,24.55666666666667,34.19809523809524,139.70857142857145,139.13380952380953,55.06714285714286,10.219047619047618,9.838095238095242,49.811904761904756,31.135714285714283,6.211904761904763,217.8519047619048,148.61571428571435,26.327142857142864,224.93380952380954,45.864285714285714,10.59,85.23095238095237,15.405238095238095,36.08619047619048,109.1852380952381,54.73809523809523,39.78904761904763,69.06000000000002,113.72761904761906,41.14571428571429,46.49857142857142,85.38476190476192,42.738095238095234,38.464285714285715,48.2595238095238,80.26190476190474,163.51476190476188,191.95714285714288,58.86952380952382,44.24190476190477,5.820952380952383,123.45571428571427,17.46109523809524,313.91761904761927,169.43333333333334,36.987619047619056,69.24476190476189,159.06809523809528,18.44047619047619,54.932380952380946,53.91952380952382,26.148095238095244,53.31190476190477,54.00380952380953,33.504761904761914,25.63285714285714,24.63047619047619,89.36000000000001,48.51428571428572,52.52571428571429,31.466666666666665,13.574761904761901,17.512698412698413,35.73047619047619,52.65380952380953,19.69714285714285,32.2795238095238,19.32571428571429,78.5861904761905,53.767142857142865,125.00619047619045,170.7514285714286,81.45952380952383,67.66095238095238,71.31752380952382,111.27333333333337,54.18619047619048,21.465238095238096,181.29809523809521,118.23380952380954,61.219523809523785,35.955238095238094,89.23857142857146,51.43333333333333,71.05700680272109,8.700000000000001,29.801428571428573,126.30666666666669,91.59714285714286,7.162857142857144,23.79095238095239,6.198095238095236,16.02809523809524,104.41428571428568],"xaxis":"x","y":[7.8,4.52,27.02,82.38,50.55,18.65,6.35,10.65,5.74,3.59,49.58,6.87,429.27,3.3,41.51,4.03,4.26,69.59,12.61,834.96,404.51,6.53,111.96,2.15,3768.66,531.96,20.79,19.06,25.15,10.77,166.13,111.62,3.79,292.8,25.38,5.37,167.98,11.32,34.2,40.06,22.78,72.5,355.05,238.77,13.86,184.83,39.88,8.93,37.62,26.44,97.97,8.1,69.79,64.06,7.85,50.4,68.05,3995.45,202.97,13.71,27.41,63.45,129.65,59.18,61.21,15.26,17.83,9.63,47.24,62.23,6.52,12.63,144.41,1.14,5.07,44.63,49.46,3.88,126.92,106.37,27.45,9.64,26.41,4.86,60.49,196.67,115.34,9.57,5.1,65.08,9.51,40.94,22.29,65.89,298.61,72.71,45.22,8.47,103.21,293.56,66.43,83.55,9.06,96.43,46.79,18.34,202.49,7.56,9,23.14,26.71,271.58,329.29,7.79,5.53,141.95,23.5,60.67,109.16,358.88,153.73,60.59,7.4,3.75,546.2,4.46,131.4,65.65,147.69,21.09,28.27,5.11,54.09,165.86,277.98,37.82,67.56,71.03,3.82,11.46,22.39,63.14,224.36,107.56,29.7,7.08,43.86,4.04,1597.56,4.67,4.41,26.35,866.23,284.42,73.48,78.15,325.94,129.89,47.38,38.97,7.05,60.53,161.06,20.76,6.19,147.84,69.42,134.71,54.11,67.48,11.17,0,122.99,28.52,33.75,94.89,64.57,152.04,14.05,549.74,21.75,31.04,115.04,272.54,3.96,198.45,7.84,14.37,16.66,14.19,2.57,68.14,9.29,7.74,8.45,16.88,10.63,189.18,37.97,43.57,12.36,139.27,46.72,7.74,18.85,19.76,86.02,14.05,12.97,85.88,57.03,64.22,36.09,4.68,187.92,29.39,10.81,74.45,234.75,3.52,40.12,30.87,5,70.85,698.11,64.49,1936.34,5.47,4.55,5.72,48.65,288.43,2.9,5.87,36.71,5.49,17.87,38.87,3.34,47.64,8.55,3.72,13.74,21.52,40.01,30.12,25.44,20.18,352.87,135.81,161.23,39.5,6.26,94.85,548.37,16.5,141.15,22.92,23.74,7.4,64.54,79.98,35.11,9.25,27.04,234.6,48.6,26.85,3.22,10.55,42.23,66.6,27.93,5.27,324.25,9.9,47.34,107.84,4.36,164.06,6.93,5.28,60.43,10.73,4.53,238.09,174.97,4.23,406.02,7.98,7.1,4.65,2.43,6.51,3.47,3.95,317.82,47.62,49.89,253.53,1.12,33.09,15.48,61.36,31.46,35.8,7.37,134.8,18.99,10.55,66.79,31.94,691.26,87.68,3.15,15.9,45.07,62.2,3.18,16.27,139.62,4.21,11.94,7.03,47.14,40.28,82.66,4.01,8.07,309.6,4.39,3,28.19,77.76,5.41,5.47,5.94,96.39,570.98,18.1,46.03,116.06,48.62,30.27,4.76,67.57,6.61,18.54,7.96,134.01,13.35,80.08,59.84,4.81,105.32,5.91,44.51,674.09,3.98,38.08,177.51,67.24,52.73,13.99,74.33,160.28,466.95,10.01,6.86,5.07,3.72,5.81,49.55,13.87,53.61,143.34,64.23,1.68,54.51,25.37,39.58,46.26,5.18,85.03,192.12,4.73,101.27,21.2,73.14,143.82,184.4,78.06,185.82,32.77,14.49,23.42,6.53,143.45,11.14,21.87,4.67,278.97,6.85,64.39,3.77,95.86,112.06,6.74,290.57,29.92,69.57,24.53,112.82,20.67,12.03,61.18,627.87,9.21,16.67,25.12,10.8,14.83,320.35,3.49,8.07,4.6,6.63,26.64,49.53,52.38,24.88,115.24,402.03,27.14,67.96,25.37,88.11,48.15,25.24,4.87,20.3,12.19,10.01,7.11,96.18,24.76,4.76,24.96,6.31,3.42,41.06,24.1,54.98,20.76,42.74,99.45,145.88,36.48,14.17,1.61,22.02,147.16,129.42,147.36,100.02,9.47,7.82,19.73,8.98,160.3,35.14,8.22,5.51,8.37,639.39,46.23,57.71,31.72,14.86,8.09,8.7,173.23,11.73,23.17,13.29,10.92,5.91,42.93,4.38,377.36,25.21,75.67,0,4.5,6.09,7.28,272.85,171.64,15.34,30.22,15.23,3.63,43.08,9.47,4.32,31.97,11.18,11.24,13.76,6.12,32.87,46.7,30.1,104.17,91.49,65.27,211.49,6.73,49.04,41.27,13.33,19.11,0.76,12.06,56.63,18.74,443.73,44.19,24.56,290.88,119.2,26.34,47.78,157.07,95.26,1623.34,24.51,6.16,38.55,7.52,4.6,5.47,6.69,5.02,23.79,7.07,49.25,43.91,8.1,0,103.19,18.1,61.22,36.24,14.65,28.03,10.01,39.54,7.05,3.09,3.5,9.24,27.13,9.68,49.32,66.39,8.2,123.99,111.69,19.12,142.39,47.71,67.27,10.89,39.57,17.74,78.39,25.94,180.38,14.49,14.73,5.32,4.39,52.09,205.45,19.7,8.29,89.02,177.59,20.65,26.94,20.19,10780.64,13.84,8.45,2.73,8.81,332.63,16.67,29.39,6.18,6.16,67.65,31.52,22.97,8.04,509.69,13.18,26.39,277.98,61.33,4.48,72.93,5.38,6.45,109.79,139.83,54.92,160.8,35.72,6.22,51.93,93.87,6.04,3.72,11.79,18.95,3.53,50.57,0.75,150.69,398.56,113.3,5.67,4.52,61.46,5.69,4.32,154.4,173.47,91.53,13.91,25.39,44.26,42.1,0,12.28,25.48,28.94,22.62,172.46,23.79,156.67,16.3,243.38,33.49,57.41,16.89,62.83,7.56,18.21,79.98,19.69,27.7,48.74,6.17,26.31,18.12,119.81,63.68,23.17,105.97,9.68,13.16,18.42,25.2,6.38,6.83,33.04,4.05,269.71,2.62,121.9,282.34,142.36,310.19,79.42,3.41,3.96,723.47,24.54,151.12,7.61,237.39,200.5,6.7,29.84,5.55,65.46,5.91,17.36,6.71,15.68,9.87,5.53,61.19,4.65,11.54,32.99,34.03,2.52,5.09,50.05,81.92,47.84,66.48,24.91,174.98,36.05,7.36,60.02,111.95,0.89,28.41,67.53,16.13,9.67,913.07,38.68,17.46,85.59,19.84,31.58,36.43,52.41,16.27,12.97,21.69,66.05,12.29,68.64,303.14,208.53,54.05,66.53,191.97,119.92,20.46,28.88,12.89,119.03,8.12,4.52,65.77,5.65,100.42,34.92,359.11,154.39,6,328.17,4.01,27.37,4,17.87,17.8,44.61,12.59,38.15,184.51,19.1,59.05,4.94,23.83,126.55,4.9,4.13,6.1,42.4,6.61,177.1,321.71,104.91,3.28,11.29,4.75,139.72,479.7,21.55,124.4,271.32,6.98,12.46,43.28,25.59,4.8,5.63,208.81,15.43,21.85,122.54,3.01,3.5,64.81,150.88,128.69,32.04,48.79,124.92,56.3,71.01,26.11,7.51,6.16,4.27,4.03,7.53,5.32,241.49,61.39,4.47,61.98,2.45,8.33,7.02,5.6,17.41,58.82,14.97,39.05,87.27,60.34,192.2,36.61,26.67,51.65,85.54,1.55,16.42,81.24,116.95,3.07,3.92,11.44,243.66,36.96,14.12,14.24,16.72,35,45.6,24.87,11.97,51.24,9.29,31.79,18.35,9,61.86,4.19,1154.79,6.18,36.86,9.99,3.18,8.1,26.27,18.72,62.41,32.33,22.53,4.8,4.46,30.45,75.02,19.35,5.24,6.25,196.34,35.69,40.76,8.44,21.08,14.51,75.58,7.79,226.73,1.6,29.57,21.69,16.16,28.08,346.25,229.98,67.17,145.02,62.67,6.91,142.16,5.48,295.12,175.27,27.81,8.14,2.18,445.62,120.68,0,13.05,50.29,935.7,87.03,13.22,34.69,45.44,10.43,8.35,50.98,7.2,119.63,39.47,515.85,190.17,9.13,4.26,148.25,220.26,3.67,6.18,6.73,56.03,54.83,23.56,190.58,29.88,41.7,31.27,6.66,4.25,37.31,138.03,42.8,4.72,5.91,31.92,82.92,64.46,71.38,68.16,127.74,66.37,21.18,726.96,70.16,16.99,25.11,7.24,38.55,121.98,4.55,2089.28,138.85,11.08,12.99,3.42,130.32,15.25,3.06,384.27,73.48,11.87,188.58,99.59,17.51,712.39,62.64,180.14,47.02,3.99,6.49,2.94,44.54,47.26,20.08,9.79,2340.63,242.19,262.45,32.17,4.16,35.98,45.3,23.23,6.48,67.52,6.82,29.26,250.05,17.24,7.63,15.6,14.4,51.6,18.41,12.22,22.72,83.45,7.25,563.25,110.42,9.5,28.46,1280.81,4.74,2.69,92.52,48.53,59.34,51.49,452.16,5.29,21.05,22.01,39.57,103.23,256.53,55.23,54.55,2.01,4.68,16.8,25.12,48.1,53.25,68.34,44.71,467.9,28.47,9.76,128.22,9.4,19.98,22.33,391.7,41.13,16.11,8,6.37,9.48,60.79,59.11,3.65,50.55,313.3,30.19,49.15,13.43,11.72,59.74,1.39,276.9,78.36,23.12,3.5,5.83,38.67,51.67,149.39,12.18,9.58,16.61,318.97,333.3,101.21,229.55,280.16,176.88,3.04,24.31,6.46,122.7,10.14,76.48,5.86,399.54,485.76,40.56,440.31,10.75,56.8,135.56,13.56,82.87,7.12,60.7,3.16,376.57,25.14,7.86,37.34,4.27,44.15,326.65,38.59,43.8,355.8,4.33,5.85,25.76,4.59,92.3,384.85,19.55,38.25,92.92,135.11,33.78,127.84,16.04,103.07,169.75,39.75,103.37,42.57,22.46,9.88,10.82,101.43,21.3,70.79,9.58,22.17,7.12,74.52,2.64,128.87,17.4,5.75,5.08,138.96,4.84,41.65,1019.53,28.07,37.13,78.85,38.62,45.08,53.31,58.85,55.71,131.35,4.71,60.57,56.96,9.65,6.76,49.72,2.35,28.57,4.52,4.47,85.41,218.12,103.39,320.96,25.49,53.87,3321.02,216.18,12307.16,93.23,79.58,8.36,6.34,103.31,17.68,8.44,69.36,4829.86,46.6,56.68,45.32,39.1,5.95,16.76,313.83,211.52,477.87,6.01,26.52,351.53,46.94,7.34,85.22,145.35,4.13,12.28,6.46,3.92,17.84,4.21,3243.48,50.42,36.08,125.13,15.5,67.23,765.56,4.72,83.59,52.46,54.12,5.09,19.57,34.87,9.71,14.44,249.01,30.49,5.6,5.68,22.08,203.73,9.63,4.62,10.43,188.67,114.33,6.13,168.62,279.35,8.04,282.72,5.31,9.2,112.01,21.37,2.28,57.59,5.6,53.73,51.14,26.1,18.62,64.67,141.2,8.29,25.77,47.96,51.98,218.49,256.76,92.7,3.13,3.16,100.65,23.72,235.32,379.34,3.98,377.27,137.33,63.07,9.83,85.55,10.38,14.35,4.84,2.17,29.77,63.17,30.14,101.83,47.92,47.84,5.19,11.22,44.97,58.36,11.58,9.99,7.98,22.94,107.71,267,97.01,53.48,8.16,6.85,37.84,181.35,31.61,303.81,225.35,39.35,51.56,157.13,62.94,124.59,4.5,49.79,13.65,65.35,11.39,3.93,4.17,19.23,92.45],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle RandomForestRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predRF"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle RandomForestRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsRF = np.logspace(0, 3, 10, dtype=int)\n","param_gridRF = {\n","    'randomforestregressor__n_estimators': n_estimatorsRF,\n","    'randomforestregressor__max_features': ['auto', 'sqrt', 'log2'],\n","}\n","\n","GridRF, \\\n","BestParametresRF, \\\n","ScoresRF, \\\n","TotalGHGEmissions_predRF, \\\n","figRF = reg_modelGrid(model=RandomForestRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=TotalGHGEmissions_train.ravel(),\n","                         y_test=TotalGHGEmissions_test,\n","                         y_test_name='TotalGHGEmissions_test',\n","                         y_pred_name='TotalGHGEmissions_predRF',\n","                         score=score,\n","                         param_grid=param_gridRF)\n","\n","print(BestParametresRF)\n","print(ScoresRF)\n","figRF.show()\n"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[414.22773300531145,355.6558577111671,391.68549023563736,341.00070130969897,328.7954786793125,341.21899789032204,337.3354401844278,335.25491888425734,337.7981771601635,336.8424860844672]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[534.9891118200721,455.416776524841,503.1943832130904,451.45852230853836,458.15356178235726,458.59890352423747,457.8425911268735,460.24347473937735,459.7867835403947,459.423642372572]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[293.46635419055076,255.89493889749318,280.1765972581843,230.5428803108596,199.43739557626776,223.83909225640662,216.82828924198208,210.2663630291373,215.8095707799323,214.26132979636247]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[281.6611087548064,240.0414403901356,246.10592759031385,270.17481575886853,199.18413906938696,244.44089314183861,233.9810182350921,236.04847378192648,233.54777012378102,234.10166444433318],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[340.49054842315275,330.24321981251654,332.2526070801769,259.7003762473581,253.2129678105541,251.1180526684363,254.86252563941238,246.5880077665343,249.58505352161515,248.82157814764838],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[367.8129635899481,340.24953545646156,362.2507003968843,246.85083509059882,255.85949615045854,251.28549776065836,247.49724332667404,238.0089704782912,251.6599773503213,245.3445363858797],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[629.5190188886446,541.7513057011782,576.3491903609209,535.5125859887854,560.4832688804462,529.652270414377,542.4708970648602,549.5908631750294,546.1176859077592,543.623695273532],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[451.65502537000515,325.99378719554335,441.46902574989065,392.76489346288423,375.2375214857167,429.59827546630015,407.86551665610017,406.0382792195052,408.08039889734084,412.3209561709431],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle RF pour le paramètre<br>randomforestregressor__max_features=log2<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE RandomForestRegressor\n","# pour le meilleur paramètre max features\n","FigRMSEGRidRF = visuRMSEGrid(RandomForestRegressor(), 'RF', n_estimatorsRF,\n","                             'n estimators', GridRF, BestParametresRF,\n","                             'randomforestregressor__max_features')\n","FigRMSEGRidRF.show()\n","if write_data is True:\n","    FigRMSEGRidRF.write_image('./Figures/EmissionsGraphRMSERF.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.7 Modèle AdaboostRegressor"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                         paramètre AdaBoostRegressor()\n","0  adaboostregressor__n_estimators                   4\n","1          adaboostregressor__loss              square\n","                           R²        RMSE        MAE\n","AdaBoostRegressor()  0.549957  357.994164  95.686241\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predAB=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,1257.4069897032418,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,1328.797808526584,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,1257.4069897032418,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,508.8408510638301,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,197.96453400503773,159.90460595446586,159.90460595446586,197.96453400503773,46.89224505042958,46.89224505042958,159.90460595446586,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,1257.4069897032418,508.8408510638301,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,508.8408510638301,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,508.8408510638301,197.96453400503773,46.89224505042958,46.89224505042958,479.80317535545015,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,556.9648387096773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,479.80317535545015,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,197.96453400503773,1257.4069897032418,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,159.90460595446586,46.89224505042958,159.90460595446586,197.96453400503773,46.89224505042958,46.89224505042958,556.9648387096773,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,508.8408510638301,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,53.6792515638961,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,508.8408510638301,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,1257.4069897032418,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,508.8408510638301,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,1257.4069897032418,46.89224505042958,46.89224505042958,508.8408510638301,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,197.96453400503773,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,479.80317535545015,197.96453400503773,159.90460595446586,197.96453400503773,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,1257.4069897032418,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,508.8408510638301,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,556.9648387096773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,508.8408510638301,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,479.80317535545015,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,508.8408510638301,46.89224505042958,46.89224505042958,479.80317535545015,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,508.8408510638301,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,197.96453400503773,479.80317535545015,46.89224505042958,197.96453400503773,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,1328.797808526584,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,508.8408510638301,46.89224505042958,197.96453400503773,1257.4069897032418,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,197.96453400503773,479.80317535545015,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,508.8408510638301,46.89224505042958,46.89224505042958,197.96453400503773,479.80317535545015,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,556.9648387096773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,1257.4069897032418,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,479.80317535545015,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,159.90460595446586,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,1257.4069897032418,46.89224505042958,53.6792515638961,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,197.96453400503773,479.80317535545015,46.89224505042958,197.96453400503773,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,1257.4069897032418,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,159.90460595446586,46.89224505042958,46.89224505042958,53.6792515638961,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,1257.4069897032418,508.8408510638301,197.96453400503773,46.89224505042958,479.80317535545015,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,556.9648387096773,46.89224505042958,46.89224505042958,508.8408510638301,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,479.80317535545015,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,508.8408510638301,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,197.96453400503773,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,556.9648387096773,46.89224505042958,159.90460595446586,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,197.96453400503773,46.89224505042958,197.96453400503773,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,159.90460595446586,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,508.8408510638301,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,197.96453400503773,46.89224505042958,53.6792515638961,159.90460595446586,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,1257.4069897032418,508.8408510638301,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,556.9648387096773,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,556.9648387096773,508.8408510638301,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,508.8408510638301,159.90460595446586,46.89224505042958,46.89224505042958,1257.4069897032418,46.89224505042958,46.89224505042958,479.80317535545015,53.6792515638961,46.89224505042958,46.89224505042958,1257.4069897032418,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,479.80317535545015,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,508.8408510638301,46.89224505042958,46.89224505042958,508.8408510638301,46.89224505042958,197.96453400503773,46.89224505042958,479.80317535545015,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,1257.4069897032418,46.89224505042958,46.89224505042958,508.8408510638301,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,1257.4069897032418,197.96453400503773,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,479.80317535545015,46.89224505042958,46.89224505042958,46.89224505042958,508.8408510638301,197.96453400503773,46.89224505042958,1257.4069897032418,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,479.80317535545015,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,159.90460595446586,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,479.80317535545015,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,479.80317535545015,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,53.6792515638961,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,1257.4069897032418,46.89224505042958,9440.045,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,1328.797808526584,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,197.96453400503773,197.96453400503773,46.89224505042958,46.89224505042958,508.8408510638301,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,4345.357384615376,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,1257.4069897032418,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,159.90460595446586,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,197.96453400503773,556.9648387096773,159.90460595446586,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,197.96453400503773,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,159.90460595446586,159.90460595446586,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,53.6792515638961,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958,46.89224505042958],"xaxis":"x","y":[7.8,4.52,27.02,82.38,50.55,18.65,6.35,10.65,5.74,3.59,49.58,6.87,429.27,3.3,41.51,4.03,4.26,69.59,12.61,834.96,404.51,6.53,111.96,2.15,3768.66,531.96,20.79,19.06,25.15,10.77,166.13,111.62,3.79,292.8,25.38,5.37,167.98,11.32,34.2,40.06,22.78,72.5,355.05,238.77,13.86,184.83,39.88,8.93,37.62,26.44,97.97,8.1,69.79,64.06,7.85,50.4,68.05,3995.45,202.97,13.71,27.41,63.45,129.65,59.18,61.21,15.26,17.83,9.63,47.24,62.23,6.52,12.63,144.41,1.14,5.07,44.63,49.46,3.88,126.92,106.37,27.45,9.64,26.41,4.86,60.49,196.67,115.34,9.57,5.1,65.08,9.51,40.94,22.29,65.89,298.61,72.71,45.22,8.47,103.21,293.56,66.43,83.55,9.06,96.43,46.79,18.34,202.49,7.56,9,23.14,26.71,271.58,329.29,7.79,5.53,141.95,23.5,60.67,109.16,358.88,153.73,60.59,7.4,3.75,546.2,4.46,131.4,65.65,147.69,21.09,28.27,5.11,54.09,165.86,277.98,37.82,67.56,71.03,3.82,11.46,22.39,63.14,224.36,107.56,29.7,7.08,43.86,4.04,1597.56,4.67,4.41,26.35,866.23,284.42,73.48,78.15,325.94,129.89,47.38,38.97,7.05,60.53,161.06,20.76,6.19,147.84,69.42,134.71,54.11,67.48,11.17,0,122.99,28.52,33.75,94.89,64.57,152.04,14.05,549.74,21.75,31.04,115.04,272.54,3.96,198.45,7.84,14.37,16.66,14.19,2.57,68.14,9.29,7.74,8.45,16.88,10.63,189.18,37.97,43.57,12.36,139.27,46.72,7.74,18.85,19.76,86.02,14.05,12.97,85.88,57.03,64.22,36.09,4.68,187.92,29.39,10.81,74.45,234.75,3.52,40.12,30.87,5,70.85,698.11,64.49,1936.34,5.47,4.55,5.72,48.65,288.43,2.9,5.87,36.71,5.49,17.87,38.87,3.34,47.64,8.55,3.72,13.74,21.52,40.01,30.12,25.44,20.18,352.87,135.81,161.23,39.5,6.26,94.85,548.37,16.5,141.15,22.92,23.74,7.4,64.54,79.98,35.11,9.25,27.04,234.6,48.6,26.85,3.22,10.55,42.23,66.6,27.93,5.27,324.25,9.9,47.34,107.84,4.36,164.06,6.93,5.28,60.43,10.73,4.53,238.09,174.97,4.23,406.02,7.98,7.1,4.65,2.43,6.51,3.47,3.95,317.82,47.62,49.89,253.53,1.12,33.09,15.48,61.36,31.46,35.8,7.37,134.8,18.99,10.55,66.79,31.94,691.26,87.68,3.15,15.9,45.07,62.2,3.18,16.27,139.62,4.21,11.94,7.03,47.14,40.28,82.66,4.01,8.07,309.6,4.39,3,28.19,77.76,5.41,5.47,5.94,96.39,570.98,18.1,46.03,116.06,48.62,30.27,4.76,67.57,6.61,18.54,7.96,134.01,13.35,80.08,59.84,4.81,105.32,5.91,44.51,674.09,3.98,38.08,177.51,67.24,52.73,13.99,74.33,160.28,466.95,10.01,6.86,5.07,3.72,5.81,49.55,13.87,53.61,143.34,64.23,1.68,54.51,25.37,39.58,46.26,5.18,85.03,192.12,4.73,101.27,21.2,73.14,143.82,184.4,78.06,185.82,32.77,14.49,23.42,6.53,143.45,11.14,21.87,4.67,278.97,6.85,64.39,3.77,95.86,112.06,6.74,290.57,29.92,69.57,24.53,112.82,20.67,12.03,61.18,627.87,9.21,16.67,25.12,10.8,14.83,320.35,3.49,8.07,4.6,6.63,26.64,49.53,52.38,24.88,115.24,402.03,27.14,67.96,25.37,88.11,48.15,25.24,4.87,20.3,12.19,10.01,7.11,96.18,24.76,4.76,24.96,6.31,3.42,41.06,24.1,54.98,20.76,42.74,99.45,145.88,36.48,14.17,1.61,22.02,147.16,129.42,147.36,100.02,9.47,7.82,19.73,8.98,160.3,35.14,8.22,5.51,8.37,639.39,46.23,57.71,31.72,14.86,8.09,8.7,173.23,11.73,23.17,13.29,10.92,5.91,42.93,4.38,377.36,25.21,75.67,0,4.5,6.09,7.28,272.85,171.64,15.34,30.22,15.23,3.63,43.08,9.47,4.32,31.97,11.18,11.24,13.76,6.12,32.87,46.7,30.1,104.17,91.49,65.27,211.49,6.73,49.04,41.27,13.33,19.11,0.76,12.06,56.63,18.74,443.73,44.19,24.56,290.88,119.2,26.34,47.78,157.07,95.26,1623.34,24.51,6.16,38.55,7.52,4.6,5.47,6.69,5.02,23.79,7.07,49.25,43.91,8.1,0,103.19,18.1,61.22,36.24,14.65,28.03,10.01,39.54,7.05,3.09,3.5,9.24,27.13,9.68,49.32,66.39,8.2,123.99,111.69,19.12,142.39,47.71,67.27,10.89,39.57,17.74,78.39,25.94,180.38,14.49,14.73,5.32,4.39,52.09,205.45,19.7,8.29,89.02,177.59,20.65,26.94,20.19,10780.64,13.84,8.45,2.73,8.81,332.63,16.67,29.39,6.18,6.16,67.65,31.52,22.97,8.04,509.69,13.18,26.39,277.98,61.33,4.48,72.93,5.38,6.45,109.79,139.83,54.92,160.8,35.72,6.22,51.93,93.87,6.04,3.72,11.79,18.95,3.53,50.57,0.75,150.69,398.56,113.3,5.67,4.52,61.46,5.69,4.32,154.4,173.47,91.53,13.91,25.39,44.26,42.1,0,12.28,25.48,28.94,22.62,172.46,23.79,156.67,16.3,243.38,33.49,57.41,16.89,62.83,7.56,18.21,79.98,19.69,27.7,48.74,6.17,26.31,18.12,119.81,63.68,23.17,105.97,9.68,13.16,18.42,25.2,6.38,6.83,33.04,4.05,269.71,2.62,121.9,282.34,142.36,310.19,79.42,3.41,3.96,723.47,24.54,151.12,7.61,237.39,200.5,6.7,29.84,5.55,65.46,5.91,17.36,6.71,15.68,9.87,5.53,61.19,4.65,11.54,32.99,34.03,2.52,5.09,50.05,81.92,47.84,66.48,24.91,174.98,36.05,7.36,60.02,111.95,0.89,28.41,67.53,16.13,9.67,913.07,38.68,17.46,85.59,19.84,31.58,36.43,52.41,16.27,12.97,21.69,66.05,12.29,68.64,303.14,208.53,54.05,66.53,191.97,119.92,20.46,28.88,12.89,119.03,8.12,4.52,65.77,5.65,100.42,34.92,359.11,154.39,6,328.17,4.01,27.37,4,17.87,17.8,44.61,12.59,38.15,184.51,19.1,59.05,4.94,23.83,126.55,4.9,4.13,6.1,42.4,6.61,177.1,321.71,104.91,3.28,11.29,4.75,139.72,479.7,21.55,124.4,271.32,6.98,12.46,43.28,25.59,4.8,5.63,208.81,15.43,21.85,122.54,3.01,3.5,64.81,150.88,128.69,32.04,48.79,124.92,56.3,71.01,26.11,7.51,6.16,4.27,4.03,7.53,5.32,241.49,61.39,4.47,61.98,2.45,8.33,7.02,5.6,17.41,58.82,14.97,39.05,87.27,60.34,192.2,36.61,26.67,51.65,85.54,1.55,16.42,81.24,116.95,3.07,3.92,11.44,243.66,36.96,14.12,14.24,16.72,35,45.6,24.87,11.97,51.24,9.29,31.79,18.35,9,61.86,4.19,1154.79,6.18,36.86,9.99,3.18,8.1,26.27,18.72,62.41,32.33,22.53,4.8,4.46,30.45,75.02,19.35,5.24,6.25,196.34,35.69,40.76,8.44,21.08,14.51,75.58,7.79,226.73,1.6,29.57,21.69,16.16,28.08,346.25,229.98,67.17,145.02,62.67,6.91,142.16,5.48,295.12,175.27,27.81,8.14,2.18,445.62,120.68,0,13.05,50.29,935.7,87.03,13.22,34.69,45.44,10.43,8.35,50.98,7.2,119.63,39.47,515.85,190.17,9.13,4.26,148.25,220.26,3.67,6.18,6.73,56.03,54.83,23.56,190.58,29.88,41.7,31.27,6.66,4.25,37.31,138.03,42.8,4.72,5.91,31.92,82.92,64.46,71.38,68.16,127.74,66.37,21.18,726.96,70.16,16.99,25.11,7.24,38.55,121.98,4.55,2089.28,138.85,11.08,12.99,3.42,130.32,15.25,3.06,384.27,73.48,11.87,188.58,99.59,17.51,712.39,62.64,180.14,47.02,3.99,6.49,2.94,44.54,47.26,20.08,9.79,2340.63,242.19,262.45,32.17,4.16,35.98,45.3,23.23,6.48,67.52,6.82,29.26,250.05,17.24,7.63,15.6,14.4,51.6,18.41,12.22,22.72,83.45,7.25,563.25,110.42,9.5,28.46,1280.81,4.74,2.69,92.52,48.53,59.34,51.49,452.16,5.29,21.05,22.01,39.57,103.23,256.53,55.23,54.55,2.01,4.68,16.8,25.12,48.1,53.25,68.34,44.71,467.9,28.47,9.76,128.22,9.4,19.98,22.33,391.7,41.13,16.11,8,6.37,9.48,60.79,59.11,3.65,50.55,313.3,30.19,49.15,13.43,11.72,59.74,1.39,276.9,78.36,23.12,3.5,5.83,38.67,51.67,149.39,12.18,9.58,16.61,318.97,333.3,101.21,229.55,280.16,176.88,3.04,24.31,6.46,122.7,10.14,76.48,5.86,399.54,485.76,40.56,440.31,10.75,56.8,135.56,13.56,82.87,7.12,60.7,3.16,376.57,25.14,7.86,37.34,4.27,44.15,326.65,38.59,43.8,355.8,4.33,5.85,25.76,4.59,92.3,384.85,19.55,38.25,92.92,135.11,33.78,127.84,16.04,103.07,169.75,39.75,103.37,42.57,22.46,9.88,10.82,101.43,21.3,70.79,9.58,22.17,7.12,74.52,2.64,128.87,17.4,5.75,5.08,138.96,4.84,41.65,1019.53,28.07,37.13,78.85,38.62,45.08,53.31,58.85,55.71,131.35,4.71,60.57,56.96,9.65,6.76,49.72,2.35,28.57,4.52,4.47,85.41,218.12,103.39,320.96,25.49,53.87,3321.02,216.18,12307.16,93.23,79.58,8.36,6.34,103.31,17.68,8.44,69.36,4829.86,46.6,56.68,45.32,39.1,5.95,16.76,313.83,211.52,477.87,6.01,26.52,351.53,46.94,7.34,85.22,145.35,4.13,12.28,6.46,3.92,17.84,4.21,3243.48,50.42,36.08,125.13,15.5,67.23,765.56,4.72,83.59,52.46,54.12,5.09,19.57,34.87,9.71,14.44,249.01,30.49,5.6,5.68,22.08,203.73,9.63,4.62,10.43,188.67,114.33,6.13,168.62,279.35,8.04,282.72,5.31,9.2,112.01,21.37,2.28,57.59,5.6,53.73,51.14,26.1,18.62,64.67,141.2,8.29,25.77,47.96,51.98,218.49,256.76,92.7,3.13,3.16,100.65,23.72,235.32,379.34,3.98,377.27,137.33,63.07,9.83,85.55,10.38,14.35,4.84,2.17,29.77,63.17,30.14,101.83,47.92,47.84,5.19,11.22,44.97,58.36,11.58,9.99,7.98,22.94,107.71,267,97.01,53.48,8.16,6.85,37.84,181.35,31.61,303.81,225.35,39.35,51.56,157.13,62.94,124.59,4.5,49.79,13.65,65.35,11.39,3.93,4.17,19.23,92.45],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle AdaBoostRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predAB"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle AdaBoostRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsAB = np.logspace(0, 2, 30, dtype=int)\n","param_gridAB = {\n","    'adaboostregressor__n_estimators': n_estimatorsAB,\n","    'adaboostregressor__loss': ['linear', 'square', 'exponential']\n","}\n","\n","GridAB, \\\n","BestParametresAB, \\\n","ScoresAB, \\\n","TotalGHGEmissions_predAB, \\\n","figAB = reg_modelGrid(model=AdaBoostRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=TotalGHGEmissions_train.ravel(),\n","                         y_test=TotalGHGEmissions_test,\n","                         y_test_name='TotalGHGEmissions_test',\n","                         y_pred_name='TotalGHGEmissions_predAB',\n","                         score=score,\n","                         param_grid=param_gridAB)\n","\n","print(BestParametresAB)\n","print(ScoresAB)\n","figAB.show()\n"]},{"cell_type":"code","execution_count":110,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[456.2177539251028,362.3011414944376,411.64774047273966,442.0752574409345,382.5046228711062,439.8523499315214,485.4465965279469,401.470986504842,404.687745644112,461.0564261812623,348.2882920114742,440.9650263461801,418.3017055380099,384.0719704353295,430.79137430393484,391.73594583000573,388.01412635195095,373.5551194353306,405.35217602831483,395.49188638493456,380.3452628237431,398.0038986663275,408.1962672807772,428.8072394445186,375.59584913839075,448.14379988366255,400.60895573409834,428.1246928429579,505.2624433080493,468.8877539585944]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[573.1577777295848,478.3710670547539,536.3430973811124,587.8433260397708,491.58067081174846,547.1166599029252,689.1923195196493,483.2310694944132,493.14595536924685,547.6649414042827,482.360818961535,527.6343264038989,537.2709603822634,488.0129786523955,555.7533956639389,492.7654247841857,496.14489720329544,483.0729177611086,496.1782028085393,503.66282362529705,482.38556516245563,491.700977416881,492.92873052583025,529.5478704920313,472.534258472025,645.4992175374175,519.2371803220572,545.7551067600281,759.520635090515,668.0578216531721]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[339.27773012062073,246.23121593412134,286.95238356436687,296.30718884209824,273.428574930464,332.5880399601176,281.7008735362445,319.71090351527084,316.22953591897715,374.44791095824195,214.21576506141338,354.2957262884613,299.33245069375647,280.1309622182635,305.8293529439308,290.70646687582575,279.88335550060646,264.0373211095526,314.52614924809035,287.32094914457207,278.30496048503056,304.30681991577393,323.46380403572414,328.0666083970059,278.6574398047565,250.78838222990765,281.9807311461396,310.4942789258878,251.0042515255835,269.7176862640166]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[569.5282899004533,209.32365183762198,270.9615700030702,224.3663724603183,249.29034042463465,497.9223805129428,190.21695843490417,380.6306638398102,368.6751482189719,371.7996109350524,180.03373156357966,384.00247172330734,277.5718883084473,273.4463039290583,267.81452251754087,304.9200435764349,259.46163537146634,222.741119727029,383.2908001394805,313.42145302053063,390.9774136999967,363.43040214515685,395.4516492075337,365.60547888500815,295.4108420537599,227.7594608672861,248.90539431993105,349.75137247622314,308.31012478773755,322.4206593119278],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[288.45855364774025,315.3968639386992,313.0153011461167,553.6059531431932,317.7602374389858,279.2285770066825,754.4455337654715,303.33822521599546,317.33616170525033,547.4110757073702,231.67474783890228,542.156425851336,552.524604534094,302.21288036860796,540.5448888972617,307.5344688562817,321.6304667129763,318.1890111984917,310.9866318911589,301.2022161331708,313.970020714122,301.19644589205484,326.002312049358,375.79857443391626,301.26015419399585,375.6631902522238,325.3470681180212,322.5677571369102,315.8407159116465,298.81821880568054],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[370.5241551437538,302.00296679619356,380.4826572799197,331.0929164960676,323.78812927121373,363.7557814387766,319.50914699871055,331.2655751301558,362.7708156310787,345.4126236078071,337.5779352721336,333.61657428200664,316.8291024576965,335.92850258105176,297.8521674696328,349.53581018870955,334.61074034669304,329.87178426328916,329.2288208526791,313.78498322267325,239.90535111560567,316.0016730984846,314.391448725791,320.60996364362603,310.0802223159772,304.10033253931374,352.26082074493166,326.31618942883097,305.872215385418,362.44212048192077],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[597.8049646835614,538.6314819599672,619.9365822055049,623.1433370002987,534.9393486525097,586.6552470004792,613.4766775440124,512.4331787984917,573.2966560866174,549.3225785753275,541.74409397975,545.1804330711107,564.000986505321,548.0378439928452,571.14402568714,575.4685734621675,556.0051250091027,521.2505009191228,562.9571841397257,562.6262582047293,543.9163297609462,548.5027263793871,536.9794591238685,600.3951532580352,542.7522439860641,777.796268350034,565.1788406675233,589.8572200778237,934.6161498269895,833.9686839502386],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[454.7728062500053,446.1507429397061,473.84259172908685,478.1677081047946,486.74505856818723,471.6997636987258,549.5846658966357,479.68728953975693,401.35994657864165,491.3362420807543,450.41095140300524,399.8692268031399,380.58194588449084,460.73432130508445,476.6012669480988,421.22083306643503,468.3626643195164,475.7231810687205,440.2974431185299,486.42452134356876,412.95719882804485,460.88824581655376,468.1564672973348,481.627027002007,428.4757831421569,555.3997474094549,511.35265482008435,552.1309250950017,661.673010628455,526.7890872432039],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle AB pour le paramètre<br>adaboostregressor__loss=square<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE AdaBoostRegressor\n","# pour le meilleur paramètre loss\n","FigRMSEGRidAB = visuRMSEGrid(AdaBoostRegressor(), 'AB', n_estimatorsAB,\n","                             'n estimators', GridAB, BestParametresAB,\n","                             'adaboostregressor__loss')\n","FigRMSEGRidAB.show()\n","if write_data is True:\n","    FigRMSEGRidAB.write_image('./Figures/EmissionsGraphRMSEAB.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.8 Modèle GradientBoostRegressor"]},{"cell_type":"code","execution_count":111,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                                 paramètre GradientBoostingRegressor()\n","0  gradientboostingregressor__n_estimators                         464\n","1          gradientboostingregressor__loss                       huber\n","                                   R²        RMSE        MAE\n","GradientBoostingRegressor()  0.787747  245.853425  68.741149\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predGB=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[44.23507676951082,15.41848941057572,59.07323539163743,30.47938105274536,23.408591607442318,27.819261730104756,24.173757241803926,48.51342584320617,21.63597977636686,11.948358841177036,48.261145149960484,22.95090440829469,447.4587101552139,9.292031022991402,33.740795336414884,18.684594491402066,12.692577987295486,103.25422849108861,40.803528615043284,877.1319514553144,264.5573974486076,19.57215549169321,31.48025907586328,50.95318370812509,2353.7932386808197,31.341120208898772,31.62971667878265,47.575570058065416,52.947779967723285,45.696019696757055,65.1134284971767,33.773208772511445,10.787608068275407,316.81171810106156,25.075699345107147,22.95090440829469,27.838750218399184,30.376595226480454,27.912615645065443,48.94356706760468,50.00057371727835,164.89752589969981,256.2662481897731,87.19561137507998,126.7274084681003,197.6842192360315,32.481087198247714,24.56313121628326,92.85272494582834,72.41015048019877,85.08662392573953,31.264737791012777,67.91571280970271,26.422768974294193,83.87764890591097,32.84701238974157,44.2512398728258,2143.9910122971132,240.42890233063312,46.61903800510036,28.872772053312065,71.94762860549028,85.02367454364494,20.78278365573456,67.07085143725939,24.128095574079232,51.41812407696762,24.173757241803926,55.30694730383619,61.72809578439476,36.40659617786904,23.281235341802592,34.62588869047455,35.47211438779939,28.18107582668402,40.744090006111,28.574025425175275,24.128095574079232,20.868204304501575,179.40447123057317,27.307077126563378,34.833706538163675,50.719369945992206,15.372827742851019,30.698236489244103,71.57403179891423,95.46132172991769,31.264737791012777,29.372908084246127,20.868204304501575,44.05076413132132,27.307077126563378,29.987205829153407,45.15378302238569,191.94501134097612,143.24312407474713,45.954215473433074,23.408591607442318,45.696019696757055,255.46241489721876,34.87309590416246,59.627209496662,23.715463520935536,93.90442572805998,47.91761320912195,46.50431819564448,197.38223434679557,45.49574536920114,27.105600180537056,34.76134061243331,44.2512398728258,302.9001253017355,268.7377762477313,25.875590375320982,10.694492294535468,178.8338460377563,70.54481529944539,23.911115406116632,106.59320495737516,115.38468590339252,53.88541693188548,35.05609937321895,40.95575276600072,26.864976191206146,203.7096142210943,15.41848941057572,262.2067938403223,29.372908084246127,43.15302096331697,24.128095574079232,27.23598834488748,16.693727115872953,63.447773947891505,234.22891155911145,211.3110901340337,36.21683726830417,63.1327794608359,46.028599260988294,15.41848941057572,35.70816483458358,43.805015656824345,20.868204304501575,211.01457392988743,28.731519058395584,60.38610786729006,22.95090440829469,28.249698220076837,20.78278365573456,228.1393009947196,12.692577987295486,25.448994947101156,102.04706149042697,1229.9090418477715,44.05076413132132,28.574025425175275,72.58582751846279,141.42270759450236,29.372908084246127,55.906171426264784,36.40659617786904,19.5428758995036,100.1430768392585,62.87008223432378,54.66018821131231,22.95090440829469,152.316277364214,76.77176857340676,77.93158081550254,73.76022448969755,120.59926620746417,31.40356955804542,40.45999667420475,352.448085194046,28.21242782674614,24.9210806889761,191.41338123152212,29.195159747436122,81.24070136627591,59.07323539163743,107.13547765930193,35.2323615724036,94.18528849453128,44.2512398728258,74.03060734239403,10.694492294535468,134.03090262943292,45.33921399887556,28.514626676144623,45.49574536920114,45.15378302238569,39.59034766399211,30.698236489244103,43.40459208752143,28.725521305301278,26.628504093855625,33.57376457487546,33.02908941450459,72.18825954586434,44.13730241168992,69.62041202788441,24.088305662347537,136.5089850288336,42.60598430739007,47.369075609504286,37.37209980805131,43.16873429772965,54.95411698692457,69.01656129430242,45.49574536920114,25.373566351686215,42.60598430739007,33.261279733168884,253.05316531506807,24.173757241803926,77.58749350703127,42.60598430739007,19.312756708501723,62.60499929894976,221.9632420421484,87.17547151103747,44.30314155225384,43.805015656824345,35.05609937321895,79.51557950996563,126.51823085954668,55.906171426264784,121.72982411398728,15.41848941057572,16.252917673925154,16.693727115872953,70.56175360365916,110.8977362130954,18.205712490775017,27.838750218399184,51.463050592921086,25.15084222135323,72.0065334275904,35.6387790897628,20.78278365573456,83.36815817518405,115.8252634463407,72.58582751846279,100.88858061415534,30.5467406091181,29.372908084246127,92.16179817021974,22.95090440829469,44.923011418510235,319.6819397635046,39.59034766399211,109.76506156884409,22.344811957417555,32.481087198247714,64.85648458674368,31.341120208898772,55.58947008535187,59.28866488963042,79.51557950996563,36.76945986702486,27.307077126563378,56.59162524523108,54.02150017330346,29.27979231050623,47.29146436126191,22.821197152204036,118.30487900021824,23.994621566228624,99.71298770639441,20.71670842437557,40.67124399618461,72.41015048019877,49.41002648246346,30.47938105274536,15.051246583711755,1230.0154330913713,35.963337993090356,24.48970252366273,78.10038786265157,13.758192038709094,104.77379109835154,20.008933508899176,23.818490759839737,30.899278663694098,45.49574536920114,20.27824662931896,273.07255637107676,65.0467775224557,15.41848941057572,83.19929071841824,30.852500817804998,45.49574536920114,20.703626672405242,12.02751582450637,26.83570676434995,24.173757241803926,12.432863633711133,304.1942995601586,73.67038822090122,15.372827742851019,37.32643814032661,27.307077126563378,59.14337461245116,85.31640812054778,52.741413182698246,14.486087890018052,43.805015656824345,28.780201996610433,35.35372295638373,25.448994947101156,52.979627415644835,49.856440341591885,29.44879969452297,261.62095626538974,144.47621106387172,12.204376837248905,15.372827742851019,45.838531531788526,72.0065334275904,47.82303358853596,34.70694875343779,45.49574536920114,10.694492294535468,18.605138107228186,27.421331582506063,120.69567284719244,19.5428758995036,49.29259221188736,10.694492294535468,121.56634157352562,117.5663534835647,73.13286417438971,56.914733343584025,31.402232565810323,67.94257689309018,28.83412548730965,15.41848941057572,23.408591607442318,59.07323539163743,111.37115168712683,37.32643814032661,123.57364943519859,42.60598430739007,113.72075675347088,86.66166357002643,30.5467406091181,58.464858092289624,15.372827742851019,37.37209980805131,87.40868724093548,159.43872255023382,18.684594491402066,62.90652400054382,20.868204304501575,12.432863633711133,59.07323539163743,22.288257870732522,23.39882983829807,620.0913153543022,16.693727115872953,53.95592407770204,190.48661832029,22.344811957417555,129.88382165942934,18.97515596736669,40.37843405526575,104.93832772943341,112.03525265127759,44.2512398728258,19.449760125763675,15.466678749624993,11.96144059314738,26.422768974294193,55.97501751262043,30.698236489244103,48.261145149960484,77.93158081550254,80.23414504919243,32.481087198247714,116.04314941923644,78.53491458239438,30.5467406091181,37.12997844498731,20.868204304501575,100.02976499804537,66.8254136657219,15.554249513278709,27.91134381712328,66.5201654577638,127.61465747219121,206.69615519767183,88.38151444086579,83.87388975312018,248.80163579401005,49.82308566020897,53.81371725395092,86.88907629445305,12.432863633711133,39.59034766399211,35.98752968000381,37.32643814032661,15.41848941057572,74.609117167773,43.54554826547558,51.84096023046976,12.02751582450637,246.1664604042705,34.76134061243331,26.124616248546268,64.40306384126362,53.63044515254045,32.296056463922035,28.574025425175275,36.606934173689716,25.08175212023719,36.617756287292174,49.97529099550079,680.2389289384331,42.60598430739007,29.372908084246127,45.69774881042188,30.5467406091181,20.868204304501575,249.37835066360842,36.606934173689716,14.960195689707344,15.41848941057572,19.312756708501723,22.821197152204036,61.367003332438195,50.721197755451705,31.264737791012777,30.999444442568077,249.66573325020227,45.54326747175545,37.37209980805131,33.261279733168884,24.128095574079232,37.37209980805131,112.73298811688397,17.66750114306599,45.49574536920114,40.67124399618461,44.2512398728258,9.92932666017387,21.05806855060508,28.574025425175275,23.281235341802592,40.0067491182063,17.873236262627422,15.372827742851019,95.27319159292152,51.81262003871631,39.59034766399211,20.868204304501575,86.66166357002643,50.9286412610758,150.52507277052513,39.31501251983928,36.90242492707511,39.59034766399211,34.83670046287626,50.278323006593034,24.636307878604427,118.74563820934347,87.44335842370221,44.05076413132132,36.7155484275175,49.22082562350115,37.32643814032661,93.67782544924447,53.07670770169138,36.606934173689716,26.422768974294193,39.07494597236556,476.041982281395,70.56175360365916,25.448994947101156,39.83093726434597,49.22082562350115,22.04126813193066,37.10010600251963,220.3273910535854,29.76240511787181,27.811255807603164,35.35372295638373,23.418033282175102,34.76134061243331,57.88979613406773,15.372827742851019,75.86769279362849,27.195701640447012,43.506169997069684,42.72434650398559,10.694492294535468,77.55579720570258,26.124616248546268,496.3262206515146,85.80320706837243,45.15378302238569,23.362929939717624,85.31640812054778,19.5428758995036,67.56821203949647,135.50822418729078,19.449760125763675,72.41015048019877,38.04343601635774,38.745361567955364,34.49024227740403,22.845218408264586,30.07789913427302,44.455819552019285,72.58582751846279,66.87412171120577,38.48620992133264,50.20869114382517,120.87892169496823,24.5797957489634,64.56706311577183,27.69248284173132,18.26999384734453,40.95101940630645,22.714840137379575,66.70791410578718,25.448994947101156,44.923011418510235,384.12756362235285,44.2512398728258,85.60297613144367,213.78072458502282,157.56156056382918,37.49283922252372,96.94167603455358,45.63050554302588,173.20747928730768,226.6741742899836,28.514626676144623,19.5428758995036,50.9286412610758,36.23614063635039,16.693727115872953,14.653323776214116,38.66243057629847,19.449760125763675,33.773208772511445,39.74986642810567,82.0673365534257,49.0766073703385,27.912615645065443,59.07323539163743,30.999444442568077,28.574025425175275,25.10186960127227,86.66166357002643,43.15302096331697,35.39938462410843,27.307077126563378,59.07323539163743,4.345107847925637,28.574025425175275,15.372827742851019,49.29259221188736,18.908027338250086,65.84171405880934,48.261145149960484,74.96283518260829,24.070182570590397,107.38972774425638,306.37034190539043,38.2160389574973,141.59697906677675,55.498159016515835,50.721197755451705,25.37139655059255,10.787608068275407,65.35954476778947,37.151236966085044,16.693727115872953,60.51801521556869,14.75643478963509,24.399792324493177,135.50822418729078,13.672771389942096,44.2512398728258,83.48910761001454,48.56343557838944,74.35860044634538,29.83191802793501,234.22891155911145,82.14582108557256,37.49283922252372,15.372827742851019,6176.77568341378,85.92937925473558,38.75866319662402,10.787608068275407,40.40749731431493,249.38065826169426,94.18528849453128,47.97825393622123,25.550446075011273,36.919504451441334,86.66166357002643,39.61841593997543,23.159366954099962,45.49574536920114,348.05561301517423,45.89761528423055,102.04706149042697,356.8270619840057,19.5428758995036,16.693727115872953,35.03689592934193,31.336370797375515,24.173757241803926,95.46132172991769,243.60241400701707,266.34672335907794,116.7177282704044,61.93922328417937,26.14669565328841,59.07323539163743,80.06261616589255,19.449760125763675,13.589544126189363,24.68509580267196,10.694492294535468,25.83854062366031,45.49574536920114,31.064874753325128,42.44878418047902,235.8859853798724,30.383005726684416,35.03689592934193,12.112936473273368,426.5622747280002,20.78278365573456,13.593614406612762,109.63124689776485,222.2807358722373,134.9542269836345,29.160727884193438,24.173757241803926,42.72434650398559,20.868204304501575,24.683829312739547,48.09733161118131,36.02987340041201,40.45999667420475,44.539347243880876,281.77621919333166,79.82567385742733,53.88541693188548,18.684594491402066,117.63295255432693,46.814015458442945,23.911115406116632,66.98619715676989,64.50254292809056,45.33921399887556,76.54336650821013,70.19883815727653,6.6536992976155345,62.90073729196459,85.1185365960689,12.325335160431532,20.78278365573456,20.868204304501575,155.7420748644446,63.447773947891505,10.694492294535468,108.68711010827786,60.01279934972295,-41.93051220583215,15.41848941057572,27.015778903362843,17.66750114306599,27.307077126563378,27.030276561743072,12.692577987295486,45.49574536920114,32.687214297576496,130.87429113200946,153.6676080750411,186.6008841280017,44.72024114266579,83.45366193246157,25.83854062366031,19.449760125763675,2115.335919705142,14.731966065902135,44.7982765287527,39.98986706433069,132.51679876641734,212.41311420378042,21.63597977636686,36.36241443634545,16.693727115872953,48.56343557838944,26.992579396840938,31.51807908182331,45.49574536920114,82.64019648397398,47.074163932194615,25.76611274572391,29.242675691146907,17.018083308286762,40.803528615043284,88.42056759549206,19.449760125763675,35.30830248305461,25.448994947101156,36.852466845687665,246.1664604042705,77.84724001967913,80.23414504919243,48.00593010344249,30.698236489244103,28.610777569614648,21.749556682611917,89.93886046384692,28.751093131540557,30.698236489244103,29.372908084246127,82.33868462437317,55.86981587622865,70.46893643140805,494.18715761970816,37.32643814032661,69.58211107767916,64.85648458674368,114.99536011354206,39.9219411215963,60.51801521556869,36.84581108810534,47.575570058065416,31.51807908182331,13.758192038709094,91.9019144417098,36.40659617786904,41.864268175405634,37.018379936244266,43.66198346814779,52.610940036654775,37.37209980805131,50.66233612070541,184.38351986427926,37.32643814032661,132.35305659830607,18.29691778639598,53.73587240397338,25.550446075011273,15.234799691625089,20.868204304501575,24.173757241803926,99.24964624774637,72.58582751846279,353.79453972104614,70.1999014761024,18.605138107228186,74.03060734239403,30.764739922541914,39.83093726434597,15.372827742851019,49.82308566020897,11.96144059314738,48.56343557838944,23.281235341802592,74.52036555984296,70.68482272630057,74.7120734525065,27.36040593845639,15.41848941057572,83.23949745388863,69.90401027353812,27.36040593845639,24.173757241803926,24.04732821710844,15.657561881333713,27.030276561743072,136.59603180031507,106.0770550871573,49.497797729623635,17.55017549046332,42.60598430739007,18.684594491402066,207.09701723717538,470.8041781315573,82.25818150084524,50.278323006593034,496.3262206515146,22.95090440829469,47.08142731719747,53.80898739468301,72.41015048019877,25.448994947101156,24.460075930018547,325.37918960852153,41.13455681333942,29.372908084246127,157.8119822889156,20.78278365573456,19.449760125763675,37.37209980805131,127.18747397432037,56.028004311191296,12.339747859971194,19.5428758995036,94.30974415805832,51.96781418915085,26.422768974294193,48.66998850955934,32.989765497906895,12.432863633711133,20.423534152956712,16.464531647964026,39.98986706433069,23.80651441493996,62.90073729196459,37.37209980805131,17.873236262627422,40.45999667420475,20.64300230529679,45.190803830911314,28.675532127178812,24.128095574079232,23.741574378210935,94.30974415805832,70.46893643140805,37.37209980805131,18.684594491402066,27.912615645065443,81.16093996944046,40.15166249688445,28.21242782674614,48.261145149960484,24.91677951800645,30.84548723725494,120.83256925403685,42.79950895098148,362.77619950128803,19.5428758995036,19.03515948170614,22.288257870732522,45.66604247818106,56.59162524523108,37.37209980805131,29.62367365610192,17.063744976011456,50.278323006593034,29.107019003386633,18.605138107228186,56.321996194869264,44.09470850250022,51.814507056050566,30.698236489244103,38.75866319662402,42.68602165169328,93.46893887549098,41.838647634839894,67.7979779500228,23.74314203794718,64.01873305365048,46.940525193132395,10.787608068275407,50.278323006593034,43.66198346814779,24.173757241803926,205.01820194579403,28.675532127178812,29.44879969452297,27.400192900303303,10.694492294535468,48.66489225359033,19.5428758995036,30.5467406091181,53.63044515254045,22.95090440829469,42.60598430739007,253.05316531506807,22.344811957417555,44.05076413132132,73.26422866966418,51.446141515790835,29.09215956938234,19.312756708501723,42.72434650398559,37.109457972364034,39.9219411215963,17.66750114306599,18.684594491402066,85.11138861620032,336.3184639673573,159.15037371992258,32.30068390958607,71.61454476691404,23.408591607442318,11.96144059314738,248.59689961087915,35.03689592934193,169.5406234282473,197.6842192360315,30.47938105274536,43.349757736015825,35.47211438779939,469.15125747482983,66.78403243979972,312.31852026106276,39.13392855889041,139.88393196934442,43.16873429772965,108.88220324196325,27.400192900303303,33.499399203856505,57.06495390177138,46.04278202512804,47.369075609504286,59.07323539163743,29.242675691146907,84.40597085355053,325.8099758011874,278.41047854152674,104.93832772943341,65.84171405880934,12.112936473273368,37.37209980805131,95.66946349104359,37.37209980805131,3.0450504146086295,34.48789309666019,46.868146106087586,34.126342763683446,60.84210053357453,138.92898877378337,43.34680834090317,56.92724775845212,91.95718698813664,19.312756708501723,10.787608068275407,27.039984618553454,39.13428578957512,47.08142731719747,18.605138107228186,20.64300230529679,16.298579341649848,31.264737791012777,24.173757241803926,45.62115085579813,71.28160516537994,66.78403243979972,62.49234849826485,55.86981587622865,89.59020963909505,48.30671257366648,88.576450720627,112.73298811688397,28.574025425175275,37.65110167424438,110.87094110121862,23.408591607442318,337.01976776331674,198.47187042105568,47.369075609504286,35.92030770718734,18.793254244022172,135.78846100805734,21.314539825282807,27.400192900303303,75.21992838481992,71.28160516537994,36.40659617786904,45.696019696757055,184.07068198161437,19.5428758995036,72.0065334275904,45.54326747175545,252.24378429589183,74.52139434227675,27.400192900303303,20.868204304501575,20.868204304501575,325.8099758011874,45.696019696757055,57.47819635872964,41.99764600462605,464.41720792337696,916.7671980083426,243.0088976626184,27.886105200405435,18.684594491402066,40.67124399618461,20.71670842437557,113.25382979056243,3.810216048970237,49.856440341591885,33.09769493085836,59.07323539163743,245.6792801371442,45.699366170994104,22.24687818610064,45.93347784741254,45.89761528423055,25.564403824445677,43.805015656824345,36.79029977898144,24.128095574079232,49.29259221188736,46.40283115007629,348.05561301517423,76.6189569831035,73.1141406916134,42.449452937064486,1229.968467196624,19.312756708501723,13.23852173539007,66.75804877254603,200.67682878096218,45.49574536920114,45.49574536920114,838.1420720365139,9.92932666017387,44.2512398728258,114.58282578068058,48.261145149960484,47.855500878236846,245.3273170293314,27.400192900303303,65.98388310918448,28.725521305301278,26.099810556844538,11.574582225609596,103.19627008042102,42.72434650398559,37.37209980805131,63.76029950502055,20.868204304501575,211.6210488401817,27.030276561743072,88.62427285117951,337.6229070631688,39.429587620907704,145.56398658303354,27.400192900303303,396.45125351391357,26.83570676434995,10.787608068275407,23.228050177108607,29.358568177122244,47.074163932194615,56.59162524523108,39.59034766399211,18.338982681308686,16.104021822584095,274.37259632949235,37.37209980805131,15.733942457391194,70.63951518586438,48.78553038913311,37.33215616012814,15.41848941057572,108.23587460892294,44.09299693797518,28.751093131540557,10.787608068275407,77.55579720570258,29.27979231050623,73.78545624302927,85.01817391533825,43.805015656824345,28.751093131540557,17.063744976011456,365.441881220284,300.68080990354713,18.605138107228186,153.64150236757678,47.08142731719747,60.702022979970955,28.574025425175275,22.670217506209028,37.28101766699749,136.55964859874106,12.112936473273368,37.97207435824911,19.5428758995036,485.3407084684976,83.19929071841824,20.662205749173822,371.89502778791655,19.5428758995036,72.96913582606814,49.29259221188736,31.820320698477243,24.128095574079232,45.54326747175545,70.56175360365916,36.606934173689716,173.6651090620294,53.51327626120195,6.173951843321301,36.606934173689716,26.112489465766807,28.293338441708286,28.574025425175275,35.35372295638373,34.10127048471208,212.84381746996797,27.400192900303303,20.64300230529679,70.89902298697778,15.41848941057572,27.095751487431308,209.23473773293054,23.1145316351545,24.173757241803926,167.40911210690965,187.2180894540103,54.493374756940774,20.868204304501575,59.01493631368319,17.505993435763465,201.4384952833462,89.60709185969469,201.8343142501429,37.65110167424438,29.23071789610075,43.40459208752143,41.349810618137035,135.22817011826208,32.9545029267798,37.48867299345468,27.195701640447012,150.78472240460528,22.95090440829469,49.590808535141825,10.787608068275407,177.91675909646597,70.54481529944539,19.92013618426192,31.133443068136263,120.29767059649203,12.432863633711133,27.912615645065443,240.14465389274235,28.32699393502073,27.039984618553454,51.41812407696762,86.66166357002643,34.48789309666019,33.773208772511445,27.36040593845639,117.63295255432693,147.59447758135647,10.694492294535468,52.201012701671864,62.90652400054382,42.15976009138861,48.99041436710312,136.65803214810384,29.697337124486765,59.620272047564335,28.212208280487182,18.338982681308686,72.17676482157727,89.03446589949425,90.38772811859717,96.7660631548322,13.589544126189363,87.69668886279145,3089.872931897659,36.42548344189995,14979.27992284459,45.49574536920114,29.82106761980306,32.09553977097915,21.63597977636686,135.44704696873353,29.81092583057106,49.2259471130004,15.372827742851019,8428.097240052683,44.2512398728258,27.36040593845639,23.095328191277467,29.94086588065943,22.24687818610064,45.43617175176174,218.96765383313732,180.36510294992047,123.92296234171019,17.063744976011456,43.469911135149225,485.3407084684976,12.112936473273368,59.07323539163743,49.29259221188736,136.55178280144585,15.372827742851019,19.5428758995036,34.171436731792774,27.400192900303303,49.82308566020897,21.99941183433372,4793.771375869917,75.86499976663522,34.290933738857355,44.7982765287527,23.281235341802592,90.67082959925456,2115.335919705142,21.36724603973099,60.51801521556869,153.56339400308346,117.63295255432693,48.00593010344249,70.56175360365916,111.37786195918156,12.02751582450637,22.95090440829469,46.940525193132395,28.514626676144623,24.399792324493177,37.11340763118829,137.3876016634647,146.2547922948056,43.54554826547558,22.714840137379575,13.576004270290273,43.586494219222295,42.60598430739007,21.63597977636686,167.6405881314733,95.97954018321998,80.96035933389688,140.68605743265587,58.41429689285447,28.675532127178812,55.73138023538049,26.422768974294193,29.697337124486765,56.24454056863198,32.31525990779907,34.5583344043768,63.95875793623326,114.58282578068058,33.60814281324958,12.112936473273368,41.27050803534127,34.76134061243331,47.01295263237241,42.60598430739007,63.95875793623326,223.01595641153196,140.3345087251125,104.66956539956148,55.97501751262043,16.111633421937462,161.84600980426268,24.128095574079232,245.2288899552804,16.104021822584095,24.173757241803926,69.21331071926485,162.3621086564239,37.32643814032661,43.54554826547558,26.54191149220177,50.719369945992206,83.86934042802999,26.802350541998948,35.35372295638373,26.432785158333672,31.264737791012777,43.91751822525081,59.07323539163743,34.126342763683446,24.070182570590397,19.03515948170614,27.307077126563378,42.60598430739007,50.278323006593034,42.60598430739007,46.940525193132395,42.105252239640485,75.36292914993385,44.2512398728258,87.19561137507998,150.44862117515106,62.49234849826485,42.60598430739007,33.09769493085836,62.90073729196459,43.586494219222295,37.37209980805131,41.71450357733196,50.9657969865252,70.56175360365916,59.07323539163743,74.35860044634538,52.741413182698246,34.883380518546794,12.112936473273368,17.053479073792108,45.019094222565606,71.5119155857986,24.675372594427184,24.128095574079232,29.09215956938234,19.938461038043634,44.694196573016846],"xaxis":"x","y":[7.8,4.52,27.02,82.38,50.55,18.65,6.35,10.65,5.74,3.59,49.58,6.87,429.27,3.3,41.51,4.03,4.26,69.59,12.61,834.96,404.51,6.53,111.96,2.15,3768.66,531.96,20.79,19.06,25.15,10.77,166.13,111.62,3.79,292.8,25.38,5.37,167.98,11.32,34.2,40.06,22.78,72.5,355.05,238.77,13.86,184.83,39.88,8.93,37.62,26.44,97.97,8.1,69.79,64.06,7.85,50.4,68.05,3995.45,202.97,13.71,27.41,63.45,129.65,59.18,61.21,15.26,17.83,9.63,47.24,62.23,6.52,12.63,144.41,1.14,5.07,44.63,49.46,3.88,126.92,106.37,27.45,9.64,26.41,4.86,60.49,196.67,115.34,9.57,5.1,65.08,9.51,40.94,22.29,65.89,298.61,72.71,45.22,8.47,103.21,293.56,66.43,83.55,9.06,96.43,46.79,18.34,202.49,7.56,9,23.14,26.71,271.58,329.29,7.79,5.53,141.95,23.5,60.67,109.16,358.88,153.73,60.59,7.4,3.75,546.2,4.46,131.4,65.65,147.69,21.09,28.27,5.11,54.09,165.86,277.98,37.82,67.56,71.03,3.82,11.46,22.39,63.14,224.36,107.56,29.7,7.08,43.86,4.04,1597.56,4.67,4.41,26.35,866.23,284.42,73.48,78.15,325.94,129.89,47.38,38.97,7.05,60.53,161.06,20.76,6.19,147.84,69.42,134.71,54.11,67.48,11.17,0,122.99,28.52,33.75,94.89,64.57,152.04,14.05,549.74,21.75,31.04,115.04,272.54,3.96,198.45,7.84,14.37,16.66,14.19,2.57,68.14,9.29,7.74,8.45,16.88,10.63,189.18,37.97,43.57,12.36,139.27,46.72,7.74,18.85,19.76,86.02,14.05,12.97,85.88,57.03,64.22,36.09,4.68,187.92,29.39,10.81,74.45,234.75,3.52,40.12,30.87,5,70.85,698.11,64.49,1936.34,5.47,4.55,5.72,48.65,288.43,2.9,5.87,36.71,5.49,17.87,38.87,3.34,47.64,8.55,3.72,13.74,21.52,40.01,30.12,25.44,20.18,352.87,135.81,161.23,39.5,6.26,94.85,548.37,16.5,141.15,22.92,23.74,7.4,64.54,79.98,35.11,9.25,27.04,234.6,48.6,26.85,3.22,10.55,42.23,66.6,27.93,5.27,324.25,9.9,47.34,107.84,4.36,164.06,6.93,5.28,60.43,10.73,4.53,238.09,174.97,4.23,406.02,7.98,7.1,4.65,2.43,6.51,3.47,3.95,317.82,47.62,49.89,253.53,1.12,33.09,15.48,61.36,31.46,35.8,7.37,134.8,18.99,10.55,66.79,31.94,691.26,87.68,3.15,15.9,45.07,62.2,3.18,16.27,139.62,4.21,11.94,7.03,47.14,40.28,82.66,4.01,8.07,309.6,4.39,3,28.19,77.76,5.41,5.47,5.94,96.39,570.98,18.1,46.03,116.06,48.62,30.27,4.76,67.57,6.61,18.54,7.96,134.01,13.35,80.08,59.84,4.81,105.32,5.91,44.51,674.09,3.98,38.08,177.51,67.24,52.73,13.99,74.33,160.28,466.95,10.01,6.86,5.07,3.72,5.81,49.55,13.87,53.61,143.34,64.23,1.68,54.51,25.37,39.58,46.26,5.18,85.03,192.12,4.73,101.27,21.2,73.14,143.82,184.4,78.06,185.82,32.77,14.49,23.42,6.53,143.45,11.14,21.87,4.67,278.97,6.85,64.39,3.77,95.86,112.06,6.74,290.57,29.92,69.57,24.53,112.82,20.67,12.03,61.18,627.87,9.21,16.67,25.12,10.8,14.83,320.35,3.49,8.07,4.6,6.63,26.64,49.53,52.38,24.88,115.24,402.03,27.14,67.96,25.37,88.11,48.15,25.24,4.87,20.3,12.19,10.01,7.11,96.18,24.76,4.76,24.96,6.31,3.42,41.06,24.1,54.98,20.76,42.74,99.45,145.88,36.48,14.17,1.61,22.02,147.16,129.42,147.36,100.02,9.47,7.82,19.73,8.98,160.3,35.14,8.22,5.51,8.37,639.39,46.23,57.71,31.72,14.86,8.09,8.7,173.23,11.73,23.17,13.29,10.92,5.91,42.93,4.38,377.36,25.21,75.67,0,4.5,6.09,7.28,272.85,171.64,15.34,30.22,15.23,3.63,43.08,9.47,4.32,31.97,11.18,11.24,13.76,6.12,32.87,46.7,30.1,104.17,91.49,65.27,211.49,6.73,49.04,41.27,13.33,19.11,0.76,12.06,56.63,18.74,443.73,44.19,24.56,290.88,119.2,26.34,47.78,157.07,95.26,1623.34,24.51,6.16,38.55,7.52,4.6,5.47,6.69,5.02,23.79,7.07,49.25,43.91,8.1,0,103.19,18.1,61.22,36.24,14.65,28.03,10.01,39.54,7.05,3.09,3.5,9.24,27.13,9.68,49.32,66.39,8.2,123.99,111.69,19.12,142.39,47.71,67.27,10.89,39.57,17.74,78.39,25.94,180.38,14.49,14.73,5.32,4.39,52.09,205.45,19.7,8.29,89.02,177.59,20.65,26.94,20.19,10780.64,13.84,8.45,2.73,8.81,332.63,16.67,29.39,6.18,6.16,67.65,31.52,22.97,8.04,509.69,13.18,26.39,277.98,61.33,4.48,72.93,5.38,6.45,109.79,139.83,54.92,160.8,35.72,6.22,51.93,93.87,6.04,3.72,11.79,18.95,3.53,50.57,0.75,150.69,398.56,113.3,5.67,4.52,61.46,5.69,4.32,154.4,173.47,91.53,13.91,25.39,44.26,42.1,0,12.28,25.48,28.94,22.62,172.46,23.79,156.67,16.3,243.38,33.49,57.41,16.89,62.83,7.56,18.21,79.98,19.69,27.7,48.74,6.17,26.31,18.12,119.81,63.68,23.17,105.97,9.68,13.16,18.42,25.2,6.38,6.83,33.04,4.05,269.71,2.62,121.9,282.34,142.36,310.19,79.42,3.41,3.96,723.47,24.54,151.12,7.61,237.39,200.5,6.7,29.84,5.55,65.46,5.91,17.36,6.71,15.68,9.87,5.53,61.19,4.65,11.54,32.99,34.03,2.52,5.09,50.05,81.92,47.84,66.48,24.91,174.98,36.05,7.36,60.02,111.95,0.89,28.41,67.53,16.13,9.67,913.07,38.68,17.46,85.59,19.84,31.58,36.43,52.41,16.27,12.97,21.69,66.05,12.29,68.64,303.14,208.53,54.05,66.53,191.97,119.92,20.46,28.88,12.89,119.03,8.12,4.52,65.77,5.65,100.42,34.92,359.11,154.39,6,328.17,4.01,27.37,4,17.87,17.8,44.61,12.59,38.15,184.51,19.1,59.05,4.94,23.83,126.55,4.9,4.13,6.1,42.4,6.61,177.1,321.71,104.91,3.28,11.29,4.75,139.72,479.7,21.55,124.4,271.32,6.98,12.46,43.28,25.59,4.8,5.63,208.81,15.43,21.85,122.54,3.01,3.5,64.81,150.88,128.69,32.04,48.79,124.92,56.3,71.01,26.11,7.51,6.16,4.27,4.03,7.53,5.32,241.49,61.39,4.47,61.98,2.45,8.33,7.02,5.6,17.41,58.82,14.97,39.05,87.27,60.34,192.2,36.61,26.67,51.65,85.54,1.55,16.42,81.24,116.95,3.07,3.92,11.44,243.66,36.96,14.12,14.24,16.72,35,45.6,24.87,11.97,51.24,9.29,31.79,18.35,9,61.86,4.19,1154.79,6.18,36.86,9.99,3.18,8.1,26.27,18.72,62.41,32.33,22.53,4.8,4.46,30.45,75.02,19.35,5.24,6.25,196.34,35.69,40.76,8.44,21.08,14.51,75.58,7.79,226.73,1.6,29.57,21.69,16.16,28.08,346.25,229.98,67.17,145.02,62.67,6.91,142.16,5.48,295.12,175.27,27.81,8.14,2.18,445.62,120.68,0,13.05,50.29,935.7,87.03,13.22,34.69,45.44,10.43,8.35,50.98,7.2,119.63,39.47,515.85,190.17,9.13,4.26,148.25,220.26,3.67,6.18,6.73,56.03,54.83,23.56,190.58,29.88,41.7,31.27,6.66,4.25,37.31,138.03,42.8,4.72,5.91,31.92,82.92,64.46,71.38,68.16,127.74,66.37,21.18,726.96,70.16,16.99,25.11,7.24,38.55,121.98,4.55,2089.28,138.85,11.08,12.99,3.42,130.32,15.25,3.06,384.27,73.48,11.87,188.58,99.59,17.51,712.39,62.64,180.14,47.02,3.99,6.49,2.94,44.54,47.26,20.08,9.79,2340.63,242.19,262.45,32.17,4.16,35.98,45.3,23.23,6.48,67.52,6.82,29.26,250.05,17.24,7.63,15.6,14.4,51.6,18.41,12.22,22.72,83.45,7.25,563.25,110.42,9.5,28.46,1280.81,4.74,2.69,92.52,48.53,59.34,51.49,452.16,5.29,21.05,22.01,39.57,103.23,256.53,55.23,54.55,2.01,4.68,16.8,25.12,48.1,53.25,68.34,44.71,467.9,28.47,9.76,128.22,9.4,19.98,22.33,391.7,41.13,16.11,8,6.37,9.48,60.79,59.11,3.65,50.55,313.3,30.19,49.15,13.43,11.72,59.74,1.39,276.9,78.36,23.12,3.5,5.83,38.67,51.67,149.39,12.18,9.58,16.61,318.97,333.3,101.21,229.55,280.16,176.88,3.04,24.31,6.46,122.7,10.14,76.48,5.86,399.54,485.76,40.56,440.31,10.75,56.8,135.56,13.56,82.87,7.12,60.7,3.16,376.57,25.14,7.86,37.34,4.27,44.15,326.65,38.59,43.8,355.8,4.33,5.85,25.76,4.59,92.3,384.85,19.55,38.25,92.92,135.11,33.78,127.84,16.04,103.07,169.75,39.75,103.37,42.57,22.46,9.88,10.82,101.43,21.3,70.79,9.58,22.17,7.12,74.52,2.64,128.87,17.4,5.75,5.08,138.96,4.84,41.65,1019.53,28.07,37.13,78.85,38.62,45.08,53.31,58.85,55.71,131.35,4.71,60.57,56.96,9.65,6.76,49.72,2.35,28.57,4.52,4.47,85.41,218.12,103.39,320.96,25.49,53.87,3321.02,216.18,12307.16,93.23,79.58,8.36,6.34,103.31,17.68,8.44,69.36,4829.86,46.6,56.68,45.32,39.1,5.95,16.76,313.83,211.52,477.87,6.01,26.52,351.53,46.94,7.34,85.22,145.35,4.13,12.28,6.46,3.92,17.84,4.21,3243.48,50.42,36.08,125.13,15.5,67.23,765.56,4.72,83.59,52.46,54.12,5.09,19.57,34.87,9.71,14.44,249.01,30.49,5.6,5.68,22.08,203.73,9.63,4.62,10.43,188.67,114.33,6.13,168.62,279.35,8.04,282.72,5.31,9.2,112.01,21.37,2.28,57.59,5.6,53.73,51.14,26.1,18.62,64.67,141.2,8.29,25.77,47.96,51.98,218.49,256.76,92.7,3.13,3.16,100.65,23.72,235.32,379.34,3.98,377.27,137.33,63.07,9.83,85.55,10.38,14.35,4.84,2.17,29.77,63.17,30.14,101.83,47.92,47.84,5.19,11.22,44.97,58.36,11.58,9.99,7.98,22.94,107.71,267,97.01,53.48,8.16,6.85,37.84,181.35,31.61,303.81,225.35,39.35,51.56,157.13,62.94,124.59,4.5,49.79,13.65,65.35,11.39,3.93,4.17,19.23,92.45],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle GradientBoostingRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predGB"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle GradientBoostRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsGB = np.logspace(0, 3, 10, dtype=int)\n","param_gridGB = {\n","    'gradientboostingregressor__n_estimators':\n","    n_estimatorsGB,\n","    'gradientboostingregressor__loss':\n","    ['squared_error', 'absolute_error', 'huber', 'quantile']\n","}\n","\n","GridGB, \\\n","BestParametresGB, \\\n","ScoresGB, \\\n","SiteTotalGHGEmissions_predGB, \\\n","figGB = reg_modelGrid(model=GradientBoostingRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=TotalGHGEmissions_train.ravel(),\n","                         y_test=TotalGHGEmissions_test,\n","                         y_test_name='TotalGHGEmissions_test',\n","                         y_pred_name='TotalGHGEmissions_predGB',\n","                         score=score,\n","                         param_grid=param_gridGB)\n","\n","print(BestParametresGB)\n","print(ScoresGB)\n","figGB.show()\n"]},{"cell_type":"code","execution_count":112,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[444.4803144403618,439.7735484099715,432.37320730020036,419.2877324978848,404.18690559062736,390.7527764075861,367.57602051748444,354.122683080704,336.0965169408912,345.04257253013844]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[596.623874536544,592.4482235437763,585.8040469582083,573.8795871551197,558.4698138870547,540.858326277306,514.4001928510359,475.4680877319354,449.93363108472647,456.04945015155954]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[292.33675434417955,287.0988732761667,278.9423676421923,264.6958778406499,249.90399729420005,240.6472265378662,220.751848183933,232.77727842947263,222.25940279705594,234.03569490871737]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[212.50211561885894,207.21587212666265,198.62873549176172,185.23485966018794,174.623838077966,169.06264300929868,203.1690892633569,230.05189702741006,261.1310185099881,274.2745540842942],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[377.327933437376,372.1182065740445,363.96654275704657,347.7209915737413,335.25913618450477,321.15406780988553,305.9629397495991,299.61556678951894,249.35606626580827,268.8611759564724],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[430.03482448054814,425.01627237255775,417.9601580364731,406.2712798798238,383.5827073411646,367.79813639238336,253.47281173632175,257.94168540657563,227.78393790988667,231.49935897537407],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[664.6771004635686,660.7667412865576,654.1965667762914,644.3962000379327,635.2553497657744,614.08058410027,592.8618657355555,555.7747539650125,512.7236467625198,519.90194915618],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[537.8595982014573,533.7506496900352,527.1140334394291,512.8153313377381,492.2134965837269,481.6684507260929,482.41339610258916,427.2295122150028,429.4879152562532,430.6758244783716],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle GB pour le paramètre<br>gradientboostingregressor__loss=huber<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["FigRMSEGRidGB = visuRMSEGrid(GradientBoostingRegressor(), 'GB', n_estimatorsGB,\n","                             'n estimators', GridGB, BestParametresGB,\n","                             'gradientboostingregressor__loss')\n","FigRMSEGRidGB.show()\n","if write_data is True:\n","    FigRMSEGRidGB.write_image('./Figures/EmissionGraphRMSEGB.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["## 1.2 Émissions au log"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[],"source":["TotalGHGEmissions_train_log = np.log2(1 + TotalGHGEmissions_train)\n","TotalGHGEmissions_test_log = np.log2(1 + TotalGHGEmissions_test)\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.1 Modèle LinearRegression"]},{"cell_type":"code","execution_count":114,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["r2 : 0.37202355178140745\n","rmse : 422.88356796671945\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_pred_logLR=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4.7570434747507,4.815731920705255,4.796695806575965,4.8766635318441,4.673630511866154,4.732819604467987,4.689082238639761,5.323133724489631,4.740950392239881,4.697883342280009,5.1915221437532875,4.876275817063937,14.535941172920722,4.671843091172197,4.962591533808712,4.6873034690483255,4.743544483178817,6.2686191537610725,5.080374168699029,18.27577894517977,6.1049857976548205,4.7576240855861744,4.885417672194163,4.824198538811133,3.5649944547921395,5.066609257923373,4.606360679486145,5.0473952676147285,4.856923616384212,4.992100355527991,5.1340866417962685,4.91006706086958,4.684644443613099,6.233669571020609,4.7924252733300685,4.873963072726469,4.8028988084132935,4.739986848826405,4.7363497323693045,4.64218362218892,4.690207183669509,5.583595817681372,6.413135664004171,5.085490308732204,5.316515556180095,6.165387417950379,4.912242116841746,4.743451300763628,5.3425370382778405,5.15866864421458,4.836930399735988,4.847835138536327,5.171855145316622,4.701246023722767,4.88310189325853,4.844841856885686,4.908249490921128,8.023789770326854,5.9631753377893375,4.7716878792817345,4.689403940277502,5.289182461398141,5.012659404274452,4.7376735001484045,4.820808762587833,4.771802105776983,5.031285769780491,4.69086992750602,5.214711100276714,5.258215114037526,4.973254740616486,4.776385559723238,4.838851523342757,4.709070130614008,4.826528531060815,5.0286482520318385,4.606474388510307,4.7669015880457,4.659879153383952,6.196091695104885,4.6326261003269344,4.875203962309748,4.835333643956131,4.713505548542362,4.595196972185058,4.864400586903431,5.233588045030948,4.859411361544409,4.587758686342932,4.657066356216761,4.842027714373981,4.725945153698044,4.661800947379196,4.6760282626249134,5.912960518084968,5.331160561048299,4.7176035740870494,4.769817452172936,4.867532689260953,5.486264072249426,4.6944584214784655,4.93424030453302,4.635040228743913,5.5524264006766835,5.0329241381463445,4.804382483678602,6.405894858646347,4.821577486535176,4.840383614740077,4.946785887875135,4.9306582821902065,6.896784980812887,6.395366676310809,4.643550708599584,4.792491965443889,6.852903907908592,4.797895257905465,4.738108217495543,5.502326721601646,5.4891225812161055,4.772874663310066,4.795749643396133,4.790394584926123,4.797231852374253,5.1012045470671685,4.708324002236616,8.347415123215164,4.684192502335301,4.945121962615682,4.685687856795499,4.864986883061905,4.719698741236305,5.063371044250141,6.078034350180559,6.0028112148688795,5.301832980742878,5.110071441069302,5.10317836463146,4.708309443779728,4.62314825393867,5.1021547202810265,4.754125238246283,6.788092473244472,4.8127807291625615,4.817666940158835,4.8756345217483466,4.748680856468283,4.640789636609286,6.155000309079175,4.840603365208339,4.6879481677749055,5.71240942877914,-1.2906018768331586,4.9331818290863865,4.612692744402111,4.793565137398397,5.620248514139335,4.682336056204955,5.148900089062331,4.973989910614731,4.749299728439459,5.569850046829583,4.763139888884573,4.727299470132647,4.871750338954946,6.709756885763831,5.453861664198077,4.599799996034531,5.382790503159865,5.646746639063286,4.950857536085345,4.9399762969102445,8.674092174394206,4.582017612471825,4.6997560852528135,6.077437546088292,4.735653269835968,5.085531199215333,4.797089234209197,4.723573034895632,4.900846758938885,5.315681501823528,4.917614265530656,4.879505952761848,4.797752705087362,5.39087072689825,4.81418928944953,4.683536322285685,4.852522218245137,4.678161374305331,4.7212658495062545,4.69225585421458,4.7498619948774286,4.626252923210254,4.790554086891586,4.8926910186785735,4.838054204238272,5.32582665935308,4.804218142171119,5.21954616184286,4.772376943800641,5.6199391777368115,5.006933370810121,4.761747303223154,4.716070870230126,4.952872607639256,4.8176639841938815,5.231780504489014,4.929931311856061,4.845662350715075,4.9614247867700305,4.726461187661537,6.08233702202069,4.6764912920688655,4.910274407383724,4.968829345185247,4.8624682821403455,5.298757654311772,6.318009007423273,5.35184437721832,4.700602557471362,5.037333118397395,4.802282604230406,5.039582811335949,5.331952295003924,5.143284531405379,5.1180787781188055,4.813199507275791,4.753806040801935,4.724849415167187,4.8714458185848875,5.148987698929953,4.775092245795638,4.810118653854489,5.319577480200224,4.898880221029938,4.8369586418156025,4.674085269936023,4.6409271511374595,5.227284655143722,5.0316805859293074,4.798405940863643,5.29736786704047,4.57813266937077,4.683842399207699,3.880901079677979,4.874369365650619,5.149238932761246,6.52970643900655,4.721337918277075,5.188067062798635,4.742536513917459,4.8152957466989115,5.040024019142769,4.969550375893851,4.805562131896857,5.593129535706471,5.050733989438767,5.05902684253241,4.648110392786147,4.7927181067084605,4.703280625793214,4.598675390993403,4.988405827061413,4.847333968048596,5.789384598940611,4.81579063233503,5.450056558088605,4.734427106058577,5.110987075880139,5.155069725595342,4.859308742401387,4.87853872995556,4.748456365913714,9.524605476173427,4.9179711605364975,4.780920834814277,5.182433168930806,4.719685819493049,5.750632566080653,4.856194208233554,4.858436432363578,4.802029634460162,4.938933690218011,4.816777674718349,5.873357288707002,4.712152909001189,4.712259861134538,5.499567419483261,4.793539884523298,4.94618302882205,4.667642473565397,4.673024465982417,4.759914895807117,4.69061738811839,4.816166771953008,6.736960374796672,5.028250872129261,4.703333918123987,4.612117781954657,4.729885003488345,5.259088508010391,5.218168663644157,4.68788818867167,4.6411095094886665,5.134392000426917,4.8269891861972605,4.735123056879691,4.6857446659672695,4.93441613951048,4.7179903005465045,4.830027007137826,5.768772332942297,5.141151157109404,4.665543255132195,4.801899209303049,4.901778622626286,4.742756311575872,4.63434995066492,4.5964986517008875,4.863963214514551,4.694908027943159,4.8242328188105414,4.893574697170405,5.3473009276955406,4.74518036825028,4.799006351933565,4.719568466744951,5.547408699644206,5.438521859260233,4.8457898162358175,4.7201259751769635,4.8662181685128365,5.044175598886047,4.8479353229394,4.7086844834020205,4.762170132071856,4.79789791485199,4.732235239261403,4.708059380912351,5.688436823752861,4.87640726967994,5.279873331551316,4.898504016053395,4.6778168287563355,4.726678007303794,4.7999108400202015,4.7168895409436296,4.985583025035766,5.806803541459887,4.653651761194413,5.409219447824578,4.752450061266712,4.8120969162960865,4.8107136649723214,4.876544769297041,4.889743902065836,9.90700228144459,4.7155218874689275,4.867112631041959,6.382770237911621,4.644740053964096,5.806473917077077,4.749696135915778,5.009924886843371,4.725822635393617,5.384375770381962,4.998395746048544,4.661391813193863,4.764887206807826,4.771693610682106,4.704383855229277,4.810630860330699,4.692049217330407,5.167045729226419,4.696858878064053,5.4742207851935865,4.907804147977957,5.424275842937648,5.557751388342511,4.580326651161178,5.057632018919739,4.7505498605137655,5.919946351847631,4.733641355398074,4.8034671536535365,4.709351387164034,4.76563164020166,5.431957005430349,6.777829175338686,5.777088323680182,5.338951392850415,6.1303731318701296,4.697452092881277,5.185918313117197,5.553335368699186,4.71244130412244,4.624279036247553,4.676233851276437,4.703288269770338,4.824822892421318,5.31885793299413,4.932367147317277,5.241220446858861,4.770327123766428,5.5071384281860585,4.823289066577189,4.912720606888864,5.219502879941976,4.689363170682224,4.817278366069833,4.611067572705512,4.366116836481174,4.809003299525415,4.916280540078865,5.078786743168062,12.220071766681478,4.88294841410457,4.686580254597227,5.047337465614592,4.6784994252083685,4.757631858714714,5.9137987802017316,4.60140434072629,4.668689148749451,4.727764010391796,4.862762063177807,4.750275086019074,4.772035761266321,4.679289772907714,4.85918927748554,4.800119143316826,5.671963942594557,4.771813556260814,4.724321741920551,4.82140734648548,4.770151931438898,4.6272628598910295,5.1195681620679245,4.733010779096498,4.933519190909322,5.126550425787352,4.695008324549638,4.704167729449529,4.7485363428952105,4.708126454735034,4.868006367229524,4.859664183156943,4.822907678811776,4.799417697135549,4.874258901965395,4.675627272995257,4.719318204692299,4.753462668246901,5.001844811756309,5.001357739171291,7.2121581278865685,4.969051019531543,4.929695158981607,4.6235731783138405,5.011097282363104,4.675711728430794,4.806034604810624,5.6669884625046425,4.240181217095284,4.868351703358398,4.880326071147564,5.166224489855333,4.608011847253592,4.213325266781937,4.721211625317291,4.709119974293273,4.796074681631424,4.901160632780021,4.954381674687072,4.766902612591109,4.6876084323541996,5.142040663216385,5.203210704810929,4.754798823250314,4.921243067776755,7.3089405142991914,4.951345221074073,4.577332584843213,4.7399485666865155,4.873455557311153,4.912235138801893,5.189534086080884,4.79876762845691,4.757120799442811,4.939923294588706,4.5898753775657095,4.734347801156493,4.6934953786991915,5.259203946582007,4.938669933359435,6.878436994316617,5.289396741632813,4.581102492275809,4.76740319419557,5.121109781614635,4.749312229760202,4.954441349555384,5.11281653036882,4.682915606906045,5.247301148713189,4.754803445553409,4.952646559320472,4.545895622334052,4.785961524968715,4.882104514219669,4.980907519021142,4.809907155947266,5.035315636452451,5.069903996674932,5.158199525638382,5.271060710469045,4.7648612009319,5.083113897703296,4.6377302739320925,4.833911422684075,4.68695228278191,4.607167160633675,4.17136036607639,4.7846673143837215,5.116118903609563,6.082383968862722,5.0148533051751105,4.851474396578196,6.703735434739565,5.475828884689774,4.872382344277396,5.094879131289539,4.664287258487719,5.787792917838471,5.589429435047641,4.591717865903635,4.749799257082417,4.983613355346463,4.926825958315789,4.825039692707602,4.799228683189798,4.653873136069963,4.771190030518786,4.813133192047489,5.008855010058142,5.166643472451814,4.789221746415664,4.842309554767891,4.915282048611925,4.897178025346348,4.60999870978198,4.689164174366761,4.995562898082917,5.04013062804334,4.657559984548985,4.730316299053981,4.807444569598009,4.723978452613245,4.607829218725185,4.723919719131663,4.893542195682878,4.766189228785211,4.902054564981554,5.069986847196897,5.110420502303782,4.891808016331717,5.484548163640932,8.20251867333192,5.04465935183071,5.8675182139397375,5.097793994516155,4.675130021787964,4.917353611284263,4.772041672317668,5.274979080554024,4.806578083015906,4.7213938511596005,4.869818537225886,4.850658502743044,4.737590026760567,5.2098754123983415,4.8025181985169585,5.025798037648539,5.83050470954684,5.111913211750042,4.7319489197934885,4.857033692182466,5.980975468151037,4.882727488890636,4.775323462247874,4.700358603787136,12.368513271906213,5.165500347668677,4.963603509427199,4.683675850131876,4.833349773122228,4.735267235277309,5.305448689584688,4.776863798232664,4.874160465928731,4.609094256490821,4.997350586949175,5.024772472423554,4.707470266039388,4.820421114366442,6.608619247153389,4.91698392909995,5.809468310808662,12.858392344249069,4.648940497733767,4.72306100225655,4.719497952007647,4.882951696177863,4.786141120669283,5.136529163001426,6.002456072874108,6.4873918932827115,5.370834142482985,4.836931191041167,4.812244197425167,4.8172607346064344,5.207223089022074,4.663735810833189,4.675143439848367,4.876506253312675,4.792729490538008,4.608504801953183,4.9200789966389875,4.696750242868895,5.061972155275402,5.90800580654256,4.786354722989113,4.816556834037169,4.786547587430561,7.412926113449947,4.732785679964886,4.751052071373151,5.70198090193328,7.546427498473852,5.748493644549623,4.71733526761469,4.785498820517158,4.735773127847278,4.75505033598127,4.774749058532646,4.616084957481526,4.713104090836224,4.940376339174023,5.276526194018663,7.351537615148107,5.158227863823807,4.869933545339588,4.6685081569353,5.216339506833583,4.934895095984549,4.641049335466021,5.1761872276377945,5.404159726245653,4.911248171479052,5.36974097680117,4.655978171715951,4.510567132415154,5.086048674481144,5.391878494215474,4.758952046558255,4.737948529204752,4.654316065653286,5.908159048296099,5.160429926279663,4.696858233979078,5.495249024889283,4.896931984366343,2.2947710994659776,4.8026575218144325,4.7311872872537615,4.8341713688012815,4.631769759856034,4.788611677572831,4.7648246548871835,4.954812219261663,4.674428851942004,4.803209850723313,5.564380788609713,5.188264357352137,4.992052174113955,5.1902803299075275,4.705563683982705,4.761306392062325,7.80122824827658,4.861469776062362,5.085149187716574,4.889664557742636,5.452248646147965,7.063909290510281,4.836065318893856,4.845678660708148,4.821908297196709,5.013666704249928,4.754039305444255,4.809918591575675,4.849124146792528,5.364654937938292,4.7863187699414,4.854929439207418,4.83495305143198,4.737168916552231,5.034252823473049,4.857564656431962,4.663385773852383,4.694152343434714,4.7834759492374115,4.740168440949323,5.4100795461565365,5.371031610537376,5.570949845660789,4.743148730959613,4.595046956336142,4.711197404926616,4.901705614547371,5.181081214946888,4.831910349640705,4.692518381950185,4.690065523343294,5.327660407708102,4.858091065251092,4.756462637550982,9.489108885952074,4.609048435568378,5.182115087534223,4.942965137113247,5.328691715876738,4.8926952337565535,4.866393175342286,4.668526259345478,5.14445414964425,4.8081314439274045,4.8145882236943915,5.138659439805582,4.897625767862303,4.842302017063873,4.731178734150995,4.729909832292703,5.239419337130126,4.7131455611762485,5.641926940914355,6.569553291646191,4.707444958917409,5.696063793086631,4.863497523688522,5.246462435713132,4.782132389411006,4.803768330591265,4.757231816450936,4.770459566428947,5.221400557457515,4.805806722743539,9.36248523792785,5.631827685365281,4.823814024565649,4.782447070732326,4.741217359137796,5.044981781186862,4.701033675107262,4.792129079468085,4.77120669333917,5.137639355035509,4.87344444175276,5.02253126551129,5.226435114105584,5.284340554453546,4.795104567642063,4.722160907163048,5.445337319731654,5.281134991457037,4.69693931872678,4.7773276895454195,4.813109302051653,4.752735453811852,4.788680434836918,5.343839079035083,4.816160050141889,4.914887341754828,4.724520696700386,4.992838131672311,4.762609056207955,9.07174334053328,6.6529597041283,5.667659591367516,4.772326813573936,6.781378112287095,4.86589972084719,5.158831250388582,5.114353417692978,5.255727526244102,4.7850070498044275,4.760667543572939,7.32396269506043,5.00701162644697,4.588858802568322,6.509830137485648,4.737942278544381,4.761751043899555,4.713962169735898,4.898071573403434,4.973688523536361,4.822664076894414,4.648121486220758,5.399962865478496,4.744817211815497,4.709718246016152,4.769242350466197,4.823148138318749,4.716955472765074,4.699056316184812,4.692186488860585,4.789767875904437,4.718545550892429,5.002513783653568,4.616903287706376,4.733179086377705,4.948225451998651,4.844997512564778,5.028500287632527,4.917855383692512,4.688493934472713,4.7704144382473075,5.3996745156001165,4.65940375552146,4.719716458894064,4.674075802902394,4.8321084770415474,5.132436823042986,5.03690713486034,4.679076494501347,5.0944632617237655,4.7105376077703065,4.661351526737305,5.195514707794768,4.7172907406315625,6.9138136060076345,4.745049279386816,4.505043887017358,4.875350893166078,5.3455618610254945,4.6952929559108405,4.608587603863772,4.820990445971635,4.8678981931286955,4.672636403527998,4.848093175370736,4.7271739367810195,5.0344937307194755,4.989615565014373,4.7685764173510155,4.688978058324329,4.869600918405464,5.049938889851767,5.648419781455411,4.9413906187493115,5.316011650592421,4.807505076717823,5.028325525460685,4.904228596408311,4.779008942400932,4.674292828526455,4.73103495115958,4.7739773355862765,5.122251108656049,4.920713137106727,4.732968125108304,4.718433793687694,4.790466751483512,4.679940533546381,4.652990925654521,4.575666297906285,4.7963293494006285,4.772666242965047,4.8577314799612585,5.985278139991168,4.7399987458066155,4.85860694030953,5.329868746587218,5.269465141233203,4.730966367732621,4.772416057114372,4.732986149192271,4.673360164318298,4.7956363517270315,4.830921025408083,4.746789241451923,6.157977526865871,6.254353880138162,5.821602408907237,4.844401490695414,4.818446079526165,4.6662314398879605,4.680124463370855,7.695809576551983,4.717322722198354,5.252469991219302,6.262446299979901,4.78238776707081,4.908429040177983,4.612011248584486,6.043051864632178,5.1031062706146315,6.5410115953722165,5.096861130208276,5.689673881194689,4.959023257444847,5.215842876966866,4.715468580227082,4.6393096162801255,5.211707268806422,5.014440240079206,4.664688421193632,4.812526356480067,4.735212658712871,4.602669940052849,5.377966501224531,6.059689366039228,4.728711426342592,4.9991134470110765,4.689488705401038,4.613229595908987,5.964468868940958,4.621124711406183,4.702449678440032,4.982831345928868,5.064307793180363,4.842573802397416,5.341917570421972,6.120595281158265,4.719854774166497,4.87518757257114,5.423355039639676,4.765703181148285,4.679656416636615,4.739326106439113,4.980029709464089,5.054209398125605,4.827095621260704,4.844859998036605,4.7272492645323165,4.752857726410527,4.781353114824688,4.98877386583178,4.685529520989153,5.2001651526441535,5.1732048075706185,4.76103218322157,5.448274563161145,4.6652046011445645,5.089211534939477,5.2166270440974465,4.712474304281465,4.811283895481904,5.205712744764864,4.760304543855956,11.250169158149191,6.521617405769526,4.762258355416962,4.884146596854236,4.664565238857813,6.419443462643008,4.839529325309788,4.717121674739596,4.9297082777758625,4.782588403018675,4.994684649891825,4.986909476001784,5.740730166637788,4.651990819995076,4.843794690180942,4.6806624252821765,8.275296723374101,5.284687888916753,4.615849327889736,4.753425164284671,4.754550283151548,5.616522842100735,5.004496301473604,5.02693306643945,5.070038732927741,6.725827449686431,6.846540233548881,6.460318286400796,4.647313606143248,4.7843623510778475,5.048276928222435,4.64728407273531,5.405721463442772,4.721118424821171,4.821363418025752,4.797431359357066,4.788242375141487,6.253040603805073,5.077296648934375,4.793781147537218,5.136419276756813,4.819925047070428,4.314831164337115,5.014398567402823,5.0041974540735135,4.76910182049648,4.895272617068943,4.831859996683478,6.705678129182911,5.031741326903817,5.206910678586992,4.959519862189568,7.392478428370341,4.891902815667029,4.644519851822268,7.972503270603877,5.074446058134553,4.960572048997323,4.823020114609466,13.812406828981153,4.700542446346127,5.011396516152525,5.464697210729012,5.18529067805422,4.63472962117423,6.557722675117054,4.718583809536611,4.968618346376243,4.720871551232593,4.6887332422405406,4.730917149696744,5.62731954274295,4.638714245817756,4.6160866791467265,4.933950751215574,4.6538160128235635,6.768463744925479,4.8855774221448,4.9393842429121,5.885416823429584,4.983821668567656,5.395336731869066,4.715317186627666,6.83814700372595,4.857786363684938,4.778965536399889,4.8087914108396195,4.7484539394170975,4.883377651970922,4.695659224678939,4.624560860251983,4.842869040124191,4.908342189204879,10.103898071548326,4.613480153771796,4.7563935862300575,5.355990575648807,4.930583498600042,4.861261557014384,4.806299674204609,5.178794712724825,4.6866441835647015,4.837910983597379,4.7809974920013145,5.162145064552485,4.601510139160308,5.237809731923365,4.9490148011066,5.129675930748699,4.9328383904402076,4.770839311099174,13.53147806477914,6.613767004184032,4.831160533108629,5.424902650731222,5.077976232339645,5.125007771818978,4.710983006524825,4.945913840459581,4.659829111602991,6.8806014329083744,4.687669763232922,4.642090189932238,4.753102384695635,6.942262934265019,5.402508537453739,4.731962713011331,5.363943293075493,4.7467370860847495,4.850934000864048,4.800156473441928,4.94610051787753,4.765445184179132,4.681431256507875,4.863961494620631,4.698463222755812,7.007621645552244,3.3145873083458444,4.789639712954715,4.698502303204417,4.834840921327466,4.621649940714519,4.606594437807795,4.738192131122115,4.698221260401898,5.431317294848256,4.621374911658172,4.747401073743305,5.257994998527003,4.805382884266138,4.865623332193303,4.990336660965761,4.747210724492421,4.677556020914651,6.61304204789241,5.54921474346755,4.703179845402422,4.655159904803443,4.774736294693761,4.843587344522032,5.501172738713093,5.481759272286464,6.000283146288154,4.923250071610456,4.8896239398456345,4.652803112847907,5.064941022487319,5.463302909099328,4.828347473228931,4.931395512439792,4.937603729623632,4.974800650553654,4.771666137305601,4.797899204720579,4.679987701636306,7.48241528484737,4.700836375875943,4.747307824746254,4.68434615514184,4.892553387379694,4.713110124782195,4.844065990332292,6.7321986609576765,4.720548676747513,4.642267224409591,5.0339735537402515,4.904785929726787,4.986788013944049,4.844867968590896,4.795135820943921,5.209096223031694,8.045567716927605,4.694207953981547,4.8460285881073775,5.312160565795056,5.11076303498666,4.722548274268956,5.174617310851548,4.59791318719254,4.982042893536415,4.8452291431767565,4.789755406797006,5.271105660660113,5.0912684789254,5.37787650369825,5.325778954970229,4.67694988069574,5.323149071368443,8.04339434021687,4.622711401322916,13.682614528043842,4.958028030226108,4.8763404619582476,4.803844536898261,4.74033782752347,5.718962888512681,4.84543280832278,5.093601521038762,4.700629985554954,7.462018840256334,5.050957195132548,4.698076938914399,4.7785088960538875,4.6792179049497165,4.793768646216475,5.121145210490033,6.153079277454185,5.5487044800721605,5.63323463524534,4.842407252698624,4.729616107662108,7.039321816294541,4.687576003327349,4.9310812595272955,4.892233579155337,5.619897948371667,4.705084103028017,4.6515032684860955,4.887776200662866,4.378293092672977,4.695070197438563,4.777340252738706,17.190572618254773,5.2002070690525555,4.690053409681061,5.072940518170652,4.869406515152749,5.616835827413604,7.704169366247058,4.704832356719235,4.872818342296352,5.457791773742026,5.112037341002172,4.7433550027518745,4.7656625037936,4.234297940279882,4.770914685841353,4.771116079192907,4.8352992379792825,4.68845268717227,4.7377529464495325,4.9081864966996775,5.258737797720082,5.772091241766046,4.953030417040128,4.608818980674654,4.639303740701612,4.6755628493534065,4.973466151709462,4.837903013043087,5.7033629939722035,5.384455334700085,5.07282590255128,5.677984232978853,4.964566846886818,4.8534251226395275,5.099151896691547,4.79590465216962,4.694972069222062,5.167200302736264,4.708671540852624,4.842564142042367,5.325057364177778,5.561756092758534,4.742205616914969,4.692707795492378,4.999301979873952,4.827656472818277,4.747949238124952,4.898985838413386,5.227998482148256,6.010855453031872,7.0354242510418,5.1421235040339495,4.70995909660643,4.625935998243337,5.9810604174845,4.674793229030433,6.723377585525983,4.805560072583172,4.698802094056797,4.868321704015154,6.118069625933345,4.611217414717301,4.855971535010606,4.746384899754854,4.832333326977794,5.341403154762072,4.73987297799149,4.643377236165973,4.736036498477019,4.752651454618266,4.530322613505409,4.903953393514836,4.969085969934736,4.900114961139095,4.848017547576099,4.63187602108235,4.995699473754825,4.78012138705724,4.895779249642789,4.916020456883358,5.004108419996443,5.273074683457825,4.891466467823558,5.182549190761726,5.228160087902048,5.076145925541097,4.8724286166025985,4.894490241386588,5.108721772118762,4.5785039673238845,4.719527485415585,4.88801193681037,4.694832672344445,4.762834665571766,4.944551663182798,4.732492727245812,4.784947070701192,4.751280712175284,4.688757378137569,4.768673376752236,4.424743338596569,5.3682527412102665,4.732099283316661,4.669842706016178,4.7328165632025945,4.753788853412466,4.809445440449346],"xaxis":"x","y":[3.137503523749935,2.4646682670034443,4.808385050656093,6.381629467033664,5.687900522480746,4.296457407371244,2.877744249949002,3.542258049766918,2.752748591407134,2.1984941536390834,5.6604951318885135,2.9763636357327616,8.749098443586298,2.1043366598147357,5.40973035386445,2.3305584000308026,2.3950627995175777,6.141391916170729,3.76659516172927,9.707290101958575,8.663593681954685,2.9126498648972037,6.819668183496455,1.6553518286125541,11.880218691860534,9.057883448899627,4.445594291341182,4.326249700837456,4.708739041359579,3.5570424152637194,7.3848269116030085,6.815319245777306,2.2600256559614555,8.198690585668917,4.72137265947682,2.6712933724815757,7.400708693037901,3.6229303509201767,5.137503523749935,5.359661722333877,4.571676809970931,6.199672344836364,8.475936042446387,7.905507349949873,3.8933622107638715,7.537839615916466,5.353323291162897,3.311793717753649,5.2712762591759255,4.778208576398088,6.628919373511362,3.1858665453113337,6.145473670317708,6.023698917266407,3.145677455195635,5.683696454306515,6.109569509370688,11.964503324303022,7.672213165334465,3.8787253414801053,4.8283269264148165,6.010108453474289,7.029563314432658,5.911212201558612,5.9590746011405145,4.0232553523003025,4.234961094839504,3.41006969175638,5.59215800212536,5.982537314399271,2.910732661902913,3.7687136570304847,7.183982678112831,1.0976107966264221,2.6016965164809576,5.511910748670928,5.6570683012015754,2.2868811477881614,6.999098033705607,6.746447139362815,4.83035674741922,3.411426245726465,4.77663042305105,2.550900664647523,5.942279901705763,7.626950122372515,6.862203399053224,3.4019034716079584,2.608809242675524,6.046141781644721,3.3936907641874536,5.390254956321868,4.541638734436141,6.063718640152248,8.226941966807852,6.203788453308596,5.530445354692906,3.2433644256936605,6.703349915166396,8.202417721575182,6.075318692553354,6.401732849522621,3.3305584000308026,6.606294160697713,5.5786368624717415,4.273515889702116,7.668814088419117,3.097610796626422,3.321928094887362,4.593353770980297,4.792334805706692,8.090535901003173,8.367589480412983,3.1358631653686793,2.7070829917717063,7.1593668097437355,4.614709844115208,5.946496941203293,6.783456654360239,8.491372117815171,7.273609132231856,5.944624223055033,3.070389327891398,2.2479275134435857,9.095924419998536,2.4489009511451276,7.048759311919856,6.058532970201611,7.216163813388774,4.46532151358055,4.871350840634026,2.611172380044005,5.783718557790366,7.3824943402931185,8.12401788912289,5.2787282129389395,6.099295204337775,6.1705259991768475,2.269033146455237,3.6392321632492775,4.547819956577703,6.003152448666922,7.816087658600206,6.762348815644129,4.940166750482817,3.01435529297707,5.487357715311547,2.333423733725192,10.642557179595622,2.503348735167504,2.4356285940520905,4.773468927905195,9.760270854333914,8.156942620908886,6.218781167784069,6.306517445249819,8.352882086522959,7.032211069322931,5.5963388642052,5.320845667645722,3.0089887832272546,5.943218087426284,7.3403842354553985,4.443606651475615,2.845991770664573,7.217618485351048,6.137913322088407,7.084383221650577,5.784242222057781,6.097610796626422,3.605257262939004,0,6.954079959320971,4.883620816285672,5.118941072723508,6.583308464847678,6.034963989749303,7.257764968571387,3.91169158187234,9.10522758436473,4.507794640198696,5.001802242633985,6.858478390423273,8.095608004811197,2.3103401206121505,7.6398833131998005,3.144046369616707,3.9420452599160467,4.142413437873741,3.925049964727359,1.835924074254375,6.111448698487674,3.363171077119244,3.127633279725874,3.24031432933371,4.160274831408593,3.5397791916984938,7.571221725048103,5.284292026394312,5.47800105583793,3.7398481026993275,7.1320626778543845,5.576522137920503,3.127633279725874,4.3110671022555955,4.375734538583156,6.4432751117658045,3.91169158187234,3.8042601156347384,6.4409521980296365,5.85872702377863,6.02724253597374,5.212958362728285,2.5058909297299574,7.561631630375841,4.92552476974757,3.5619370596224327,6.237448995639321,7.881113960675097,2.176322772640463,5.361768359419154,4.9941271140031285,2.584962500721156,6.166916251569974,9.4493756610678,6.0332027259372225,10.919861451442168,2.693765712217783,2.4724877714627436,2.7484612330040354,5.633721812641012,8.17707065762942,1.9634741239748859,2.7803100990433753,5.23687524536694,2.698218478224414,4.238022517825721,5.317231700222606,2.1176950426697547,5.604071323668861,3.2555007331483865,2.2387868595871168,3.881664619320345,4.493134922305505,5.357903838552492,4.9597701552114675,4.724650271732967,4.4046306842176115,8.46707564951531,7.096029876307993,7.341896820977107,5.339850002884624,2.859969548221026,6.582706526780788,9.101634319640722,4.129283016944966,7.151270288790165,4.58014548442338,4.628773595201645,3.070389327891398,6.034303767768035,6.339493737901733,5.174326515173915,3.357552004618084,4.809414444235898,7.880195728943098,5.632268215499513,4.79960542245267,2.077242998932461,3.5298209465286954,5.433960931121179,6.078951341394822,4.854494418154875,2.6484654430273142,8.345405246717796,3.4462562298895643,5.595145567990858,6.766065051474954,2.4222330006830477,7.366846735262642,2.9873208659292536,2.6507645591169022,5.940871478189196,3.552131108253784,2.4672794804599825,7.901409979999464,7.459185683748918,2.3868109464722167,8.668955876629637,3.1667154449664223,3.017921907997262,2.4982508675278257,1.7782085763980877,2.9088129077395473,2.1602748314085933,2.3074285251922473,8.316598324108684,5.603477988254004,5.669310286202474,7.991691898336964,1.0840642647884746,5.091276694365222,4.042644337408494,5.96254902292306,5.020591094773247,5.20163386116965,3.065227622775619,7.08533966935737,4.321206566969903,3.5298209465286954,6.083000565676004,5.04176864995163,9.435170178881494,6.4705368650925825,2.0531113364595623,4.078951341394822,5.525755692829486,5.9818526532897405,2.063502942306158,4.110196177754199,7.135657989368776,2.3812833725037836,3.693765712217783,3.00539998774259,5.589164236699772,5.36737106564853,6.38646609286931,2.3248106034204836,3.1811025507537978,8.27891401949994,2.430285272977781,2,4.867402305727543,6.299391206126829,2.6803243568440163,2.693765712217783,2.7949356628035362,6.605701738949982,9.159820892013935,4.255500733148386,5.555509427083257,6.871104373555458,5.632849830117706,4.966707314150316,2.526068811667588,6.09950561709114,2.927896453728821,4.28835856219366,3.1634987322828794,7.07692245939234,3.8429788317883253,6.341274183692174,5.926948247949772,2.538538163629804,6.73226919950145,2.788685710613534,5.508111681193465,9.398936038454162,2.3161457422933562,5.28835856219366,7.479861085016818,6.0925457415435655,5.747655932932393,3.9059284781731347,6.235152624217932,7.3334237337251915,8.870210577263595,3.460742563789644,2.974529312483882,2.6016965164809576,2.2387868595871168,2.7676547982373463,5.659639187015652,3.894332742277694,5.771093251699539,7.173327349650988,6.027463723408577,1.4222330006830475,5.794675787986858,4.720825666089835,5.342696959981934,5.562547724199122,2.62760683812965,6.426767932655541,7.593353770980297,2.5185351389821804,6.676239195086378,4.472487771462744,6.212180210044431,7.178117045706399,7.534497433738168,6.304876050044889,7.545505100691072,5.077670274232752,3.953265239014844,4.609991295212678,2.9126498648972037,7.174426393677253,3.6016965164809576,4.515384460636948,2.503348735167504,8.129128434194934,2.9726926540042644,6.030998118171895,2.2539892662307865,6.597829097826899,6.8209447918806925,2.9523335663696857,8.18769847638827,4.950468414150123,6.1409831048784245,4.674121632632394,6.830610274357907,4.4376272483189485,3.7037651787934096,5.95837871156634,9.296618003796326,3.351910961103077,4.143230134776893,4.707082991771706,3.560714954474479,3.9845893503624565,8.328001662149257,2.1667154449664223,3.1811025507537978,2.4854268271702415,2.931683057059806,4.788685710613533,5.659068274843228,5.738227400367242,4.6937657122177825,6.860962797858116,8.65474342118394,4.814550423461808,6.107687869314374,4.720825666089835,6.4775154361842375,5.619119511453218,4.7136958148433585,2.5533605033353277,4.412781525338476,3.72137265947682,3.460742563789644,3.0197019144425483,6.602587527342561,4.687060688339892,2.526068811667588,4.698218478224414,2.8698714061777126,2.144046369616707,5.394376944957055,4.649615459063409,5.80683958179819,4.443606651475615,5.450881315273369,6.6503337538459295,7.198494153639083,5.228049047884462,3.9231491804723087,1.3840498067951599,4.524815928357506,7.211012193485511,7.027021314622255,7.212958362728285,6.658497136656478,3.3881895371560837,3.140778655782796,4.37364821133469,3.319039815562536,7.33360262826828,5.175524601089875,3.2047667506546134,2.702657543390911,3.228049047884462,9.322806969423008,5.561631630375841,5.8755343515732354,5.032100843167024,3.9873208659292536,3.1842802944193824,3.277984747299765,7.444849247263915,3.6701605141266334,4.595145567990858,3.8369340113210915,3.575312330687437,2.788685710613534,5.4571345943180365,2.4276061727818994,8.563615765629766,4.712045448553693,6.260590274730996,0,2.4594316186372973,2.8257856274647914,3.049630767724601,8.097242070141174,7.431622959713292,4.030336078370959,4.964398632203184,4.020591094773247,2.211012193485512,5.462052318796433,3.3881895371560837,2.411426245726465,5.0430819818956625,3.6064422281316078,3.613531652917927,3.883620816285671,2.8318772411916733,5.081936081719432,5.575917361118149,4.958842675243241,6.716579420796345,6.531225485053606,6.050284014301706,7.73125113789623,2.950468414150123,5.645009884391095,5.4015622072560685,3.840966704487421,4.3298411765306275,0.8155754288625726,3.7070829917717063,5.848748114611959,4.303050084681673,8.796785917134267,5.497931651896279,4.67581593117227,8.189231548647284,6.9092930858238235,4.772941337831336,5.6082178530214595,7.304419775319909,6.588864518190148,10.665637927637095,4.672990993827748,2.8399595874895316,5.305605789585429,3.0908534304511135,2.4854268271702415,2.693765712217783,2.942983598187102,2.5897634869849773,4.631686366311998,3.0125686735030555,5.651051691178929,5.488964817004848,3.1858665453113337,0,6.703073006328523,4.255500733148386,5.9593064897596575,5.218781167784069,3.968090752045256,4.8594726668519375,3.460742563789644,5.341274183692174,3.0089887832272546,2.032100843167024,2.169925001442312,3.356143810225275,4.814037647539975,3.4168397419128294,5.653060017104565,6.074462620704536,3.2016338611696504,6.9656688644419456,6.816215687622865,4.330558400030802,7.16380060395775,5.6061460780657235,6.093179846765504,3.5716768099709313,5.342341397431548,4.228049047884462,6.310885391185277,4.751677945687579,7.502871574574138,3.953265239014844,3.975446765640962,2.6599245584023783,2.430285272977781,5.7303682363490065,7.689648608288815,4.371558862611963,3.215678596607928,6.492173659610827,7.480507489974544,4.436295119839363,4.804260115634738,4.405311682597445,13.396289023308677,3.8914191868460786,3.24031432933371,1.899175630480513,3.2942531364445142,8.382105211486744,4.143230134776893,4.92552476974757,2.8439838440483265,2.8399595874895316,6.101187815259738,5.0232553523003025,4.583158003874408,3.176322772640463,8.996303999184056,3.8257856274647914,4.7755773609306535,8.12401788912289,5.9618548076361595,2.454175893185802,6.208088008224774,2.673556423990145,2.8972404255747994,6.791683858152045,7.137810883416096,5.8052924556007115,7.338067797542616,5.198494153639083,2.851998837112446,5.726013748860151,6.567880042052731,2.8155754288625725,2.2387868595871168,3.6769443591069124,4.318316841334983,2.1795110502715103,5.6884601404244926,0.8073549220576041,7.244982170376553,8.642268351758643,6.836681593327116,2.73768676140986,2.4646682670034443,5.964860664245848,2.7420062108667365,2.411426245726465,7.279842693520348,7.446835177086041,6.531849285751203,3.8982083525087177,4.721919445551544,5.500164679492167,5.429615964201735,0,3.7311832415722,4.726831217032493,4.904002316283692,4.561937059622433,7.438459204423481,4.631686366311998,7.300764373248062,4.112700132749362,7.9329824099498465,5.1081062236861285,5.868143479666726,4.161081482277184,5.996162742681061,3.097610796626422,4.263785613890803,6.339493737901733,4.370861740085285,4.842978831788326,5.636334602442182,2.84197311892718,4.7713574089905615,4.257010618206024,6.916596067892185,6.015247773698937,4.595145567990858,6.7410624357490905,3.4168397419128294,3.823749360308273,4.279471295644468,4.7114949066500875,2.883620816285671,2.969012307516316,5.089159131911238,2.3362833878644325,8.080604371320732,1.8559896973084806,6.941341105485299,8.14639047567022,7.163498732282879,8.281651890258825,6.329482431025636,2.140778655782796,2.3103401206121505,9.500782139394977,4.674686619927999,7.249066033855867,3.1060132376221192,7.897179908573977,7.6546360285279675,2.944858445807539,4.94673086014031,2.711494906650088,6.054414387917307,2.788685710613534,4.198494153639083,2.9467308601403097,4.060047383669939,3.442280035252584,2.7070829917717063,5.9586107120558465,2.4982508675278257,3.6484654430273142,5.0870384567669475,5.130519083027338,1.8155754288625725,2.6064422281316078,5.673839055990439,6.37364821133469,5.609991295212678,6.076388068512841,4.695437110405369,7.459267666703387,5.211401637418471,3.063502942306158,5.931210274803932,6.819540460505722,0.918386234446348,4.878234879112339,6.098663781886574,4.098453246309274,3.4154882710497003,9.8361608416947,5.3103401206121505,4.20633064787105,6.436128517259249,4.381283372503783,5.0259146987507926,5.226123143125237,5.73903797911741,4.110196177754199,3.8042601156347384,4.503984703897613,6.067165427017112,3.7322691995014496,6.121844297876376,8.248591759535907,7.711013010084339,5.782670658677007,6.0774566524005875,7.592232766722112,6.917909073852982,4.423578170981798,4.901108243014512,3.795974694206668,6.907251224292033,3.189033824390017,2.4646682670034443,6.061128135118254,2.733354340613827,6.664198369291911,5.166715444966422,8.492293852480074,7.279749853014451,2.807354922057604,8.362689046726734,2.3248106034204836,4.826294245499635,2.321928094887362,4.238022517825721,4.232660756790275,5.51127826508303,3.7644735509926663,5.290940402403678,7.535353147842131,4.329123596291566,5.908092340818271,2.570462931026041,4.634012356402116,6.99491908871511,2.560714954474479,2.3589588258323295,2.8278190246173196,5.439623137557117,2.927896453728821,7.476543706214256,8.334094473911568,6.726695004499616,2.097610796626422,3.619413010597937,2.523561956057013,7.136683577697236,8.908992993250534,4.4950555283680185,6.970393537914677,8.089159131911238,2.996388746447621,3.7506065048355923,5.468583317006828,4.732811872082943,2.5360529002402097,2.729008870337863,7.712939631305699,4.038260575175349,4.514122260170708,6.948834425578686,2.0036022366801953,2.169925001442312,6.04023491675437,7.246788093844365,7.018923431842335,5.046141781644721,5.637784110291324,6.976363635732762,5.840463233869542,6.170125361839952,4.760753208062987,3.089159131911238,2.8399595874895316,2.3978029618624896,2.3305584000308026,3.0925457415435655,2.6599245584023783,7.9217814432688,5.963242904318827,2.4515408330178317,5.976821852360685,1.7865963618908067,3.221877081077034,3.0036022366801953,2.722466024471091,4.2024177215751815,5.9025560053446275,3.997292407636508,5.323730337521348,6.463851292611099,5.938756261492295,7.593951283948411,5.233044401265581,4.79025073883523,5.718361626138354,6.435295215647167,1.3504972470841332,4.12267271882414,6.361768359419154,6.8820316083521,2.0250287944915226,2.298658315564515,3.636914580355878,7.934634441094905,5.246408087246385,3.918386234446348,3.929790997718597,4.147306698780294,5.169925001442312,5.542258049766918,4.693208148910016,3.6971065744769747,5.707082991771706,3.363171077119244,5.035183996618336,4.274261661257048,3.321928094887362,5.9740703670240265,2.3757345385831563,10.174663577413781,2.8439838440483265,5.24260250592898,3.4581194811745064,2.063502942306158,3.1858665453113337,4.7692427951405385,4.301587646603187,5.986638471730853,5.058749412335524,4.556429415449573,2.5360529002402097,2.4489009511451276,4.974988111991927,6.248307120087241,4.346956889378885,2.641546029087524,2.8579809951275723,7.624539603781834,5.197314999473818,5.38404980679516,3.2387868595871163,4.464668267003444,3.955126781261366,6.258895755075436,3.1358631653686793,7.831180547126736,1.3785116232537298,4.934044647112246,4.503984703897613,4.100977647724821,4.86195536414487,8.439830883981392,7.851624127346895,6.091065077930535,7.190022174802645,5.992541859109112,2.983677694698067,7.161484638641659,2.6959938131099,8.21003812347289,7.461643148073906,4.848497755385146,3.192194165283345,1.6690267655096307,8.80290404741688,6.926948247949772,0,3.812498225333564,5.6806056662025926,9.871443254934135,6.459923362676901,3.8298495598446904,5.157447996274298,5.537296067090842,3.5147534984397533,3.2249663650002742,5.699884728697476,3.035623909730721,6.914444931746792,5.338780943894699,9.013601832683456,7.578712331048165,3.3405622690264134,2.3950627995175777,7.221587121264805,7.78959885005233,2.2234225499349374,2.8439838440483265,2.950468414150123,5.833649127594984,5.802968651005583,4.618238655595454,7.581803148516525,4.948600847493355,5.416164164733128,5.012121672712217,2.9373443921502322,2.3923174227787602,5.259649120649155,7.119252412157343,5.452858964713811,2.5160151470036647,2.788685710613534,5.040892430646901,6.390942772802543,6.032541697270038,6.177519202597814,6.111865963867557,7.0083165637349865,6.074034394212556,4.471187460386985,9.507715369062607,6.152994605492435,4.169123281476757,4.706530552538117,3.0426443374084937,5.305605789585429,6.942279901705763,2.4724877714627436,11.02948049374083,7.127736443588829,3.594548549550354,3.8063240573900288,2.144046369616707,7.036942844798256,4.022367813028454,2.0214797274104517,8.589726041133547,6.218781167784069,3.685940148445977,7.566662962962611,6.652343078740216,4.210232990095849,9.47854718263086,5.991861930556323,7.500961352460057,5.5855634984556914,2.319039815562536,2.9049657186840263,1.9781956296816516,5.509062386361898,5.592756010441026,4.39780296186249,3.431622959713292,11.193297418854996,7.925940095994448,8.041385369486115,5.051807107013298,2.3673710656485296,5.208673319629471,5.532940288372874,4.598722499676621,2.903038270112912,6.098453246309274,2.967168607532628,4.919340082442012,7.971830914780502,4.189033824390017,3.10936055940423,4.053111336459563,3.944858445807539,5.71699089440494,4.2787282129389395,3.7246502717329673,4.568032104771279,6.400025517914023,3.0443941193584534,9.140190703191836,6.7998644108818525,3.3923174227787602,4.880685525261125,10.32396671481508,2.521050736900963,1.8836208162856714,6.547203024756931,5.63023071585969,5.915042791371977,5.713970692436053,8.823876711241038,2.6530600171045644,4.462706750670158,4.524189078449365,5.342341397431548,6.70362677086498,8.008596693255853,5.81326814176608,5.795715006501729,1.5897634869849773,2.5058909297299574,4.153805336079036,4.707082991771706,5.617651119427331,5.7615512324444795,6.115615931428109,5.51443791382931,8.873136468833458,4.8811751553492755,3.4276061727818994,7.013685569928653,3.37851162325373,4.390942772802543,4.544114402209474,8.617283787779035,5.39677601078753,4.096767854714689,3.169925001442312,2.881664619320345,3.389566811762726,5.949301468328278,5.9095331149468935,2.2172307162206693,5.687900522480746,8.295998461911388,4.963011647599427,5.648177795724818,3.8509993947164736,3.669026765509631,5.924575003392735,1.2570106182060237,8.1184220243132,6.3103401206121505,4.59215800212536,2.169925001442312,2.771885578515366,5.309976492370556,5.718909554379232,7.2325648297287755,3.720278465233327,3.403267722339301,4.138323004056484,8.321792835886914,8.384999544730041,6.675392543042777,7.848935855524565,8.135247549809566,7.474760499576783,2.01435529297707,4.661635602335959,2.899175630480513,6.950701690089008,3.4776773275653072,6.275752048828529,2.7782085763980877,8.645802514606062,8.927066807759255,5.377123749129487,8.785648628652076,3.554588851677637,5.8529975876133165,7.093391153253013,3.8639384504239715,6.390082950951663,3.0214797274104517,5.947198584262056,2.0565835283663674,8.560600329022595,4.708187236020708,3.1473066987802936,5.260778431893426,2.3978029618624896,5.496654082593496,8.356011721502778,5.307064162255372,5.485426827170242,8.478971805032943,2.414135532984451,2.776103988073164,4.7420062108667365,2.4828482830684653,6.543805175964397,8.5918962955207,4.361066488794321,5.294620748891627,6.553360503335328,7.088629255297176,5.1201860275318625,7.00943675560809,4.090853430451113,6.7014104362328135,7.415741768290091,5.348728154231077,6.705563274566321,5.445263208140443,4.552131108253784,3.4436066514756147,3.5631581304028077,6.678494507773734,4.478971805032942,6.1657109924849305,3.403267722339301,4.534186139090097,3.0214797274104517,6.238786859587116,1.8639384504239715,7.020924396158786,4.20163386116965,2.7548875021634687,2.604071323668861,7.128870759465125,2.5459683691052923,5.414473836430928,9.995102877826515,4.861459166361513,5.25285462595139,6.31922050252387,5.308156975114762,5.526068811667588,5.763145958149476,5.903279342056139,5.825531251189622,7.048214384608036,2.513490745588118,5.944155663440363,5.856985689782205,3.412781525338476,2.956056652412403,5.664482840364682,1.74416109557041,4.886062338345599,2.4646682670034443,2.4515408330178317,6.433126376206435,7.7755773609306535,6.705839705883682,8.330737650206952,4.727375938594897,5.777945670695148,11.69784504363786,7.7627474419531515,13.587327482778312,6.558114538598396,6.332349900373874,3.2265085298086795,2.875780063068488,6.70473366267406,4.223422549934937,3.2387868595871163,6.136683577697236,12.23806432823473,5.572889668420581,5.849999259466098,5.533563348214512,5.325530331567558,2.797012977836145,4.150559676575382,8.298429210890312,7.731454807698346,8.903490246967104,2.8094144442358986,4.7824085649273735,8.461602224618334,5.583158003874408,3.060047383669939,6.429950657403768,7.193278935521388,2.3589588258323295,3.7311832415722,2.899175630480513,2.298658315564515,4.235727059838059,2.3812833725037836,11.663771557482411,5.684257705058932,5.212569338850806,6.9787676510367875,4.044394119358453,6.092334311186202,9.582254908357664,2.5160151470036647,6.402415216824399,5.740387932468353,5.784503982929566,2.6064422281316078,4.362469888750209,5.164705840182799,3.4208865749755315,3.948600847493356,7.965841991309597,4.976821852360685,2.722466024471091,2.7398481026993275,4.528571318870758,7.677578712165378,3.41006969175638,2.4905701304462013,3.5147534984397533,7.5673476962842585,6.849624029924543,2.833902076669163,7.406162478777607,8.131085259578953,3.176322772640463,8.148324042480084,2.657640005207824,3.350497247084133,6.8203066288938246,4.483493351284395,1.713695814843359,5.872582544833223,2.722466024471091,5.7742599514327635,5.704318677760832,4.760220946466509,4.294253136444514,6.037162550127378,7.151777654732053,3.215678596607928,4.742545233866053,5.613531652917927,5.727375938594897,7.7780114016125745,8.009884588931843,6.549977142771825,2.0461417816447205,2.0565835283663674,6.66746640495737,4.62760683812965,7.884597920990064,8.571145863602528,2.3161457422933562,8.563272552819562,7.111970261358359,6.001577085389535,3.4369613378336026,6.435461914479276,3.5084286525318573,3.940166750482817,2.5459683691052923,1.6644828403646825,4.94345253855882,6.003827078111476,4.960697039304312,6.684117412839159,5.612352498752662,5.609991295212678,2.62993940943954,3.611172380044005,5.522620761042079,5.891419186846079,3.6530600171045644,3.4581194811745064,3.1667154449664223,4.581351247168778,6.764340846670181,8.066089190457772,6.614857050384495,5.767654798237347,3.1953475983222193,2.9726926540042644,5.279471295644468,7.5105663894487495,5.02724253597374,8.25176642434399,7.822411496360477,5.334496768390418,5.715893370547605,7.3049672876759635,5.998646839005795,6.9725777851765525,2.4594316186372973,5.666472568842073,3.8728287595348854,6.052024560482831,3.631104282365877,2.3015876466031866,2.37016428054021,4.3384244147835584,6.546122758857796],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données d'émissions prédites par le modèle de régression linéaire<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_pred_logLR"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle régression linéaire\n","pipeLR = make_pipeline(scaler, LinearRegression())\n","\n","pipeLR.fit(BEBNumM_train, TotalGHGEmissions_train_log)\n","\n","TotalGHGEmissions_pred_logLR = pipeLR.predict(BEBNumM_test)\n","\n","LRr2_log = metrics.r2_score(TotalGHGEmissions_test_log,\n","                            TotalGHGEmissions_pred_logLR)\n","print(\"r2 :\", LRr2)\n","LRrmse_log = metrics.mean_squared_error(TotalGHGEmissions_test_log,\n","                                        TotalGHGEmissions_pred_logLR,\n","                                        squared=False)\n","print(\"rmse :\", LRrmse)\n","\n","fig = px.scatter(\n","    x=TotalGHGEmissions_pred_logLR.squeeze(),\n","    y=TotalGHGEmissions_test_log.squeeze(),\n","    labels={\n","        'x': f'{TotalGHGEmissions_pred_logLR=}'.partition('=')[0],\n","        'y': f'{TotalGHGEmissions_test_log=}'.partition('=')[0]\n","    },\n","    title=\n","    \"Visualisation des données d'émissions prédites par le modèle de régression linéaire<br>vs les données test\"\n",")\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.2 Modèle Ridge"]},{"cell_type":"code","execution_count":115,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre      Ridge()\n","0  ridge__alpha  9794.696671\n","               R²      RMSE       MAE\n","Ridge()  0.193604  1.823562  1.511262\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_pred_logRidge=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4.861971695941575,4.905383946187531,4.9229743810020015,4.9405640988309045,4.867271059186977,4.8596038525756935,4.874581809464909,5.491456501595741,4.905267865217631,4.8839131565441205,5.106620773494044,4.935404513417085,10.665066409824654,4.871001151270702,4.98813866076098,4.915203073117823,4.950018549610129,6.081855768564612,5.037086134065933,13.265368689456238,5.617457773204258,4.975241404511438,4.940294035240828,4.899242036251114,5.590798983401248,5.0265291609563,4.829518606791471,5.0572179301563205,4.917046081684606,4.998330113359285,5.058481950769547,4.9571271704058315,4.877348675706602,5.618400106837497,4.939959243953118,4.934257743865677,4.894117209775661,4.857251326766803,4.89801929959395,4.841492790798866,4.870172273325254,5.34662327632219,5.726357456437562,5.075389071115186,5.138794474504705,5.657875873014428,4.953238329630058,4.974211896265752,5.193543377437374,5.122860652607759,4.9378529060280005,4.916334923282406,5.217804365565025,4.880613197429881,5.042817386132292,5.0235926824709605,4.993190119094263,6.501060209992037,5.501336020814246,4.872854825251581,4.833494404506372,5.155045442335109,5.1186349035811185,4.861711500109407,4.894689368796894,4.878634099435586,5.007298543726367,4.875468231334376,5.253619337745328,5.134177574354246,4.99401460915397,4.927805658401027,4.913001939413801,4.837593902138884,4.957636286055942,5.060517844469933,4.828828375390177,4.876204187737468,4.860101519345513,5.677791142559709,4.841620810872367,5.040003863632394,4.905168744920943,4.906434549169122,4.8180943163730285,4.956545716186417,5.097675037643823,4.922074969793777,4.814406057545528,4.858706799620827,4.969729054013327,4.851002117356891,4.81505802642371,4.821886605179095,5.59402251227864,5.266800253885818,4.884199638804219,4.877956677725675,4.973000776235155,5.363337410798785,4.835864693492453,5.099842549505405,4.848203050625468,5.264332669700695,5.027980276661968,4.889749467547953,5.7465474855665795,4.940279307251908,4.947613089034542,4.971510165901336,5.009268810263784,6.076756191143669,5.839855567430577,4.80510623068522,4.893860461884466,6.428610855334641,4.88220623098757,4.862951892117893,5.609414092270767,5.394379043523871,4.912812014871633,4.89101150235593,4.951771370483476,4.891487285406969,5.068688608209475,4.889360901223076,6.78591458710793,4.825258381018336,5.006505920966772,4.891936571641219,4.925430647560161,4.895106702342844,5.085185889632372,5.553504157239041,6.24524226998032,5.458223544277529,5.087922854544551,5.095892685615176,4.889082917656953,4.837406880013212,5.0588749792384595,4.869869060810232,5.895936617236153,4.9509068852695055,4.891629066891181,4.934961344899191,4.904653220610238,4.85063602148065,5.590594618314491,4.961180810799534,4.874284069943607,5.389535816433568,4.495873256717205,4.963621243136047,4.831736897090099,4.916447772042989,5.314236529901012,4.824337866000044,5.117483564013515,4.989621725247041,4.867476341638105,5.274670902489345,4.906335806097754,4.847816182094107,4.933160564348925,6.494890723079131,5.216831937031553,4.822017972876226,5.294537555965431,5.350332154136465,5.044406904512883,4.966990266115452,6.988459154779473,4.870589281731932,4.885680971626342,5.755990783098145,4.925390069170978,5.137964473469167,4.9233882411008345,4.881749324607655,5.073825936411157,5.1557527346228,5.008730850344437,4.922922919438042,4.916130889009531,5.2317275296064185,4.937309348064185,4.825256639427886,4.987305687840186,4.822980495577411,4.843974714092775,4.829256577562433,4.863949363582013,4.838209151352902,4.887932230933487,5.0626876371855944,4.979647673350911,5.330779329687023,4.889851791487539,5.273140895838914,4.9212603004832145,5.296662950144203,5.000190794317289,4.8639797563777885,4.841065204564766,5.0053817246230405,4.896337403589631,5.123657678399447,4.957042142980215,4.920428865631291,4.980688367594509,4.8931160849173905,5.532940457550366,4.8683507946852735,4.996204566257773,4.981296991113555,4.9335254132869935,5.218078094447128,5.923120716232584,5.3016190750234,4.87606676814085,5.057378327233482,4.894098690850514,5.07818082884651,5.201593936888713,5.114909488848292,5.122070231326251,4.904256468226894,4.921836038621889,4.897355445736961,4.918177198594511,5.261865066915625,4.8856757094922845,4.897633281462962,5.415574386791404,4.952426528102835,4.901007158954565,4.820685805376612,4.850704207778301,5.161323381757693,5.034523823872491,4.918855068038609,5.155494957735309,4.809633016709939,4.825307700726953,4.942739072468012,4.934459203381465,5.124383827704702,6.181081158721076,4.8441017143451806,5.1078181298454055,4.86412281554453,4.942131857229641,5.006664048974859,5.015366899766895,4.885428656900846,5.488983185343065,5.083710117710595,5.121624253331183,4.874655430355176,4.8789661438893885,4.871920491515769,4.819811332104865,4.992220176554585,4.9600265735711195,5.400166907812567,4.953467013056779,5.256876707810736,4.860307099571952,5.155618229397853,5.120742954447209,4.960838635348637,4.941493911980695,4.9399388694085395,7.242913318561156,5.090947033812334,4.956062816527422,5.114366821883098,4.899691208128542,5.666973908281255,4.925800609868152,4.926558890985415,4.91386825280192,4.9613545447358005,4.957768681706564,5.485936002739238,4.877450584743663,4.891041724025844,5.497905928274054,4.890579713655761,4.965100523611728,4.863950945785644,4.87158693355507,4.914671375539175,4.891945935950771,4.9106359065988645,6.255657684520944,5.230692861660507,4.886615813432844,4.826476014831077,4.852882252131083,5.260400420154314,5.099970630113466,4.86902240439668,4.9355002022603,5.068540588422887,4.905998500433907,4.855479530196163,4.873205979577806,4.954089725409758,4.884162359392373,4.907504797736567,5.490826749072581,5.264168803146281,4.884860442214926,4.8985250245192455,5.011381654110544,4.891261313130007,4.837082395185712,4.8191268996846075,4.961296183814329,4.88243785301312,4.909599099133246,4.9442677537112,5.178896027194739,4.865660063127734,4.929087456305601,4.926094640129325,5.253282367638352,5.3389561406338935,4.942658445746395,4.880071344701119,5.035066228429221,5.272044969625491,4.921351937346591,4.889268880286911,4.874371019488979,4.923435294656954,4.890077642773469,4.837095288246979,5.529571651903773,4.972704878860906,5.3237649987355455,4.9684884939744025,4.8220970163090495,4.88298406075881,4.897613110156127,4.841616838560826,4.990568963265388,5.475651586756631,4.857241551013764,5.1946961853122176,4.869038427729753,4.9085987031620375,4.929925165707928,4.940505210664751,4.942676208653237,8.663706897527842,4.892938932883956,4.957980460686952,5.734505008569549,4.852594827849541,5.391674001718072,4.914443548110769,4.996619957706859,4.882864493019757,5.228590149282109,5.001220593172924,4.860851568619676,4.914164300417376,4.883615710207882,4.882169084767196,4.887952582331512,4.829145375218087,5.095030093896052,4.83318023406563,5.277591475220116,4.951037771842222,5.234990418547038,5.557727278017717,4.810720898095194,5.081379215567091,4.868096217071299,6.11631009522262,4.849888340596633,4.975902913925264,4.908483231646694,4.905098344337588,5.234253465049096,6.427809433675079,6.067937237569013,5.220562081197622,5.876462570771821,4.873759110229676,5.114728141415621,5.286025121368867,4.896099029993187,4.832939453155776,4.826989868973738,4.834726978260364,4.917484141482084,5.245929637190114,4.963523308498218,5.132028139239465,4.882870070453947,5.249981607309554,4.946690416629297,4.959050951818818,5.262523787691838,4.864786433378319,4.901233238773565,4.830931059026948,4.769190094857682,4.92580684460582,4.993222688815382,5.139936734274011,10.292074807686252,4.976204603514342,4.826442343095735,5.015257744288569,4.822436063155431,4.871607811400339,5.471920327042391,4.821512577426103,4.869937543322616,4.906321880292679,4.9336710840137945,4.948864312381715,4.87020966755853,4.8603044159302655,4.921987345607011,4.934606569470674,5.451863566059297,4.873753214160225,4.845156382423843,4.903230756624699,4.877815863863771,4.833994121234438,5.088856196099613,4.901330441481563,4.95882118547348,5.193343461803251,4.9516707626309815,4.9128692743927385,4.867497613230337,4.842093320216352,4.93627146145604,4.984613137813961,4.90894203117406,4.897294571784357,4.959039626808581,4.862538863575376,4.842902677281332,4.869540526830639,4.982765629215603,5.029722532636659,6.661417990934879,5.115663340537072,4.993889234091636,4.832421028289677,5.155901996294541,4.858017393647847,4.95833421439766,5.624221269661314,4.922305517152127,4.96892390262291,4.932445485791102,5.084163055229653,4.824410140037032,4.835951581835771,4.886968588730426,4.8379649698272145,4.8906638951949475,5.040185192108267,5.272611940310637,4.903234207122137,4.874143761644113,5.138078956154261,5.107449184764873,4.92906721788403,4.95770143274905,6.6705628300488655,4.987791793450675,4.809236296432696,4.857872249368289,4.9350008013809425,4.953786190045268,5.0957003234962075,4.896972236559097,4.927923093997198,4.977216689838398,4.816209163564405,4.850127783398054,4.881737393773611,5.130245981388073,4.976969889688831,5.918237364143187,5.322124237595653,4.8118182343880065,4.894041561375505,5.088808368924062,4.867482540392437,5.001192244262168,5.074754857764077,4.896878199576299,5.129719295062659,4.864987993265338,4.978240127660397,4.794405272036752,4.979526390183691,4.990799950329458,5.035236729704403,4.924557922023989,5.058721308015748,5.108931433836884,5.0801631457632155,5.153150150837061,4.974163631571249,5.096634083427278,4.802802213612531,4.9151814281513015,4.869047881970953,4.828997047675385,4.890121800358952,4.885306022833518,5.108822694335576,5.518548761141954,5.014052553245628,4.969098178315977,5.85942451507858,5.272032077768186,4.934804513452823,5.246348942240571,4.816061530371983,5.655997865349418,5.300985688429732,4.816579013463276,4.867921015315333,5.020624461525266,5.064453778435085,4.910149779147953,4.897461415545072,4.810840304130625,4.902529256276043,4.946026896759746,5.046079494825586,5.064480708196318,4.8888203602093565,4.9135950738676915,4.944654343646964,4.945768830660078,4.830401065531568,4.87427346176542,4.9796507551638065,5.016651586445739,4.853984226387409,4.853096109155534,4.9283041914501275,4.957980135997993,4.829414844552683,4.923931629313773,4.938956246923684,4.972396934847184,4.975216364234201,5.083867832706647,5.083501526091235,4.953949836026464,5.252217754630001,6.689140748317502,5.113604347733947,5.431877367179964,5.072273360186451,4.858102641306775,4.991930807403736,4.883869711786967,5.336248111945202,4.934280037492458,4.8958168487443965,4.922268008745582,4.9731264222824345,4.908253783662918,5.085917118953482,4.903799286888206,5.014512138633707,5.824960623870032,5.052244833819702,4.88590249001005,4.973003191986801,5.542341896049637,4.96300389347375,4.923642252263418,4.885140509901843,8.696088749442342,5.119416476523814,4.989182092930775,4.888581201575271,4.985534694608551,4.887418315477863,5.150499665584209,4.872083440404867,4.934571078782938,4.830596578476857,4.980537177033273,5.184889155770706,4.884339088584021,4.939705922476204,6.042236923638239,4.955812523451601,5.400698077622973,9.93762315575744,4.85467760930507,4.896397447768635,4.889663378754504,4.943682072259867,4.885744070654314,5.086512776454418,5.736002004671478,6.249477147033717,5.175662910136015,5.025263935317601,4.987855081420174,4.933665490610044,5.130149560797308,4.8620138350569135,4.872637622414333,4.936327863763498,4.893978238216772,4.829660314388901,4.952156309831359,4.831593801544252,5.074380525766414,5.500494366576474,4.952568405783224,4.900825639943909,4.8909129541996315,6.3377354058775754,4.859469190802369,4.939860834229866,5.362237839088072,6.400065886018054,5.416192753045248,4.846389250982659,4.88543199814009,4.850835752585139,4.870327768630795,4.8807752241321865,4.834106904005399,4.886656177296776,4.9671886262540745,5.469632153377394,6.177660495141331,5.148503037225367,4.923974276061038,4.892866081593647,5.1807272884343085,5.053911930202044,4.851789630928488,5.200933654774173,5.393616541475948,4.94847160925359,5.232532300270109,4.854764915179134,4.9139948060914795,5.0592893072250495,5.516393856274977,4.966479361871595,4.861847872704709,4.857343073667802,5.929969269447626,5.096348150821776,4.883404858688902,5.287027153542716,4.936474446949029,4.459761264638933,4.8989934046306605,4.859150207162202,4.914527108827133,4.84119619620063,4.928900616108129,4.960060299449621,4.969397939367083,4.82098298517392,4.883067153204865,5.30362946811721,5.075440265878156,5.034967652688071,5.162838560999863,4.840822575578306,4.873436290230461,6.372287431073285,4.937504952253694,5.055324192782843,4.937075955277056,5.206097449663674,6.075490405814324,4.915466220108421,4.915265638160148,4.908517706926365,5.040493690968764,4.917506137534008,4.94924619153109,4.953938262422324,5.187828297786468,4.922817500034162,5.029743977561125,4.910346054179074,4.908360166061751,5.0150497679339905,5.138421352236048,4.86184026993562,4.83558610995814,4.884811330848886,4.858549436699259,5.238819346120149,5.47539064448858,5.288598295859639,4.854491706447734,4.818019931321046,4.891641151254345,5.000548943394713,5.204033169890338,4.96030484979584,4.829386751403404,4.828257925155869,5.3032971295042906,4.918345172166162,4.861093379811244,7.272233805247696,4.824962536172811,5.221091708546237,4.995501787785454,5.170315352660317,4.940353308943972,4.920569550058632,4.8180838792839165,5.068380191345725,4.948370003752737,4.909784184195688,5.055572220653815,4.9935835767542365,4.955041941889993,4.976946183811617,4.847927225610217,5.15809108406014,4.839614696051094,5.494567422333537,6.882190547638155,4.836788064075733,5.391488311599468,5.000807222922968,5.12506165813019,4.926020648087003,4.9044191623214,4.871409451261718,4.878234273383715,5.091645067928237,4.922524730603114,8.085133286854779,5.325622254358856,4.909391440863126,4.911760658248637,4.905590346693048,5.126916694964857,4.885475242635768,4.88374584994978,4.883401431308143,5.06513699627618,4.938967919590431,5.176498671802276,5.119742170957845,5.3959168156927575,4.89018857751031,4.895951137456735,5.402791319918508,5.311438658372201,4.878477726562529,4.8813739488503005,4.8994833467044625,4.9605648203038255,4.928934709256954,5.189672501113659,4.928044156312614,4.943924850611533,4.8554611735446205,4.993201698808034,4.887354453325484,7.934047392390909,5.88591231610774,5.3459428484330935,4.868959599058468,5.907075102953782,4.9302595473215804,5.086835253536743,5.181199650521246,5.134022913797164,4.885446331133012,4.978786123308704,6.240689950852251,5.050874117453113,4.814951547926738,5.865393241716557,4.861844773327544,4.873650300952711,4.84017579777373,4.948449812791794,5.050474618792418,4.913788579494117,4.854497801938329,5.18508677195766,4.8979482034619535,4.896745787122708,4.87367720505165,4.9864047492583525,4.8984676833947765,4.9092687241900155,4.88633289674395,4.924506576854303,4.927428910373406,5.0552833870106175,4.829013536584325,4.901528589608266,4.971033503273095,4.919895230078587,5.021640534377502,4.960988994354626,4.892072003692283,4.878247854883291,5.184996012350257,4.849931118621839,4.8428478022295005,4.896782021971486,4.908536890332834,5.214990738547161,5.017632832568088,4.881751542921337,5.095458512304639,4.885833506711258,4.814688551854843,5.289905696355971,4.876143251935698,6.0029399873254485,4.865368765165248,4.853313499310893,4.939913229626051,5.290502139507278,4.8676377130465776,4.824894884081587,4.954890237886896,4.941410585012813,4.856492500082191,4.993729241219369,4.898436837943841,5.1555021290525715,5.002018633042799,4.867099972758888,4.827593582581855,4.979510577042811,5.038256820004278,5.678762908964005,4.9570449413294515,5.2279208148717125,4.993921976527351,5.100164118308983,4.9442975040737585,4.887259378917047,4.857313835031173,4.848485113500091,4.879712682689343,5.037501507534093,4.962569478470174,4.896342536547162,4.847204193163032,4.892856263682693,4.861335127274425,4.856686005708617,4.808569056897241,4.880861207375781,4.920994104957747,4.9631735288093894,5.521778196360962,4.862864468415147,4.964185170098609,5.155513309028171,5.219498985046406,4.853418444380795,4.9257875017224055,4.849314722660824,4.820936212493837,4.929191047754568,4.91291543270083,4.866582420371061,5.708943698937774,5.721065650811682,5.451041633992029,4.976719965479139,4.943693943321288,4.863589306509406,4.875050514045552,7.172964110405509,4.888584795500748,5.105351189202756,5.6690381342038325,4.93243901503049,4.946380285529289,4.82643164094948,5.980371408374859,5.135878819868199,5.967242177653837,5.054701862079464,5.384967254841579,5.008431511754352,5.127207314376045,4.8459757564730745,4.8450470058968556,5.101135164024114,4.998945722264086,4.8528174951883845,4.930823985086058,4.897893626359789,5.098310341693902,5.756719451461058,5.541488016799456,4.8842127390585075,4.9863786254236055,4.879750693010227,4.827058951068669,5.484638439508703,4.830950532857459,4.931338076369978,5.00314186805391,5.189402042271674,4.960624963211449,5.257954561946429,6.080536099801067,4.889790246529577,4.924905523598431,5.355741282620579,4.92250882282439,4.874875372728161,4.868264766360599,5.001975017178177,5.072367811810743,4.9110186138752585,4.919827043780936,4.90382517832584,4.906204754689269,4.883369947745184,5.023415940340397,4.862900620307491,5.147041081057604,5.08317184795701,4.907182910976758,5.431354731033889,4.853262033189549,5.073716039690188,5.100018457289018,4.844189689544827,4.936013304898576,5.085149720827698,4.873475518899389,9.076844103213602,5.759386160547241,4.864206177331345,4.971304072683235,4.86759100003702,6.13601320378421,4.986066705476344,4.84675161411022,4.946951068934819,4.874062881496896,5.004745837943641,4.995616589590611,5.378944448804552,4.856190105362062,4.904437536664359,4.865439524392823,7.229673327408784,5.14288214209372,4.833302082558912,4.869521930567643,4.870079818457517,5.810791285446506,5.009140729391978,5.107677371405002,5.129537688134386,5.977933395928636,5.949168358717253,5.714433417066665,4.8069720557391316,4.926365334307227,5.174058448903112,4.853856274356089,5.438047063149629,4.9399432679734865,4.8982416369550075,4.933273837289308,4.918928520383595,5.659242249015455,5.139155018782995,4.936431225053913,5.232353044537688,4.944650262262196,4.827293337307924,5.047164632773115,5.037248311579211,4.877295168499888,4.939885839587825,4.950345206719453,6.053399184827643,5.230274668943076,5.208312307338743,4.977620828627927,6.175524658565556,4.953087804391061,4.875773493005799,6.5087675610716715,5.013797470968778,4.9722352898477835,4.940994048641955,10.334152192992612,4.908438490388672,5.007371173643319,5.539809074482002,5.103691181543342,4.843458240049631,6.420755040059602,4.847278578215015,5.00943117086247,4.848412950257758,4.875032346882942,4.925631033488186,5.480592490929901,4.839673491395734,4.828452434861689,4.991964547144494,4.857095123494525,5.888544042432815,4.940265080883116,4.958818200985596,5.643112410000935,4.990517683508753,5.215021013806687,4.845735991477145,5.890354199645517,4.9262365557601555,4.887153409697314,4.981094657009813,4.939090673813166,4.933979761223567,4.8678038826999845,4.833062294067798,4.924087433484628,5.02779738146833,7.681433385845434,4.827159994583481,4.952654060261568,5.185845977699205,5.062441579396108,4.922992385434903,4.900706986044087,5.107400244490225,4.832025045349133,4.963280251875168,4.8883732615570965,5.1190837201986685,4.8212246873106555,5.131019916202722,5.006168269635647,5.072756063472435,4.973385625450978,4.930248323823408,9.458842802168956,6.541667603758372,4.913126564507469,5.219704099278596,5.083145543074926,5.1596670167928425,4.8435097355812,5.03425446990164,4.813634227311625,6.549141469883283,4.878848774254931,4.84708500104713,4.869445802899123,6.1203879003552695,5.4867436670846494,4.901188991535715,6.663345481886813,4.86626860211385,4.945212559366075,4.92965774170414,5.083680374415503,4.875482032857798,4.865820747784237,4.9143964683115415,4.832674838615508,6.549473452776161,4.65849539472219,5.002539876529218,4.832899592190014,4.915217871332498,4.8361783045689295,4.828781017685378,4.857001324384653,4.837531524568725,5.2359236175259385,4.836041931973627,4.90846642245291,5.1616399376973225,4.9005231624124805,4.9303922746126405,5.151358067800187,4.909283115627176,4.8688665579708665,6.582215430935855,5.288952519235623,4.871606631915992,4.857761489585207,4.871868965294115,4.944100389770795,5.235323698771226,5.251377559143684,5.623021044232289,4.95470427331826,4.991912724377699,4.852787102392608,5.035518946234337,5.266544072477997,4.958538204811239,4.999699688710308,4.976070850947513,4.969306649598015,4.920498204611192,5.032861768459754,4.875039639717957,6.899516800841525,4.871043969798166,4.965947560500577,4.825188172302385,4.965537886912402,4.896430663349945,4.914465998851329,6.489535611102438,4.848573971547952,4.857102505171194,5.008631275907732,4.971603368026199,5.005103773799967,4.966729858660889,4.89020407439614,5.131343229060482,8.246396490742432,4.882090722770532,4.932035044106541,5.183533924122813,5.052158121149261,4.891175874811496,5.283885602651714,4.824642066571395,4.9779142263833505,4.9208088572435775,4.939531379260585,5.174478604018476,5.074005034454712,5.384015990219209,5.173407313312286,4.873533342415297,5.186789729943562,6.541994698298199,4.794881327100575,9.966143333488818,4.970973843341235,4.940061806862298,4.9364537982615895,4.904964126255367,5.7201115456536,5.0599429310648345,5.058787770776156,4.885274549484517,8.36302418069686,5.032360507328956,4.879041813206736,4.924299126376983,4.875631606839271,4.936425026299581,5.057619293629995,5.952659792027037,5.305791078244025,5.668911237761574,4.923721034972098,4.848830606987539,6.1315501615446735,4.878802283597441,4.952645004349223,4.9383497992922685,5.348091824556495,4.887483639039314,4.855948353943117,4.941976071608237,4.77972614892002,4.872583588760376,4.958875510439839,11.059483513044787,5.255279245223713,4.8757744261663145,5.037887641219428,4.936965721941217,5.317511693997924,6.361125169883881,4.840459948449888,4.923844972682952,5.292853568005356,5.120180967871077,4.854593985894212,4.90270963434409,4.804539136115517,4.883161411907548,4.920225459420587,4.947503518655203,4.827603901434368,4.911694474691014,4.951575984361684,5.360065609205143,5.520347296685178,4.974154373518935,4.829771690577238,4.8499114816476,4.826750974899786,4.983867140050311,4.916377436995215,5.569255589864168,5.204338134708522,5.105107450357486,5.385109059784187,4.989509751389327,4.970972906001098,5.141596770395745,4.890585297787554,4.8358043277608,5.079658824852678,4.88461799123458,4.9139143308880655,5.222112090175879,5.550971335671406,4.86471636948422,4.8813468722507,5.055952338562037,4.948992359222465,4.856872028111197,4.983629418104771,5.210949828986475,5.679805598816825,6.418604986275106,5.0892867190179585,4.874998881140179,4.862142624451772,5.67741777453931,4.867496633263509,6.3059362968461725,5.013937835341421,4.896356783582976,4.9183305041058265,5.843633159340031,4.826038020049402,4.96299211232953,4.902674727768797,4.903681043881279,5.178674031273192,4.936519626412836,4.846951739597831,4.897945981928734,4.9061024752427915,4.859610897447835,4.9391937074489265,4.99135891363642,4.958007566272288,4.9218379024516175,4.841248885612451,4.99458552195857,4.872824522384429,4.98203943761863,4.9506501036152795,5.008724723897331,5.152025771407943,4.984868291403642,5.086551332304591,5.131950638906953,5.072009586767605,4.9703104224436165,4.944436098478712,5.0710065163135765,4.815588713710382,4.842779160137546,4.95604805803469,4.872465812428069,4.901225690939808,4.9593238102290815,4.8861721358234895,4.880184665586085,4.869417066758042,4.879388065881809,4.891363483644462,4.913867283098203,5.31908890825513,4.895911723121093,4.865041926548064,4.854335860021921,4.869954322266078,4.89329974034584],"xaxis":"x","y":[3.137503523749935,2.4646682670034443,4.808385050656093,6.381629467033664,5.687900522480746,4.296457407371244,2.877744249949002,3.542258049766918,2.752748591407134,2.1984941536390834,5.6604951318885135,2.9763636357327616,8.749098443586298,2.1043366598147357,5.40973035386445,2.3305584000308026,2.3950627995175777,6.141391916170729,3.76659516172927,9.707290101958575,8.663593681954685,2.9126498648972037,6.819668183496455,1.6553518286125541,11.880218691860534,9.057883448899627,4.445594291341182,4.326249700837456,4.708739041359579,3.5570424152637194,7.3848269116030085,6.815319245777306,2.2600256559614555,8.198690585668917,4.72137265947682,2.6712933724815757,7.400708693037901,3.6229303509201767,5.137503523749935,5.359661722333877,4.571676809970931,6.199672344836364,8.475936042446387,7.905507349949873,3.8933622107638715,7.537839615916466,5.353323291162897,3.311793717753649,5.2712762591759255,4.778208576398088,6.628919373511362,3.1858665453113337,6.145473670317708,6.023698917266407,3.145677455195635,5.683696454306515,6.109569509370688,11.964503324303022,7.672213165334465,3.8787253414801053,4.8283269264148165,6.010108453474289,7.029563314432658,5.911212201558612,5.9590746011405145,4.0232553523003025,4.234961094839504,3.41006969175638,5.59215800212536,5.982537314399271,2.910732661902913,3.7687136570304847,7.183982678112831,1.0976107966264221,2.6016965164809576,5.511910748670928,5.6570683012015754,2.2868811477881614,6.999098033705607,6.746447139362815,4.83035674741922,3.411426245726465,4.77663042305105,2.550900664647523,5.942279901705763,7.626950122372515,6.862203399053224,3.4019034716079584,2.608809242675524,6.046141781644721,3.3936907641874536,5.390254956321868,4.541638734436141,6.063718640152248,8.226941966807852,6.203788453308596,5.530445354692906,3.2433644256936605,6.703349915166396,8.202417721575182,6.075318692553354,6.401732849522621,3.3305584000308026,6.606294160697713,5.5786368624717415,4.273515889702116,7.668814088419117,3.097610796626422,3.321928094887362,4.593353770980297,4.792334805706692,8.090535901003173,8.367589480412983,3.1358631653686793,2.7070829917717063,7.1593668097437355,4.614709844115208,5.946496941203293,6.783456654360239,8.491372117815171,7.273609132231856,5.944624223055033,3.070389327891398,2.2479275134435857,9.095924419998536,2.4489009511451276,7.048759311919856,6.058532970201611,7.216163813388774,4.46532151358055,4.871350840634026,2.611172380044005,5.783718557790366,7.3824943402931185,8.12401788912289,5.2787282129389395,6.099295204337775,6.1705259991768475,2.269033146455237,3.6392321632492775,4.547819956577703,6.003152448666922,7.816087658600206,6.762348815644129,4.940166750482817,3.01435529297707,5.487357715311547,2.333423733725192,10.642557179595622,2.503348735167504,2.4356285940520905,4.773468927905195,9.760270854333914,8.156942620908886,6.218781167784069,6.306517445249819,8.352882086522959,7.032211069322931,5.5963388642052,5.320845667645722,3.0089887832272546,5.943218087426284,7.3403842354553985,4.443606651475615,2.845991770664573,7.217618485351048,6.137913322088407,7.084383221650577,5.784242222057781,6.097610796626422,3.605257262939004,0,6.954079959320971,4.883620816285672,5.118941072723508,6.583308464847678,6.034963989749303,7.257764968571387,3.91169158187234,9.10522758436473,4.507794640198696,5.001802242633985,6.858478390423273,8.095608004811197,2.3103401206121505,7.6398833131998005,3.144046369616707,3.9420452599160467,4.142413437873741,3.925049964727359,1.835924074254375,6.111448698487674,3.363171077119244,3.127633279725874,3.24031432933371,4.160274831408593,3.5397791916984938,7.571221725048103,5.284292026394312,5.47800105583793,3.7398481026993275,7.1320626778543845,5.576522137920503,3.127633279725874,4.3110671022555955,4.375734538583156,6.4432751117658045,3.91169158187234,3.8042601156347384,6.4409521980296365,5.85872702377863,6.02724253597374,5.212958362728285,2.5058909297299574,7.561631630375841,4.92552476974757,3.5619370596224327,6.237448995639321,7.881113960675097,2.176322772640463,5.361768359419154,4.9941271140031285,2.584962500721156,6.166916251569974,9.4493756610678,6.0332027259372225,10.919861451442168,2.693765712217783,2.4724877714627436,2.7484612330040354,5.633721812641012,8.17707065762942,1.9634741239748859,2.7803100990433753,5.23687524536694,2.698218478224414,4.238022517825721,5.317231700222606,2.1176950426697547,5.604071323668861,3.2555007331483865,2.2387868595871168,3.881664619320345,4.493134922305505,5.357903838552492,4.9597701552114675,4.724650271732967,4.4046306842176115,8.46707564951531,7.096029876307993,7.341896820977107,5.339850002884624,2.859969548221026,6.582706526780788,9.101634319640722,4.129283016944966,7.151270288790165,4.58014548442338,4.628773595201645,3.070389327891398,6.034303767768035,6.339493737901733,5.174326515173915,3.357552004618084,4.809414444235898,7.880195728943098,5.632268215499513,4.79960542245267,2.077242998932461,3.5298209465286954,5.433960931121179,6.078951341394822,4.854494418154875,2.6484654430273142,8.345405246717796,3.4462562298895643,5.595145567990858,6.766065051474954,2.4222330006830477,7.366846735262642,2.9873208659292536,2.6507645591169022,5.940871478189196,3.552131108253784,2.4672794804599825,7.901409979999464,7.459185683748918,2.3868109464722167,8.668955876629637,3.1667154449664223,3.017921907997262,2.4982508675278257,1.7782085763980877,2.9088129077395473,2.1602748314085933,2.3074285251922473,8.316598324108684,5.603477988254004,5.669310286202474,7.991691898336964,1.0840642647884746,5.091276694365222,4.042644337408494,5.96254902292306,5.020591094773247,5.20163386116965,3.065227622775619,7.08533966935737,4.321206566969903,3.5298209465286954,6.083000565676004,5.04176864995163,9.435170178881494,6.4705368650925825,2.0531113364595623,4.078951341394822,5.525755692829486,5.9818526532897405,2.063502942306158,4.110196177754199,7.135657989368776,2.3812833725037836,3.693765712217783,3.00539998774259,5.589164236699772,5.36737106564853,6.38646609286931,2.3248106034204836,3.1811025507537978,8.27891401949994,2.430285272977781,2,4.867402305727543,6.299391206126829,2.6803243568440163,2.693765712217783,2.7949356628035362,6.605701738949982,9.159820892013935,4.255500733148386,5.555509427083257,6.871104373555458,5.632849830117706,4.966707314150316,2.526068811667588,6.09950561709114,2.927896453728821,4.28835856219366,3.1634987322828794,7.07692245939234,3.8429788317883253,6.341274183692174,5.926948247949772,2.538538163629804,6.73226919950145,2.788685710613534,5.508111681193465,9.398936038454162,2.3161457422933562,5.28835856219366,7.479861085016818,6.0925457415435655,5.747655932932393,3.9059284781731347,6.235152624217932,7.3334237337251915,8.870210577263595,3.460742563789644,2.974529312483882,2.6016965164809576,2.2387868595871168,2.7676547982373463,5.659639187015652,3.894332742277694,5.771093251699539,7.173327349650988,6.027463723408577,1.4222330006830475,5.794675787986858,4.720825666089835,5.342696959981934,5.562547724199122,2.62760683812965,6.426767932655541,7.593353770980297,2.5185351389821804,6.676239195086378,4.472487771462744,6.212180210044431,7.178117045706399,7.534497433738168,6.304876050044889,7.545505100691072,5.077670274232752,3.953265239014844,4.609991295212678,2.9126498648972037,7.174426393677253,3.6016965164809576,4.515384460636948,2.503348735167504,8.129128434194934,2.9726926540042644,6.030998118171895,2.2539892662307865,6.597829097826899,6.8209447918806925,2.9523335663696857,8.18769847638827,4.950468414150123,6.1409831048784245,4.674121632632394,6.830610274357907,4.4376272483189485,3.7037651787934096,5.95837871156634,9.296618003796326,3.351910961103077,4.143230134776893,4.707082991771706,3.560714954474479,3.9845893503624565,8.328001662149257,2.1667154449664223,3.1811025507537978,2.4854268271702415,2.931683057059806,4.788685710613533,5.659068274843228,5.738227400367242,4.6937657122177825,6.860962797858116,8.65474342118394,4.814550423461808,6.107687869314374,4.720825666089835,6.4775154361842375,5.619119511453218,4.7136958148433585,2.5533605033353277,4.412781525338476,3.72137265947682,3.460742563789644,3.0197019144425483,6.602587527342561,4.687060688339892,2.526068811667588,4.698218478224414,2.8698714061777126,2.144046369616707,5.394376944957055,4.649615459063409,5.80683958179819,4.443606651475615,5.450881315273369,6.6503337538459295,7.198494153639083,5.228049047884462,3.9231491804723087,1.3840498067951599,4.524815928357506,7.211012193485511,7.027021314622255,7.212958362728285,6.658497136656478,3.3881895371560837,3.140778655782796,4.37364821133469,3.319039815562536,7.33360262826828,5.175524601089875,3.2047667506546134,2.702657543390911,3.228049047884462,9.322806969423008,5.561631630375841,5.8755343515732354,5.032100843167024,3.9873208659292536,3.1842802944193824,3.277984747299765,7.444849247263915,3.6701605141266334,4.595145567990858,3.8369340113210915,3.575312330687437,2.788685710613534,5.4571345943180365,2.4276061727818994,8.563615765629766,4.712045448553693,6.260590274730996,0,2.4594316186372973,2.8257856274647914,3.049630767724601,8.097242070141174,7.431622959713292,4.030336078370959,4.964398632203184,4.020591094773247,2.211012193485512,5.462052318796433,3.3881895371560837,2.411426245726465,5.0430819818956625,3.6064422281316078,3.613531652917927,3.883620816285671,2.8318772411916733,5.081936081719432,5.575917361118149,4.958842675243241,6.716579420796345,6.531225485053606,6.050284014301706,7.73125113789623,2.950468414150123,5.645009884391095,5.4015622072560685,3.840966704487421,4.3298411765306275,0.8155754288625726,3.7070829917717063,5.848748114611959,4.303050084681673,8.796785917134267,5.497931651896279,4.67581593117227,8.189231548647284,6.9092930858238235,4.772941337831336,5.6082178530214595,7.304419775319909,6.588864518190148,10.665637927637095,4.672990993827748,2.8399595874895316,5.305605789585429,3.0908534304511135,2.4854268271702415,2.693765712217783,2.942983598187102,2.5897634869849773,4.631686366311998,3.0125686735030555,5.651051691178929,5.488964817004848,3.1858665453113337,0,6.703073006328523,4.255500733148386,5.9593064897596575,5.218781167784069,3.968090752045256,4.8594726668519375,3.460742563789644,5.341274183692174,3.0089887832272546,2.032100843167024,2.169925001442312,3.356143810225275,4.814037647539975,3.4168397419128294,5.653060017104565,6.074462620704536,3.2016338611696504,6.9656688644419456,6.816215687622865,4.330558400030802,7.16380060395775,5.6061460780657235,6.093179846765504,3.5716768099709313,5.342341397431548,4.228049047884462,6.310885391185277,4.751677945687579,7.502871574574138,3.953265239014844,3.975446765640962,2.6599245584023783,2.430285272977781,5.7303682363490065,7.689648608288815,4.371558862611963,3.215678596607928,6.492173659610827,7.480507489974544,4.436295119839363,4.804260115634738,4.405311682597445,13.396289023308677,3.8914191868460786,3.24031432933371,1.899175630480513,3.2942531364445142,8.382105211486744,4.143230134776893,4.92552476974757,2.8439838440483265,2.8399595874895316,6.101187815259738,5.0232553523003025,4.583158003874408,3.176322772640463,8.996303999184056,3.8257856274647914,4.7755773609306535,8.12401788912289,5.9618548076361595,2.454175893185802,6.208088008224774,2.673556423990145,2.8972404255747994,6.791683858152045,7.137810883416096,5.8052924556007115,7.338067797542616,5.198494153639083,2.851998837112446,5.726013748860151,6.567880042052731,2.8155754288625725,2.2387868595871168,3.6769443591069124,4.318316841334983,2.1795110502715103,5.6884601404244926,0.8073549220576041,7.244982170376553,8.642268351758643,6.836681593327116,2.73768676140986,2.4646682670034443,5.964860664245848,2.7420062108667365,2.411426245726465,7.279842693520348,7.446835177086041,6.531849285751203,3.8982083525087177,4.721919445551544,5.500164679492167,5.429615964201735,0,3.7311832415722,4.726831217032493,4.904002316283692,4.561937059622433,7.438459204423481,4.631686366311998,7.300764373248062,4.112700132749362,7.9329824099498465,5.1081062236861285,5.868143479666726,4.161081482277184,5.996162742681061,3.097610796626422,4.263785613890803,6.339493737901733,4.370861740085285,4.842978831788326,5.636334602442182,2.84197311892718,4.7713574089905615,4.257010618206024,6.916596067892185,6.015247773698937,4.595145567990858,6.7410624357490905,3.4168397419128294,3.823749360308273,4.279471295644468,4.7114949066500875,2.883620816285671,2.969012307516316,5.089159131911238,2.3362833878644325,8.080604371320732,1.8559896973084806,6.941341105485299,8.14639047567022,7.163498732282879,8.281651890258825,6.329482431025636,2.140778655782796,2.3103401206121505,9.500782139394977,4.674686619927999,7.249066033855867,3.1060132376221192,7.897179908573977,7.6546360285279675,2.944858445807539,4.94673086014031,2.711494906650088,6.054414387917307,2.788685710613534,4.198494153639083,2.9467308601403097,4.060047383669939,3.442280035252584,2.7070829917717063,5.9586107120558465,2.4982508675278257,3.6484654430273142,5.0870384567669475,5.130519083027338,1.8155754288625725,2.6064422281316078,5.673839055990439,6.37364821133469,5.609991295212678,6.076388068512841,4.695437110405369,7.459267666703387,5.211401637418471,3.063502942306158,5.931210274803932,6.819540460505722,0.918386234446348,4.878234879112339,6.098663781886574,4.098453246309274,3.4154882710497003,9.8361608416947,5.3103401206121505,4.20633064787105,6.436128517259249,4.381283372503783,5.0259146987507926,5.226123143125237,5.73903797911741,4.110196177754199,3.8042601156347384,4.503984703897613,6.067165427017112,3.7322691995014496,6.121844297876376,8.248591759535907,7.711013010084339,5.782670658677007,6.0774566524005875,7.592232766722112,6.917909073852982,4.423578170981798,4.901108243014512,3.795974694206668,6.907251224292033,3.189033824390017,2.4646682670034443,6.061128135118254,2.733354340613827,6.664198369291911,5.166715444966422,8.492293852480074,7.279749853014451,2.807354922057604,8.362689046726734,2.3248106034204836,4.826294245499635,2.321928094887362,4.238022517825721,4.232660756790275,5.51127826508303,3.7644735509926663,5.290940402403678,7.535353147842131,4.329123596291566,5.908092340818271,2.570462931026041,4.634012356402116,6.99491908871511,2.560714954474479,2.3589588258323295,2.8278190246173196,5.439623137557117,2.927896453728821,7.476543706214256,8.334094473911568,6.726695004499616,2.097610796626422,3.619413010597937,2.523561956057013,7.136683577697236,8.908992993250534,4.4950555283680185,6.970393537914677,8.089159131911238,2.996388746447621,3.7506065048355923,5.468583317006828,4.732811872082943,2.5360529002402097,2.729008870337863,7.712939631305699,4.038260575175349,4.514122260170708,6.948834425578686,2.0036022366801953,2.169925001442312,6.04023491675437,7.246788093844365,7.018923431842335,5.046141781644721,5.637784110291324,6.976363635732762,5.840463233869542,6.170125361839952,4.760753208062987,3.089159131911238,2.8399595874895316,2.3978029618624896,2.3305584000308026,3.0925457415435655,2.6599245584023783,7.9217814432688,5.963242904318827,2.4515408330178317,5.976821852360685,1.7865963618908067,3.221877081077034,3.0036022366801953,2.722466024471091,4.2024177215751815,5.9025560053446275,3.997292407636508,5.323730337521348,6.463851292611099,5.938756261492295,7.593951283948411,5.233044401265581,4.79025073883523,5.718361626138354,6.435295215647167,1.3504972470841332,4.12267271882414,6.361768359419154,6.8820316083521,2.0250287944915226,2.298658315564515,3.636914580355878,7.934634441094905,5.246408087246385,3.918386234446348,3.929790997718597,4.147306698780294,5.169925001442312,5.542258049766918,4.693208148910016,3.6971065744769747,5.707082991771706,3.363171077119244,5.035183996618336,4.274261661257048,3.321928094887362,5.9740703670240265,2.3757345385831563,10.174663577413781,2.8439838440483265,5.24260250592898,3.4581194811745064,2.063502942306158,3.1858665453113337,4.7692427951405385,4.301587646603187,5.986638471730853,5.058749412335524,4.556429415449573,2.5360529002402097,2.4489009511451276,4.974988111991927,6.248307120087241,4.346956889378885,2.641546029087524,2.8579809951275723,7.624539603781834,5.197314999473818,5.38404980679516,3.2387868595871163,4.464668267003444,3.955126781261366,6.258895755075436,3.1358631653686793,7.831180547126736,1.3785116232537298,4.934044647112246,4.503984703897613,4.100977647724821,4.86195536414487,8.439830883981392,7.851624127346895,6.091065077930535,7.190022174802645,5.992541859109112,2.983677694698067,7.161484638641659,2.6959938131099,8.21003812347289,7.461643148073906,4.848497755385146,3.192194165283345,1.6690267655096307,8.80290404741688,6.926948247949772,0,3.812498225333564,5.6806056662025926,9.871443254934135,6.459923362676901,3.8298495598446904,5.157447996274298,5.537296067090842,3.5147534984397533,3.2249663650002742,5.699884728697476,3.035623909730721,6.914444931746792,5.338780943894699,9.013601832683456,7.578712331048165,3.3405622690264134,2.3950627995175777,7.221587121264805,7.78959885005233,2.2234225499349374,2.8439838440483265,2.950468414150123,5.833649127594984,5.802968651005583,4.618238655595454,7.581803148516525,4.948600847493355,5.416164164733128,5.012121672712217,2.9373443921502322,2.3923174227787602,5.259649120649155,7.119252412157343,5.452858964713811,2.5160151470036647,2.788685710613534,5.040892430646901,6.390942772802543,6.032541697270038,6.177519202597814,6.111865963867557,7.0083165637349865,6.074034394212556,4.471187460386985,9.507715369062607,6.152994605492435,4.169123281476757,4.706530552538117,3.0426443374084937,5.305605789585429,6.942279901705763,2.4724877714627436,11.02948049374083,7.127736443588829,3.594548549550354,3.8063240573900288,2.144046369616707,7.036942844798256,4.022367813028454,2.0214797274104517,8.589726041133547,6.218781167784069,3.685940148445977,7.566662962962611,6.652343078740216,4.210232990095849,9.47854718263086,5.991861930556323,7.500961352460057,5.5855634984556914,2.319039815562536,2.9049657186840263,1.9781956296816516,5.509062386361898,5.592756010441026,4.39780296186249,3.431622959713292,11.193297418854996,7.925940095994448,8.041385369486115,5.051807107013298,2.3673710656485296,5.208673319629471,5.532940288372874,4.598722499676621,2.903038270112912,6.098453246309274,2.967168607532628,4.919340082442012,7.971830914780502,4.189033824390017,3.10936055940423,4.053111336459563,3.944858445807539,5.71699089440494,4.2787282129389395,3.7246502717329673,4.568032104771279,6.400025517914023,3.0443941193584534,9.140190703191836,6.7998644108818525,3.3923174227787602,4.880685525261125,10.32396671481508,2.521050736900963,1.8836208162856714,6.547203024756931,5.63023071585969,5.915042791371977,5.713970692436053,8.823876711241038,2.6530600171045644,4.462706750670158,4.524189078449365,5.342341397431548,6.70362677086498,8.008596693255853,5.81326814176608,5.795715006501729,1.5897634869849773,2.5058909297299574,4.153805336079036,4.707082991771706,5.617651119427331,5.7615512324444795,6.115615931428109,5.51443791382931,8.873136468833458,4.8811751553492755,3.4276061727818994,7.013685569928653,3.37851162325373,4.390942772802543,4.544114402209474,8.617283787779035,5.39677601078753,4.096767854714689,3.169925001442312,2.881664619320345,3.389566811762726,5.949301468328278,5.9095331149468935,2.2172307162206693,5.687900522480746,8.295998461911388,4.963011647599427,5.648177795724818,3.8509993947164736,3.669026765509631,5.924575003392735,1.2570106182060237,8.1184220243132,6.3103401206121505,4.59215800212536,2.169925001442312,2.771885578515366,5.309976492370556,5.718909554379232,7.2325648297287755,3.720278465233327,3.403267722339301,4.138323004056484,8.321792835886914,8.384999544730041,6.675392543042777,7.848935855524565,8.135247549809566,7.474760499576783,2.01435529297707,4.661635602335959,2.899175630480513,6.950701690089008,3.4776773275653072,6.275752048828529,2.7782085763980877,8.645802514606062,8.927066807759255,5.377123749129487,8.785648628652076,3.554588851677637,5.8529975876133165,7.093391153253013,3.8639384504239715,6.390082950951663,3.0214797274104517,5.947198584262056,2.0565835283663674,8.560600329022595,4.708187236020708,3.1473066987802936,5.260778431893426,2.3978029618624896,5.496654082593496,8.356011721502778,5.307064162255372,5.485426827170242,8.478971805032943,2.414135532984451,2.776103988073164,4.7420062108667365,2.4828482830684653,6.543805175964397,8.5918962955207,4.361066488794321,5.294620748891627,6.553360503335328,7.088629255297176,5.1201860275318625,7.00943675560809,4.090853430451113,6.7014104362328135,7.415741768290091,5.348728154231077,6.705563274566321,5.445263208140443,4.552131108253784,3.4436066514756147,3.5631581304028077,6.678494507773734,4.478971805032942,6.1657109924849305,3.403267722339301,4.534186139090097,3.0214797274104517,6.238786859587116,1.8639384504239715,7.020924396158786,4.20163386116965,2.7548875021634687,2.604071323668861,7.128870759465125,2.5459683691052923,5.414473836430928,9.995102877826515,4.861459166361513,5.25285462595139,6.31922050252387,5.308156975114762,5.526068811667588,5.763145958149476,5.903279342056139,5.825531251189622,7.048214384608036,2.513490745588118,5.944155663440363,5.856985689782205,3.412781525338476,2.956056652412403,5.664482840364682,1.74416109557041,4.886062338345599,2.4646682670034443,2.4515408330178317,6.433126376206435,7.7755773609306535,6.705839705883682,8.330737650206952,4.727375938594897,5.777945670695148,11.69784504363786,7.7627474419531515,13.587327482778312,6.558114538598396,6.332349900373874,3.2265085298086795,2.875780063068488,6.70473366267406,4.223422549934937,3.2387868595871163,6.136683577697236,12.23806432823473,5.572889668420581,5.849999259466098,5.533563348214512,5.325530331567558,2.797012977836145,4.150559676575382,8.298429210890312,7.731454807698346,8.903490246967104,2.8094144442358986,4.7824085649273735,8.461602224618334,5.583158003874408,3.060047383669939,6.429950657403768,7.193278935521388,2.3589588258323295,3.7311832415722,2.899175630480513,2.298658315564515,4.235727059838059,2.3812833725037836,11.663771557482411,5.684257705058932,5.212569338850806,6.9787676510367875,4.044394119358453,6.092334311186202,9.582254908357664,2.5160151470036647,6.402415216824399,5.740387932468353,5.784503982929566,2.6064422281316078,4.362469888750209,5.164705840182799,3.4208865749755315,3.948600847493356,7.965841991309597,4.976821852360685,2.722466024471091,2.7398481026993275,4.528571318870758,7.677578712165378,3.41006969175638,2.4905701304462013,3.5147534984397533,7.5673476962842585,6.849624029924543,2.833902076669163,7.406162478777607,8.131085259578953,3.176322772640463,8.148324042480084,2.657640005207824,3.350497247084133,6.8203066288938246,4.483493351284395,1.713695814843359,5.872582544833223,2.722466024471091,5.7742599514327635,5.704318677760832,4.760220946466509,4.294253136444514,6.037162550127378,7.151777654732053,3.215678596607928,4.742545233866053,5.613531652917927,5.727375938594897,7.7780114016125745,8.009884588931843,6.549977142771825,2.0461417816447205,2.0565835283663674,6.66746640495737,4.62760683812965,7.884597920990064,8.571145863602528,2.3161457422933562,8.563272552819562,7.111970261358359,6.001577085389535,3.4369613378336026,6.435461914479276,3.5084286525318573,3.940166750482817,2.5459683691052923,1.6644828403646825,4.94345253855882,6.003827078111476,4.960697039304312,6.684117412839159,5.612352498752662,5.609991295212678,2.62993940943954,3.611172380044005,5.522620761042079,5.891419186846079,3.6530600171045644,3.4581194811745064,3.1667154449664223,4.581351247168778,6.764340846670181,8.066089190457772,6.614857050384495,5.767654798237347,3.1953475983222193,2.9726926540042644,5.279471295644468,7.5105663894487495,5.02724253597374,8.25176642434399,7.822411496360477,5.334496768390418,5.715893370547605,7.3049672876759635,5.998646839005795,6.9725777851765525,2.4594316186372973,5.666472568842073,3.8728287595348854,6.052024560482831,3.631104282365877,2.3015876466031866,2.37016428054021,4.3384244147835584,6.546122758857796],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Ridge()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_pred_logRidge"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression ridge\n","# réglage des paramètre pour la gridsearch\n","alphasridge_log = np.logspace(-3, 5, 1000)\n","param_gridRidge_log = {'ridge__alpha': alphasridge_log}\n","\n","GridRidge_log, \\\n","BestParametresRidge_log, \\\n","ScoresRidge_log, \\\n","TotalGHGEmissions_pred_logRidge_log, \\\n","figRidge_log = reg_modelGrid(model=Ridge(),\n","                            scaler=scaler,\n","                            X_train=BEBNumM_train,\n","                            X_test=BEBNumM_test,\n","                            y_train=TotalGHGEmissions_train_log,\n","                            y_test=TotalGHGEmissions_test_log,\n","                            y_test_name='TotalGHGEmissions_test_log',\n","                            y_pred_name='TotalGHGEmissions_pred_logRidge',\n","                            score=score,\n","                            param_grid=param_gridRidge_log)\n","\n","print(BestParametresRidge_log)\n","print(ScoresRidge_log)\n","figRidge_log.show()\n"]},{"cell_type":"code","execution_count":116,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[2.057187486718974,2.0571874847997975,2.0571874828449053,2.0571874808536323,2.057187478825301,2.0571874767592226,2.057187474654694,2.057187472511,2.0571874703274116,2.0571874681031863,2.0571874658375684,2.057187463529787,2.057187461179057,2.05718745878458,2.0571874563455417,2.057187453861112,2.057187451330447,2.0571874487526864,2.0571874461269535,2.057187443452355,2.057187440727982,2.0571874379529085,2.0571874351261905,2.057187432246866,2.0571874293139585,2.0571874263264687,2.057187423283381,2.057187420183662,2.0571874170262565,2.057187413810091,2.057187410534073,2.0571874071970884,2.057187403798001,2.0571874003356565,2.057187396808878,2.057187393216466,2.057187389557199,2.057187385829832,2.0571873820330993,2.0571873781657084,2.0571873742263453,2.0571873702136703,2.0571873661263194,2.057187361962902,2.057187357722004,2.057187353402182,2.057187349001968,2.0571873445198663,2.0571873399543517,2.0571873353038734,2.0571873305668484,2.057187325741668,2.0571873208266904,2.057187315820245,2.057187310720629,2.057187305526109,2.0571873002349195,2.05718729484526,2.057187289355299,2.0571872837631697,2.057187278066971,2.0571872722647653,2.057187266354581,2.0571872603344077,2.0571872542021987,2.0571872479558695,2.057187241593296,2.057187235112315,2.057187228510723,2.0571872217862746,2.0571872149366848,2.057187207959624,2.05718720085272,2.057187193613557,2.057187186239673,2.0571871787285616,2.057187171077668,2.0571871632843917,2.0571871553460825,2.057187147260042,2.05718713902352,2.057187130633717,2.05718712208778,2.0571871133828035,2.057187104515828,2.0571870954838385,2.057187086283764,2.0571870769124763,2.0571870673667907,2.0571870576434597,2.0571870477391783,2.0571870376505794,2.057187027374232,2.057187016906643,2.057187006244253,2.057186995383437,2.057186984320503,2.0571869730516883,2.057186961573162,2.0571869498810225,2.057186937971293,2.0571869258399254,2.057186913482794,2.057186900895699,2.0571868880743596,2.057186875014417,2.0571868617114317,2.057186848160879,2.0571868343581534,2.0571868202985613,2.057186805977323,2.057186791389569,2.0571867765303398,2.0571867613945836,2.0571867459771536,2.057186730272809,2.05718671427621,2.057186697981918,2.057186681384393,2.057186664477993,2.0571866472569686,2.0571866297154653,2.0571866118475195,2.0571865936470566,2.057186575107888,2.057186556223711,2.0571865369881053,2.057186517394531,2.057186497436326,2.0571864771067063,2.0571864563987594,2.057186435305444,2.05718641381959,2.057186391933893,2.0571863696409105,2.0571863469330647,2.057186323802635,2.057186300241758,2.0571862762424225,2.05718625179647,2.057186226895589,2.057186201531314,2.0571861756950223,2.0571861493779293,2.0571861225710877,2.057186095265385,2.057186067451537,2.0571860391200882,2.057186010261406,2.0571859808656803,2.057185950922917,2.057185920422936,2.057185889355369,2.057185857709653,2.0571858254750306,2.0571857926405426,2.0571857591950256,2.057185725127111,2.0571856904252153,2.057185655077542,2.0571856190720745,2.0571855823965706,2.057185545038564,2.0571855069853533,2.057185468224003,2.0571854287413345,2.0571853885239264,2.0571853475581063,2.0571853058299476,2.0571852633252643,2.0571852200296075,2.0571851759282582,2.057185131006224,2.0571850852482347,2.057185038638734,2.057184991161877,2.0571849428015243,2.057184893541236,2.0571848433642663,2.0571847922535587,2.057184740191738,2.0571846871611057,2.057184633143635,2.0571845781209643,2.057184522074388,2.0571844649848545,2.0571844068329574,2.057184347598929,2.0571842872626327,2.05718422580356,2.0571841632008185,2.0571840994331287,2.057184034478813,2.057183968315793,2.057183900921578,2.05718383227326,2.057183762347503,2.0571836911205397,2.057183618568158,2.0571835446656968,2.0571834693880353,2.057183392709587,2.0571833146042873,2.0571832350455894,2.057183154006449,2.0571830714593218,2.0571829873761507,2.057182901728355,2.0571828144868247,2.0571827256219057,2.0571826351033944,2.0571825429005246,2.057182448981958,2.057182353315773,2.057182255869455,2.057182156609883,2.057182055503321,2.057181952515408,2.0571818476111376,2.057181740754859,2.057181631910254,2.057181521040331,2.0571814081074087,2.0571812930731075,2.0571811758983314,2.0571810565432584,2.0571809349673265,2.057180811129218,2.0571806849868466,2.0571805564973453,2.057180425617048,2.0571802923014784,2.05718015650533,2.057180018182456,2.057179877285853,2.057179733767639,2.057179587579046,2.0571794386703965,2.057179286991089,2.0571791324895843,2.057178975113381,2.057178814809003,2.0571786515219803,2.0571784851968298,2.0571783157770365,2.057178143205035,2.057177967422189,2.057177788368772,2.0571776059839473,2.057177420205748,2.0571772309710536,2.0571770382155705,2.057176841873811,2.057176641879068,2.057176438163396,2.057176230657585,2.05717601929114,2.057175803992254,2.057175584687786,2.057175361303235,2.057175133762716,2.0571749019889323,2.057174665903151,2.0571744254251763,2.0571741804733215,2.057173930964381,2.057173676813604,2.0571734179346626,2.057173154239628,2.0571728856389333,2.05717261204135,2.057172333353953,2.057172049482091,2.0571717603293527,2.0571714657975377,2.057171165786618,2.05717086019471,2.057170548918034,2.057170231850885,2.0571699088855913,2.0571695799124834,2.0571692448198533,2.057168903493918,2.05716855581878,2.0571682016763897,2.057167840946505,2.05716747350665,2.0571670992320734,2.0571667179957074,2.057166329668125,2.0571659341174926,2.0571655312095323,2.0571651208074693,2.057164702771992,2.057164276961198,2.0571638432305543,2.0571634014328413,2.057162951418106,2.0571624930336148,2.0571620261237937,2.057161550530185,2.057161066091386,2.0571605726430007,2.05716007001758,2.0571595580445674,2.0571590365502415,2.057158505357655,2.057157964286579,2.0571574131534396,2.057156851771255,2.057156279949575,2.057155697494417,2.0571551042081984,2.05715449988967,2.057153884333851,2.057153257331958,2.057152618671334,2.057151968135379,2.057151305503475,2.0571506305509137,2.0571499430488185,2.05714924276407,2.057148529459226,2.057147802892442,2.05714706281739,2.057146308983176,2.057145541134256,2.057144759010348,2.0571439623463474,2.0571431508722373,2.0571423243129945,2.0571414823885035,2.057140624813457,2.0571397512972616,2.0571388615439425,2.0571379552520415,2.0571370321145173,2.0571360918186423,2.0571351340458985,2.0571341584718694,2.0571331647661326,2.057132152592149,2.0571311216071484,2.0571300714620175,2.057129001801183,2.057127912262489,2.0571268024770806,2.057125672069278,2.057124520656452,2.0571233478488944,2.057122153249691,2.0571209364545866,2.057119697051851,2.0571184346221414,2.0571171487383633,2.0571158389655286,2.057114504860608,2.0571131459723855,2.0571117618413095,2.057110351999337,2.0571089159697786,2.0571074532671436,2.057105963396972,2.057104445855676,2.057102900130371,2.057101325698704,2.057099722028681,2.057098088578493,2.057096424796331,2.05709473012021,2.0570930039777773,2.0570912457861246,2.0570894549515986,2.0570876308695976,2.0570857729243794,2.0570838804888525,2.0570819529243716,2.057079989580525,2.057077989794923,2.0570759528929754,2.057073878187675,2.057071764979365,2.0570696125555132,2.057067420190476,2.0570651871452608,2.057062912667282,2.057060595990116,2.057058236333246,2.057055832901812,2.057053384886345,2.057050891462504,2.057048351790807,2.057045765016357,2.0570431302685597,2.0570404466608436,2.0570377132903674,2.0570349292377252,2.057032093566653,2.0570292053237162,2.057026263538005,2.057023267220816,2.0570202153653345,2.057017106946303,2.057013940919694,2.057010716222368,2.0570074317717344,2.0570040864653945,2.057000679180792,2.056997208774848,2.056993674083595,2.056990073921795,2.0569864070825683,2.056982672336999,2.0569788684337422,2.056974994098623,2.0569710480342285,2.056967028919492,2.0569629354092713,2.0569587661339197,2.0569545196988464,2.056950194684077,2.056945789643798,2.0569413031058987,2.056936733571504,2.0569320795144987,2.056927339381045,2.0569225115890903,2.0569175945278695,2.056912586557396,2.056907486007943,2.056902291179523,2.0568970003413503,2.0568916117312988,2.056886123555353,2.0568805339870426,2.056874841166876,2.0568690432017585,2.056863138164405,2.056857124092739,2.056850998989286,2.056844760820552,2.0568384075163975,2.0568319369693993,2.056825347034196,2.056818635526834,2.056811800224093,2.056804838862806,2.0567977491391667,2.056790528708026,2.0567831751821783,2.056775686131633,2.05676805908288,2.0567602915181356,2.0567523808745864,2.056744324543613,2.0567361198700067,2.056727764151167,2.0567192546362985,2.056710588525582,2.056701762969341,2.056692775067194,2.056683621867193,2.0566743003649472,2.0566648075027345,2.056655140168601,2.056645295195446,2.0566352693600893,2.0566250593823328,2.056614661923996,2.0566040735879527,2.0565932909171356,2.0565823103935417,2.0565711284372132,2.0565597414052066,2.056548145590548,2.0565363372211705,2.0565243124588366,2.0565120673980486,2.0564995980649377,2.056486900416139,2.056473970337657,2.0564608036437004,2.0564473960755167,2.056433743300199,2.056419840909481,2.0564056844185146,2.0563912692646285,2.056376590806073,2.056361644320744,2.0563464250048935,2.0563309279718167,2.056315148250529,2.0562990807844184,2.0562827204298855,2.056266061954959,2.0562491000378977,2.0562318292657755,2.056214244133041,2.0561963390400666,2.056178108291673,2.0561595460956363,2.056140646561177,2.056121403697431,2.0561018114118963,2.0560818635088673,2.056061553687842,2.056040875541919,2.056019822556163,2.0559983881059614,2.055976565455355,2.0559543477553497,2.055931728042208,2.055908699235723,2.0558852541374653,2.0558613854290186,2.055837085670187,2.055812347297187,2.055787162620814,2.0557615238245917,2.0557354229629015,2.0557088519590865,2.0556818026035373,2.0556542665517576,2.0556262353224066,2.055597700295322,2.0555686527095203,2.0555390836611744,2.055508984101574,2.0554783448350635,2.0554471565169536,2.055415409651414,2.0553830945893514,2.055350201526251,2.0553167205000125,2.055282641388751,2.0552479539085846,2.055212647611397,2.0551767118825746,2.055140135938731,2.0551029088253983,2.055065019414704,2.0550264564030223,2.0549872083086056,2.0549472634691903,2.0549066100395827,2.0548652359892223,2.0548231290997214,2.054780276962384,2.0547366669757015,2.0546922863428225,2.0546471220690075,2.0546011609590518,2.054554389614693,2.05450679443199,2.054458361598685,2.054409077091536,2.0543589266736317,2.0543078958916783,2.05425597007327,2.0542031343241285,2.0541493735253225,2.0540946723304687,2.0540390151628984,2.0539823862128124,2.053924769434403,2.05386614854296,2.0538065070119447,2.053745828070049,2.0536840946982258,2.0536212896266948,2.0535573953319273,2.053492394033606,2.0534262676915622,2.053358998002686,2.053290566397817,2.0532209540386073,2.053150141814366,2.0530781103388724,2.0530048399471745,2.052930310692358,2.0528545023422957,2.05277739437637,2.0526989659821786,2.0526191960522113,2.052538063180511,2.0524555456593077,2.052371621475634,2.0522862683079204,2.052199463522567,2.0521111841704984,2.0520214069837017,2.051930108371738,2.051837264418244,2.0517428508774196,2.0516468431704853,2.0515492163821447,2.0514499452570165,2.051349004196067,2.0512463672530257,2.05114200813079,2.0510359001778307,2.0509280163845847,2.050818329379847,2.050706811427161,2.050593434421209,2.0504781698842085,2.0503609889623116,2.050241862422015,2.05012076064658,2.0499976536324716,2.0498725109858116,2.0497453019188567,2.0496159952465023,2.049484559382816,2.049350962337608,2.0492151717130356,2.049077154700262,2.048936878076153,2.0487943082000375,2.048649411010527,2.0485021520224,2.0483524963235604,2.048200408572081,2.0480458529933294,2.0478887933771905,2.047729193075393,2.0475670149989442,2.047402221615692,2.0472347749480013,2.0470646365705845,2.0468917676084706,2.046716128735129,2.046537680170773,2.0463563816808286,2.046172192574607,2.0459850717041683,2.0457949774634065,2.045601867787358,2.0454057001517514,2.0452064315728085,2.045004018607317,2.044798417352978,2.0445895834490546,2.0443774720773265,2.044162037963379,2.0439432353782214,2.043721018140278,2.0434953396177447,2.043266152731339,2.0430334099574634,2.042797063331789,2.042557064453294,2.0423133644887557,2.04206591417773,2.041814663838032,2.041559563371738,2.0413005622717226,2.0410376096287646,2.0407706541392225,2.0404996441133214,2.04022452748405,2.0399452518167087,2.0396617643191117,2.0393740118524786,2.039081940943025,2.038785497794284,2.038484628300165,2.0381792780587897,2.0378693923871074,2.037554916336325,2.0372357947081636,2.0369119720719646,2.0365833927826715,2.0362500009996958,2.035911740706699,2.035568555732302,2.0352203897717454,2.0348671864095182,2.0345088891429697,2.034145441406938,2.0337767865993817,2.0334028681080687,2.033023629338309,2.0326390137417634,2.0322489648463327,2.031853426287152,2.031452341838694,2.031045655447994,2.0306333112690154,2.0302152536981604,2.0297914274109337,2.0293617773997696,2.028926249013031,2.0284847879951826,2.0280373405281464,2.0275838532738373,2.027124273417886,2.026658548714546,2.0261866275327884,2.0257084589035697,2.0252239925682867,2.024733179028394,2.02423596959619,2.0237323164467558,2.0232221726710335,2.0227054923300387,2.0221822305101846,2.021652343379706,2.0211157882461612,2.0205725236149936,2.020022509249126,2.019465706229571,2.01890207701702,2.018331585514389,2.017754197130288,2.017169878843386,2.0165785992676186,2.015980328718228,2.015375039278567,2.0147627048676493,2.0141433013083834,2.0135168063964572,2.012883199969813,2.0122424639786765,2.0115945825560675,2.0109395420887606,2.010277331288616,2.00960794126424,2.0089313655928986,2.008247600392637,2.0075566443945214,2.0068584990149536,2.0061531684279776,2.005440659637516,2.00472098254945,2.003994150043493,2.0032601780447536,2.0025190855949355,2.001770894923078,2.001015631515771,2.0002533241867497,1.9994840051458023,1.9987077100668906,1.9979244781554162,1.9971343522145304,1.9963373787104175,1.9955336078364527,1.994723093576156,1.993905893764848,1.9930820701499266,1.9922516884496695,1.991414818410482,1.9905715338624947,1.9897219127734285,1.9888660373006382,1.9880039938412435,1.9871358730802708,1.9862617700367058,1.9853817841073833,1.984496019108627,1.9836045833155478,1.9827075894989323,1.9818051549596292,1.9808974015603567,1.9799844557548532,1.9790664486142984,1.9781435158509186,1.9772157978387135,1.9762834396312268,1.9753465909762913,1.9744054063276817,1.9734600448536088,1.9725106704419868,1.9715574517024206,1.970600561964843,1.969640179274755,1.9686764863850037,1.9677096707440533,1.9667399244806945,1.965767444385147,1.9647924318865102,1.9638150930265108,1.9628356384295274,1.9618542832688273,1.9608712472290044,1.9598867544645748,1.9589010335547,1.9579143174540126,1.9569268434395242,1.9559388530535866,1.9549505920428938,1.9539623102935024,1.9529742617618655,1.9519867044018564,1.9509999000877898,1.950014114533419,1.949029617206922,1.9480466812418555,1.9470655833441086,1.9460866036948257,1.9451100258493397,1.9441361366321037,1.9431652260276402,1.9421975870675285,1.9412335157134428,1.940273310736261,1.9393172735912714,1.9383657082895058,1.9374189212652162,1.936477221239543,1.9355409190803943,1.934610327658584,1.9336857617002614,1.9327675376356823,1.9318559734443599,1.9309513884966514,1.9300541033918255,1.9291644397926753,1.928282720256726,1.9274092680641046,1.9265444070421345,1.9256884613867293,1.9248417554806507,1.9240046137087188,1.9231773602700382,1.9223603189873484,1.9215538131135674,1.9207581651356338,1.9199736965757452,1.9192007277900907,1.9184395777651961,1.91769056391199,1.9169540018577078,1.9162302052357678,1.9155194854737416,1.9148221515795647,1.9141385099261226,1.9134688640343687,1.9128135143551284,1.9121727580497516,1.9115468887697902,1.910936196435864,1.9103409670159213,1.909761482303064,1.9091980196931515,1.908650851962387,1.9081202470450953,1.907606467811921,1.9071097718486747,1.9066304112360601,1.9061686323305338,1.905724675546543,1.9052987751404042,1.9048911589960862,1.9045020484131705,1.90413165789727,1.90378019495319,1.9034478598811184,1.903134845576148,1.902841337331429,1.9025675126452508,1.9023135410323753,1.9020795838399187,1.9018657940681098,1.9016723161962301,1.9014992860140605,1.9013468304591477,1.9012150674602086,1.9011041057869886,1.9010140449068849,1.9009449748486449,1.9008969760734444,1.9008701193536446,1.900864465659522,1.9008800660542569,1.9009169615974517,1.9009751832574602,1.9010547518327652,1.9011556778826662,1.9012779616675004,1.901421593098619,1.9015865516983177,1.9017728065699155,1.9019803163781503,1.9022090293400375,1.9024588832263436,1.9027298053737696,1.903021712707956,1.9033345117773792,1.9036680987981875,1.9040223597100205,1.9043971702428066,1.9047923959945432,1.9052078925200076,1.9056435054303484,1.9060990705034768,1.906574413805144,1.9070693518205872,1.9075836915965845,1.9081172308937462,1.9086697583488506,1.9092410536470006,1.9098308877033627,1.9104390228542307,1.9110652130571295,1.91170920409966,1.9123707338167688,1.913049532316105,1.9137453222111094,1.914457818861474,1.9151867306205805,1.915931759089529,1.9166925993773465,1.9174689403669543,1.9182604649864683,1.9190668504853918,1.9198877687152613,1.9207228864142898,1.921571865495558,1.9224343633382883,1.9233100330817514,1.9241985239213366,1.92509948140633,1.9260125477389423,1.926937362074127,1.927873560819744,1.9288207779366187,1.9297786452380563,1.9307467926883848,1.9317248487000955,1.9327124404291716,1.9337091940681994,1.9347147351368683,1.935728688769481,1.9367506799990948,1.9377803340379571,1.9388172765538685,1.9398611339421694,1.9409115335930167,1.941968104153665,1.9430304757854646,1.9440982804153097,1.945171151981281,1.946248726672259,1.9473306431612716,1.9484165428323916,1.9495060700009823,1.9505988721271272,1.9516946000220927,1.9527929080476774,1.9538934543083328,1.9549959008359437,1.95609991376718,1.9572051635133434,1.958311324922645,1.9594180774348715,1.9605251052284,1.961632097359556,1.9627387478942904,1.9638447560322017,1.9649498262229101,1.9660536682748244,1.9671559974563386,1.9682565345895207,1.9693550061363514,1.9704511442775978,1.971544686984399,1.9726353780826642,1.9737229673103875,1.974807210367985,1.9758878689617831,1.9769647108407775,1.9780375098267988,1.9791060458382297,1.980170104907409,1.9812294791918852,1.9822839669796675,1.9833333726886384,1.9843775068602931,1.985416186147966,1.9864492332997308,1.9874764771361266,1.9884977525229082,1.9895129003389784,1.990521767439698,1.991524206615739,1.9925200765476763,1.9935092417564868,1.9944915725501453,1.9954669449664935,1.996435240712567,1.997396347100557,1.9983501569805846,1.9992965686704665,2.0002354858826488,2.001166817648476,2.0020904782399747,2.0030063870893158,2.003914468706121,2.0048146525927804]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[2.553917974058127,2.5539179684251265,2.5539179626872954,2.553917956842683,2.553917950889301,2.5539179448251255,2.553917938648095,2.5539179323561094,2.553917925947029,2.553917919418675,2.553917912768828,2.5539179059952257,2.5539178990955667,2.5539178920675036,2.5539178849086475,2.5539178776165636,2.5539178701887737,2.5539178626227517,2.5539178549159254,2.553917847065674,2.5539178390693276,2.55391783092417,2.553917822627429,2.553917814176284,2.553917805567864,2.5539177967992392,2.553917787867429,2.5539177787693976,2.5539177695020507,2.553917760062237,2.553917750446748,2.5539177406523144,2.553917730675604,2.5539177205132257,2.5539177101617256,2.5539176996175827,2.553917688877213,2.553917677936963,2.5539176667931143,2.553917655441877,2.5539176438793927,2.5539176321017285,2.5539176201048823,2.553917607884773,2.5539175954372464,2.5539175827580705,2.553917569842933,2.5539175566874457,2.553917543287132,2.553917529637439,2.5539175157337226,2.553917501571258,2.5539174871452284,2.5539174724507294,2.5539174574827648,2.5539174422362447,2.553917426705987,2.553917410886709,2.553917394773034,2.553917378359482,2.5539173616404742,2.553917344610323,2.5539173272632407,2.553917309593328,2.5539172915945776,2.5539172732608693,2.5539172545859694,2.55391723556353,2.553917216187082,2.553917196450036,2.553917176345684,2.553917155867189,2.5539171350075884,2.553917113759791,2.5539170921165706,2.5539170700705704,2.5539170476142923,2.553917024740103,2.5539170014402246,2.5539169777067356,2.553916953531566,2.5539169289064962,2.553916903823154,2.5539168782730104,2.553916852247379,2.5539168257374105,2.553916798734092,2.553916771228241,2.553916743210508,2.553916714671365,2.553916685601109,2.5539166559898563,2.553916625827539,2.5539165951039022,2.5539165638084995,2.553916531930691,2.5539164994596386,2.5539164663843,2.5539164326934323,2.553916398375579,2.553916363419072,2.5539163278120274,2.553916291542338,2.5539162545976724,2.5539162169654697,2.5539161786329343,2.553916139587035,2.553916099814494,2.55391605930179,2.5539160180351494,2.553915976000541,2.5539159331836734,2.5539158895699887,2.553915845144659,2.5539157998925797,2.553915753798365,2.553915706846343,2.553915659020552,2.5539156103047294,2.553915560682314,2.5539155101364335,2.553915458649902,2.5539154062052156,2.5539153527845437,2.5539152983697235,2.553915242942253,2.5539151864832896,2.5539151289736366,2.553915070393741,2.553915010723687,2.5539149499431884,2.5539148880315774,2.5539148249678076,2.553914760730438,2.553914695297627,2.553914628647129,2.553914560756284,2.5539144916020105,2.5539144211607963,2.553914349408693,2.5539142763213047,2.553914201873784,2.5539141260408207,2.5539140487966314,2.553913970114954,2.5539138899690403,2.553913808331641,2.5539137251749997,2.5539136404708462,2.5539135541903826,2.5539134663042757,2.5539133767826447,2.5539132855950557,2.5539131927105063,2.553913098097418,2.5539130017236245,2.55391290355636,2.5539128035622523,2.5539127017073042,2.553912597956889,2.553912492275734,2.5539123846279086,2.5539122749768186,2.553912163285183,2.553912049515033,2.553911933627687,2.5539118155837492,2.5539116953430883,2.553911572864826,2.553911448107322,2.5539113210281665,2.553911191584155,2.5539110597312815,2.553910925424722,2.5539107886188157,2.5539106492670545,2.5539105073220645,2.55391036273559,2.553910215458478,2.5539100654406606,2.553909912631139,2.553909756977963,2.5539095984282185,2.553909436928006,2.5539092724224237,2.5539091048555473,2.553908934170413,2.553908760308996,2.5539085832121913,2.5539084028197987,2.5539082190704914,2.5539080319018077,2.553907841250119,2.5539076470506155,2.553907449237281,2.5539072477428713,2.55390704249889,2.5539068334355686,2.553906620481838,2.553906403565309,2.553906182612244,2.5539059575475345,2.553905728294675,2.553905494775737,2.5539052569113387,2.553905014620626,2.5539047678212388,2.553904516429285,2.5539042603593103,2.553903999524273,2.553903733835511,2.553903463202712,2.5539031875338867,2.553902906735331,2.5539026207115985,2.5539023293654686,2.5539020325979105,2.553901730308054,2.5539014223931473,2.5539011087485335,2.5539007892676038,2.553900463841769,2.55390013236042,2.553899794710889,2.5538994507784127,2.5538991004460936,2.55389874359486,2.553898380103424,2.5538980098482433,2.5538976327034764,2.5538972485409412,2.5538968572300744,2.5538964586378787,2.5538960526288874,2.5538956390651144,2.553895217806004,2.5538947887083885,2.553894351626437,2.5538939064116057,2.5538934529125896,2.5538929909752697,2.5538925204426577,2.553892041154849,2.553891552948965,2.5538910556590957,2.553890549116248,2.5538900331482846,2.553889507579867,2.553888972232397,2.5538884269239555,2.5538878714692395,2.5538873056795,2.5538867293624787,2.5538861423223422,2.553885544359617,2.553884935271118,2.553884314849883,2.553883682885103,2.553883039162048,2.553882383461995,2.553881715562156,2.553881035235599,2.5538803422511744,2.5538796363734337,2.5538789173625536,2.5538781849742485,2.5538774389596943,2.553876679065439,2.5538759050333213,2.5538751166003792,2.553874313498763,2.553873495455642,2.5538726621931187,2.553871813428124,2.553870948872332,2.553870068232055,2.5538691712081487,2.553868257495906,2.5538673267849585,2.553866378759169,2.553865413096524,2.553864429469028,2.5538634275425864,2.5538624069768994,2.5538613674253394,2.5538603085348424,2.5538592299457807,2.5538581312918436,2.5538570121999156,2.5538558722899487,2.553854711174831,2.553853528460261,2.55385232374461,2.5538510966187897,2.553849846666109,2.553848573462138,2.553847276574562,2.553845955563034,2.553844609979032,2.5538432393656962,2.553841843257687,2.5538404211810186,2.553838972652903,2.5538374971815863,2.5538359942661826,2.5538344633965027,2.5538329040528835,2.5538313157060144,2.5538296978167554,2.5538280498359542,2.5538263712042646,2.553824661351956,2.553822919698719,2.553821145653473,2.553819338614164,2.5538174979675627,2.553815623089057,2.553813713342442,2.5538117680797043,2.5538097866408056,2.553807768353457,2.553805712532895,2.5538036184816497,2.5538014854893087,2.5537993128322807,2.5537970997735475,2.55379484556242,2.553792549434281,2.553790210610331,2.5537878282973274,2.553785401687309,2.5537829299573374,2.553780412269208,2.553777847769173,2.553775235587656,2.553772574838955,2.55376986462095,2.5537671040147933,2.553764292084608,2.5537614278771663,2.5537585104215745,2.553755538728945,2.5537525117920623,2.553749428585049,2.553746288063019,2.553743089161725,2.5537398307972055,2.5537365118654187,2.553733131241873,2.5537296877812485,2.5537261803170157,2.5537226076610446,2.553718968603202,2.553715261910953,2.553711486328942,2.5537076405785797,2.553703723357607,2.553699733339666,2.553695669173853,2.5536915294842695,2.553687312869558,2.5536830179024395,2.5536786431292313,2.5536741870693667,2.5536696482148966,2.5536650250299897,2.5536603159504168,2.5536555193830353,2.5536506337052467,2.5536456572644712,2.5536405883775863,2.553635425330368,2.553630166376926,2.5536248097391145,2.5536193536059497,2.5536137961330003,2.5536081354417797,2.553602369619119,2.553596496716535,2.553590514749579,2.553584421697186,2.5535782155009956,2.553571894064678,2.5535654552532336,2.55355889689229,2.5535522167673808,2.5535454126232153,2.553538482162929,2.5535314230473296,2.55352423289412,2.5535169092771164,2.5535094497254427,2.5535018517227224,2.553494112706244,2.5534862300661203,2.5534782011444284,2.5534700232343326,2.5534616935792025,2.5534532093716984,2.5534445677528557,2.5534357658111424,2.5534268005815113,2.55341766904442,2.5534083681248476,2.5533988946912864,2.553389245554722,2.5533794174675832,2.553369407122689,2.5533592111521664,2.553348826126355,2.5533382485526808,2.5533274748745347,2.5533165014701003,2.5533053246511854,2.553293940662022,2.5532823456780482,2.5532705358046677,2.553258507075987,2.553246255453535,2.5532337768249516,2.5532210670026676,2.5532081217225455,2.5531949366425066,2.553181507341135,2.55316782931625,2.5531538979834627,2.553139708674699,2.5531252566367084,2.5531105370295335,2.553095544924963,2.5530802753049566,2.55306472306004,2.553048882987673,2.553032749790597,2.5530163180751404,2.5529995823495066,2.552982537022032,2.552965176399406,2.552947494684873,2.5529294859763927,2.5529111442647774,2.552892463431794,2.5528734372482376,2.552854059371963,2.5528343233458997,2.5528142225960186,2.552793750429273,2.552772900031502,2.5527516644653003,2.552730036667855,2.552708009448738,2.5526855754876787,2.5526627273322746,2.5526394573956925,2.5526157579543116,2.5525916211453374,2.552567038964373,2.5525420032629498,2.552516505746028,2.552490537969438,2.5524640913372996,2.5524371570993893,2.5524097263484657,2.552381790017552,2.5523533388771793,2.5523243635325845,2.552294854420855,2.552264801808044,2.55223419578622,2.552203026270496,2.5521712829959804,2.552138955514707,2.552106033192505,2.552072505205814,2.552038360538467,2.5520035879784073,2.551968176114358,2.551932113332451,2.551895387812787,2.5518579875259557,2.551819900229502,2.551781113464326,2.5517416145510454,2.551701390586292,2.551660428438956,2.55161871474637,2.551576235910445,2.551532978093738,2.5514889272154666,2.551444068947468,2.5513983887100875,2.5513518716680195,2.5513045027260803,2.5512562665249203,2.551207147436673,2.551157129560542,2.55110619671833,2.551054332449891,2.551001520008531,2.5509477423563367,2.5508929821594335,2.5508372217831874,2.5507804432873318,2.550722628421029,2.55066375861786,2.5506038149907497,2.550542778326819,2.5504806290821644,2.550417347376571,2.5503529129881506,2.5502873053479083,2.5502205035342307,2.550152486267312,2.550083231903492,2.5500127184295285,2.5499409234567887,2.5498678242153714,2.549793397548137,2.5497176199046763,2.549640467335197,2.5495619154843228,2.5494819395848203,2.549400514451247,2.5493176144735124,2.549233213610364,2.549147285382789,2.5490598028673275,2.548970738689313,2.5488800650160264,2.548787753549756,2.5486937755207837,2.548598101680284,2.5485007022931283,2.5484015471306143,2.5483006054631008,2.5481978460525525,2.5480932371450056,2.5479867464629327,2.5478783411975274,2.5477679880008925,2.547655652978142,2.547541301679405,2.5474248990917454,2.547306409630983,2.5471857971334253,2.5470630248474997,2.546938055425298,2.5468108509140226,2.5466813727473365,2.546549581736617,2.546415438062115,2.546278901264016,2.5461399302333994,2.545998483203105,2.5458545177384977,2.545707990728131,2.545558858374313,2.5454070761835705,2.545252598957012,2.5450953807805883,2.544935375015249,2.5447725342870022,2.5446068104768633,2.5444381547107047,2.5442665173490004,2.544091847976463,2.54391409539158,2.5437332075960377,2.543549131784051,2.543361814331571,2.543171200785398,2.5429772358521845,2.542779863387327,2.5425790263837564,2.5423746669606158,2.5421667263518337,2.5419551448945916,2.541739862017675,2.5415208162297303,2.5412979451074023,2.541071185283374,2.540840472434293,2.540605741268598,2.5403669255142347,2.540123957906273,2.5398767701744145,2.5396252930304004,2.5393694561553226,2.5391091881868233,2.5388444167062105,2.5385750682254664,2.5383010681741642,2.5380223408862936,2.537738809587001,2.5374503963792274,2.537157022230282,2.536858606958319,2.536555069218746,2.536246326490555,2.535932295062583,2.5356128900197157,2.535288025229021,2.534957613325836,2.534621565699803,2.534279792480863,2.533932202525215,2.533578703401248,2.5332192013754486,2.532853601398305,2.5324818070901984,2.5321037207273087,2.53171924322753,2.531328274136422,2.5309307116131925,2.5305264524167357,2.5301153918917305,2.529697423954823,2.5292724410808933,2.528840334289433,2.5284009931310503,2.5279543056741085,2.527500158491521,2.527038436647738,2.5265690236859104,2.526091801615284,2.525606650898828,2.525113450441124,2.5246120775765473,2.5241024080577446,2.5235843160444626,2.5230576740927346,2.5225223531444483,2.521978222517352,2.521425149895495,2.5208630013201643,2.5202916411813225,2.5197109322096094,2.5191207354689173,2.5185209103495927,2.517911314562294,2.517291804132552,2.5166622333960618,2.51602245499476,2.515372319873717,2.514711677278908,2.514040374755876,2.5133582581493745,2.512665171604,2.5119609575658837,2.5112454567854883,2.5105185083215598,2.509779949546287,2.509029616151721,2.5082673421575112,2.5074929599200138,2.506706300142825,2.505907191888805,2.5050954625936384,2.5042709380810018,2.5034334425793987,2.5025827987407068,2.5017188276605262,2.5008413489003587,2.4999501805117137,2.4990451390621766,2.4981260396635228,2.497192696001925,2.496244920370336,2.4952825237030947,2.4943053156128334,2.4933131044297365,2.492305697243233,2.49128289994617,2.490244517281544,2.489190352891842,2.488120209371068,2.4870338883195036,2.485931190401276,2.484811915404783,2.483675862306054,2.482522829335066,2.481352614045125,2.4801650133853195,2.4789598237761314,2.47773684118824,2.476495861224578,2.475236679205683,2.4739590902583863,2.4726628894078964,2.4713478716733035,2.470013832166552,2.468660566194906,2.467287869366958,2.4658955377021896,2.4644833677441262,2.4630511566770936,2.4615987024466115,2.46012580388342,2.458632260831166,2.457117874277746,2.4555824464903084,2.454025781153917,2.4524476835138658,2.4508479605216325,2.4492264209844543,2.4475828757185014,2.445917137705629,2.444229022253663,2.442518347160191,2.4407849328798115,2.4390286026947803,2.4372491828890244,2.4354465029254286,2.4336203956263547,2.431770697357302,2.429897248213639,2.427999892210303,2.4260784774744004,2.4241328564405773,2.4221628860490796,2.4201684279463724,2.4181493486882073,2.4161055199450034,2.4140368187094228,2.41194312750598,2.4098243346025585,2.4076803342236675,2.4055110267652884,2.4033163190111337,2.401096124350162,2.3988503629951494,2.3965789622021516,2.3942818564906507,2.3919589878642014,2.3896103060313547,2.3872357686266774,2.384835341431624,2.382408998595064,2.37995672285322,2.3774785057488095,2.374974347849123,2.372444258962838,2.3698882583552883,2.367306374961968,2.3646986475999983,2.3620651251773213,2.359405866899345,2.356720942472785,2.3540104323064437,2.3512744277086544,2.3485130310811195,2.3457263561088855,2.3429145279461663,2.3400776833977557,2.3372159710957536,2.334329551671325,2.33141859792123,2.328483294968845,2.325523840419403,2.322540444509192,2.3195333302484276,2.3165027335575443,2.313448903396642,2.3103721018878165,2.307272604430123,2.304150699806923,2.3010066902853494,2.2978408917076574,2.2946536335742143,2.2914452591178933,2.2882161253696407,2.284966603214996,2.2816970774413377,2.2784079467756637,2.2750996239126744,2.2717725355329983,2.2684271223113393,2.265063838914394,2.2616831539883475,2.258285550135809,2.254871523882021,2.2514415856302024,2.2479962596059155,2.244536083790305,2.241061609842133,2.2375734030084993,2.2340720420241604,2.230558118999387,2.2270322392962862,2.223495021393561,2.219947096739655,2.2163891095942794,2.212821716858306,2.2092455878920365,2.205661404321876,2.2020698598354453,2.198471659965181,2.194867521860491,2.1912581740485564,2.1876443561838546,2.1840268187865477,2.1804063229698327,2.176783640156416,2.173159551784264,2.1695348490018063,2.165910332352778,2.1622868114509077,2.1586651046446805,2.155046038672393,2.1514304483077806,2.1478191759964558,2.144213071483463,2.140612991432235,2.1370197990352753,2.1334343636168867,2.129857560228294,2.1262902692355183,2.122733375900385,2.1191877699550385,2.1156543451703835,2.1121339989188517,2.1086276317319506,2.1051361468530074,2.1016604497856046,2.098201447838139,2.0947600496650267,2.0913371648050334,2.087933703217238,2.084550574815171,2.0811886889996436,2.077848954190828,2.0745322773601416,2.071239563562497,2.067971715469506,2.0647296329042106,2.0615142123779426,2.0583263466298996,2.055166924170044,2.052036828825929,2.0489369392940477,2.045868128696314,2.0428312641422526,2.0398272062975003,2.036856808959167,2.033920918638614,2.031020374152168,2.028156006220261,2.025328637075428,2.022539080079579,2.0197881393508563,2.017076609400351,2.0144052747788317,2.0117749097335627,2.009186277875138,2.006640131854132,2.0041372130471857,2.001678251251966,1.9992639643901813,1.9968950582176253,1.994572226039872,1.9922961484319486,1.990067492959919,1.9878869139018749,1.9857550519653784,1.9836725339978327,1.9816399726857035,1.979657966237842,1.9777270980474677,1.9758479363266057,1.974021033705954,1.9722469267922986,1.9705261356746948,1.9688591633697174,1.9672464951951523,1.9656885980606207,1.9641859196627662,1.9627388875719403,1.961347908196682,1.9600133656119823,1.9587356202371988,1.9575150073498098,1.9563518354219347,1.9552463842678887,1.9541989029930218,1.9532096077368626,1.9522786792072455,1.9514062600066708,1.950592451757809,1.9498373120416863,1.9491408511697816,1.9485030288198626,1.9479237505747309,1.9474028644129286,1.9469401572104443,1.9465353513222068,1.9461881013210736,1.945897990979534,1.945664530584854,1.945487154681212,1.9453652203320064,1.9452980059914282,1.9452847110663436,1.9453244562373981,1.9454162845922556,1.9455591636044214,1.9457519879689442,1.9459935832823656,1.9462827105297438,1.9466180713176615,1.9469983137700717,1.9474220389847805,1.947887807933291,1.9483941486763112,1.9489395637618598,1.9495225376726402,1.9501415441939483,1.9507950535822798,1.9514815394272798,1.9521994851148554,1.952947389816172,1.9537237739450062,1.9545271840436014,1.9553561970741,1.9562094241081702,1.957085513421213,1.9579831530092406,1.9589010725560516,1.9598380448856818,1.9607928869404163,1.9617644603280362,1.9627516714837185,1.9637534714923346,1.9647688556160345,1.9657968625702804,1.966836573589035,1.9678871113169427,1.968947638563106,1.9700173569477553,1.9710955054697035,1.9721813590192143,1.9732742268577157,1.9743734510828423,1.9754784050945144,1.9765884920752652,1.9777031434957324,1.978821817654226,1.9799439982574663,1.9810691930480349,1.9821969324827111,1.9833267684646867,1.984458273131665,1.9855910377009915,1.9867246713722742,1.9878588002873407,1.9889930665469246,1.9901271272830696,1.9912606537859316,1.9923933306834367,1.9935248551720335,1.9946549362966874,1.9957832942781282,1.996909659885336,1.9980337738511946,1.9991553863292446,2.0002742563894667,2.0013901515510666,2.002502847350244,2.0036121269409923,2.004717780727017,2.005819606022919,2.0069174067428484,2.0080109931149,2.0091001814195755,2.0101847937507205,2.0112646577973843,2.012339606645136,2.013409478595427,2.014474117001646,2.015533370120583,2.016587090978076,2.0176351372476735,2.018677371141198,2.019713659310164,2.0207438727570453,2.0217678867554456,2.022785580778278,2.0237968384331175,2.0248015474039094,2.025799599398312,2.026790890099946,2.0277753191249155,2.0287527899819704,2.029723210035741,2.0306864904725193,2.03164254626809,2.0325912961571557,2.033532662603938,2.034466571773582,2.0353929535039996,2.0363117412778466]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[1.560456999379821,1.5604570011744685,1.560457003002515,1.5604570048645814,1.5604570067613015,1.5604570086933196,1.5604570106612932,1.5604570126658908,1.560457014707794,1.5604570167876974,1.5604570189063087,1.5604570210643478,1.5604570232625476,1.5604570255016565,1.560457027782436,1.5604570301056606,1.5604570324721203,1.5604570348826212,1.5604570373379816,1.5604570398390363,1.5604570423866364,1.560457044981647,1.5604570476249517,1.5604570503174484,1.560457053060053,1.5604570558536983,1.5604570586993332,1.560457061597926,1.5604570645504623,1.560457067557945,1.5604570706213974,1.5604570737418622,1.5604570769203983,1.560457080158087,1.5604570834560305,1.560457086815349,1.5604570902371848,1.5604570937227011,1.5604570972730842,1.5604571008895396,1.5604571045732982,1.5604571083256118,1.5604571121477564,1.5604571160410314,1.5604571200067614,1.5604571240462937,1.560457128161003,1.560457132352287,1.5604571366215712,1.5604571409703079,1.5604571453999743,1.5604571499120783,1.5604571545081525,1.5604571591897605,1.5604571639584934,1.5604571688159736,1.560457173763852,1.560457178803811,1.5604571839375643,1.5604571891668573,1.5604571944934682,1.5604571999192074,1.5604572054459211,1.5604572110754873,1.5604572168098199,1.5604572226508695,1.5604572286006222,1.5604572346611,1.5604572408343644,1.560457247122513,1.5604572535276855,1.560457260052059,1.5604572666978516,1.560457273467323,1.5604572803627754,1.5604572873865528,1.5604572945410433,1.5604573018286803,1.5604573092519403,1.560457316813348,1.5604573245154747,1.560457332360938,1.5604573403524062,1.5604573484925968,1.560457356784277,1.5604573652302665,1.5604573738334364,1.5604573825967116,1.5604573915230735,1.5604574006155545,1.5604574098772477,1.5604574193113026,1.560457428920925,1.560457438709384,1.5604574486800062,1.5604574588361833,1.5604574691813675,1.5604574797190762,1.560457490452892,1.560457501386466,1.5604575125235138,1.560457523867823,1.5604575354232504,1.560457547193726,1.5604575591832497,1.5604575713959,1.5604575838358283,1.560457596507264,1.5604576094145164,1.5604576225619733,1.5604576359541045,1.5604576495954647,1.5604576634906908,1.560457677644508,1.5604576920617277,1.5604577067472534,1.5604577217060767,1.5604577369432846,1.5604577524640568,1.5604577682736718,1.560457784377504,1.5604578007810286,1.5604578174898236,1.5604578345095694,1.560457851846053,1.5604578695051687,1.5604578874929211,1.5604579058154253,1.5604579244789112,1.5604579434897254,1.5604579628543305,1.5604579825793106,1.5604580026713724,1.5604580231373482,1.5604580439841942,1.5604580652190008,1.5604580868489863,1.560458108881505,1.5604581313240486,1.5604581541842475,1.5604581774698736,1.560458201188844,1.5604582253492238,1.5604582499592274,1.5604582750272216,1.5604583005617298,1.5604583265714331,1.5604583530651766,1.5604583800519662,1.560458407540978,1.5604584355415583,1.5604584640632273,1.5604584931156822,1.5604585227088,1.5604585528526433,1.560458583557461,1.5604586148336914,1.5604586466919694,1.560458679143126,1.5604587121981952,1.560458745868415,1.5604587801652325,1.5604588151003096,1.5604588506855235,1.5604588869329732,1.560458923854982,1.5604589614641036,1.5604589997731246,1.5604590387950694,1.5604590785432062,1.5604591190310484,1.5604591602723614,1.5604592022811667,1.5604592450717476,1.5604592886586526,1.5604593330566994,1.5604593782809841,1.560459424346882,1.5604594712700546,1.5604595190664565,1.560459567752337,1.5604596173442487,1.560459667859052,1.5604597193139225,1.5604597717263522,1.5604598251141617,1.560459879495502,1.5604599348888621,1.560459991313074,1.5604600487873217,1.5604601073311457,1.5604601669644498,1.5604602277075073,1.5604602895809707,1.5604603526058751,1.5604604168036482,1.5604604821961163,1.560460548805511,1.5604606166544777,1.5604606857660843,1.5604607561638266,1.5604608278716392,1.5604609009138994,1.5604609753154417,1.5604610511015593,1.5604611282980174,1.5604612069310626,1.5604612870274255,1.560461368614339,1.5604614517195383,1.560461536371278,1.560461622598337,1.5604617104300291,1.5604617998962154,1.5604618910273114,1.5604619838542972,1.560462078408732,1.560462174722762,1.5604622728291282,1.5604623727611848,1.560462474552904,1.5604625782388926,1.5604626838543976,1.560462791435326,1.5604629010182498,1.5604630126404233,1.5604631263397928,1.5604632421550115,1.56046336012545,1.5604634802912145,1.5604636026931549,1.5604637273728823,1.5604638543727807,1.560463983736025,1.5604641155065913,1.5604642497292742,1.5604643864497034,1.5604645257143561,1.560464667570573,1.560464812066579,1.5604649592514925,1.5604651091753483,1.5604652618891113,1.5604654174446946,1.5604655758949773,1.5604657372938218,1.5604659016960931,1.5604660691576768,1.5604662397354976,1.5604664134875403,1.5604665904728678,1.5604667707516413,1.5604669543851428,1.5604671414357933,1.5604673319671747,1.5604675260440526,1.560467723732397,1.5604679250994051,1.5604681302135244,1.560468339144475,1.5604685519632764,1.5604687687422656,1.560468989555128,1.560469214476919,1.5604694435840893,1.5604696769545134,1.5604699146675136,1.5604701568038866,1.5604704034459345,1.5604706546774874,1.5604709105839374,1.5604711712522636,1.560471436771063,1.5604717072305814,1.5604719827227436,1.560472263341181,1.560472549181271,1.5604728403401618,1.560473136916811,1.5604734390120134,1.5604737467284426,1.560474060170679,1.5604743794452491,1.560474704660661,1.56047503592744,1.5604753733581678,1.5604757170675194,1.5604760671723032,1.560476423791499,1.560476787046301,1.5604771570601543,1.5604775339588035,1.560477917870329,1.5604783089251941,1.5604787072562876,1.5604791129989706,1.5604795262911202,1.5604799472731778,1.5604803760881978,1.5604808128818914,1.5604812578026828,1.5604817110017533,1.5604821726330982,1.5604826428535732,1.5604831218229525,1.5604836097039803,1.5604841066624264,1.560484612867144,1.5604851284901238,1.5604856537065555,1.5604861886948858,1.5604867336368784,1.5604872887176775,1.5604878541258667,1.560488430053538,1.560489016696353,1.5604896142536113,1.5604902229283164,1.5604908429272462,1.5604914744610219,1.5604921177441804,1.5604927729952451,1.5604934404368023,1.5604941202955749,1.5604948128024994,1.5604955181928042,1.5604962367060917,1.5604969685864154,1.5604977140823635,1.5604984734471472,1.5604992469386796,1.5605000348196694,1.560500837357706,1.5605016548253503,1.5605024875002291,1.5605033356651277,1.5605041996080844,1.5605050796224913,1.5605059760071889,1.5605068890665725,1.5605078191106907,1.5605087664553527,1.5605097314222343,1.5605107143389858,1.5605117155393469,1.5605127353632533,1.560513774156956,1.5605148322731375,1.5605159100710313,1.5605170079165402,1.560518126182366,1.5605192652481286,1.5605204255004999,1.56052160733333,1.5605228111477847,1.5605240373524778,1.5605252863636085,1.560526558605105,1.5605278545087657,1.5605291745144045,1.5605305190699996,1.5605318886318478,1.5605332836647126,1.560534704641986,1.5605361520458456,1.5605376263674182,1.5605391281069447,1.5605406577739507,1.5605422158874154,1.5605438029759482,1.5605454195779682,1.560547066241881,1.560548743526271,1.5605504520000806,1.5605521922428092,1.5605539648447047,1.5605557704069635,1.560557609541931,1.560559482873311,1.5605613910363716,1.5605633346781638,1.5605653144577338,1.5605673310463481,1.560569385127719,1.5605714773982318,1.560573608567183,1.5605757793570163,1.560577990503563,1.5605802427562947,1.5605825368785695,1.560584873647892,1.5605872538561716,1.5605896783099915,1.5605921478308755,1.560594663255567,1.5605972254363065,1.5605998352411181,1.5606024935541036,1.5606052012757343,1.5606079593231548,1.5606107686304898,1.5606136301491578,1.5606165448481861,1.5606195137145407,1.5606225377534497,1.5606256179887468,1.5606287554632057,1.560631951238895,1.56063520639753,1.560638522040835,1.5606418992909086,1.560645339290602,1.560648843203898,1.560652412216299,1.5606560475352236,1.5606597503904085,1.5606635220343161,1.5606673637425557,1.5606712768143047,1.5606752625727411,1.5606793223654862,1.5606834575650506,1.5606876695692908,1.5606919598018731,1.5606963297127474,1.5607007807786273,1.5607053145034813,1.5607099324190306,1.5607146360852586,1.5607194270909233,1.5607243070540897,1.560729277622661,1.5607343404749243,1.560739497320109,1.5607447498989448,1.5607500999842447,1.560755549381485,1.5607610999294041,1.5607667535006051,1.560772512002179,1.5607783773763262,1.5607843516010005,1.5607904366905612,1.560796634696429,1.5608029477077685,1.5608093778521672,1.560815927296339,1.5608225982468316,1.5608293929507517,1.560836313696502,1.5608433628145275,1.560850542678081,1.5608578557039965,1.5608653043534804,1.5608728911329144,1.5608806185946758,1.5608884893379618,1.560896506009647,1.5609046713051358,1.560912987969244,1.5609214587970883,1.5609300866349964,1.5609388743814288,1.5609478249879172,1.5609569414600224,1.5609662268583075,1.5609756842993239,1.5609853169566217,1.5609951280617715,1.5610051209054094,1.5610152988382908,1.5610256652723762,1.5610362236819217,1.5610469776045992,1.5610579306426287,1.5610690864639338,1.5610804488033154,1.5610920214636466,1.5611038083170885,1.5611158133063223,1.5611280404458117,1.5611404938230748,1.5611531775999878,1.5611660960141056,1.5611792533800064,1.5611926540906593,1.561206302618812,1.5612202035184084,1.5612343614260216,1.5612487810623192,1.5612634672335461,1.5612784248330382,1.5612936588427564,1.5613091743348506,1.561324976473245,1.5613410705152535,1.561357461813221,1.5613741558161913,1.561391158071602,1.5614084742270091,1.5614261100318392,1.5614440713391669,1.5614623641075303,1.5614809944027639,1.5614999683998745,1.5615192923849341,1.5615389727570184,1.5615590160301611,1.5615794288353515,1.561600217922559,1.5616213901627911,1.5616429525501856,1.5616649122041344,1.561687276371439,1.561710052428509,1.561733247883585,1.5617568703790026,1.5617809276934917,1.561805427744507,1.561830378590606,1.5618557884338504,1.5618816656222543,1.5619080186522685,1.561934856171301,1.5619621869802796,1.5619900200362515,1.5620183644550214,1.5620472295138352,1.5620766246541007,1.5621065594841512,1.5621370437820445,1.562168087498419,1.562199700759374,1.5622318938694104,1.562264677314401,1.5622980617646167,1.5623320580777882,1.5623666773022165,1.5624019306799346,1.562437829649904,1.562474385851266,1.56251161112664,1.5625495175254658,1.5625881173073972,1.5626274229457402,1.5626674471309447,1.5627082027741446,1.5627497030107456,1.5627919612040664,1.5628349909490282,1.5628788060758998,1.5629234206540878,1.5629688489959865,1.5630151056608748,1.5630622054588725,1.5631101634549411,1.5631589949729503,1.563208715599786,1.563259341189528,1.5633108878676687,1.563363372035396,1.5634168103739352,1.5634712198489338,1.56352661771492,1.5635830215198059,1.5636404491094567,1.5636989186323096,1.5637584485440603,1.5638190576124007,1.5638807649218185,1.5639435898784564,1.5640075522150276,1.5640726719957976,1.564138969621616,1.5642064658350185,1.564275181725381,1.5643451387341405,1.5644163586600697,1.564488863664619,1.5645626762773137,1.5646378194012176,1.5647143163184476,1.5647921906957594,1.5648714665901877,1.5649521684547485,1.5650343211442008,1.5651179499208676,1.5652030804605184,1.5652897388583105,1.5653779516347863,1.565467745741937,1.5655591485693114,1.5656521879501946,1.5657468921678384,1.5658432899617434,1.5659414105340073,1.5660412835557138,1.566142939173388,1.5662464080154963,1.5663517211989963,1.5664589103359456,1.5665680075401482,1.566679045433858,1.5667920571545189,1.566907076361555,1.567024137243202,1.5671432745233753,1.567264523468581,1.5673879198948548,1.5675135001747444,1.5676413012443144,1.5677713606101835,1.5679037163565828,1.5680384071524398,1.56817547225848,1.5683149515343406,1.568456885445701,1.5686013150714126,1.5687482821106422,1.5688978288900037,1.5690499983706918,1.5692048341555997,1.5693623804964238,1.5695226823007484,1.569685785139097,1.5698517352519576,1.5700205795567643,1.5701923656548367,1.570367141838258,1.5705449570967063,1.5707258611242065,1.5709099043258101,1.5710971378241938,1.5712876134661622,1.5714813838290493,1.571678502227014,1.5718790227172037,1.5720830001057984,1.57229048995391,1.5725015485833231,1.5727162330820823,1.5729346013098946,1.5731567119033492,1.573382624280936,1.5736123986478503,1.5738460960005667,1.5740837781311812,1.5743255076314893,1.5745713478967946,1.5748213631294383,1.575075618342018,1.5753341793603015,1.5755971128257902,1.5758644861979487,1.5761363677560507,1.576412826600651,1.5766939326546405,1.5769797566638908,1.577270370197443,1.5775658456472443,1.577866256227393,1.5781716759728912,1.5784821797378648,1.5787978431932432,1.5791187428238735,1.579444955925045,1.5797765605984053,1.5801136357472438,1.5804562610711201,1.580804517059817,1.5811584849865905,1.5815182469006963,1.581883885619173,1.5822554847178476,1.582633128521556,1.5830169020935363,1.5834068912239874,1.5838031824177605,1.5842058628811562,1.5846150205078224,1.5850307438636972,1.5854531221710124,1.5858822452912988,1.5863182037073955,1.586761088504425,1.587210991349726,1.587668004471705,1.5881322206376016,1.5886037331301346,1.5890826357230172,1.5895690226553154,1.5900629886046331,1.5905646286591035,1.5910740382881756,1.5915913133121669,1.5921165498705807,1.5926498443891601,1.5931912935456725,1.5937409942344105,1.5942990435293931,1.594865538646265,1.5954405769028708,1.5960242556785142,1.5966166723718789,1.5972179243576128,1.5978281089415762,1.5984473233147407,1.5990756645057491,1.5997132293321314,1.6003601143501756,1.6010164158034714,1.6016822295701179,1.6023576511086115,1.6030427754024228,1.6037376969032742,1.6044425094731332,1.605157306324934,1.6058821799620557,1.6066172221165573,1.607362523686219,1.6081181746703943,1.6088842641047074,1.6096608799946228,1.6104481092479301,1.611246037606155,1.6120547495749626,1.6128743283535645,1.6137048557631912,1.6145464121746633,1.6153990764351118,1.6162629257938934,1.6171380358277556,1.6180244803653046,1.6189223314108303,1.6198316590675452,1.6207525314603082,1.6216850146578832,1.622629172594807,1.6235850669929357,1.6245527572827327,1.6255323005243763,1.6265237513287665,1.627527161778493,1.6285425813488645,1.6295700568290625,1.6306096322435135,1.6316613487735605,1.632725244679527,1.6338013552232522,1.6348897125911988,1.6359903458182192,1.6371032807120787,1.6382285397788234,1.6393661421491013,1.6405161035055225,1.641678436011162,1.6428531482393114,1.6440402451045666,1.6452397277953636,1.6464515937080615,1.647675836382668,1.6489124454403206,1.6501614065226164,1.651422701232897,1.6526963070795833,1.6539821974216737,1.6552803414164878,1.6565907039697696,1.6579132456882393,1.6592479228346892,1.6605946872857227,1.6619534864922216,1.6633242634426357,1.6647069566291774,1.6661015000170118,1.667507823016512,1.6689258504586681,1.6703555025737127,1.6717966949730414,1.6732493386344849,1.6747133398909995,1.6761886004228193,1.6776750172531394,1.6791724827473495,1.680680884615876,1.6822001059206504,1.6837300250852392,1.685270515908638,1.686821447582762,1.6883826847136125,1.6899540873461327,1.6915355109927255,1.6931268066654248,1.6947278209116765,1.6963383958537035,1.6979583692313929,1.699587574448663,1.70122584062322,1.7028729926396609,1.7045288512057968,1.7061932329121317,1.7078659502943747,1.7095468118988644,1.7112356223507932,1.7129321824250794,1.7146362891197442,1.716347735731635,1.7180663119343313,1.7197918038580393,1.721523994171305,1.7232626621643328,1.7250075838337051,1.7267585319682879,1.7285152762360891,1.7302775832718331,1.7320452167650089,1.7338179375481328,1.7355955036849657,1.7373776705584134,1.7391641909578257,1.740954815165417,1.742749291041508,1.744547364108294,1.7463487776318327,1.7481532727019373,1.7499605883096698,1.7517704614221015,1.7535826270540298,1.7553968183363193,1.7572127665805377,1.7590302013395642,1.7608488504638387,1.762668440152919,1.7644886950020298,1.7663093380432726,1.7681300907811868,1.7699506732223456,1.7717708038986935,1.7735901998843278,1.7754085768054555,1.7772256488432665,1.7790411287294752,1.780854727734342,1.782666155646961,1.7844751207476892,1.786281329772606,1.7880844878699296,1.7898842985484138,1.7916804636177706,1.7934726831212637,1.7952606552607167,1.7970440763142543,1.7988226405472458,1.8005960401170342,1.802363964972209,1.8041261027473543,1.8058821386544144,1.8076317553720556,1.809374632934671,1.811110448622986,1.812838876858553,1.8145595891048316,1.8162722537779625,1.8179765361708413,1.8196720983946275,1.8213585993424175,1.8230356946804533,1.8247030368729495,1.8263602752473715,1.8280070561078157,1.8296430229049911,1.831267816472194,1.8328810753375717,1.8344824361238916,1.836071534047893,1.8376480035321372,1.83921147894298,1.8407615954688483,1.84229799015335,1.843820303097802,1.845328178847428,1.8468212679747007,1.8482992288719424,1.849761729763279,1.8512084509432125,1.8526390872454417,1.8540533507408683,1.855450973658103,1.8568317115130721,1.8581953464265935,1.8595416906001785,1.8608705899108824,1.8621819275761577,1.8634756278295708,1.86475165953849,1.86601003968588,1.8672508366307539,1.8684741730563206,1.869680228511957,1.870869241455486,1.872041510706273,1.8731973962276576,1.8743373191693273,1.8754617611162059,1.8765712625098376,1.8776664202303757,1.878747884351172,1.8798163541024662,1.8808725731045572,1.8819173239528764,1.8829514222563806,1.883975710245767,1.8849910500783817,1.8859983169720487,1.8869983923002964,1.8879921567768354,1.888980483848243,1.8899642334012998,1.8909442458762606,1.8919213368604046,1.8928962922184966,1.8938698637990719,1.89484276573856,1.8958156713697143,1.896789210727041,1.8977639686302472,1.8987404833171857,1.8997192455904308,1.9007006984363533,1.9016852370721549,1.9026732093746246,1.9036649166440642,1.904660614657702,1.9056605149686814,1.9066647864091546,1.9076735567589715,1.908686914544631,1.9097049109365836,1.91072756171633,1.9117548492881156,1.9127867247132135,1.9138231097477771,1.9148638988680478,1.9159089612692528,1.9169581428268108,1.9180112680105572,1.9190681417444984,1.9201285512062196,1.9211922675614743,1.922259047630668,1.9233286354850008,1.9244007639708958,1.9254751561620858,1.926551526739346,1.9276295832983652,1.9287090275866734,1.9297895566708685,1.9308708640356753,1.9319526406165473,1.9330345757677159,1.9341163581676921,1.9351976766643129,1.9362782210614826,1.9373576828497967,1.9384357558832364,1.9395121370041293,1.940586526618554,1.941658629224336,1.9427281538937577,1.9437948147130508,1.9448583311807177,1.9459184285666549,1.9469748382340222,1.9480272979257387,1.9490755520174339,1.9501193517386344,1.9511584553639079,1.952192628375631,1.9532216436000032,1.9542452813178561,1.955263329351788,1.9562755831310554,1.957281845735652,1.9582819279209114,1.9592756481239506,1.9602628324531997,1.961243314662235,1.962216936109064,1.9631835457019788,1.9641429998330413,1.9650951623002182,1.9660399042191434,1.9669771039254282,1.9679066468684137,1.9688284254972073,1.9697423391397966,1.9706482938760115,1.9715462024050494,1.9724359839082424,1.9733175639077138]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.8283332946035373,1.828333294450149,1.8283332942939057,1.828333294134755,1.8283332939726424,1.8283332938075127,1.8283332936393104,1.8283332934679777,1.8283332932934564,1.8283332931156873,1.8283332929346099,1.8283332927501628,1.828333292562283,1.8283332923709068,1.8283332921759692,1.8283332919774036,1.8283332917751427,1.828333291569118,1.828333291359259,1.8283332911454946,1.828333290927752,1.8283332907059573,1.8283332904800351,1.828333290249908,1.828333290015499,1.8283332897767273,1.828333289533512,1.8283332892857709,1.828333289033419,1.8283332887763708,1.8283332885145394,1.828333288247835,1.8283332879761673,1.828333287699444,1.8283332874175708,1.828333287130452,1.82833328683799,1.8283332865400852,1.8283332862366368,1.828333285927541,1.828333285612693,1.8283332852919858,1.8283332849653102,1.8283332846325557,1.8283332842936082,1.828333283948353,1.8283332835966728,1.828333283238448,1.8283332828735566,1.8283332825018745,1.8283332821232758,1.8283332817376317,1.82833328134481,1.8283332809446788,1.8283332805371009,1.8283332801219385,1.8283332796990497,1.8283332792682911,1.8283332788295161,1.828333278382576,1.8283332779273183,1.8283332774635885,1.8283332769912288,1.8283332765100786,1.8283332760199744,1.8283332755207498,1.8283332750122347,1.8283332744942562,1.8283332739666385,1.828333273429202,1.828333272881764,1.8283332723241386,1.8283332717561356,1.8283332711775628,1.8283332705882231,1.8283332699879158,1.8283332693764371,1.8283332687535794,1.8283332681191304,1.828333267472875,1.828333266814593,1.8283332661440606,1.82833326546105,1.828333264765329,1.8283332640566614,1.8283332633348057,1.8283332625995168,1.8283332618505448,1.8283332610876346,1.8283332603105276,1.828333259518959,1.8283332587126597,1.8283332578913563,1.8283332570547686,1.8283332562026129,1.8283332553345992,1.8283332544504327,1.8283332535498125,1.8283332526324325,1.8283332516979809,1.82833325074614,1.8283332497765863,1.8283332487889903,1.8283332477830156,1.8283332467583213,1.8283332457145582,1.8283332446513718,1.8283332435684005,1.8283332424652765,1.8283332413416244,1.8283332401970624,1.8283332390312015,1.8283332378436454,1.8283332366339904,1.8283332354018251,1.8283332341467307,1.8283332328682806,1.8283332315660403,1.8283332302395672,1.82833322888841,1.8283332275121098,1.8283332261101983,1.8283332246822,1.8283332232276281,1.8283332217459891,1.828333220236779,1.8283332186994852,1.8283332171335849,1.8283332155385452,1.8283332139138249,1.8283332122588711,1.8283332105731218,1.8283332088560036,1.8283332071069331,1.8283332053253154,1.8283332035105453,1.828333201662006,1.828333199779069,1.8283331978610944,1.82833319590743,1.8283331939174123,1.8283331918903645,1.8283331898255977,1.8283331877224103,1.8283331855800875,1.8283331833979009,1.828333181175109,1.8283331789109563,1.8283331766046733,1.8283331742554763,1.8283331718625666,1.8283331694251312,1.8283331669423415,1.828333164413354,1.828333161837309,1.8283331592133312,1.8283331565405287,1.8283331538179932,1.8283331510447993,1.828333148220005,1.82833314534265,1.8283331424117566,1.8283331394263287,1.8283331363853517,1.8283331332877928,1.8283331301325987,1.8283331269186975,1.8283331236449971,1.828333120310385,1.8283331169137285,1.8283331134538727,1.8283331099296425,1.8283331063398403,1.828333102683246,1.8283330989586177,1.8283330951646894,1.8283330913001723,1.8283330873637533,1.8283330833540947,1.8283330792698347,1.8283330751095852,1.8283330708719332,1.8283330665554383,1.8283330621586347,1.8283330576800279,1.828333053118097,1.8283330484712923,1.8283330437380347,1.8283330389167167,1.8283330340057,1.8283330290033166,1.8283330239078672,1.8283330187176208,1.8283330134308142,1.8283330080456521,1.8283330025603048,1.8283329969729094,1.8283329912815678,1.8283329854843473,1.8283329795792786,1.828332973564356,1.8283329674375373,1.828332961196741,1.8283329548398473,1.8283329483646984,1.8283329417690943,1.8283329350507957,1.8283329282075211,1.8283329212369466,1.8283329141367048,1.828332906904385,1.8283328995375316,1.8283328920336432,1.8283328843901716,1.8283328766045217,1.82833286867405,1.8283328605960643,1.828332852367822,1.8283328439865294,1.8283328354493413,1.8283328267533596,1.8283328178956324,1.8283328088731527,1.828332799682858,1.8283327903216287,1.8283327807862872,1.8283327710735977,1.828332761180263,1.8283327511029257,1.8283327408381658,1.8283327303824997,1.828332719732379,1.82833270888419,1.8283326978342518,1.828332686578814,1.8283326751140587,1.8283326634360952,1.8283326515409617,1.8283326394246227,1.8283326270829672,1.8283326145118093,1.8283326017068837,1.8283325886638475,1.8283325753782764,1.828332561845664,1.8283325480614208,1.8283325340208718,1.8283325197192555,1.828332505151722,1.8283324903133316,1.8283324751990524,1.8283324598037602,1.828332444122235,1.8283324281491606,1.828332411879122,1.8283323953066033,1.8283323784259873,1.828332361231552,1.82833234371747,1.8283323258778053,1.8283323077065121,1.8283322891974327,1.8283322703442957,1.8283322511407125,1.8283322315801769,1.8283322116560625,1.8283321913616193,1.8283321706899731,1.8283321496341218,1.8283321281869342,1.8283321063411468,1.8283320840893618,1.828332061424044,1.8283320383375194,1.8283320148219722,1.8283319908694404,1.828331966471817,1.828331941620843,1.828331916308108,1.8283318905250454,1.8283318642629303,1.8283318375128765,1.8283318102658335,1.8283317825125838,1.828331754243739,1.8283317254497375,1.8283316961208411,1.8283316662471314,1.828331635818507,1.8283316048246803,1.8283315732551724,1.8283315410993126,1.828331508346232,1.8283314749848611,1.8283314410039269,1.8283314063919478,1.8283313711372304,1.8283313352278656,1.8283312986517244,1.8283312613964546,1.828331223449476,1.8283311847979764,1.8283311454289077,1.8283311053289806,1.8283310644846618,1.8283310228821685,1.828330980507464,1.828330937346252,1.828330893383975,1.8283308486058059,1.8283308029966454,1.8283307565411167,1.8283307092235588,1.828330661028024,1.8283306119382714,1.8283305619377601,1.8283305110096464,1.8283304591367768,1.8283304063016828,1.8283303524865742,1.8283302976733353,1.828330241843517,1.828330184978332,1.8283301270586483,1.8283300680649823,1.8283300079774945,1.82832994677598,1.8283298844398645,1.8283298209481968,1.8283297562796417,1.8283296904124728,1.828329623324567,1.8283295549933953,1.8283294853960173,1.8283294145090723,1.8283293423087734,1.8283292687708976,1.828329193870781,1.8283291175833079,1.8283290398829044,1.8283289607435298,1.8283288801386686,1.8283287980413212,1.8283287144239961,1.8283286292587007,1.8283285425169324,1.8283284541696694,1.828328364187362,1.8283282725399226,1.8283281791967163,1.8283280841265515,1.8283279872976699,1.8283278886777359,1.8283277882338274,1.8283276859324242,1.828327581739399,1.8283274756200052,1.828327367538867,1.8283272574599672,1.8283271453466374,1.8283270311615456,1.8283269148666845,1.8283267964233605,1.8283266757921806,1.8283265529330404,1.8283264278051128,1.8283263003668337,1.8283261705758909,1.8283260383892095,1.8283259037629398,1.8283257666524428,1.828325627012278,1.8283254847961876,1.8283253399570847,1.8283251924470367,1.8283250422172521,1.8283248892180657,1.828324733398923,1.8283245747083647,1.8283244130940137,1.8283242485025553,1.8283240808797239,1.8283239101702877,1.8283237363180291,1.8283235592657308,1.8283233789551574,1.828323195327039,1.8283230083210529,1.8283228178758064,1.8283226239288193,1.8283224264165046,1.8283222252741504,1.828322020435902,1.828321811834741,1.8283215994024682,1.828321383069682,1.8283211627657598,1.828320938418837,1.8283207099557879,1.828320477302203,1.8283202403823697,1.8283199991192505,1.8283197534344604,1.828319503248247,1.8283192484794668,1.8283189890455631,1.8283187248625432,1.8283184558449561,1.8283181819058676,1.8283179029568388,1.8283176189078998,1.8283173296675275,1.828317035142619,1.8283167352384693,1.8283164298587447,1.8283161189054558,1.8283158022789352,1.828315479877809,1.8283151515989713,1.828314817337557,1.8283144769869155,1.8283141304385835,1.8283137775822569,1.8283134183057634,1.8283130524950344,1.8283126800340763,1.8283123008049422,1.8283119146877032,1.8283115215604178,1.8283111212991043,1.82831071377771,1.8283102988680802,1.8283098764399304,1.8283094463608138,1.8283090084960902,1.828308562708897,1.8283081088601163,1.8283076468083428,1.8283071764098542,1.8283066975185776,1.8283062099860576,1.8283057136614242,1.8283052083913613,1.8283046940200711,1.8283041703892438,1.8283036373380241,1.8283030947029777,1.828302542318057,1.8283019800145692,1.8283014076211426,1.8283008249636918,1.8283002318653852,1.8282996281466106,1.8282990136249422,1.828298388115105,1.8282977514289436,1.8282971033753856,1.8282964437604103,1.828295772387014,1.828295089055175,1.828294393561823,1.828293685700803,1.8282929652628441,1.828292232035525,1.828291485803242,1.8282907263471762,1.8282899534452612,1.828289166872151,1.8282883663991887,1.828287551794375,1.8282867228223367,1.8282858792442975,1.828285020818047,1.8282841472979117,1.8282832584347264,1.8282823539758049,1.828281433664914,1.828280497242245,1.828279544444389,1.8282785750043093,1.8282775886513198,1.82827658511106,1.8282755641054704,1.8282745253527743,1.8282734685674553,1.8282723934602372,1.828271299738068,1.8282701871040996,1.8282690552576752,1.8282679038943137,1.8282667327056963,1.8282655413796554,1.8282643296001657,1.8282630970473355,1.8282618433974,1.828260568322717,1.8282592714917656,1.8282579525691438,1.8282566112155705,1.8282552470878901,1.8282538598390778,1.8282524491182486,1.8282510145706665,1.8282495558377592,1.8282480725571346,1.8282465643625982,1.8282450308841758,1.8282434717481368,1.8282418865770251,1.828240274989685,1.8282386366013004,1.8282369710234294,1.8282352778640467,1.8282335567275885,1.8282318072150001,1.8282300289237903,1.828228221448087,1.8282263843786979,1.8282245173031761,1.82822261980589,1.8282206914680967,1.828218731868021,1.8282167405809402,1.828214717179272,1.8282126612326686,1.828210572308116,1.8282084499700388,1.8282062937804098,1.8282041032988676,1.828201878082837,1.8281996176876574,1.8281973216667167,1.8281949895715925,1.828192620952199,1.8281902153569396,1.8281877723328688,1.8281852914258587,1.8281827721807735,1.8281802141416532,1.8281776168519002,1.8281749798544775,1.8281723026921144,1.8281695849075172,1.8281668260435913,1.8281640256436695,1.8281611832517488,1.8281582984127371,1.8281553706727072,1.8281523995791598,1.8281493846812964,1.8281463255303003,1.828143221679626,1.8281400726853019,1.828136878106237,1.8281336375045423,1.8281303504458577,1.8281270164996928,1.8281236352397745,1.8281202062444066,1.8281167290968392,1.828113203385648,1.8281096287051255,1.8281060046556807,1.8281023308442512,1.8280986068847263,1.8280948323983786,1.8280910070143097,1.8280871303699053,1.828083202111301,1.8280792218938604,1.8280751893826641,1.8280711042530107,1.8280669661909263,1.8280627748936913,1.8280585300703722,1.8280542314423691,1.828049878743974,1.8280454717229402,1.8280410101410633,1.828036493774776,1.8280319224157504,1.8280272958715178,1.828022613966096,1.8280178765406292,1.8280130834540431,1.8280082345837068,1.8280033298261114,1.8279983690975574,1.8279933523348568,1.8279882794960458,1.8279831505611097,1.8279779655327204,1.827972724436987,1.8279674273242184,1.8279620742696967,1.8279566653744668,1.827951200766135,1.8279456805996834,1.8279401050582957,1.8279344743541956,1.8279287887295013,1.8279230484570903,1.8279172538414796,1.8279114052197198,1.8279055029623035,1.8278995474740876,1.827893539195231,1.8278874786021466,1.8278813662084714,1.8278752025660492,1.8278689882659305,1.8278627239393923,1.8278564102589716,1.8278500479395188,1.8278436377392684,1.8278371804609308,1.8278306769528023,1.8278241281098968,1.8278175348750976,1.8278108982403338,1.8278042192477764,1.827797498991063,1.8277907386165433,1.8277839393245534,1.8277771023707166,1.8277702290672724,1.827763320784435,1.8277563789517848,1.8277494050596892,1.827742400660759,1.8277353673713403,1.8277283068730412,1.8277212209143001,1.8277141113119912,1.8277069799530725,1.8276998287962791,1.8276926598738594,1.8276854752933613,1.8276782772394657,1.8276710679758723,1.8276638498472402,1.8276566252811817,1.827649396790317,1.8276421669743865,1.8276349385224284,1.8276277142150188,1.8276204969265812,1.8276132896277655,1.8276060953878992,1.8275989173775138,1.8275917588709498,1.8275846232490414,1.827577514001883,1.827570434731686,1.8275633891557186,1.8275563811093414,1.8275494145491347,1.827542493556124,1.8275356223391048,1.827528805238071,1.8275220467277469,1.82751535142123,1.8275087240737424,1.8275021695864988,1.827495693010688,1.8274892995515772,1.8274829945727362,1.827476783600387,1.8274706723278802,1.8274646666203045,1.8274587725192244,1.8274529962475554,1.8274473442145776,1.8274418230210854,1.827436439464684,1.8274312005452262,1.8274261134703969,1.8274211856614453,1.827416424759068,1.827411838629443,1.8274074353704166,1.8274032233178454,1.8273992110520947,1.8273954074046939,1.8273918214651481,1.8273884625879127,1.8273853403995233,1.827382464805889,1.8273798459997437,1.8273774944682577,1.8273754210008137,1.8273736366969355,1.8273721529743818,1.827370981577397,1.8273701345851168,1.827369624420133,1.8273694638572098,1.8273696660321548,1.827370244450838,1.82737121299836,1.8273725859483636,1.8273743779724885,1.827376604149962,1.8273792799773263,1.8273824213782952,1.827386044713734,1.827390166791764,1.8273948048779798,1.8273999767057767,1.8274057004867819,1.8274119949213827,1.827418879209345,1.8274263730605156,1.8274344967055967,1.827443270906992,1.8274527169697077,1.827462856752303,1.827473712677883,1.8274853077451196,1.8274976655392912,1.8275108102433342,1.82752476664889,1.8275395601673379,1.8275552168408022,1.82757176335312,1.8275892270407608,1.8276076359036737,1.827627018616064,1.827647404537071,1.8276688237213423,1.8276913069294822,1.8277148856383658,1.8277395920512987,1.8277654591080077,1.8277925204944474,1.8278208106524056,1.8278503647888913,1.8278812188852878,1.8279134097062557,1.827946974808368,1.827981952548459,1.8280183820916733,1.8280563034191943,1.8280957573356384,1.8281367854760948,1.8281794303127972,1.8282237351614086,1.8282697441869054,1.8283175024090415,1.8283670557073783,1.8284184508258676,1.8284717353769675,1.8285269578452805,1.828584167590699,1.8286434148510446,1.8287047507441867,1.8287682272696317,1.8288338973095695,1.8289018146293612,1.8289720338774706,1.8290446105848166,1.8291196011635438,1.8291970629052063,1.8292770539783525,1.829359633425507,1.8294448611595455,1.829532797959455,1.8296235054654795,1.829717046173647,1.8298134834296724,1.8299128814222418,1.8300153051756718,1.8301208205419481,1.8302294941921422,1.8303413936072073,1.8304565870681602,1.8305751436456448,1.8306971331888873,1.8308226263140421,1.8309516943919346,1.8310844095352063,1.8312208445848592,1.8313610730962164,1.8315051693242905,1.8316532082085732,1.8318052653572428,1.8319614170307987,1.8321217401251233,1.8322863121539723,1.8324552112308978,1.8326285160506035,1.8328063058697308,1.8329886604870764,1.8331756602232383,1.833367385899686,1.8335639188172497,1.833765340734024,1.8339717338426738,1.834183180747139,1.8343997644387207,1.8346215682715385,1.8348486759373455,1.835081171439681,1.8353191390673471,1.8355626633671842,1.8358118291161265,1.8360667212925135,1.8363274250466333,1.836594025670465,1.8368666085666014,1.8371452592163133,1.8374300631467269,1.8377211058970857,1.8380184729840576,1.8383222498660547,1.838632521906535,1.838949374336242,1.8392728922143546,1.839603160388509,1.8399402634536521,1.8402842857097006,1.8406353111179616,1.8409934232562908,1.8413587052729488,1.8417312398391303,1.8421111091001363,1.8424983946251614,1.8428931773556787,1.8432955375523936,1.8437055547407588,1.8441233076550285,1.844548874180848,1.844982331296371,1.8454237550119024,1.8458732203080745,1.8463308010725612,1.8467965700353457,1.8472705987025626,1.8477529572889393,1.848243714648866,1.8487429382061356,1.849250693882393,1.8497670460243474,1.8502920573298027,1.8508257887725672,1.8513682995263179,1.8519196468874886,1.8524798861972693,1.8530490707628062,1.8536272517776942,1.8542144782418692,1.8548107968810048,1.8554162520655266,1.8560308857293686,1.8566547372885922,1.8572878435600002,1.8579302386798822,1.8585819540230275,1.859243018122157,1.8599134565879127,1.8605932920295618,1.8612825439765703,1.8619812288011999,1.8626893596422864,1.8634069463303662,1.8641339953143021,1.8648705095895814,1.8656164886284403,1.8663719283119797,1.8671368208644339,1.8679111547897473,1.8686949148106224,1.8694880818101887,1.8702906327764455,1.87110254074963,1.8719237747726496,1.8727542998447237,1.8735940768783665,1.8744430626598412,1.8753012098132122,1.8761684667681084,1.8770447777313157,1.8779300826622967,1.8788243172527412,1.8797274129102355,1.8806392967461292,1.8815598915676852,1.8824891158745682,1.8834268838597372,1.884373105414791,1.8853276861398083,1.886290527357715,1.8872615261332073,1.8882405752962401,1.8892275634700957,1.8902223751040261,1.8912248905104638,1.8922349859067797,1.893252533461562,1.8942774013453847,1.89530945378602,1.8963485511280422,1.8973945498967721,1.8984473028664848,1.8995066591328176,1.9005724641892903,1.9016445600078549,1.9027227851233768,1.9038069747219493,1.9048969607329334,1.9059925719246085,1.9070936340033198,1.908199969715994,1.9093113989558954,1.9104277388714919,1.9115488039782855,1.912674406273476,1.9138043553532973,1.9149384585328908,1.91607652096855,1.9172183457821874,1.9183637341878652,1.9195124856202201,1.920664397864629,1.9218192671889405,1.9229768884766123,1.9241370553610806,1.9252995603611953,1.9264641950175465,1.9276307500295184,1.9287990153928902,1.92996878053782,1.9311398344670414,1.9323119658940933,1.933484963381425,1.9346586154782044,1.9358327108576554,1.9370070384537732,1.9381813875972391,1.939355548150385,1.9405293106410428,1.9417024663951257,1.942874807667781,1.9440461277729777,1.9452162212113637,1.9463848837962672,1.9475519127776875,1.94871710696415,1.9498802668422872,1.951041194694021,1.9521996947112235,1.9533555731077388,1.9545086382286498,1.9556587006566855,1.9568055733156622,1.957949071570867,1.959089013326284,1.960225219118584,1.9613575122077913,1.9624857186645572,1.96360966745397,1.9647291905158373,1.965844122841386,1.9669543025463294,1.9680595709402544,1.9691597725922936,1.97025475539305,1.9713443706127465,1.9724284729555817,1.9735069206102789,1.9745795752968216,1.9756463023093707,1.976706970555373,1.9777614525908658,1.978809624651999,1.9798513666827928,1.9808865623591605,1.981915099109227,1.9829368681299833,1.983951764400317,1.984959686690471,1.9859605375679785,1.9869542234001325,1.9879406543530538,1.9889197443874194,1.9898914112509238,1.9908555764675442,1.9918121653236853,1.9927611068512892,1.9937023338079836,1.9946357826543646,1.9955613935284935,1.9964791102177026,1.9973888801278037,1.998290654249787,1.9991843871241157,2.0000700368027062,2.0009475648086963,2.0018169360940994,2.0026781189954446],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.8284906757568566,1.8284906758245814,1.8284906758935666,1.8284906759638355,1.8284906760354125,1.8284906761083215,1.828490676182587,1.828490676258235,1.8284906763352906,1.8284906764137803,1.8284906764937305,1.828490676575169,1.8284906766581228,1.8284906767426206,1.828490676828691,1.8284906769163631,1.8284906770056666,1.8284906770966323,1.8284906771892908,1.828490677283674,1.8284906773798133,1.828490677477742,1.8284906775774932,1.8284906776791008,1.828490677782599,1.828490677888024,1.8284906779954109,1.828490678104796,1.828490678216217,1.8284906783297115,1.828490678445318,1.8284906785630763,1.828490678683026,1.8284906788052082,1.828490678929664,1.8284906790564361,1.8284906791855677,1.8284906793171023,1.8284906794510847,1.8284906795875608,1.8284906797265768,1.8284906798681797,1.828490680012418,1.828490680159341,1.8284906803089978,1.82849068046144,1.8284906806167192,1.8284906807748884,1.8284906809360013,1.8284906811001123,1.8284906812672777,1.828490681437554,1.8284906816109994,1.8284906817876727,1.8284906819676339,1.8284906821509441,1.8284906823376663,1.828490682527863,1.8284906827215996,1.8284906829189422,1.828490683119957,1.828490683324713,1.8284906835332795,1.8284906837457278,1.8284906839621298,1.8284906841825592,1.828490684407091,1.8284906846358013,1.8284906848687683,1.8284906851060707,1.8284906853477896,1.8284906855940075,1.8284906858448073,1.8284906861002745,1.8284906863604966,1.8284906866255615,1.8284906868955595,1.8284906871705824,1.8284906874507234,1.8284906877360785,1.8284906880267442,1.8284906883228194,1.8284906886244048,1.8284906889316028,1.8284906892445183,1.8284906895632576,1.8284906898879287,1.8284906902186424,1.828490690555511,1.828490690898649,1.8284906912481733,1.8284906916042025,1.8284906919668575,1.8284906923362625,1.8284906927125422,1.8284906930958247,1.828490693486241,1.8284906938839234,1.8284906942890067,1.8284906947016293,1.8284906951219313,1.8284906955500557,1.828490695986148,1.8284906964303567,1.8284906968828325,1.8284906973437294,1.8284906978132045,1.8284906982914169,1.8284906987785297,1.8284906992747083,1.8284906997801214,1.828490700294941,1.828490700819342,1.8284907013535032,1.828490701897606,1.8284907024518353,1.8284907030163797,1.8284907035914308,1.8284907041771852,1.8284907047738412,1.8284907053816017,1.828490706000674,1.8284907066312681,1.828490707273599,1.8284907079278847,1.8284907085943478,1.828490709273215,1.8284907099647174,1.8284907106690897,1.828490711386572,1.828490712117408,1.8284907128618464,1.8284907136201403,1.8284907143925477,1.8284907151793315,1.8284907159807586,1.8284907167971023,1.8284907176286398,1.8284907184756543,1.8284907193384334,1.8284907202172715,1.8284907211124664,1.8284907220243234,1.8284907229531522,1.828490723899269,1.8284907248629951,1.828490725844659,1.8284907268445942,1.828490727863141,1.8284907289006458,1.8284907299574615,1.8284907310339475,1.8284907321304702,1.828490733247402,1.828490734385124,1.8284907355440219,1.828490736724491,1.828490737926932,1.8284907391517544,1.8284907403993749,1.8284907416702179,1.8284907429647155,1.8284907442833085,1.8284907456264448,1.828490746994582,1.828490748388185,1.8284907498077285,1.8284907512536948,1.8284907527265764,1.8284907542268738,1.8284907557550978,1.8284907573117686,1.8284907588974153,1.8284907605125775,1.828490762157805,1.8284907638336574,1.828490765540705,1.828490767279528,1.828490769050719,1.82849077085488,1.8284907726926254,1.82849077456458,1.828490776471381,1.828490778413677,1.8284907803921293,1.8284907824074108,1.8284907844602079,1.828490786551218,1.828490788681154,1.8284907908507395,1.828490793060714,1.8284907953118283,1.8284907976048497,1.8284907999405582,1.8284908023197481,1.8284908047432302,1.8284908072118282,1.8284908097263832,1.8284908122877506,1.8284908148968022,1.828490817554426,1.8284908202615269,1.8284908230190264,1.8284908258278625,1.828490828688992,1.8284908316033883,1.8284908345720439,1.8284908375959692,1.8284908406761937,1.828490843813766,1.8284908470097543,1.8284908502652466,1.828490853581351,1.828490856959197,1.8284908603999348,1.8284908639047353,1.8284908674747922,1.8284908711113212,1.8284908748155606,1.8284908785887712,1.8284908824322386,1.8284908863472713,1.8284908903352026,1.8284908943973908,1.8284908985352193,1.828490902750097,1.82849090704346,1.8284909114167704,1.8284909158715177,1.8284909204092192,1.8284909250314212,1.8284909297396976,1.8284909345356526,1.8284909394209197,1.8284909443971638,1.82849094946608,1.8284909546293948,1.8284909598888686,1.8284909652462924,1.8284909707034924,1.828490976262328,1.8284909819246933,1.8284909876925184,1.8284909935677687,1.8284909995524465,1.828491005648592,1.8284910118582827,1.8284910181836354,1.828491024626806,1.8284910311899916,1.828491037875429,1.8284910446853984,1.8284910516222204,1.828491058688261,1.8284910658859297,1.8284910732176802,1.8284910806860135,1.8284910882934762,1.8284910960426628,1.8284911039362166,1.8284911119768301,1.828491120167246,1.828491128510258,1.8284911370087131,1.82849114566551,1.8284911544836029,1.8284911634660006,1.8284911726157684,1.8284911819360286,1.8284911914299624,1.82849120110081,1.8284912109518727,1.8284912209865127,1.8284912312081565,1.8284912416202932,1.8284912522264787,1.8284912630303338,1.8284912740355486,1.8284912852458814,1.828491296665161,1.828491308297288,1.8284913201462363,1.8284913322160534,1.8284913445108637,1.828491357034868,1.8284913697923464,1.8284913827876583,1.828491396025246,1.8284914095096343,1.828491423245433,1.8284914372373382,1.828491451490134,1.8284914660086948,1.8284914807979853,1.8284914958630647,1.8284915112090863,1.8284915268412996,1.8284915427650537,1.828491558985797,1.8284915755090816,1.8284915923405627,1.8284916094860015,1.8284916269512688,1.8284916447423438,1.8284916628653196,1.828491681326403,1.8284917001319174,1.8284917192883057,1.828491738802131,1.8284917586800806,1.828491778928967,1.8284917995557317,1.8284918205674454,1.828491841971313,1.828491863774675,1.8284918859850094,1.8284919086099354,1.828491931657216,1.8284919551347592,1.8284919790506238,1.828492003413019,1.8284920282303094,1.8284920535110176,1.8284920792638264,1.828492105497582,1.8284921322212986,1.8284921594441597,1.8284921871755224,1.8284922154249206,1.8284922442020681,1.8284922735168625,1.8284923033793878,1.8284923337999197,1.828492364788927,1.8284923963570772,1.8284924285152393,1.8284924612744884,1.8284924946461087,1.8284925286415987,1.8284925632726743,1.8284925985512734,1.82849263448956,1.8284926710999296,1.8284927083950118,1.8284927463876757,1.8284927850910364,1.8284928245184557,1.8284928646835505,1.8284929056001966,1.8284929472825326,1.8284929897449667,1.8284930330021807,1.8284930770691363,1.8284931219610794,1.8284931676935465,1.8284932142823704,1.8284932617436855,1.8284933100939333,1.82849335934987,1.8284934095285708,1.8284934606474366,1.8284935127242017,1.828493565776939,1.8284936198240662,1.828493674884354,1.8284937309769325,1.8284937881212975,1.8284938463373186,1.8284939056452458,1.8284939660657182,1.8284940276197705,1.828494090328841,1.8284941542147801,1.8284942192998572,1.8284942856067719,1.8284943531586582,1.828494421979097,1.8284944920921227,1.8284945635222334,1.8284946362943988,1.8284947104340714,1.828494785967195,1.828494862920214,1.8284949413200844,1.828495021194284,1.8284951025708225,1.8284951854782516,1.8284952699456771,1.8284953560027692,1.8284954436797745,1.8284955330075263,1.8284956240174577,1.828495716741614,1.8284958112126637,1.8284959074639122,1.8284960055293138,1.8284961054434865,1.828496207241723,1.8284963109600072,1.828496416635026,1.8284965243041855,1.8284966340056237,1.8284967457782275,1.828496859661647,1.8284969756963112,1.828497093923444,1.8284972143850806,1.828497337124085,1.8284974621841659,1.828497589609895,1.8284977194467247,1.828497851741006,1.8284979865400077,1.8284981238919353,1.8284982638459508,1.8284984064521923,1.8284985517617942,1.8284986998269088,1.8284988507007274,1.828499004437502,1.8284991610925676,1.8284993207223657,1.8284994833844663,1.828499649137594,1.82849981804165,1.8284999901577388,1.8285001655481938,1.8285003442766028,1.828500526407835,1.8285007120080685,1.828500901144819,1.8285010938869675,1.8285012903047908,1.828501490469991,1.8285016944557257,1.8285019023366418,1.8285021141889044,1.828502330090234,1.8285025501199363,1.8285027743589397,1.8285030028898301,1.8285032357968867,1.8285034731661192,1.828503715085307,1.8285039616440373,1.828504212933746,1.8285044690477574,1.828504730081327,1.8285049961316853,1.8285052672980795,1.8285055436818214,1.8285058253863318,1.828506112517189,1.8285064051821762,1.8285067034913327,1.828507007557004,1.828507317493894,1.8285076334191186,1.8285079554522619,1.8285082837154312,1.8285086183333141,1.8285089594332402,1.8285093071452392,1.8285096616021046,1.8285100229394573,1.8285103912958098,1.8285107668126357,1.8285111496344364,1.8285115399088119,1.8285119377865349,1.8285123434216228,1.8285127569714155,1.8285131785966526,1.8285136084615543,1.8285140467339027,1.828514493585127,1.8285149491903896,1.8285154137286752,1.8285158873828822,1.8285163703399154,1.8285168627907828,1.8285173649306938,1.82851787695916,1.8285183990801002,1.828518931501945,1.8285194744377484,1.8285200281052978,1.8285205927272314,1.8285211685311553,1.8285217557497648,1.8285223546209692,1.828522965388022,1.828523588299648,1.8285242236101833,1.8285248715797102,1.8285255324742016,1.8285262065656664,1.8285268941323,1.8285275954586386,1.8285283108357178,1.8285290405612342,1.828529784939714,1.828530544282684,1.8285313189088483,1.8285321091442683,1.8285329153225507,1.8285337377850366,1.8285345768809997,1.8285354329678474,1.828536306411327,1.82853719758574,1.8285381068741613,1.8285390346686625,1.8285399813705432,1.8285409473905694,1.828541933149216,1.8285429390769181,1.8285439656143285,1.8285450132125811,1.8285460823335644,1.8285471734501995,1.8285482870467267,1.8285494236190016,1.8285505836747977,1.828551767734117,1.8285529763295107,1.8285542100064072,1.8285554693234507,1.828556754852848,1.8285580671807242,1.8285594069074904,1.8285607746482198,1.8285621710330346,1.8285635967075036,1.82856505233305,1.8285665385873724,1.8285680561648754,1.8285696057771121,1.8285711881532407,1.8285728040404907,1.828574454204645,1.8285761394305318,1.8285778605225338,1.8285796183051082,1.8285814136233216,1.8285832473434005,1.828585120353297,1.8285870335632666,1.828588987906467,1.8285909843395687,1.8285930238433845,1.8285951074235145,1.8285972361110103,1.828599410963056,1.8286016330636665,1.8286039035244084,1.8286062234851346,1.828608594114745,1.8286110166119607,1.8286134922061257,1.8286160221580248,1.8286186077607256,1.8286212503404424,1.8286239512574236,1.828626711906861,1.8286295337198253,1.828632418164223,1.828635366745784,1.828638381009067,1.8286414625384995,1.828644612959439,1.8286478339392647,1.8286511271884967,1.8286544944619438,1.828657937559882,1.8286614583292602,1.8286650586649436,1.8286687405109796,1.8286725058619056,1.8286763567640798,1.8286802953170573,1.8286843236749892,1.8286884440480673,1.8286926587039962,1.82869696996951,1.8287013802319196,1.8287058919407049,1.8287105076091412,1.8287152298159697,1.828720061207105,1.8287250044973884,1.828730062472381,1.828735237990202,1.8287405339834095,1.8287459534609296,1.8287514995100282,1.8287571752983334,1.8287629840759017,1.8287689291773377,1.8287750140239598,1.8287812421260203,1.8287876170849726,1.8287941425957959,1.8288008224493701,1.8288076605349068,1.8288146608424358,1.8288218274653474,1.8288291646029922,1.8288366765633406,1.828844367765701,1.8288522427434966,1.828860306147107,1.8288685627467691,1.8288770174355427,1.8288856752323408,1.8288945412850235,1.8289036208735603,1.8289129194132554,1.8289224424580477,1.8289321957038729,1.828942184992099,1.8289524163130326,1.828962895809496,1.828973629780475,1.8289846246848451,1.8289958871451653,1.8290074239515517,1.8290192420656242,1.8290313486245298,1.8290437509450446,1.8290564565277507,1.8290694730612926,1.8290828084267132,1.8290964707018675,1.8291104681659167,1.8291248093039032,1.8291395028114033,1.8291545575992645,1.82916998279842,1.829185787764787,1.8292019820842431,1.8292185755776877,1.8292355783061793,1.8292530005761583,1.8292708529447468,1.8292891462251293,1.8293078914920142,1.8293271000871714,1.8293467836250497,1.8293669539984712,1.829387623384401,1.8294088042497931,1.8294305093575083,1.8294527517723052,1.8294755448669027,1.8294989023281087,1.829522838163019,1.829547366705278,1.8295725026214034,1.829598260917171,1.8296246569440562,1.8296517064057298,1.8296794253646058,1.8297078302484364,1.8297369378569521,1.8297667653685425,1.8297973303469737,1.8298286507481374,1.8298607449268296,1.8298936316435517,1.8299273300713288,1.829961859802544,1.829997240855778,1.830033493682649,1.8300706391746533,1.8301086986699897,1.830147693960371,1.8301876472978071,1.830228581401362,1.8302705194638669,1.8303134851585894,1.830357502645847,1.8304025965795587,1.8304487921137234,1.8304961149088173,1.8305445911381044,1.8305942474938428,1.830645111193386,1.8306972099851615,1.8307505721545219,1.8308052265294543,1.8308612024861377,1.8309185299543393,1.8309772394226351,1.8310373619434461,1.8310989291378759,1.8311619732003375,1.8312265269029597,1.831292623599758,1.831360297230556,1.831429582324649,1.8315005140041902,1.831573127987295,1.8316474605908393,1.8317235487329473,1.831801429935151,1.8318811423242074,1.831962724633562,1.8320462162044433,1.8321316569865744,1.8322190875384887,1.8323085490274353,1.8324000832288616,1.832493732525458,1.8325895399057521,1.832687548962241,1.832787803889044,1.832890349479067,1.8329952311206665,1.8331024947937953,1.8332121870656244,1.8333243550856264,1.8334390465801083,1.8335563098461851,1.8336761937451826,1.8337987476954574,1.8339240216646318,1.8340520661612234,1.8341829322256733,1.8343166714207566,1.8344533358213724,1.8345929780037054,1.8347356510337562,1.8348814084552316,1.8350303042767948,1.8351823929586715,1.8353377293986088,1.8354963689171873,1.8356583672424847,1.8358237804940953,1.8359926651665017,1.8361650781118084,1.8363410765218358,1.8365207179095835,1.8367040600900693,1.8368911611605476,1.8370820794801241,1.8372768736487672,1.837475602485737,1.837678325007438,1.8378851004047134,1.8380959880195933,1.8383110473215176,1.8385303378830493,1.838753919355097,1.8389818514416676,1.8392141938741753,1.8394510063853236,1.8396923486825913,1.8399382804213447,1.8401888611776036,1.8404441504204927,1.840704207484404,1.8409690915409045,1.841238861570418,1.8415135763337183,1.8417932943432624,1.8420780738344027,1.8423679727365148,1.8426630486440707,1.842963358787706,1.8432689600053056,1.8435799087131597,1.8438962608772156,1.844218071984479,1.8445453970145886,1.8448782904116163,1.8452168060561256,1.8455609972375275,1.8459109166267762,1.84626661624944,1.8466281474591855,1.846995560911712,1.8473689065391763,1.8477482335251363,1.8481335902800529,1.8485250244173792,1.8489225827302709,1.8493263111689466,1.8497362548187257,1.8501524578787678,1.850574963641542,1.851003814473043,1.8514390517937778,1.8518807160605342,1.852328846748952,1.8527834823369032,1.8532446602886932,1.8537124170400874,1.8541867879841663,1.8546678074580096,1.8551555087302078,1.855649923989189,1.8561510843323596,1.85665901975604,1.8571737591461825,1.857695330269849,1.8582237597674316,1.8587590731455848,1.8593012947708445,1.8598504478638993,1.8604065544944828,1.860969635576846,1.861539710865773,1.8621167989530931,1.862700917264647,1.86329208205766,1.8638903084184697,1.8644956102605592,1.8651080003228402,1.8657274901681344,1.866354090181793,1.8669878095704004,1.8676286563605042,1.8682766373973094,1.8689317583432845,1.8695940236766135,1.8702634366894433,1.8709399994858622,1.871623712979557,1.8723145768910907,1.8730125897447492,1.8737177488649017,1.8744300503718272,1.8751494891769565,1.8758760589774874,1.8766097522503278,1.8773505602453275,1.8780984729777657,1.878853479220055,1.8796155664926428,1.8803847210540734,1.8811609278902022,1.8819441707025362,1.8827344318956922,1.88353169256397,1.8843359324770275,1.8851471300646718,1.8859652624007643,1.8867903051862587,1.88762223273138,1.8884610179369796,1.8893066322750798,1.8901590457686486,1.8910182269706375,1.8918841429423265,1.892756759231018,1.8936360398471355,1.8945219472407797,1.8954144422777994,1.8963134842154463,1.8972190306776733,1.8981310376301552,1.8990494593551017,1.89997424842594,1.9009053556819508,1.9018427302029386,1.9027863192840233,1.90373606841064,1.904691921233839,1.9056538195459738,1.906621703256875,1.907595510370597,1.9085751769628392,1.9095606371591303,1.910551823113874,1.9115486649903495,1.912551090941761,1.913559027093427,1.914572397526208,1.9155911242612527,1.9166151272461638,1.9176443243426586,1.9186786313158175,1.9197179618249993,1.9207622274165013,1.9218113375180454,1.9228651994351578,1.9239237183495164,1.924986797319332,1.9260543372818226,1.9271262370578441,1.928202393358729,1.9292827007953843,1.930367051889696,1.9314553370882792,1.932547444778612,1.933643261307589,1.9347426710025146,1.9358455561945667,1.9369517972447425,1.9380612725723045,1.9391738586857303,1.9402894302161722,1.9414078599534237,1.9425290188843858,1.943652776234022,1.9447789995087825,1.9459075545424807,1.9470383055445901,1.9481711151509327,1.949305844476723,1.9504423531719255,1.9515804994788792,1.9527201402921397,1.9538611312204854,1.955003326651027,1.9561465798153541,1.9572907428576611,1.9584356669047658,1.9595812021379584,1.9607271978665946,1.961873502603352,1.9630199641410624,1.964166429631029,1.9653127456627395,1.9664587583448732,1.967604313387509,1.9687492561854276,1.9698934319024084,1.97103668555641,1.972178862105531,1.9733198065346376,1.9744593639425436,1.9755973796296393,1.9767336991858413,1.9778681685787578,1.9790006342419504,1.9801309431631713,1.981258942972466,1.982384482030018,1.983507409513623,1.98462757550567,1.9857448310795238,1.9868590283851806,1.9879700207340942,1.9890776626830529,1.9901818101170003,1.9912823203306895,1.9923790521090627,1.993471865806255,1.994560623423114,1.9956451886831443,1.9967254271067738,1.9978012060838495,1.9988723949442762,1.99993886502671,2.001000489745222,2.0020571446538544,2.0031087075090004,2.0041550583295233,2.005196079454565,2.0062316555989708,2.007261673906277,2.0082860239992084,2.009304598027637,2.010317290713963,2.011323999395871,2.012324624066438,2.013319067411557,2.0143072348446545,2.0152890345386907,2.0162643774554114,2.0172331773718626,2.01819535090415,2.0191508175284496,2.02009949959927,2.021041322364987,2.021976213980651,2.0229041055180965,2.023824930973374,2.024738627271527,2.025645134268757,2.0265443947519977,2.027436354435951,2.0283209619576166,2.0291981688683713,2.030067929623638,2.030930201570206,2.031784944931253,2.0326321227891277,2.033471701065957,2.034303648502136,2.0351279366327732,2.035944539762152,2.036753434936282,2.0375546019136084,2.0383480231339552,2.0391336836857725,2.0399115712717633],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[3.0481363358316247,3.0481363264932773,3.0481363169811413,3.0481363072919843,3.0481362974225092,3.0481362873693625,3.048136277129125,3.0481362666983154,3.048136256073388,3.0481362452507286,3.0481362342266585,3.048136222997428,3.0481362115592217,3.048136199908148,3.0481361880402464,3.0481361759514813,3.0481361636377438,3.048136151094845,3.0481361383185224,3.0481361253044303,3.048136112048143,3.0481360985451578,3.0481360847908787,3.0481360707806298,3.0481360565096507,3.048136041973085,3.048136027165993,3.0481360120833396,3.048135996719996,3.0481359810707387,3.048135965130248,3.0481359488931026,3.0481359323537816,3.0481359155066623,3.048135898346016,3.0481358808660093,3.0481358630606974,3.0481358449240266,3.0481358264498306,3.048135807631828,3.04813578846362,3.04813576893869,3.0481357490504,3.0481357287919866,3.0481357081565617,3.048135687137111,3.048135665726484,3.0481356439174063,3.0481356217024578,3.048135599074088,3.0481355760246025,3.0481355525461633,3.048135528630789,3.048135504270348,3.0481354794565574,3.0481354541809793,3.048135428435022,3.048135402209929,3.0481353754967864,3.04813534828651,3.0481353205698496,3.0481352923373795,3.0481352635795016,3.0481352342864376,3.0481352044482284,3.0481351740547287,3.0481351430956027,3.048135111560327,3.0481350794381785,3.0481350467182344,3.0481350133893717,3.0481349794402566,3.0481349448593473,3.0481349096348866,3.048134873754896,3.0481348372071797,3.048134799979308,3.048134762058624,3.0481347234322356,3.04813468408701,3.0481346440095676,3.0481346031862846,3.0481345616032804,3.0481345192464144,3.048134476101288,3.0481344321532293,3.048134387387298,3.048134341788272,3.048134295340649,3.048134248028636,3.048134199836147,3.048134150746796,3.048134100743894,3.0481340498104372,3.048133997929112,3.048133945082276,3.048133891251963,3.0481338364198685,3.048133780567352,3.0481337236754213,3.048133665724735,3.04813360669559,3.0481335465679154,3.0481334853212676,3.0481334229348254,3.0481333593873745,3.048133294657311,3.0481332287226257,3.0481331615609006,3.048133093149302,3.0481330234645707,3.0481329524830127,3.048132880180495,3.048132806532435,3.048132731513794,3.048132655099063,3.0481325772622636,3.048132497976932,3.0481324172161113,3.048132334952342,3.0481322511576567,3.048132165803564,3.0481320788610455,3.0481319903005413,3.048131900091941,3.0481318082045736,3.0481317146071993,3.048131619267996,3.0481315221545477,3.048131423233838,3.0481313224722353,3.0481312198354797,3.048131115288678,3.048131008796284,3.048130900322092,3.0481307898292207,3.048130677280106,3.048130562636482,3.0481304458593703,3.0481303269090687,3.0481302057451365,3.0481300823263786,3.0481299566108357,3.0481298285557648,3.04812969811763,3.048129565252085,3.0481294299139576,3.0481292920572334,3.048129151635045,3.0481290085996515,3.048128862902424,3.0481287144938265,3.048128563323404,3.0481284093397627,3.04812825249055,3.0481280927224406,3.0481279299811166,3.0481277642112516,3.0481275953564855,3.0481274233594124,3.0481272481615593,3.0481270697033596,3.048126887924146,3.0481267027621155,3.0481265141543203,3.0481263220366364,3.0481261263437522,3.0481259270091368,3.0481257239650232,3.04812551714238,3.048125306470897,3.0481250918789513,3.0481248732935895,3.0481246506405,3.0481244238439875,3.0481241928269505,3.048123957510852,3.0481237178156917,3.048123473659984,3.048123224960725,3.048122971633368,3.0481227135917903,3.04812245074827,3.048122183013451,3.048121910296316,3.048121632504154,3.0481213495425274,3.048121061315243,3.0481207677243174,3.0481204686699455,3.0481201640504616,3.0481198537623135,3.0481195377000185,3.0481192157561328,3.048118887821212,3.0481185537837803,3.0481182135302793,3.0481178669450477,3.048117513910265,3.0481171543059222,3.0481167880097755,3.048116414897308,3.0481160348416876,3.0481156477137206,3.0481152533818086,3.048114851711908,3.0481144425674787,3.0481140258094443,3.048113601296135,3.048113168883251,3.0481127284238045,3.0481122797680746,3.0481118227635564,3.0481113572549057,3.0481108830838877,3.0481104000893273,3.048109908107048,3.0481094069698207,3.0481088965073053,3.048108376545993,3.048107846909146,3.04810730741674,3.0481067578854026,3.048106198128349,3.04810562795532,3.0481050471725166,3.048104455582538,3.0481038529843074,3.048103239173011,3.0481026139400225,3.048101977072837,3.0481013283549974,3.0481006675660174,3.0480999944813116,3.0480993088721196,3.0480986105054217,3.048097899143866,3.0480971745456857,3.048096436464619,3.048095684649821,3.048094918845785,3.0480941387922478,3.0480933442241094,3.048092534871338,3.0480917104588787,3.0480908707065635,3.0480900153290116,3.048089144035535,3.048088256530044,3.048087352510936,3.0480864316710057,3.04808549369733,3.048084538271169,3.0480835650678557,3.0480825737566857,3.048081564000804,3.0480805354570926,3.0480794877760538,3.0480784206016907,3.0480773335713858,3.04807622631578,3.048075098458648,3.048073949616767,3.0480727793997895,3.0480715874101127,3.048070373242737,3.0480691364851373,3.048067876717116,3.048066593510666,3.048065286429822,3.0480639550305124,3.0480625988604118,3.048061217458789,3.048059810356341,3.048058377075046,3.048056917127997,3.0480554300192337,3.0480539152435773,3.048052372286459,3.0480508006237486,3.0480491997215684,3.0480475690361244,3.0480459080135107,3.0480442160895302,3.048042492689497,3.0480407372280487,3.0480389491089435,3.048037127724857,3.048035272457182,3.0480333826758153,3.0480314577389436,3.04802949699283,3.0480274997715884,3.0480254653969645,3.048023393178097,3.0480212824112924,3.0480191323797827,3.0480169423534824,3.0480147115887473,3.0480124393281134,3.048010124800049,3.0480077672186905,3.0480053657835753,3.0480029196793748,3.0480004280756163,3.0479978901264033,3.0479953049701267,3.047992671729179,3.047989989509655,3.047987257401047,3.0479844744759403,3.047981639789703,3.0479787523801583,3.0479758112672695,3.047972815452801,3.0479697639199865,3.0479666556331817,3.047963489537521,3.0479602645585526,3.0479569796018855,3.04795363355281,3.047950225275933,3.047946753614787,3.0479432173914405,3.0479396154061065,3.0479359464367324,3.0479322092385894,3.047928402543853,3.047924525061181,3.0479205754752674,3.04791655244641,3.047912454610054,3.047908280576333,3.0479040289296027,3.0478996982279623,3.0478952870027696,3.0478907937581496,3.0478862169704857,3.0478815550879155,3.047876806529798,3.047871969686194,3.047867042917316,3.04786202455298,3.0478569128920467,3.047851706201846,3.047846402717597,3.0478410006418177,3.0478354981437175,3.0478298933585837,3.0478241843871587,3.0478183692949994,3.0478124461118323,3.0478064128308873,3.0478002674082303,3.047794007762074,3.047787631772087,3.047781137278675,3.047774522082263,3.0477677839425588,3.0477609205778036,3.047753929664004,3.047746808834162,3.047739555677476,3.047732167738542,3.047724642516529,3.0477169774643444,3.0477091699877836,3.0477012174446654,3.047693117143945,3.0476848663448255,3.047676462255836,3.0476679020339037,3.0476591827834074,3.0476503015552114,3.047641255345686,3.047632041095703,3.047622655689624,3.047613095954259,3.047603358657814,3.0475934405088165,3.0475833381550226,3.047573048182303,3.0475625671135114,3.047551891407327,3.0475410174570876,3.0475299415895853,3.0475186600638575,3.047507169069943,3.047495464727623,3.0474835430851375,3.0474714001178786,3.0474590317270596,3.0474464337383615,3.047433601900557,3.0474205318841046,3.047407219279724,3.047393659596938,3.0473798482626018,3.047365780619387,3.0473514519242593,3.0473368573469086,3.0473219919681735,3.047306850778416,3.047291428675882,3.0472757204650245,3.0472597208548087,3.0472434244569673,3.0472268257842456,3.0472099192486026,3.047192699159388,3.0471751597214745,3.0471572950333803,3.04713909908533,3.0471205657573064,3.0471016888170506,3.0470824619180368,3.0470628785974103,3.047042932273884,3.0470226162456036,3.0470019236879744,3.046980847651449,3.046959381059277,3.046937516705214,3.0469152472511962,3.046892565224967,3.0468694630176674,3.0468459328813826,3.0468219669266494,3.0467975571199157,3.0467726952809593,3.046747373080262,3.0467215820363363,3.0466953135130086,3.046668558716658,3.0466413086934034,3.0466135543262376,3.04658528633213,3.046556495259057,3.0465271714830062,3.0464973052049036,3.0464668864475137,3.046435905052268,3.046404350676049,3.0463722127879205,3.0463394806657966,3.046306143393062,3.04627218985513,3.046237608735945,3.046202388514427,3.046166517460855,3.046129983633192,3.0460927748733484,3.046054878803378,3.0460162828216233,3.0459769740987843,3.045936939573929,3.045896165950439,3.0458546396918793,3.045812347017818,3.045769273899556,3.045725406055806,3.0456807289482897,3.0456352277772667,3.045588887476989,3.045541692711089,3.045493627867885,3.045444677055612,3.0453948240975834,3.0453440525272617,3.04529234558327,3.045239686204301,3.0451860570239644,3.045131440365543,3.0450758182366653,3.0450191723239004,3.044961483987264,3.0449027342546335,3.04484290381609,3.044781973018154,3.0447199218579466,3.044656729977253,3.0445923766564924,3.044526840808604,3.044460100972831,3.0443921353084105,3.044322921588171,3.0442524371920316,3.0441806591003995,3.0441075638874695,3.044033127714427,3.043957326322542,3.0438801350261646,3.0438015287056177,3.0437214817999783,3.0436399682997557,3.0435569617394647,3.043472435190086,3.0433863612514163,3.0432987120443094,3.0432094592028056,3.043118573866141,3.043026026670652,3.0429317877415563,3.042835826684618,3.042738112577695,3.0426386139621693,3.0425372988342514,3.0424341346361654,3.042329088247215,3.042222125974716,3.042113213544815,3.042002316093164,3.04188939815549,3.041774423658018,3.041657355907767,3.041538157582719,3.041416790721858,3.0412932167150597,3.041167396292866,3.0410392895161107,3.040908855765412,3.040776053730524,3.0406408413995543,3.0405031760480337,3.040363014227846,3.040220311756022,3.0400750237033756,3.0399271043830045,3.0397765073386456,3.0396231853328692,3.0394670903351435,3.0393081735097334,3.039146385203454,3.038981674933272,3.0388139913737566,3.038643282344363,3.038469494796579,3.0382925748009,3.0381124675336486,3.037929117263641,3.037742467338689,3.0375524601719346,3.037359037228035,3.03716213900917,3.0369617050408966,3.0367576738578284,3.036549982989152,3.036338568943973,3.0361233671964984,3.035904312171036,3.035681337226839,3.0354543746427614,3.0352233556017514,3.0349882101751637,3.0347488673068965,3.034505254797354,3.034257299287227,3.0340049262411,3.0337480599308675,3.0334866234189817,3.0332205385415087,3.032949725891005,3.0326741047992085,3.0323935933195467,3.032108108209458,3.0318175649125196,3.0315218775403996,3.0312209588546106,3.030914720248079,3.0306030717265227,3.0302859218896363,3.0299631779120935,3.0296347455243433,3.0293005289932315,3.028960431102414,3.0286143531325895,3.0282621948415365,3.02790385444395,3.0275392285911,3.0271682123502863,3.0267906991841054,3.026406580929528,3.026015747776791,3.025618088248082,3.0252134891760623,3.024801835682182,3.0243830111548196,3.0239568972272415,3.0235233737553693,3.0230823187953857,3.022633608581147,3.022177117501443,3.0217127180770667,3.0212402809377434,3.02075967479888,3.0202707664381676,3.019773420672037,3.0192675003319636,3.0187528662406384,3.018229377188009,3.0176968899071954,3.0171552590502912,3.0166043371640576,3.016043974665525,3.0154740198175025,3.014894318704014,3.0143047152056752,3.013705050975009,3.013095165411737,3.012474895638038,3.011844076473804,3.011202540411901,3.0105501175934637,3.0098866357832312,3.009211920344944,3.0085257942168355,3.0078280778872286,3.007118589370262,3.006397144181771,3.0056635553153686,3.0049176332187084,3.004159185770011,3.0033880182548574,3.0026039333432695,3.001806731067149,3.0009962087980684,3.000172161225484,2.9993343803353927,2.99848265538947,2.9976167729047494,2.9967365166338698,2.995841667545942,2.99493200380809,2.9940073007677026,2.993067330935463,2.9921118639691904,2.991140666658579,2.990153502910858,2.989150133737452,2.9881303172417093,2.9870938086077423,2.9860403600904615,2.9849697210068715,2.98388163772868,2.9827758536763245,2.9816521093144592,2.980510142148995,2.979349686725775,2.978170474630951,2.9769722344931537,2.975754691987542,2.9745175698418116,2.9732605878442544,2.9719834628539568,2.9706859088132367,2.9693676367623976,2.9680283548569117,2.9666677683871145,2.9652855798005167,2.963881488726838,2.96245519200585,2.9610063837181433,2.9595347552189093,2.9580399951748633,2.956521789604385,2.954979821921008,2.953413772980347,2.9518233211305835,2.95020814226661,2.9485679098879407,2.946902295160499,2.9452109669823927,2.943493592053776,2.941749834950916,2.9399793582045595,2.938181822382714,2.9363568861779483,2.9345042064993034,2.9326234385689367,2.9307142360235847,2.928776251020941,2.926809134351059,2.924812535552866,2.9227861030358757,2.9207294842072025,2.9186423256039395,2.916524273031016,2.9143749717045737,2.9121940664009753,2.909981201611488,2.9077360217027244,2.9054581710828895,2.9031472943739116,2.90080303658948,2.8984250433190675,2.8960129609179464,2.89356643670326,2.891085119156162,2.8885686581300476,2.8860167050648933,2.8834289132077124,2.880804937839127,2.878144436506045,2.8754470692604284,2.8727124989041353,2.869940391239786,2.8671304153276367,2.8642822437483857,2.8613955528718686,2.8584700231315776,2.8555053393048935,2.8525011907989977,2.8494572719423066,2.8463732822813657,2.843248926883056,2.840083916642001,2.8368779685930106,2.8336308062284417,2.8303421598202894,2.827011766746842,2.8236393718237167,2.820224727639081,2.816767594892852,2.813267742739649,2.8097249491352874,2.806139001186546,2.8025096955039848,2.7988368385575275,2.795120247034546,2.7913597482001604,2.787555180259455,2.7837063927213053,2.7798132467634975,2.77587561559882,2.77189338484177,2.767866452875553,2.7637947312189977,2.7596781448930257,2.7555166327863008,2.7513101480196767,2.747058658309036,2.742762146326134,2.738420610057033,2.734034063157705,2.7296025353063844,2.72512607255224,2.7206047376599343,2.71603861044961,2.711427788131888,2.7067723856373913,2.702072535940368,2.6973283903759326,2.6925401189504807,2.6877079106447948,2.6828319737093946,2.6779125359516462,2.672949845014173,2.6679441686440946,2.662895794952629,2.6578050326645895,2.6526722113573116,2.6474976816885474,2.6422818156128725,2.6370250065861383,2.6317276697575367,2.626390242148816,2.6210131828202177,2.615596973022696,2.6101421163359935,2.604649138792165,2.5991185889841235,2.593551038158828,2.587947080294705,2.582307332162943,2.5766324333722697,2.5709230463968877,2.5651798565871946,2.559403572162986,2.553594924188803,2.5477546665311497,2.5418835757972755,2.535982451255269,2.530052114735211,2.5240934105111457,2.5181072051636635,2.512094387422908,2.506055867991804,2.4999925793493887,2.493905475534068,2.487795531906722,2.4816637448935386,2.4755111317085214,2.469338730055612,2.463147597810401,2.4569388126814315,2.45071347185111,2.444472691596262,2.438217606888415,2.431949370973879,2.4256691549337552,2.4193781472240072,2.413077553195751,2.406768594595961,2.400452509048792,2.3941305495177594,2.3878039837490324,2.381474093696119,2.375142174926252,2.368809536008797,2.3624774978860406,2.356147393226717,2.34982056576268,2.3434983696091227,2.337182168568782,2.3308733354205913,2.324573251193238,2.318283304424132,2.3120048904042885,2.3057394104096525,2.2994882709194173,2.2932528828218817,2.287034660608446,2.2808350215563067,2.2746553849004894,2.268497170995806,2.262361800469396,2.256250693364481,2.2501652682759867,2.2441069414787145,2.238077126048725,2.23207723097864,2.22610866028755,2.2201728121262394,2.214271077878457,2.208404841258953,2.2025754774090247,2.1967843519903196,2.1910328202776483,2.185322226251583,2.1796539016916063,2.174029165270607,2.168449321651507,2.162915660586828,2.157429456022018,2.15199196520335,2.1466044277912353,2.1412680649797986,2.135984078623555,2.1307536503720743,2.125577940813496,2.120458088627786,2.1153952097506368,2.110390396548908,2.1054447170085466,2.1005592139358966,2.0957349041733497,2.0909727778302893,2.086273797530267,2.081638897675405,2.0770689837289766,2.0725649315171544,2.068127586550909,2.063757763369044,2.0594562449033638,2.0552237818669563,2.051061092166585,2.0469688603401712,2.042947737020348,2.0389983384250536,2.03512124587612,2.031317005346804,2.0275861270391826,2.0239290849923264,2.0203463167221196,2.016838222893598,2.013405167026609,2.0100474752356066,2.0067654360043083,2.0035592999959464,2.0004292798997616,1.997375550314365,1.9943982476685262,1.9914974701798966,1.988673277852114,1.985925692510673,1.9832546978778822,1.9806602396871453,1.9781422258367583,1.9757005265833079,1.9733349747747078,1.9710453661228082,1.968831459515444,1.9666929773677027,1.9646296060121078,1.962640996127325,1.9607267632049232,1.9588864880536219,1.957119717340385,1.9554259641676317,1.9538047086857429,1.9522553987399744,1.950777450550795,1.9493702494265952,1.9480331505076338,1.9467654795400193,1.9455665336784473,1.9444355823163568,1.9433718679420973,1.9423746070196473,1.9414429908923638,1.9405761867082028,1.9397733383647884,1.9390335674726942,1.9383559743352403,1.937739638943095,1.937183621981948,1.93668696585149,1.9362486956939347,1.9358678204303088,1.9355433338027248,1.9352742154208702,1.9350594318109393,1.9348979374652666,1.93478867589092,1.9347305806555568,1.9347225764288556,1.9347635800178915,1.9348525013948392,1.9349882447154483,1.9351697093267763,1.9353957907627068,1.9356653817258502,1.9359773730544656,1.936330654673111,1.93672411652579,1.9371566494904249,1.9376271462735526,1.9381345022842138,1.9386776164860615,1.939255392226807,1.9398667380441648,1.940510568447557,1.9411858046748869,1.9418913754237803,1.9426262175567497,1.9433892767798198,1.9441795082942157,1.9449958774207792,1.9458373601968626,1.9467029439454946,1.9475916278166943,1.9485024233008614,1.9494343547142374,1.950386459656486,1.9513577894405032,1.9523474094946176,1.9533543997373919,1.95437785492529,1.9554168849735205,1.9564706152504043,1.9575381868456663,1.9586187568130802,1.95971149838794,1.9608156011798574,1.96193027134142,1.9630547317132758,1.9641882219462237,1.9653299986009303,1.9664793352258967,1.9676355224143272,1.9687978678405682,1.9699656962767884,1.9711383495905965,1.9723151867242923,1.973495583656455,1.9746789333465804,1.9758646456634803,1.9770521472981561,1.978240881661861,1.9794303087700609,1.980619905113001,1.9818091635135806,1.9829975929732273,1.984184718506461,1.9853700809648203,1.986553236850824,1.9877337581226149,1.9889112319899365,1.9900852607020663,1.9912554613283246,1.992421465531759,1.9935829193365893,1.994739482889984,1.995890830218719,1.9970366489812619,1.9981766402157934,1.9993105180846757,2.000438009615848,2.0015588544416203,2.002672804535317],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.8394434410518032,1.83944344060354,1.8394434401469353,1.8394434396818327,1.839443439208075,1.8394434387255,1.8394434382339448,1.8394434377332418,1.8394434372232205,1.8394434367037082,1.8394434361745273,1.8394434356354985,1.8394434350864386,1.8394434345271609,1.8394434339574746,1.8394434333771865,1.8394434327860993,1.8394434321840125,1.8394434315707207,1.8394434309460157,1.839443430309685,1.8394434296615119,1.8394434290012767,1.8394434283287544,1.8394434276437166,1.8394434269459303,1.8394434262351587,1.8394434255111594,1.8394434247736868,1.83944342402249,1.8394434232573127,1.8394434224778964,1.8394434216839755,1.8394434208752795,1.839443420051534,1.8394434192124582,1.839443418357768,1.8394434174871719,1.8394434166003744,1.8394434156970738,1.8394434147769627,1.839443413839729,1.8394434128850534,1.8394434119126115,1.8394434109220725,1.8394434099131003,1.8394434088853513,1.8394434078384758,1.8394434067721188,1.839443405685917,1.8394434045795014,1.839443403452496,1.8394434023045172,1.8394434011351746,1.8394433999440714,1.8394433987308019,1.8394433974949542,1.8394433962361076,1.8394433949538345,1.8394433936476988,1.8394433923172564,1.8394433909620553,1.8394433895816344,1.8394433881755243,1.8394433867432474,1.839443385284316,1.8394433837982356,1.839443382284499,1.8394433807425932,1.839443379171993,1.839443377572165,1.8394433759425648,1.8394433742826388,1.839443372591823,1.8394433708695415,1.8394433691152097,1.8394433673282309,1.8394433655079974,1.8394433636538907,1.8394433617652803,1.8394433598415243,1.8394433578819682,1.8394433558859462,1.8394433538527801,1.839443351781778,1.8394433496722362,1.8394433475234375,1.8394433453346513,1.839443343105134,1.8394433408341266,1.8394433385208586,1.8394433361645421,1.8394433337643767,1.8394433313195466,1.8394433288292202,1.8394433262925516,1.8394433237086782,1.8394433210767211,1.8394433183957863,1.8394433156649619,1.8394433128833196,1.8394433100499146,1.8394433071637826,1.8394433042239433,1.8394433012293967,1.8394432981791253,1.839443295072092,1.839443291907241,1.8394432886834957,1.839443285399761,1.8394432820549202,1.8394432786478365,1.8394432751773515,1.839443271642286,1.8394432680414379,1.839443264373583,1.8394432606374749,1.839443256831844,1.8394432529553957,1.839443249006813,1.8394432449847538,1.8394432408878507,1.8394432367147113,1.8394432324639167,1.8394432281340227,1.8394432237235567,1.8394432192310204,1.8394432146548862,1.839443209993599,1.8394432052455743,1.839443200409198,1.8394431954828272,1.8394431904647863,1.8394431853533704,1.8394431801468423,1.839443174843432,1.8394431694413371,1.8394431639387216,1.8394431583337154,1.8394431526244133,1.8394431468088748,1.8394431408851235,1.8394431348511466,1.8394431287048925,1.8394431224442729,1.8394431160671598,1.8394431095713861,1.8394431029547442,1.8394430962149853,1.8394430893498193,1.8394430823569126,1.8394430752338888,1.8394430679783278,1.8394430605877632,1.8394430530596841,1.8394430453915316,1.8394430375807005,1.8394430296245363,1.8394430215203352,1.8394430132653437,1.8394430048567558,1.8394429962917156,1.8394429875673113,1.8394429786805793,1.8394429696284995,1.8394429604079963,1.8394429510159367,1.8394429414491293,1.839442931704324,1.8394429217782091,1.8394429116674125,1.8394429013684985,1.8394428908779683,1.8394428801922578,1.8394428693077363,1.839442858220706,1.8394428469274002,1.839442835423982,1.8394428237065432,1.8394428117711035,1.8394427996136082,1.839442787229927,1.8394427746158533,1.839442761767102,1.8394427486793081,1.839442735348026,1.839442721768728,1.8394427079368003,1.8394426938475452,1.839442679496177,1.8394426648778213,1.8394426499875127,1.8394426348201938,1.8394426193707134,1.839442603633824,1.8394425876041811,1.839442571276341,1.839442554644758,1.8394425377037844,1.839442520447667,1.839442502870546,1.8394424849664521,1.8394424667293063,1.839442448152915,1.8394424292309715,1.8394424099570497,1.8394423903246067,1.8394423703269753,1.8394423499573669,1.8394423292088657,1.8394423080744273,1.8394422865468765,1.8394422646189055,1.8394422422830696,1.8394422195317872,1.8394421963573355,1.8394421727518475,1.839442148707311,1.8394421242155647,1.8394420992682958,1.8394420738570374,1.839442047973165,1.8394420216078944,1.8394419947522775,1.8394419673972018,1.839441939533384,1.8394419111513691,1.839441882241527,1.839441852794048,1.8394418227989409,1.8394417922460296,1.839441761124948,1.8394417294251393,1.8394416971358485,1.8394416642461233,1.8394416307448072,1.8394415966205366,1.839441561861738,1.8394415264566217,1.839441490393181,1.8394414536591857,1.8394414162421784,1.839441378129472,1.8394413393081426,1.8394412997650276,1.8394412594867207,1.8394412184595657,1.839441176669655,1.8394411341028216,1.8394410907446375,1.8394410465804059,1.8394410015951583,1.8394409557736486,1.8394409091003492,1.839440861559443,1.8394408131348219,1.8394407638100776,1.8394407135684994,1.8394406623930657,1.8394406102664407,1.8394405571709678,1.8394405030886625,1.8394404480012079,1.8394403918899482,1.8394403347358819,1.8394402765196558,1.8394402172215591,1.8394401568215155,1.8394400952990786,1.8394400326334215,1.8394399688033338,1.839439903787213,1.8394398375630554,1.8394397701084524,1.8394397014005794,1.839439631416191,1.8394395601316116,1.8394394875227278,1.8394394135649814,1.8394393382333596,1.8394392615023876,1.83943918334612,1.8394391037381335,1.8394390226515147,1.8394389400588544,1.8394388559322374,1.8394387702432333,1.839438682962886,1.8394385940617064,1.8394385035096603,1.8394384112761597,1.839438317330053,1.8394382216396132,1.8394381241725286,1.8394380248958921,1.8394379237761902,1.8394378207792912,1.8394377158704343,1.8394376090142197,1.8394375001745942,1.8394373893148415,1.8394372763975682,1.8394371613846943,1.8394370442374375,1.8394369249163025,1.8394368033810677,1.8394366795907713,1.8394365535036983,1.8394364250773672,1.8394362942685156,1.839436161033085,1.8394360253262096,1.839435887102198,1.8394357463145208,1.8394356029157934,1.8394354568577622,1.8394353080912884,1.8394351565663312,1.839435002231933,1.8394348450362017,1.839434684926293,1.8394345218483965,1.8394343557477149,1.8394341865684467,1.8394340142537708,1.8394338387458247,1.8394336599856882,1.8394334779133625,1.8394332924677537,1.8394331035866494,1.8394329112067016,1.8394327152634047,1.8394325156910762,1.8394323124228342,1.8394321053905767,1.8394318945249595,1.8394316797553751,1.839431461009928,1.8394312382154143,1.8394310112972958,1.8394307801796788,1.8394305447852886,1.839430305035444,1.8394300608500354,1.8394298121474961,1.8394295588447789,1.8394293008573284,1.8394290380990554,1.839428770482309,1.83942849791785,1.8394282203148216,1.8394279375807219,1.8394276496213753,1.8394273563409016,1.8394270576416878,1.839426753424357,1.8394264435877363,1.839426128028828,1.8394258066427762,1.8394254793228335,1.83942514596033,1.8394248064446383,1.839424460663141,1.8394241085011946,1.8394237498420958,1.8394233845670442,1.8394230125551079,1.8394226336831851,1.8394222478259696,1.8394218548559076,1.8394214546431649,1.8394210470555843,1.839420631958647,1.839420209215432,1.839419778686575,1.8394193402302275,1.8394188937020135,1.8394184389549875,1.839417975839591,1.8394175042036072,1.8394170238921177,1.839416534747457,1.8394160366091636,1.839415529313937,1.839415012695588,1.8394144865849906,1.8394139508100336,1.8394134051955706,1.8394128495633701,1.8394122837320643,1.839411707517097,1.8394111207306716,1.8394105231816975,1.8394099146757368,1.8394092950149488,1.8394086639980354,1.8394080214201838,1.8394073670730111,1.839406700744506,1.8394060222189694,1.839405331276957,1.8394046276952174,1.839403911246633,1.8394031817001588,1.8394024388207575,1.8394016823693407,1.8394009121027,1.8394001277734469,1.8393993291299455,1.8393985159162465,1.8393976878720206,1.8393968447324907,1.839395986228364,1.8393951120857615,1.8393942220261499,1.8393933157662699,1.8393923930180645,1.839391453488608,1.8393904968800316,1.8393895228894515,1.8393885312088925,1.8393875215252147,1.8393864935200372,1.839385446869661,1.8393843812449928,1.8393832963114665,1.839382191728964,1.8393810671517388,1.8393799222283334,1.8393787566014994,1.8393775699081178,1.8393763617791166,1.8393751318393885,1.8393738797077093,1.8393726049966523,1.8393713073125086,1.8393699862551989,1.8393686414181911,1.839367272388416,1.83936587874618,1.8393644600650805,1.8393630159119199,1.83936154584662,1.839360049422133,1.8393585261843575,1.839356975672051,1.839355397416742,1.8393537909426432,1.8393521557665655,1.8393504913978291,1.8393487973381786,1.8393470730816948,1.839345318114707,1.8393435319157103,1.8393417139552752,1.839339863695965,1.8393379805922485,1.8393360640904166,1.8393341136284973,1.8393321286361721,1.839330108534695,1.8393280527368072,1.8393259606466594,1.8393238316597293,1.8393216651627449,1.8393194605336038,1.8393172171412997,1.8393149343458448,1.8393126114981981,1.839310247940193,1.839307843004465,1.8393053960143857,1.8393029062839954,1.8393003731179383,1.8392977958114005,1.8392951736500502,1.8392925059099803,1.839289791857653,1.8392870307498486,1.8392842218336165,1.8392813643462278,1.8392784575151337,1.8392755005579264,1.839272492682302,1.8392694330860295,1.839266320956923,1.8392631554728152,1.839259935801541,1.8392566611009202,1.839253330518748,1.8392499431927885,1.8392464982507746,1.8392429948104125,1.8392394319793939,1.8392358088554088,1.839232124526171,1.8392283780694436,1.8392245685530766,1.839220695035046,1.8392167565635043,1.839212752176835,1.8392086809037167,1.8392045417631935,1.8392003337647542,1.8391960559084197,1.8391917071848394,1.8391872865753955,1.8391827930523164,1.839178225578801,1.8391735831091511,1.8391688645889153,1.839164068955041,1.8391591951360404,1.8391542420521638,1.8391492086155863,1.8391440937306067,1.8391388962938557,1.8391336151945188,1.8391282493145702,1.839122797529021,1.8391172587061781,1.839111631707919,1.8391059153899811,1.8391001086022603,1.8390942101891299,1.8390882189897708,1.8390821338385184,1.8390759535652232,1.8390696769956283,1.8390633029517645,1.8390568302523576,1.8390502577132573,1.8390435841478772,1.8390368083676603,1.8390299291825525,1.8390229454015017,1.83901585583297,1.839008659285467,1.839001354568102,1.8389939404911513,1.8389864158666493,1.838978779508997,1.838971030235588,1.8389631668674584,1.8389551882299535,1.8389470931534153,1.8389388804738915,1.8389305490338619,1.8389220976829903,1.838913525278892,1.8389048306879254,1.8388960127860028,1.8388870704594233,1.8388780026057245,1.8388688081345588,1.8388594859685872,1.838850035044395,1.8388404543134307,1.8388307427429615,1.838820899317054,1.8388109230375718,1.8388008129251954,1.8387905680204644,1.8387801873848346,1.8387696701017608,1.8387590152777948,1.8387482220437061,1.8387372895556207,1.838726216996176,1.8387150035757005,1.8387036485334052,1.8386921511385965,1.838680510691906,1.8386687265265356,1.838656798009522,1.838644724543015,1.8386325055655712,1.8386201405534657,1.8386076290220141,1.8385949705269131,1.8385821646655907,1.83856921107857,1.8385561094508494,1.838542859513289,1.838529461044011,1.8385159138698106,1.8385022178675774,1.8384883729657227,1.8384743791456215,1.8384602364430584,1.838445944949681,1.8384315048144633,1.8384169162451724,1.8384021795098449,1.838387294938264,1.8383722629234465,1.83835708392313,1.8383417584612687,1.8383262871295276,1.8383106705887848,1.8382949095706356,1.8382790048788962,1.8382629573911156,1.8382467680600845,1.8382304379153516,1.838213968064737,1.8381973596958523,1.8381806140776222,1.8381637325618059,1.8381467165845267,1.8381295676677993,1.8381122874210656,1.8380948775427315,1.8380773398217112,1.8380596761389736,1.8380418884691008,1.8380239788818467,1.8380059495437113,1.8379878027195182,1.8379695407740098,1.8379511661734458,1.8379326814872234,1.837914089389509,1.8378953926608868,1.8378765941900257,1.837857696975369,1.8378387041268442,1.8378196188675977,1.8378004445357592,1.8377811845862313,1.8377618425925135,1.8377424222485608,1.8377229273706757,1.837703361899445,1.8376837299017135,1.8376640355726088,1.8376442832376088,1.8376244773546635,1.8376046225163727,1.837584723452217,1.8375647850308534,1.837544812262474,1.837524810301231,1.8375047844477324,1.8374847401516148,1.837464683014189,1.8374446187911697,1.8374245533954874,1.8374044929001887,1.8373844435414262,1.8373644117215429,1.8373444040122533,1.8373244271579243,1.8373044880789589,1.837284593875286,1.8372647518299596,1.837244969412868,1.8372252542845577,1.8372056143001727,1.8371860575135144,1.8371665921812192,1.8371472267670605,1.837127969946375,1.8371088306106125,1.8370898178720159,1.837070941068427,1.8370522097682203,1.8370336337753694,1.8370152231346406,1.8369969881369161,1.836978939324649,1.8369610874974442,1.8369434437177712,1.8369260193168002,1.836908825900367,1.83689187535506,1.8368751798544307,1.8368587518653217,1.8368426041543138,1.8368267497942856,1.8368112021710843,1.8367959749903022,1.8367810822841584,1.836766538418476,1.8367523580997538,1.8367385563823297,1.8367251486756215,1.836712150751451,1.8366995787514366,1.8366874491944511,1.8366757789841384,1.8366645854164818,1.8366538861874173,1.8366436994004804,1.836634043574486,1.836624937651224,1.8366164010031714,1.8366084534412035,1.8366011152223045,1.8365944070572577,1.8365883501183167,1.8365829660468398,1.8365782769608818,1.8365743054627337,1.8365710746463972,1.8365686081049861,1.8365669299380454,1.836566064758774,1.8365660377011441,1.836566874426905,1.8365686011324616,1.836571244555615,1.8365748319821606,1.8365793912523227,1.836584950767028,1.8365915394939976,1.8365991869736513,1.836607923324817,1.8366177792502278,1.8366287860418065,1.8366409755857203,1.8366543803672022,1.8366690334751252,1.836684968606327,1.8367022200696712,1.8367208227898417,1.8367408123108582,1.8367622247993132,1.836785097047315,1.8368094664751395,1.83683537113358,1.8368628497059898,1.8368919415100184,1.8369226864990325,1.836955125263221,1.8369892990303818,1.8370252496663877,1.8370630196753315,1.837102652199349,1.8371441910181217,1.8371876805480585,1.8372331658411605,1.8372806925835694,1.8373303070938065,1.837382056320701,1.8374359878410207,1.8374921498568009,1.8375505911923902,1.8376113612912102,1.837674510212243,1.8377400886262538,1.8378081478117623,1.8378787396507659,1.8379519166242344,1.8380277318073823,1.8381062388647371,1.8381874920450085,1.8382715461757841,1.8383584566580555,1.8384482794605963,1.8385410711142056,1.8386368887058322,1.8387357898725976,1.8388378327957335,1.8389430761944483,1.8390515793197433,1.8391634019481928,1.8392786043757052,1.8393972474112816,1.8395193923707873,1.839645101070756,1.839774435822233,1.839907459424684,1.8400442351599704,1.8401848267864163,1.8403292985329687,1.8404777150934684,1.8406301416210369,1.8407866437225897,1.8409472874534811,1.8411121393122847,1.8412812662357143,1.8414547355936821,1.841632615184497,1.8418149732301954,1.8420018783720011,1.842193399665906,1.842389606578352,1.842590568982016,1.8427963571516623,1.8430070417600561,1.8432226938739062,1.8434433849498173,1.8436691868302142,1.8439001717392123,1.8441364122783925,1.8443779814224441,1.8446249525146285,1.844877399262021,1.8451353957304781,1.8453990163392764,1.8456683358553694,1.8459434293871952,1.8462243723779832,1.8465112405984787,1.8468041101390318,1.8471030574009655,1.8474081590871587,1.8477194921917595,1.8480371339889572,1.848361162020726,1.848691654083463,1.8490286882134332,1.8493723426709416,1.8497226959231436,1.8500798266254088,1.8504438136011556,1.85081473582007,1.8511926723746228,1.85157770245481,1.8519699053210248,1.8523693602749927,1.852776146628687,1.8531903436711576,1.8536120306331956,1.854041286649778,1.854478190720224,1.8549228216660087,1.855375258086189,1.8558355783103846,1.8563038603492883,1.8567801818426657,1.8572646200048208,1.8577572515675076,1.8582581527202837,1.8587673990482954,1.859285065467506,1.859811226157381,1.8603459544910506,1.8608893229629884,1.86144140311424,1.8620022654552602,1.8625719793864142,1.863150613116215,1.863738233577378,1.8643349063407781,1.8649406955274108,1.865555663718467,1.8661798718636322,1.8668133791877435,1.8674562430959343,1.868108519077412,1.868770260608022,1.8694415190517515,1.8701223435613465,1.870812780978213,1.8715128757317805,1.8722226697385196,1.872942202300798,1.8736715100057795,1.8744106266245606,1.8751595830117538,1.8759184070057242,1.8766871233296878,1.8774657534938883,1.8782543156990616,1.879052824741404,1.8798612919192572,1.8806797249417246,1.8815081278394272,1.88234650087761,1.8831948404718084,1.8840531391062703,1.884921385255338,1.8857995633079834,1.8866876534956827,1.8875856318238153,1.8884934700067615,1.8894111354068694,1.890338590977454,1.891275795209977,1.892222702085561,1.8931792610309675,1.89414541687917,1.895121109834641,1.8961062754434592,1.8971008445683373,1.8981047433686606,1.899117893285611,1.900140211032447,1.9011716085899983,1.9022119932074195,1.9032612674082425,1.9043193290017486,1.9053860710996844,1.9064613821383158,1.9075451459058268,1.9086372415750374,1.9097375437414235,1.910845922466398,1.9119622433258143,1.9130863674636323,1.9142181516506904,1.915357448348507,1.9165041057780354,1.917657967993278,1.9188188749596717,1.919986662637136,1.92116116306767,1.9223422044673877,1.9235296113228622,1.9247232044916485,1.925922801306851,1.9271282156855898,1.9283392582412227,1.9295557363991662,1.930777454516165,1.9320042140028417,1.9332358134493686,1.9344720487540894,1.9357127132549212,1.9369575978633615,1.9382064912009247,1.939459179737827,1.9407154479337407,1.9419750783804328,1.9432378519461053,1.944503547921254,1.9457719441658512,1.9470428172576786,1.9483159426416095,1.9495910947796666,1.950868047301655,1.9521465731561989,1.9534264447619785,1.9547074341590036,1.955989313159721,1.957271853499786,1.9585548269883117,1.959838005657425,1.9611211619109408,1.9624040686719952,1.963686499529458,1.9649682288829569,1.9662490320863508,1.967528685589493,1.9688069670781183,1.9700836556117096,1.9713585317591862,1.9726313777322713,1.9739019775163982,1.975170116999015,1.9764355840951593,1.9776981688701711,1.97895766365943,1.9802138631849897,1.9814665646690086,1.9827155679438653,1.98396067555886,1.9852016928834126,1.9864384282066647,1.9876706928334062,1.988898301176253,1.9901210708440076,1.9913388227261342,1.9925513810733042,1.9937585735739514,1.994960231426801,1.9961561894093365,1.9973462859421725,1.9985303631493136,1.9997082669142807,2.000879846932097,2.002044956757133,2.0032034538468046,2.004355199601151,2.0055000593982872,2.006637902625772,2.0077686027079076,2.008892037129008,2.010008087452682,2.0111166393371613,2.0122175825467385,2.013310810959367,2.014396222570473,2.015473719493061,2.016543207954172,2.0176045982877677,2.0186578049241257,2.019702746375821,2.020739345220387,2.021767528079731,2.022787225596419,2.0237983724068935,2.024800907111753,2.025794772243175,2.0267799142295826,2.0277562833576717,2.028723833731891,2.029682523231491],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.741533686351046,1.7415336866274391,1.7415336869089764,1.7415336871957525,1.741533687487866,1.7415336877854157,1.7415336880885026,1.7415336883972303,1.7415336887117034,1.7415336890320288,1.7415336893583158,1.7415336896906748,1.741533690029219,1.7415336903740637,1.7415336907253258,1.7415336910831254,1.7415336914475834,1.7415336918188242,1.7415336921969737,1.7415336925821605,1.7415336929745158,1.7415336933741732,1.741533693781268,1.7415336941959385,1.7415336946183269,1.7415336950485756,1.7415336954868315,1.741533695933243,1.7415336963879626,1.7415336968511446,1.7415336973229463,1.7415336978035285,1.7415336982930543,1.7415336987916905,1.7415336992996062,1.7415336998169744,1.7415337003439708,1.7415337008807747,1.7415337014275685,1.7415337019845387,1.7415337025518738,1.7415337031297669,1.741533703718415,1.7415337043180181,1.7415337049287798,1.7415337055509077,1.7415337061846135,1.7415337068301129,1.741533707487625,1.7415337081573734,1.7415337088395861,1.741533709534495,1.741533710242336,1.74153371096335,1.7415337116977825,1.7415337124458827,1.7415337132079054,1.7415337139841092,1.7415337147747585,1.7415337155801218,1.741533716400473,1.741533717236091,1.7415337180872603,1.7415337189542697,1.7415337198374143,1.7415337207369945,1.741533721653316,1.7415337225866905,1.741533723537435,1.7415337245058733,1.7415337254923342,1.7415337264971533,1.7415337275206726,1.7415337285632393,1.7415337296252085,1.741533730706941,1.7415337318088049,1.7415337329311746,1.741533734074432,1.7415337352389653,1.741533736425171,1.7415337376334519,1.7415337388642194,1.7415337401178912,1.7415337413948948,1.7415337426956632,1.741533744020639,1.741533745370273,1.7415337467450238,1.7415337481453592,1.7415337495717547,1.741533751024696,1.7415337525046766,1.7415337540121998,1.7415337555477783,1.7415337571119343,1.7415337587051993,1.7415337603281154,1.7415337619812343,1.7415337636651176,1.7415337653803387,1.74153376712748,1.741533768907136,1.7415337707199117,1.7415337725664235,1.7415337744472992,1.7415337763631782,1.741533778314712,1.7415337803025643,1.7415337823274109,1.7415337843899399,1.741533786490853,1.7415337886308646,1.7415337908107023,1.7415337930311066,1.7415337952928334,1.7415337975966512,1.7415337999433431,1.741533802333708,1.7415338047685573,1.7415338072487199,1.7415338097750384,1.7415338123483726,1.7415338149695965,1.741533817639602,1.7415338203592967,1.7415338231296054,1.7415338259514697,1.7415338288258495,1.7415338317537221,1.7415338347360827,1.7415338377739455,1.7415338408683434,1.7415338440203287,1.7415338472309727,1.7415338505013676,1.7415338538326246,1.741533857225877,1.7415338606822786,1.741533864203004,1.7415338677892507,1.7415338714422381,1.7415338751632083,1.741533878953426,1.7415338828141804,1.741533886746784,1.741533890752574,1.7415338948329122,1.7415338989891862,1.7415339032228092,1.7415339075352203,1.741533911927886,1.7415339164023,1.7415339209599838,1.7415339256024864,1.7415339303313868,1.7415339351482926,1.7415339400548422,1.7415339450527028,1.7415339501435745,1.7415339553291882,1.7415339606113067,1.7415339659917262,1.7415339714722764,1.7415339770548202,1.7415339827412557,1.7415339885335168,1.741533994433573,1.7415340004434297,1.7415340065651308,1.7415340128007575,1.7415340191524302,1.7415340256223084,1.7415340322125918,1.7415340389255214,1.7415340457633794,1.7415340527284913,1.7415340598232245,1.7415340670499917,1.7415340744112502,1.7415340819095024,1.7415340895472984,1.7415340973272344,1.7415341052519564,1.7415341133241584,1.7415341215465852,1.7415341299220322,1.7415341384533476,1.7415341471434314,1.7415341559952393,1.7415341650117804,1.7415341741961203,1.7415341835513818,1.7415341930807462,1.741534202787453,1.741534212674803,1.7415342227461579,1.7415342330049417,1.7415342434546428,1.7415342540988137,1.7415342649410743,1.7415342759851105,1.7415342872346773,1.7415342986936002,1.7415343103657748,1.74153432225517,1.7415343343658278,1.7415343467018667,1.7415343592674806,1.7415343720669416,1.7415343851046023,1.7415343983848952,1.7415344119123355,1.7415344256915228,1.7415344397271426,1.7415344540239666,1.741534468586856,1.7415344834207618,1.7415344985307282,1.7415345139218927,1.741534529599488,1.7415345455688451,1.741534561835393,1.7415345784046632,1.741534595282289,1.7415346124740085,1.7415346299856678,1.7415346478232203,1.7415346659927313,1.741534684500378,1.741534703352454,1.7415347225553681,1.7415347421156502,1.7415347620399504,1.7415347823350436,1.7415348030078297,1.7415348240653377,1.7415348455147275,1.7415348673632915,1.7415348896184588,1.7415349122877963,1.7415349353790115,1.7415349588999554,1.741534982858625,1.7415350072631672,1.741535032121879,1.741535057443213,1.7415350832357774,1.7415351095083431,1.7415351362698421,1.7415351635293737,1.741535191296206,1.7415352195797802,1.7415352483897126,1.7415352777357984,1.741535307628016,1.7415353380765282,1.741535369091688,1.7415354006840402,1.7415354328643269,1.741535465643489,1.7415354990326712,1.7415355330432267,1.7415355676867188,1.7415356029749258,1.7415356389198464,1.741535675533702,1.7415357128289408,1.7415357508182436,1.7415357895145267,1.7415358289309468,1.7415358690809053,1.7415359099780536,1.7415359516362967,1.741535994069798,1.7415360372929853,1.7415360813205543,1.7415361261674738,1.7415361718489928,1.741536218380642,1.7415362657782425,1.7415363140579094,1.7415363632360576,1.7415364133294073,1.7415364643549907,1.741536516330156,1.7415365692725746,1.741536623200247,1.7415366781315076,1.7415367340850338,1.7415367910798492,1.7415368491353318,1.7415369082712198,1.7415369685076199,1.7415370298650115,1.741537092364256,1.7415371560266026,1.7415372208736963,1.741537286927585,1.741537354210726,1.741537422745995,1.741537492556694,1.7415375636665582,1.7415376360997636,1.7415377098809375,1.7415377850351645,1.7415378615879966,1.7415379395654609,1.7415380189940688,1.7415380999008256,1.7415381823132383,1.7415382662593262,1.7415383517676302,1.7415384388672221,1.7415385275877144,1.7415386179592713,1.7415387100126176,1.7415388037790502,1.7415388992904486,1.741538996579285,1.741539095678636,1.741539196622194,1.7415392994442782,1.7415394041798464,1.7415395108645066,1.7415396195345298,1.7415397302268618,1.7415398429791362,1.7415399578296862,1.7415400748175587,1.741540193982527,1.7415403153651052,1.7415404390065599,1.741540564948927,1.7415406932350241,1.741540823908465,1.7415409570136755,1.7415410925959083,1.7415412307012572,1.7415413713766754,1.7415415146699877,1.7415416606299103,1.7415418093060655,1.7415419607489984,1.741542115010195,1.7415422721420997,1.7415424321981319,1.7415425952327057,1.7415427613012475,1.7415429304602146,1.741543102767115,1.7415432782805271,1.741543457060119,1.7415436391666685,1.7415438246620851,1.7415440136094298,1.7415442060729371,1.7415444021180364,1.7415446018113745,1.7415448052208387,1.7415450124155787,1.7415452234660311,1.7415454384439437,1.741545657422398,1.7415458804758364,1.7415461076800853,1.7415463391123827,1.7415465748514032,1.7415468149772853,1.741547059571658,1.7415473087176705,1.741547562500017,1.7415478210049693,1.7415480843204028,1.7415483525358284,1.7415486257424222,1.7415489040330572,1.7415491875023335,1.7415494762466108,1.7415497703640428,1.741550069954608,1.7415503751201455,1.7415506859643892,1.741551002593002,1.7415513251136132,1.7415516536358544,1.7415519882713961,1.741552329133987,1.7415526763394908,1.7415530300059274,1.741553390253512,1.7415537572046966,1.74155413098421,1.7415545117191016,1.741554899538785,1.7415552945750798,1.741555696962259,1.741556106837092,1.7415565243388933,1.741556949609568,1.7415573827936615,1.7415578240384073,1.7415582734937776,1.7415587313125345,1.7415591976502809,1.7415596726655145,1.7415601565196805,1.7415606493772273,1.7415611514056613,1.7415616627756054,1.7415621836608546,1.7415627142384371,1.7415632546886732,1.7415638051952373,1.7415643659452185,1.741564937129187,1.741565518941255,1.7415661115791468,1.7415667152442622,1.7415673301417467,1.7415679564805608,1.7415685944735506,1.7415692443375204,1.741569906293306,1.7415705805658495,1.7415712673842754,1.7415719669819687,1.7415726795966544,1.7415734054704772,1.7415741448500839,1.7415748979867074,1.7415756651362513,1.7415764465593773,1.7415772425215927,1.7415780532933411,1.7415788791500943,1.7415797203724448,1.7415805772462005,1.7415814500624833,1.7415823391178265,1.7415832447142754,1.7415841671594892,1.7415851067668464,1.7415860638555503,1.7415870387507357,1.741588031783582,1.7415890432914225,1.7415900736178602,1.7415911231128836,1.7415921921329849,1.7415932810412815,1.741594390207638,1.7415955200087916,1.7415966708284796,1.7415978430575698,1.7415990370941914,1.741600253343871,1.7416014922196685,1.7416027541423182,1.7416040395403694,1.7416053488503322,1.741606682516826,1.741608040992727,1.7416094247393241,1.7416108342264727,1.7416122699327543,1.7416137323456387,1.7416152219616465,1.7416167392865196,1.7416182848353898,1.7416198591329526,1.7416214627136457,1.741623096121828,1.7416247599119639,1.7416264546488098,1.741628180907605,1.7416299392742653,1.7416317303455795,1.741633554729413,1.7416354130449094,1.7416373059227015,1.7416392340051225,1.741641197946422,1.7416431984129868,1.7416452360835646,1.7416473116494928,1.7416494258149298,1.7416515792970937,1.7416537728265007,1.741656007147213,1.7416582830170868,1.7416606012080276,1.7416629625062494,1.741665367712538,1.7416678176425193,1.741670313126934,1.7416728550119143,1.7416754441592686,1.7416780814467696,1.7416807677684487,1.7416835040348935,1.741686291173555,1.7416891301290545,1.7416920218635021,1.7416949673568167,1.741697967607053,1.7417010236307349,1.7417041364631949,1.741707307158918,1.7417105367918946,1.7417138264559775,1.7417171772652449,1.7417205903543733,1.7417240668790124,1.74172760801617,1.7417312149646036,1.7417348889452164,1.7417386312014642,1.7417424429997654,1.7417463256299228,1.7417502804055476,1.741754308664496,1.7417584117693108,1.7417625911076713,1.74176684809285,1.7417711841641803,1.7417756007875298,1.7417800994557824,1.7417846816893305,1.7417893490365726,1.7417941030744228,1.7417989454088294,1.7418038776752975,1.741808901539427,1.7418140186974573,1.7418192308768192,1.7418245398367016,1.7418299473686218,1.7418354552970112,1.7418410654798075,1.741846779809058,1.7418526002115344,1.7418585286493555,1.7418645671206248,1.7418707176600732,1.741876982339718,1.74188336326953,1.7418898625981123,1.7418964825133918,1.7419032252433202,1.741910093056589,1.7419170882633546,1.7419242132159767,1.7419314703097692,1.7419388619837612,1.7419463907214736,1.7419540590517066,1.7419618695493415,1.7419698248361535,1.7419779275816398,1.7419861805038601,1.7419945863702917,1.7420031479986968,1.7420118682580052,1.7420207500692104,1.742029796406281,1.7420390102970835,1.7420483948243253,1.7420579531265068,1.7420676883988913,1.742077603894491,1.7420877029250657,1.7420979888621384,1.7421084651380283,1.7421191352468948,1.7421300027458035,1.742141071255804,1.7421523444630254,1.742163826119789,1.7421755200457365,1.7421874301289764,1.7421995603272455,1.7422119146690898,1.7422244972550602,1.742237312258929,1.7422503639289195,1.7422636565889575,1.7422771946399391,1.7422909825610158,1.7423050249108987,1.7423193263291816,1.74233389153768,1.7423487253417933,1.74236383263188,1.7423792183846565,1.742394887664613,1.7424108456254468,1.7424270975115195,1.7424436486593284,1.7424605044989991,1.7424776705558003,1.7424951524516736,1.7425129559067865,1.7425310867411044,1.7425495508759803,1.7425683543357682,1.7425875032494536,1.742607003852305,1.7426268624875452,1.7426470856080434,1.7426676797780283,1.7426886516748183,1.742710008090575,1.7427317559340771,1.7427539022325114,1.7427764541332873,1.742799418905871,1.7428228039436393,1.7428466167657526,1.7428708650190514,1.74289555647997,1.7429206990564703,1.7429463007899992,1.742972369857462,1.742998914573216,1.7430259433910886,1.7430534649064078,1.7430814878580594,1.743110021130558,1.7431390737561405,1.743168654916879,1.7431987739468098,1.7432294403340844,1.7432606637231365,1.7432924539168697,1.7433248208788603,1.7433577747355815,1.7433913257786422,1.743425484467044,1.7434602614294565,1.7434956674665067,1.7435317135530863,1.7435684108406748,1.7436057706596775,1.7436438045217786,1.7436825241223097,1.74372194134263,1.7437620682525237,1.7438029171126073,1.7438445003767504,1.74388683069451,1.7439299209135728,1.7439737840822103,1.7440184334517435,1.7440638824790173,1.7441101448288814,1.7441572343766798,1.7442051652107484,1.744253951634916,1.7443036081710124,1.7443541495613784,1.7444055907713802,1.744457946991926,1.7445112336419808,1.7445654663710843,1.7446206610618644,1.744676833832549,1.7447340010394725,1.744792179279579,1.7448513853929155,1.7449116364651178,1.7449729498298874,1.7450353430714545,1.7450988340270277,1.745163440789231,1.7452291817085197,1.7452960753955813,1.7453641407237115,1.74543339683117,1.7455038631235094,1.7455755592758777,1.74564850523529,1.745722721222868,1.7457982277360469,1.7458750455507415,1.7459531957234757,1.7460326995934676,1.7461135787846693,1.7461958552077597,1.7462795510620832,1.7463646888375384,1.746451291316403,1.7465393815751038,1.74662898298592,1.7467201192186157,1.7468128142420072,1.74690709232545,1.7470029780402503,1.7471004962609926,1.7471996721667808,1.7473005312423902,1.747403099279323,1.7475074023767674,1.747613466942452,1.747721319693394,1.747830987656537,1.7479424981692697,1.748055878879829,1.7481711577475725,1.7482883630431283,1.748407523348404,1.748528667556464,1.7486518248712566,1.7487770248072,1.748904297188612,1.7490336721489823,1.7491651801300863,1.7492988518809318,1.7494347184565324,1.7495728112165119,1.7497131618235222,1.7498558022414856,1.750000764733644,1.7501480818604187,1.7502977864770766,1.7504499117311962,1.7506044910599314,1.7507615581870704,1.750921147119888,1.7510832921457853,1.7512480278287166,1.7514153890054012,1.7515854107813196,1.7517581285264883,1.7519335778710186,1.752111794700453,1.752292815150883,1.752476675603845,1.752663412681,1.7528530632385921,1.7530456643616936,1.7532412533582344,1.7534398677528207,1.753641545280347,1.7538463238794024,1.7540542416854814,1.7542653370239967,1.754479648403107,1.7546972145063635,1.7549180741851813,1.7551422664511442,1.7553698304681542,1.7556008055444294,1.755835231124366,1.7560731467802715,1.7563145922039805,1.7565596071983656,1.756808231668757,1.7570605056142812,1.7573164691191339,1.7575761623438013,1.7578396255162458,1.7581068989230662,1.7583780229006536,1.7586530378263565,1.7589319841096696,1.759214902183465,1.759501832495284,1.7597928154986964,1.7600878916447602,1.760387101373582,1.760690485106005,1.7609980832354366,1.7613099361198306,1.7616260840738427,1.7619465673611705,1.7622714261870942,1.76260070069123,1.7629344309405086,1.7632726569223935,1.7636154185383424,1.7639627555975266,1.764314707810815,1.7646713147850257,1.7650326160174528,1.7653986508906698,1.7657694586676107,1.7661450784869264,1.766525549358616,1.7669109101599247,1.7673011996315027,1.7676964563738131,1.7680967188437788,1.768502025351651,1.7689124140580839,1.7693279229713923,1.7697485899449747,1.7701744526748682,1.7706055486974157,1.7710419153870076,1.7714835899538675,1.7719306094418446,1.772383010726174,1.772840830511162,1.7733041053277527,1.7737728715309313,1.774247165296911,1.7747270226200538,1.7752124793094755,1.7757035709852715,1.7762003330743164,1.776702800805566,1.7772110092048192,1.777724993088857,1.7782447870589173,1.7787704254934267,1.7793019425399366,1.7798393721061945,1.7803827478502885,1.7809321031698049,1.7814874711899316,1.7820488847504528,1.78261637639157,1.7831899783384932,1.783769722484749,1.7843556403741465,1.7849477631813568,1.7855461216910526,1.7861507462755668,1.786761666871027,1.7873789129519333,1.788002513504137,1.788632496996207,1.7892688913491446,1.7899117239044404,1.7905610213904548,1.7912168098871135,1.7918791147889235,1.7925479607663022,1.7932233717252435,1.7939053707653247,1.7945939801360875,1.7952892211918157,1.7959911143447482,1.796699679016771,1.7974149335896341,1.7981368953537507,1.7988655804556422,1.799601003844096,1.800343179215108,1.8010921189556985,1.801847834086682,1.8026103342044855,1.8033796274221159,1.8041557203093792,1.8049386178324611,1.8057283232929806,1.8065248382666428,1.8073281625416024,1.8081382940566777,1.808955228839532,1.8097789609449695,1.8106094823934697,1.8114467831101084,1.8122908508639992,1.8131416712084032,1.8139992274216432,1.8148635004489735,1.8157344688455441,1.8166121087206046,1.8174963936830903,1.818387294788734,1.8192847804888408,1.8201888165808684,1.8210993661609414,1.8220163895784431,1.822939844392806,1.8238696853326286,1.824805864257246,1.825748330120867,1.8266970289393916,1.827651903760021,1.8286128946337594,1.829579938590908,1.8305529696196396,1.8315319186477463,1.8325167135276335,1.8335072790246352,1.8345035368087228,1.8355054054496605,1.836512800415664,1.8375256340756096,1.8385438157048348,1.839567251494557,1.8405958445649415,1.841629494981839,1.8426680997771947,1.8437115529731452,1.8447597456097902,1.8458125657766375,1.8468698986477012,1.8479316265202277,1.8489976288570251,1.8500677823323564,1.8511419608813524,1.8522200357528986,1.8533018755659385,1.8543873463691307,1.8554763117037996,1.8565686326700954,1.8576641679962966,1.8587627741111654,1.8598643052192678,1.8609686133791674,1.8620755485843916,1.8631849588470724,1.864296690284148,1.865410587206022,1.866526492207561,1.8676442462613119,1.8687636888128196,1.8698846578779218,1.871006990141887,1.8721305210602734,1.8732550849613683,1.87438051515008,1.8755066440131414,1.876633303125484,1.877760323357651,1.878887534984097,1.8800147677922419,1.8811418511921283,1.8822686143265432,1.8833948861814578,1.8845204956966422,1.8856452718763106,1.8867690438996554,1.8878916412311237,1.8890128937302988,1.8901326317612412,1.8912506863011573,1.8923668890482475,1.8934810725286098,1.8945930702020561,1.8957027165667135,1.896809847262285,1.897914299171837,1.899015910521997,1.9001145209814387,1.9012099717575386,1.9023021056910938,1.9033907673489887,1.9044758031147087,1.9055570612766004,1.906634392113781,1.9077076479796053,1.9087766833826059,1.9098413550648186,1.9109015220774257,1.9119570458536335,1.913007790278727,1.914053621757234,1.915094409277144,1.9161300244711321,1.9171603416747405,1.9181852379814786,1.919204593294806,1.9202182903769722,1.9212262148946841,1.922228255461589,1.9232243036775571,1.9242142541647556,1.9251980046005164,1.9261754557469952,1.927146511477636,1.9281110788004516,1.9290690678781381,1.93002039204505,1.9309649678210616,1.9319027149223482,1.9328335562691252,1.9337574179903865,1.9346742294256853,1.9355839231240135,1.9364864348398256,1.9373817035262684,1.9382696713256788,1.9391502835574075,1.9400234887030399,1.9408892383890848,1.9417474873671967,1.9425981934920136,1.9434413176966807,1.9442768239661443,1.94510467930829,1.9459248537230134,1.9467373201693003,1.9475420545304072,1.94833903557722,1.9491282449298855],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Ridge en fonction de alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Ridge pour tout les paramètres de GridSearchCV\n","FigRMSEGRidRidge_log = visuRMSEGrid(Ridge(), 'Ridge', alphasridge_log, 'alpha',\n","                                    GridRidge_log)\n","FigRMSEGRidRidge_log.show()\n","if write_data is True:\n","    FigRMSEGRidRidge_log.write_image(\n","        './Figures/EmissionsGraphRMSERidge_log.pdf')"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.3 Modèle Lasso"]},{"cell_type":"code","execution_count":117,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre  Lasso()\n","0  lasso__alpha     0.55\n","               R²     RMSE       MAE\n","Lasso()  0.134489  1.88922  1.566199\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_pred_logLasso=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[5.014361677760583,4.995607691347489,5.039616267148434,5.004741252747013,4.995876440062443,4.983420521360662,4.996238666591294,5.244357302609943,5.0025484190293446,4.989130457826207,5.083137979335332,5.014470735210129,7.698861827702037,4.981017362561726,5.0216626845168335,5.011912870392537,5.008559285869126,5.613449522147948,5.0841467607436375,8.980653941081517,5.4449380156604645,5.040562906724224,5.021841850326803,5.012803714195632,5.987309924072295,5.03252169056454,4.992004900603541,5.0980243211984355,5.043008732811114,5.047390505337538,5.107305889136629,5.0151484493608836,4.985005749288,5.417679495637195,4.989155774734137,5.01375017706134,5.002269933042109,4.983591897352807,5.01096531718899,5.002599052845206,5.006439433032809,5.127898272556365,5.568500105996704,5.1736439777322305,5.181182573932135,5.377961162003233,5.025676388151037,5.0435867106683245,5.208596890311899,5.119757912929498,5.060677987092111,5.015460042073873,5.1164990279699385,5.000028412963037,5.073060134962193,5.053839302407643,5.0448218129098255,5.985259254529929,5.402602303237385,4.992004900603541,4.984633785486867,5.113565007758819,5.086950123893941,4.981138104738009,5.048179224392294,4.9917712060687975,5.072615882908546,4.9967956385657635,5.152352747895084,5.106848237339425,5.032451582204117,5.003738313702075,5.026887704822786,4.991927002425293,5.009510568710218,5.106236736640182,4.982569483763307,4.990244401775146,4.9871401593719815,5.302791367448818,4.988499482582402,5.0760195046046785,5.0214153578008975,5.000017187739247,4.986688349938146,5.060711093817867,5.155345695661884,5.0190667277267345,4.984370879135282,4.986263804866697,5.039648065553187,4.988265788047658,4.988647489121072,4.990213242503847,5.352878367959292,5.2003890104796895,5.020593532020386,4.995042929555194,5.0321360945822144,5.2442559341898685,4.984483831493741,5.105266685548439,4.984701946392834,5.290434768924303,5.033725217418464,5.010856259739444,5.50210748867633,5.037518858699119,5.011698546123911,5.0509426622656255,5.0419532124958595,5.517494326334674,5.291441602878153,4.971513784815526,4.98836705567938,5.806212286783501,5.0268137015534515,4.994273685045,5.261669042913063,5.297383576156345,5.053116021438732,5.005613712343385,5.033506670820909,5.002787955927456,5.140426247073025,4.995818016428757,5.9152541091933255,4.9841761336896635,5.066160071386282,5.002625201635753,5.028301556757979,5.000702232204878,5.065846531218835,5.379182215947264,5.682788043863484,5.27256513806284,5.112821080156555,5.1094052450404,4.992378811859129,4.991706940071744,5.0846063599952975,4.986263804866697,5.6237396516464875,5.006336217946631,5.028141865492572,5.012682972019348,5.021397830710792,4.981192633462783,5.462412524495843,5.008559285869126,4.999241641362737,5.337841652751297,5.800492565544601,5.032200360579268,4.982289050321617,5.048403181654756,5.226562157670237,4.983597739716175,5.109948584833677,5.045193776710957,4.98476037002652,5.244550794482049,5.029162331627615,5.012616758567838,5.013060778183849,5.742020634534797,5.204273541419161,5.008941912009011,5.222531858619395,5.305954033485668,5.052946218384535,5.034317243573145,5.9486802174793425,5.005663774969835,5.000359480220588,5.362631694854057,5.021585740889485,5.070775338070536,5.0425140793792425,5.026685169559343,5.108337782991774,5.182701588407962,5.039143914469147,5.055414017697034,4.9964503674923355,5.185428024646626,5.044013619310507,4.988076884965409,5.059548324016573,4.9913369237250675,4.999958304602614,4.986688349938146,5.009520305982499,4.983323148637853,4.997613569437363,5.045410272911427,5.027450935941904,5.225392842221609,5.01313672890764,5.182960251120358,5.01753602852417,5.224073310875228,5.055178375707835,5.011713139700166,4.994108151416224,5.0784251395513555,5.015023812275687,5.13143679730326,5.041037908901451,5.007520270255993,5.042197374934438,5.0078844442393,5.384052799542191,4.99247034221857,5.070954704257417,5.0433066933429105,5.000318583677009,5.139781924898607,5.432822217039659,5.21264934478353,5.019064780272278,5.087352270778523,5.005718874884019,5.046338879931196,5.220078567000356,5.110867783336998,5.060944788352609,4.99644509676365,4.990139239234512,4.998435395217874,5.043400171156808,5.15654742204984,4.988563748579455,5.003709101885232,5.251832974108316,5.022398822301272,5.031771920598907,4.986596819578706,4.981235477460819,5.124591494889757,5.122679094613781,5.05000009430883,5.143193579855269,4.981371799272752,4.986894780110502,5.236468858488867,5.013876761600992,5.096169653838605,5.489513148858889,5.001138462003064,5.046120765032103,4.982653224304924,5.025711442331249,5.085188648877698,5.03252169056454,5.021855482507996,5.373583578599866,5.049813138681036,5.085205653103741,4.998688951588219,5.016663568927798,5.02331996825905,4.987673761892977,5.064833854901617,5.012903000395064,5.305447695327059,5.020817489282847,5.2234773898116345,4.982731122483171,5.115623881763451,5.114410202992804,5.07274636235711,5.0053254890838685,5.016988572140621,6.787688388657786,5.095982817175083,5.008580340759654,5.16138669938498,4.986072954329991,5.383824314607192,5.0126985516549984,5.008912700192168,5.010558370968471,5.041922053224561,4.996622315119163,5.2356684547073735,5.0404283556566645,4.9936096030754396,5.319556140569681,5.013347053988909,5.046101290487541,4.9895588978065675,4.981385431453945,5.00845699584942,5.003208949875779,4.98676819557085,5.647527341571239,5.108970831267117,4.990828638112003,4.991852999155958,4.988561801124999,5.266625208675884,5.130924616781282,5.005716927429563,5.024085317860331,5.087352270778523,5.008965281462485,4.990193767959285,4.99873919811304,5.049635920325523,5.017808672148036,5.009911744328193,5.254126327457066,5.150285449634873,4.986284166125074,4.991297974635944,5.03938092835438,5.0326619072853855,4.993484965990244,4.992004900603541,5.050724547366532,4.988203469505061,4.998256229407906,5.023483554433369,5.141522663931859,4.9863475454083135,5.030486600657823,5.007599092485774,5.253119594089278,5.205141052982103,5.068672287634764,5.02600745540859,5.055898066197648,5.15728202780785,5.005640976705772,4.9924956591265,4.995276624089937,5.038276418482576,5.007683856430313,4.9916446215291455,5.352702359244205,5.048188961664575,5.175678715091889,5.081186629970231,4.982189730144351,5.023785409874079,4.991617357166759,4.996211402228908,5.060763675088183,5.283018862355138,4.988090517146602,5.190364821693064,4.985741887072439,4.985256970912848,5.043984407493665,5.0047042511123445,5.026196358490839,6.959959474521478,4.998174436320745,5.062697497363178,5.56045517163819,4.9824234246790935,5.31413334220166,4.993796558703234,5.064859171809547,5.027382358254658,5.257320253351276,5.046420673018356,4.98761144335038,5.004850329928025,4.982750597027733,5.001006035100043,5.023569242429442,4.986513079037089,5.082436895731104,5.008941912009011,5.2689231870012465,5.0242936954871436,5.245695897702288,5.353331924239318,4.9820553557868745,5.093606754885057,4.985149860917757,5.603421859252762,5.000988508009938,5.00214674477218,5.011879818723522,5.071429883144727,5.191913047985733,5.806212286783501,5.587599753636566,5.208460568499966,5.500883067704025,5.008626424387108,5.116399323867329,5.19586443307734,4.983815854615268,5.001138462003064,4.99081066239381,4.990125607053319,5.00140883636195,5.175164076823632,5.035828468231148,5.108320512908303,4.981461382177737,5.233413302447108,5.0357505700529,5.0236880371512695,5.144209131145318,5.0160267513206245,5.006571859935829,4.981782712163008,4.995588216802927,5.017603379184469,5.070084192115501,5.098008701637146,7.403463593776863,5.053478247967583,4.984920061291927,5.0776169459520375,4.982409792497901,4.987356326816619,5.397511657288908,4.992939678742511,4.986380652134068,5.00140883636195,5.000410114036449,5.012903000395064,5.0292324399880375,5.019393900075374,5.019282895171371,5.020983022911624,5.28039478890613,5.002649686661067,4.996678791298392,5.007226204633109,4.991257078092364,4.996678791298392,5.140747577058296,5.00006736205216,5.0421557477593035,5.121194830102268,5.052709003457388,5.002484078576585,4.989593951986779,4.981782712163008,5.002044028325192,5.0078961289660375,4.997843369063193,4.990524835216838,5.106271790820394,4.99677226911229,4.9980030603286005,4.986057374694341,5.0831438216987,5.097331027412032,5.852091615051655,5.099865545190873,5.07120397842781,4.998782042111076,5.132339872560463,5.011773510788308,5.022079078265648,5.373323046456707,5.18720410311067,5.04838565456465,5.025582910337141,5.095219986781524,4.990193767959285,5.06547067250879,5.034147815035457,4.996336039314103,4.999261115907299,5.064201839657764,5.361156577500784,5.040184923849641,4.9994928629875846,5.1249290884910135,5.096629943807804,5.021290181931142,5.028480722567949,6.22964511920336,5.011635241521919,4.98112252510236,4.991697202799463,5.026209990672033,5.032667749648754,5.102219138097065,4.990322299953394,5.040766808392792,5.01863439283746,4.9945891726669025,4.999802508246119,4.987763344797962,5.123925465465741,5.022996690819323,5.657964216259542,5.214298279289727,4.9913369237250675,5.001526872867467,5.130924616781282,4.984764264935432,5.08876417525926,5.147957053455105,5.00423122532177,5.11554167403185,4.993153898732691,5.0284145091164385,4.980929727111198,5.03977020866466,5.0250824145419,5.070952756802961,5.0535834105082165,5.116449187808434,5.105107713256141,5.092456548908193,5.196374666144861,5.050360390758122,5.093038837790593,4.977083504560225,5.011206801541558,5.011635241521919,4.980567500582346,5.1175222352137935,4.9994928629875846,5.096777259628936,5.419796378631072,5.037945351225024,5.005771456154336,5.66489325921466,5.261649444607383,5.0301360588557085,5.172258715060114,4.986513079037089,5.287695785553668,5.242720187293232,4.988265788047658,4.987414750450305,5.08800272056689,5.063080389016366,5.000417903854275,4.9937712417953035,4.982540271946465,4.997994610518146,5.015187398450007,5.071517052135374,5.134488458436107,5.0168446821922235,5.0137384923346024,5.0443368967502344,5.020983022911624,4.981449697451,4.991837419520309,5.081186629970231,5.065521306324651,4.996267878408137,4.988696175482477,5.042965888813078,5.0101945454782735,4.98190929670266,5.007842131617806,5.029162331627615,5.015496268669564,5.072442559461945,5.082436895731104,5.127510729119583,5.011140588090047,5.251107873636035,5.991074353536107,5.0949086038542815,5.313451733141994,5.133427095757484,5.016332501670246,5.0472814478879915,4.983891805339059,5.233910410366521,5.051088721349839,4.999576603529201,5.032159464035688,5.028492335528051,5.009559687328128,5.147957053455105,4.981640547987706,5.051205568617211,5.526896173331717,5.087885873299519,5.029294758530636,5.0220716499526326,5.379182215947264,5.10593098629056,5.0301360588557085,4.989901649790857,7.198024832394502,5.180948879397393,5.0288487914601685,4.9892838549600045,5.049942114590095,5.028685205285848,5.17724092611281,5.024137899130649,5.01654477420597,4.9893368879985625,5.081743601944701,5.133490942717836,5.010081172865881,5.037158579624724,5.483279032302665,5.02998220995367,5.337841652751297,7.523408117193135,4.983732114073653,4.996974804375733,5.005714979975107,5.006700391929938,4.996238666591294,5.155345695661884,5.3103320338726085,5.689942957693708,5.1784055038776104,5.10403320851141,5.0314382314281705,5.052290300749307,5.069153308885443,4.98834173877145,4.982045618514593,5.024805876009121,4.988441058948716,4.9809842558359705,5.037960930860674,4.989467367447127,5.082865335711466,5.430300947967752,5.0263866302162,5.005714979975107,4.986515026491546,5.525286091613884,4.981915139066029,5.005470928766095,5.346840839793344,5.662077240071012,5.243166154363699,4.981223792734082,4.996119871869467,5.000262107497779,4.986552028126213,5.001313732904121,4.992073061509507,5.005792878153354,5.034441880658341,5.251907331694172,5.756117868305892,5.083361936597793,5.053116021438732,5.000967018619107,5.071858323125088,5.0612756834888195,4.994273685045,5.078737813407266,5.229760161446311,5.044013619310507,5.1762340921589605,5.0156275231571055,5.024278106876306,5.040426408202208,5.307661388693503,5.019958481560535,4.981223792734082,4.985406924905974,5.517981593012613,5.065846531218835,4.988811075295391,5.118665390979576,5.050276632841609,5.05549775823865,4.992705984207769,4.987424487722586,5.001352681993245,4.988232681321904,5.017397759257781,5.006886528117145,5.049026367080736,4.988312526954608,5.005962306691043,5.278796781094122,5.144254942533891,5.084002568265096,5.07358961013664,4.9809842558359705,4.988583223124017,5.9737536936027675,4.983907384974709,5.066714408779994,5.028492407294686,5.223471547448266,5.813820943842754,5.00194276069347,5.014788170286488,4.998435395217874,5.087515856952843,5.006680917385376,5.002380937946112,5.046101290487541,5.223942831426664,5.026802016826714,5.041047855401437,5.01650387766239,4.99151998444395,5.080341434736245,5.198068951521745,4.988232681321904,4.982779808844576,5.000340005676026,4.9989728926477826,5.233413302447108,5.287830567052234,5.2689231870012465,5.002544524120432,4.986641611031198,5.007565061708486,5.043330438535938,5.155884592453999,5.011187326996996,4.986770143025306,4.9871148424640515,5.197667539976042,5.052508415648401,5.006692602112113,6.351772130639037,4.99100390901306,5.1779546865521215,5.085188648877698,5.143927770185252,5.0519475487650185,5.031092258993698,4.986824671750079,5.0980243211984355,5.001950550511294,4.985401082542606,5.115919480196351,5.0404634098368755,5.0280250182252,5.050817826032375,4.998419815582225,5.156767337414902,4.993196742730728,5.410097829264487,5.848057843887258,4.991420664266684,5.241744512610682,5.042767030249841,5.134334609534068,5.01960033024773,4.982030038878944,4.987231689731423,4.9947254944788355,5.151715640555549,5.052305880384957,6.550071605669692,5.301767006404863,4.998125749959341,5.055414017697034,5.005043108187722,5.1249290884910135,4.990111974872125,5.007954552599723,4.982943395018896,5.097625093034917,5.003738313702075,5.153603215148955,5.113724699024226,5.270135967833279,4.999031316281468,4.996694370934042,5.220672037589668,5.195426370746648,4.998686616842723,4.993492755808068,5.009298296174493,5.033639855489156,5.017419181256799,5.224571859216012,5.060424818012807,5.037442907975327,4.98048376004073,5.050786865909131,4.996813962106254,6.3252643482997915,5.4301100974310454,5.272459764293685,5.011635241521919,5.657964216259542,5.011237960812857,5.100454744359759,5.137176213675247,5.119757912929498,4.999241641362737,5.045407972363495,5.698958132562307,5.077559796210436,4.984713631119571,5.546825545631245,4.9812218452796255,4.988639699303247,4.995432420446432,5.049649552506716,5.015958590414657,4.987917193700001,4.9863475454083135,5.196668731767745,5.032303575665447,5.005192611421735,5.0170569547279475,5.039243825787018,4.98687530556594,5.005639086658811,4.981021257470638,5.027608262971576,5.020369793406089,5.050354531019857,4.995432420446432,5.00157469180125,5.036290014937264,5.004725673111363,5.052613578189035,5.017574977613294,5.003008365094174,4.99516756664039,5.197241283377864,5.006692602112113,4.994926082287823,5.002612440022335,5.010560246662103,5.146138020873733,5.097239497052592,5.005663774969835,5.083137979335332,5.010700463382949,4.986647453394566,5.2064387969125425,5.00428148664519,5.646594977144314,4.983436100996312,5.013567116342458,5.004332287311213,5.175679112592787,5.016745362014958,4.992900729653387,5.007785124062035,4.99501956010172,5.010815363195864,5.03712828150074,4.998256229407906,5.127196045225875,5.036173167669893,5.0104667688482065,4.985188810006881,5.029487556521798,5.06236448265117,5.402132224726882,5.0454274712457,5.1805959790701825,5.035059710019259,5.0595653325104815,5.0330299761776045,4.985237496368286,5.011331438626754,4.9987703573843385,4.992448920219552,5.130507861527658,5.020539003295613,5.009911744328193,4.9849940645612625,4.9877360804355755,5.028578095290758,4.9849940645612625,4.9826201175791685,5.019113466633683,5.012429802940044,5.038932710634311,5.384052799542191,4.981862557795711,5.046531677922359,5.1677100840042245,5.1718248023000255,4.98889871074592,5.001868757424135,4.9976291490730125,4.994108151416224,5.0519475487650185,5.000340005676026,4.988429374221979,5.2685843299258694,5.356213938090978,5.319535580863126,4.999015736645819,5.026692959377168,4.993407067811996,4.982875234112929,6.1081741008909916,5.005037265824353,5.1398361683728,5.377961162003233,5.005958998328183,5.034338665572164,4.991927002425293,5.5815306024401306,5.090799786338472,5.373183853488723,5.063316787880247,5.342947878335423,5.080341434736245,5.19739707973436,4.987138211917526,4.992004900603541,5.111463704400592,5.067367493149118,5.011713139700166,5.044549169285959,5.016168915495926,5.252769052287164,5.442483373967195,5.409289861839936,5.027214877171426,5.072442559461945,4.986515026491546,4.992600821667134,5.4759901769643875,4.994766391022416,5.009992127166661,5.018118317406571,5.133704628191322,5.024359908938655,5.179556525929366,5.504072993040187,5.005191114726392,5.033518787246108,5.217075610274625,5.000410114036449,4.983451680631961,4.981525648174791,5.020065771862758,5.103742047481805,4.999148163548839,5.004682829113327,4.993296062907993,5.016108544407784,4.994746916477854,5.092561711448827,5.015023812275687,5.090799786338472,5.113777280294544,5.052508415648401,5.29016387696091,5.01426625249223,5.13017484681565,5.140747577058296,4.982382528135513,5.044942555086109,5.163108249124251,4.995070193917581,6.949640285998223,5.5575184103182576,5.011530078981285,5.0570128778055645,4.981268584186574,5.60669702623558,5.041748819026451,4.987097315373946,5.060087908391886,5.015023812275687,5.0404634098368755,5.04400193458377,5.254249117673867,4.984682471848272,5.034418511204867,5.003465670078209,6.087407265808493,5.131865237283621,4.983272514821992,4.986045689967604,4.986396231769718,5.442483373967195,5.037555860333787,5.125022358751335,5.062883063482873,5.577401920315941,5.51091582518167,5.572338538729851,4.972686152398152,5.011912870392537,5.123169041763361,4.983216038642762,5.2419302122677065,5.009833318804101,5.017062797091317,5.020145617495462,5.03883144300259,5.599889176921549,5.110023012725957,5.009158079453647,5.155489154018778,5.02998220995367,5.010501750365425,5.094900604250709,5.040651323042952,4.990929905743725,5.030609290288563,5.030872196640148,5.483279032302665,5.218015419747711,5.167089500968068,5.052329249838432,5.77571315504406,4.9996389220717985,4.991460717150617,6.316772479152419,5.115613729846729,5.050584330645687,5.037960930860674,7.388048096494373,4.999033142634813,5.046718633550153,5.29590437829841,5.083229509694773,4.999231904090456,5.775909796539091,4.985040803468211,5.108521100717291,4.985753571799176,5.004040169142785,4.9975374702532465,5.283388395300682,5.000262107497779,4.993196742730728,5.094209257918763,4.985251128549479,5.574449579360359,5.02051952875105,5.007054828640964,5.528156639482305,5.0706431115444275,5.23825077931628,4.985001854379087,5.690852827115651,5.008710164928725,4.984152764236189,5.040181077040534,5.0284492116575725,5.026802016826714,5.016663568927798,5.001011877463411,4.997767418339402,5.044702765217873,6.482734547908793,4.992384654222497,5.022894420976354,5.0305255497469465,5.085892399461722,5.019643174245766,4.9926689825731,5.167628290917064,4.982493533039516,5.0130568832749365,4.987479016447359,5.123925465465741,4.9886552789388965,5.1159214276508065,5.111465651855048,5.096158659829407,5.012392801305377,4.99501956010172,7.518116236631448,5.680726526647032,5.001586376527987,5.195249037469184,5.0983729155460935,5.123631398044331,4.982672698849486,5.064492010258965,4.982374738317689,5.902376487276573,4.9859483172447945,5.001233887271417,4.987009679923418,5.453269885140042,5.319556140569681,5.004538717483569,6.376121552061328,4.98476037002652,5.070317886650243,5.030844932277762,5.082691469218252,4.989790644886854,5.003705206976321,5.040184923849641,4.992939678742511,5.7746162166279165,5.089276355781237,5.022727622531829,4.995557057531628,5.006112260684169,4.9850797525573345,4.981251057096468,4.9911499680972735,4.983132298101146,5.144062144542729,4.9849940645612625,5.004558192028131,5.163595112738298,4.995818016428757,5.014573950296308,5.1470196311905205,5.016055963137467,4.992647560574083,5.807925130815956,5.184771732494891,5.019941134777563,4.985669831257559,5.0341361303087195,5.014024310091115,5.228864048837451,5.184261499427369,5.242678213299116,5.051324363339038,5.04992962488887,5.009520305982499,5.083669634401871,5.122681042068237,5.0100772779569684,5.061883461400492,5.017966415958988,5.074094000840793,5.012118210227054,5.08150818061611,4.9835548957181395,5.983128987785077,5.0268137015534515,5.032757960919291,4.982366948499864,5.079332653327939,4.9840242322420805,5.014285727036792,5.905749660827039,4.9897263788898005,4.981525648174791,5.073453288324707,5.0831438216987,5.019351056077338,5.015224400084675,4.999041053553749,5.073096904159224,6.588510241773354,4.987985354605968,5.029309992807515,5.190364821693064,5.093662023216574,5.006665337749727,5.161372878141195,4.98064734621505,5.067124061342095,5.01493033446179,4.9996895558876595,5.175213626023918,5.121543728665823,5.202759008273904,5.12770157965629,4.982608432852431,5.094596801355544,6.094382917527988,4.966393927050207,8.380673027094122,5.0497917166820185,5.000301056586903,5.022143705767512,5.002357568492638,5.45542759232957,5.085934118467458,5.061772456496489,4.989979547969105,6.7953958077735725,5.054342917746131,4.999041053553749,5.0194250593466725,4.995666114981175,5.009154184544736,5.100869552158927,5.5658426590449075,5.2542121160391995,5.39035859301615,4.995880334971355,5.011635241521919,5.453269885140042,4.985919105427952,5.051246465160791,5.029292811076179,5.148085585449214,4.991373925359736,4.98453057040069,5.029078591085999,4.983584107534982,5.007954552599723,5.027792017316797,9.225926684713562,5.160061300311023,5.005561131073068,5.065893270125784,5.002480258123378,5.215648622897757,5.9737536936027675,4.980756403664596,5.034229608122617,5.196435037233003,5.073096904159224,5.0026087901174865,5.040944431087555,5.053135495983294,4.981644442896618,5.01194683423491,5.047125651531497,4.988460533493278,5.0088972807880605,5.028835159278975,5.183973364733905,5.3354302685065225,5.047152915893883,4.980518814220941,4.980884935658705,4.981839188342237,5.048188961664575,5.002515312303589,5.358221814863644,5.169275837387,5.0434449626093,5.3417696683894285,5.027246036442725,5.0178904652351966,5.096252146048825,4.9992805904518605,4.98064734621505,5.105111107964505,5.006437485578353,5.016266288218735,5.165221474918032,5.29590437829841,4.992160696960036,4.987517965536482,5.0718230662113974,5.038841180274871,5.004040169142785,5.051785910045155,5.165221474918032,5.322910405392669,5.7641884369789755,5.157088667400173,5.022443613753765,4.983943973300752,5.346587764889584,4.991786785704448,5.660438294751593,5.044702765217873,5.005896799313714,5.064021766393386,5.320917781043741,4.991679675709357,5.047152915893883,5.0092558402510825,5.0204805796619265,5.173965307717501,5.019839581438364,4.991849104247045,5.011907885145786,5.01604427841073,5.015187398450007,5.042794512820933,5.023686089696813,5.012951720734303,5.011313911536648,4.988265788047658,5.051234780434053,5.014063717228786,5.050786865909131,5.043117790260661,5.034597677014837,5.0986961929858206,5.03959289769496,5.1736439777322305,5.183930432169817,5.113777280294544,5.04160072323929,5.020145617495462,5.0535133021477945,4.981839188342237,4.9951850937304965,5.034273306316846,5.007880549330388,5.039026188448209,5.0554373871505085,5.029464187068323,5.005716927429563,4.997535671259116,4.9862871743201715,4.988959827602529,5.082845861166904,5.204918761954215,5.009641048158782,4.990244401775146,4.989475157264952,4.989356362543124,5.02562380688072],"xaxis":"x","y":[3.137503523749935,2.4646682670034443,4.808385050656093,6.381629467033664,5.687900522480746,4.296457407371244,2.877744249949002,3.542258049766918,2.752748591407134,2.1984941536390834,5.6604951318885135,2.9763636357327616,8.749098443586298,2.1043366598147357,5.40973035386445,2.3305584000308026,2.3950627995175777,6.141391916170729,3.76659516172927,9.707290101958575,8.663593681954685,2.9126498648972037,6.819668183496455,1.6553518286125541,11.880218691860534,9.057883448899627,4.445594291341182,4.326249700837456,4.708739041359579,3.5570424152637194,7.3848269116030085,6.815319245777306,2.2600256559614555,8.198690585668917,4.72137265947682,2.6712933724815757,7.400708693037901,3.6229303509201767,5.137503523749935,5.359661722333877,4.571676809970931,6.199672344836364,8.475936042446387,7.905507349949873,3.8933622107638715,7.537839615916466,5.353323291162897,3.311793717753649,5.2712762591759255,4.778208576398088,6.628919373511362,3.1858665453113337,6.145473670317708,6.023698917266407,3.145677455195635,5.683696454306515,6.109569509370688,11.964503324303022,7.672213165334465,3.8787253414801053,4.8283269264148165,6.010108453474289,7.029563314432658,5.911212201558612,5.9590746011405145,4.0232553523003025,4.234961094839504,3.41006969175638,5.59215800212536,5.982537314399271,2.910732661902913,3.7687136570304847,7.183982678112831,1.0976107966264221,2.6016965164809576,5.511910748670928,5.6570683012015754,2.2868811477881614,6.999098033705607,6.746447139362815,4.83035674741922,3.411426245726465,4.77663042305105,2.550900664647523,5.942279901705763,7.626950122372515,6.862203399053224,3.4019034716079584,2.608809242675524,6.046141781644721,3.3936907641874536,5.390254956321868,4.541638734436141,6.063718640152248,8.226941966807852,6.203788453308596,5.530445354692906,3.2433644256936605,6.703349915166396,8.202417721575182,6.075318692553354,6.401732849522621,3.3305584000308026,6.606294160697713,5.5786368624717415,4.273515889702116,7.668814088419117,3.097610796626422,3.321928094887362,4.593353770980297,4.792334805706692,8.090535901003173,8.367589480412983,3.1358631653686793,2.7070829917717063,7.1593668097437355,4.614709844115208,5.946496941203293,6.783456654360239,8.491372117815171,7.273609132231856,5.944624223055033,3.070389327891398,2.2479275134435857,9.095924419998536,2.4489009511451276,7.048759311919856,6.058532970201611,7.216163813388774,4.46532151358055,4.871350840634026,2.611172380044005,5.783718557790366,7.3824943402931185,8.12401788912289,5.2787282129389395,6.099295204337775,6.1705259991768475,2.269033146455237,3.6392321632492775,4.547819956577703,6.003152448666922,7.816087658600206,6.762348815644129,4.940166750482817,3.01435529297707,5.487357715311547,2.333423733725192,10.642557179595622,2.503348735167504,2.4356285940520905,4.773468927905195,9.760270854333914,8.156942620908886,6.218781167784069,6.306517445249819,8.352882086522959,7.032211069322931,5.5963388642052,5.320845667645722,3.0089887832272546,5.943218087426284,7.3403842354553985,4.443606651475615,2.845991770664573,7.217618485351048,6.137913322088407,7.084383221650577,5.784242222057781,6.097610796626422,3.605257262939004,0,6.954079959320971,4.883620816285672,5.118941072723508,6.583308464847678,6.034963989749303,7.257764968571387,3.91169158187234,9.10522758436473,4.507794640198696,5.001802242633985,6.858478390423273,8.095608004811197,2.3103401206121505,7.6398833131998005,3.144046369616707,3.9420452599160467,4.142413437873741,3.925049964727359,1.835924074254375,6.111448698487674,3.363171077119244,3.127633279725874,3.24031432933371,4.160274831408593,3.5397791916984938,7.571221725048103,5.284292026394312,5.47800105583793,3.7398481026993275,7.1320626778543845,5.576522137920503,3.127633279725874,4.3110671022555955,4.375734538583156,6.4432751117658045,3.91169158187234,3.8042601156347384,6.4409521980296365,5.85872702377863,6.02724253597374,5.212958362728285,2.5058909297299574,7.561631630375841,4.92552476974757,3.5619370596224327,6.237448995639321,7.881113960675097,2.176322772640463,5.361768359419154,4.9941271140031285,2.584962500721156,6.166916251569974,9.4493756610678,6.0332027259372225,10.919861451442168,2.693765712217783,2.4724877714627436,2.7484612330040354,5.633721812641012,8.17707065762942,1.9634741239748859,2.7803100990433753,5.23687524536694,2.698218478224414,4.238022517825721,5.317231700222606,2.1176950426697547,5.604071323668861,3.2555007331483865,2.2387868595871168,3.881664619320345,4.493134922305505,5.357903838552492,4.9597701552114675,4.724650271732967,4.4046306842176115,8.46707564951531,7.096029876307993,7.341896820977107,5.339850002884624,2.859969548221026,6.582706526780788,9.101634319640722,4.129283016944966,7.151270288790165,4.58014548442338,4.628773595201645,3.070389327891398,6.034303767768035,6.339493737901733,5.174326515173915,3.357552004618084,4.809414444235898,7.880195728943098,5.632268215499513,4.79960542245267,2.077242998932461,3.5298209465286954,5.433960931121179,6.078951341394822,4.854494418154875,2.6484654430273142,8.345405246717796,3.4462562298895643,5.595145567990858,6.766065051474954,2.4222330006830477,7.366846735262642,2.9873208659292536,2.6507645591169022,5.940871478189196,3.552131108253784,2.4672794804599825,7.901409979999464,7.459185683748918,2.3868109464722167,8.668955876629637,3.1667154449664223,3.017921907997262,2.4982508675278257,1.7782085763980877,2.9088129077395473,2.1602748314085933,2.3074285251922473,8.316598324108684,5.603477988254004,5.669310286202474,7.991691898336964,1.0840642647884746,5.091276694365222,4.042644337408494,5.96254902292306,5.020591094773247,5.20163386116965,3.065227622775619,7.08533966935737,4.321206566969903,3.5298209465286954,6.083000565676004,5.04176864995163,9.435170178881494,6.4705368650925825,2.0531113364595623,4.078951341394822,5.525755692829486,5.9818526532897405,2.063502942306158,4.110196177754199,7.135657989368776,2.3812833725037836,3.693765712217783,3.00539998774259,5.589164236699772,5.36737106564853,6.38646609286931,2.3248106034204836,3.1811025507537978,8.27891401949994,2.430285272977781,2,4.867402305727543,6.299391206126829,2.6803243568440163,2.693765712217783,2.7949356628035362,6.605701738949982,9.159820892013935,4.255500733148386,5.555509427083257,6.871104373555458,5.632849830117706,4.966707314150316,2.526068811667588,6.09950561709114,2.927896453728821,4.28835856219366,3.1634987322828794,7.07692245939234,3.8429788317883253,6.341274183692174,5.926948247949772,2.538538163629804,6.73226919950145,2.788685710613534,5.508111681193465,9.398936038454162,2.3161457422933562,5.28835856219366,7.479861085016818,6.0925457415435655,5.747655932932393,3.9059284781731347,6.235152624217932,7.3334237337251915,8.870210577263595,3.460742563789644,2.974529312483882,2.6016965164809576,2.2387868595871168,2.7676547982373463,5.659639187015652,3.894332742277694,5.771093251699539,7.173327349650988,6.027463723408577,1.4222330006830475,5.794675787986858,4.720825666089835,5.342696959981934,5.562547724199122,2.62760683812965,6.426767932655541,7.593353770980297,2.5185351389821804,6.676239195086378,4.472487771462744,6.212180210044431,7.178117045706399,7.534497433738168,6.304876050044889,7.545505100691072,5.077670274232752,3.953265239014844,4.609991295212678,2.9126498648972037,7.174426393677253,3.6016965164809576,4.515384460636948,2.503348735167504,8.129128434194934,2.9726926540042644,6.030998118171895,2.2539892662307865,6.597829097826899,6.8209447918806925,2.9523335663696857,8.18769847638827,4.950468414150123,6.1409831048784245,4.674121632632394,6.830610274357907,4.4376272483189485,3.7037651787934096,5.95837871156634,9.296618003796326,3.351910961103077,4.143230134776893,4.707082991771706,3.560714954474479,3.9845893503624565,8.328001662149257,2.1667154449664223,3.1811025507537978,2.4854268271702415,2.931683057059806,4.788685710613533,5.659068274843228,5.738227400367242,4.6937657122177825,6.860962797858116,8.65474342118394,4.814550423461808,6.107687869314374,4.720825666089835,6.4775154361842375,5.619119511453218,4.7136958148433585,2.5533605033353277,4.412781525338476,3.72137265947682,3.460742563789644,3.0197019144425483,6.602587527342561,4.687060688339892,2.526068811667588,4.698218478224414,2.8698714061777126,2.144046369616707,5.394376944957055,4.649615459063409,5.80683958179819,4.443606651475615,5.450881315273369,6.6503337538459295,7.198494153639083,5.228049047884462,3.9231491804723087,1.3840498067951599,4.524815928357506,7.211012193485511,7.027021314622255,7.212958362728285,6.658497136656478,3.3881895371560837,3.140778655782796,4.37364821133469,3.319039815562536,7.33360262826828,5.175524601089875,3.2047667506546134,2.702657543390911,3.228049047884462,9.322806969423008,5.561631630375841,5.8755343515732354,5.032100843167024,3.9873208659292536,3.1842802944193824,3.277984747299765,7.444849247263915,3.6701605141266334,4.595145567990858,3.8369340113210915,3.575312330687437,2.788685710613534,5.4571345943180365,2.4276061727818994,8.563615765629766,4.712045448553693,6.260590274730996,0,2.4594316186372973,2.8257856274647914,3.049630767724601,8.097242070141174,7.431622959713292,4.030336078370959,4.964398632203184,4.020591094773247,2.211012193485512,5.462052318796433,3.3881895371560837,2.411426245726465,5.0430819818956625,3.6064422281316078,3.613531652917927,3.883620816285671,2.8318772411916733,5.081936081719432,5.575917361118149,4.958842675243241,6.716579420796345,6.531225485053606,6.050284014301706,7.73125113789623,2.950468414150123,5.645009884391095,5.4015622072560685,3.840966704487421,4.3298411765306275,0.8155754288625726,3.7070829917717063,5.848748114611959,4.303050084681673,8.796785917134267,5.497931651896279,4.67581593117227,8.189231548647284,6.9092930858238235,4.772941337831336,5.6082178530214595,7.304419775319909,6.588864518190148,10.665637927637095,4.672990993827748,2.8399595874895316,5.305605789585429,3.0908534304511135,2.4854268271702415,2.693765712217783,2.942983598187102,2.5897634869849773,4.631686366311998,3.0125686735030555,5.651051691178929,5.488964817004848,3.1858665453113337,0,6.703073006328523,4.255500733148386,5.9593064897596575,5.218781167784069,3.968090752045256,4.8594726668519375,3.460742563789644,5.341274183692174,3.0089887832272546,2.032100843167024,2.169925001442312,3.356143810225275,4.814037647539975,3.4168397419128294,5.653060017104565,6.074462620704536,3.2016338611696504,6.9656688644419456,6.816215687622865,4.330558400030802,7.16380060395775,5.6061460780657235,6.093179846765504,3.5716768099709313,5.342341397431548,4.228049047884462,6.310885391185277,4.751677945687579,7.502871574574138,3.953265239014844,3.975446765640962,2.6599245584023783,2.430285272977781,5.7303682363490065,7.689648608288815,4.371558862611963,3.215678596607928,6.492173659610827,7.480507489974544,4.436295119839363,4.804260115634738,4.405311682597445,13.396289023308677,3.8914191868460786,3.24031432933371,1.899175630480513,3.2942531364445142,8.382105211486744,4.143230134776893,4.92552476974757,2.8439838440483265,2.8399595874895316,6.101187815259738,5.0232553523003025,4.583158003874408,3.176322772640463,8.996303999184056,3.8257856274647914,4.7755773609306535,8.12401788912289,5.9618548076361595,2.454175893185802,6.208088008224774,2.673556423990145,2.8972404255747994,6.791683858152045,7.137810883416096,5.8052924556007115,7.338067797542616,5.198494153639083,2.851998837112446,5.726013748860151,6.567880042052731,2.8155754288625725,2.2387868595871168,3.6769443591069124,4.318316841334983,2.1795110502715103,5.6884601404244926,0.8073549220576041,7.244982170376553,8.642268351758643,6.836681593327116,2.73768676140986,2.4646682670034443,5.964860664245848,2.7420062108667365,2.411426245726465,7.279842693520348,7.446835177086041,6.531849285751203,3.8982083525087177,4.721919445551544,5.500164679492167,5.429615964201735,0,3.7311832415722,4.726831217032493,4.904002316283692,4.561937059622433,7.438459204423481,4.631686366311998,7.300764373248062,4.112700132749362,7.9329824099498465,5.1081062236861285,5.868143479666726,4.161081482277184,5.996162742681061,3.097610796626422,4.263785613890803,6.339493737901733,4.370861740085285,4.842978831788326,5.636334602442182,2.84197311892718,4.7713574089905615,4.257010618206024,6.916596067892185,6.015247773698937,4.595145567990858,6.7410624357490905,3.4168397419128294,3.823749360308273,4.279471295644468,4.7114949066500875,2.883620816285671,2.969012307516316,5.089159131911238,2.3362833878644325,8.080604371320732,1.8559896973084806,6.941341105485299,8.14639047567022,7.163498732282879,8.281651890258825,6.329482431025636,2.140778655782796,2.3103401206121505,9.500782139394977,4.674686619927999,7.249066033855867,3.1060132376221192,7.897179908573977,7.6546360285279675,2.944858445807539,4.94673086014031,2.711494906650088,6.054414387917307,2.788685710613534,4.198494153639083,2.9467308601403097,4.060047383669939,3.442280035252584,2.7070829917717063,5.9586107120558465,2.4982508675278257,3.6484654430273142,5.0870384567669475,5.130519083027338,1.8155754288625725,2.6064422281316078,5.673839055990439,6.37364821133469,5.609991295212678,6.076388068512841,4.695437110405369,7.459267666703387,5.211401637418471,3.063502942306158,5.931210274803932,6.819540460505722,0.918386234446348,4.878234879112339,6.098663781886574,4.098453246309274,3.4154882710497003,9.8361608416947,5.3103401206121505,4.20633064787105,6.436128517259249,4.381283372503783,5.0259146987507926,5.226123143125237,5.73903797911741,4.110196177754199,3.8042601156347384,4.503984703897613,6.067165427017112,3.7322691995014496,6.121844297876376,8.248591759535907,7.711013010084339,5.782670658677007,6.0774566524005875,7.592232766722112,6.917909073852982,4.423578170981798,4.901108243014512,3.795974694206668,6.907251224292033,3.189033824390017,2.4646682670034443,6.061128135118254,2.733354340613827,6.664198369291911,5.166715444966422,8.492293852480074,7.279749853014451,2.807354922057604,8.362689046726734,2.3248106034204836,4.826294245499635,2.321928094887362,4.238022517825721,4.232660756790275,5.51127826508303,3.7644735509926663,5.290940402403678,7.535353147842131,4.329123596291566,5.908092340818271,2.570462931026041,4.634012356402116,6.99491908871511,2.560714954474479,2.3589588258323295,2.8278190246173196,5.439623137557117,2.927896453728821,7.476543706214256,8.334094473911568,6.726695004499616,2.097610796626422,3.619413010597937,2.523561956057013,7.136683577697236,8.908992993250534,4.4950555283680185,6.970393537914677,8.089159131911238,2.996388746447621,3.7506065048355923,5.468583317006828,4.732811872082943,2.5360529002402097,2.729008870337863,7.712939631305699,4.038260575175349,4.514122260170708,6.948834425578686,2.0036022366801953,2.169925001442312,6.04023491675437,7.246788093844365,7.018923431842335,5.046141781644721,5.637784110291324,6.976363635732762,5.840463233869542,6.170125361839952,4.760753208062987,3.089159131911238,2.8399595874895316,2.3978029618624896,2.3305584000308026,3.0925457415435655,2.6599245584023783,7.9217814432688,5.963242904318827,2.4515408330178317,5.976821852360685,1.7865963618908067,3.221877081077034,3.0036022366801953,2.722466024471091,4.2024177215751815,5.9025560053446275,3.997292407636508,5.323730337521348,6.463851292611099,5.938756261492295,7.593951283948411,5.233044401265581,4.79025073883523,5.718361626138354,6.435295215647167,1.3504972470841332,4.12267271882414,6.361768359419154,6.8820316083521,2.0250287944915226,2.298658315564515,3.636914580355878,7.934634441094905,5.246408087246385,3.918386234446348,3.929790997718597,4.147306698780294,5.169925001442312,5.542258049766918,4.693208148910016,3.6971065744769747,5.707082991771706,3.363171077119244,5.035183996618336,4.274261661257048,3.321928094887362,5.9740703670240265,2.3757345385831563,10.174663577413781,2.8439838440483265,5.24260250592898,3.4581194811745064,2.063502942306158,3.1858665453113337,4.7692427951405385,4.301587646603187,5.986638471730853,5.058749412335524,4.556429415449573,2.5360529002402097,2.4489009511451276,4.974988111991927,6.248307120087241,4.346956889378885,2.641546029087524,2.8579809951275723,7.624539603781834,5.197314999473818,5.38404980679516,3.2387868595871163,4.464668267003444,3.955126781261366,6.258895755075436,3.1358631653686793,7.831180547126736,1.3785116232537298,4.934044647112246,4.503984703897613,4.100977647724821,4.86195536414487,8.439830883981392,7.851624127346895,6.091065077930535,7.190022174802645,5.992541859109112,2.983677694698067,7.161484638641659,2.6959938131099,8.21003812347289,7.461643148073906,4.848497755385146,3.192194165283345,1.6690267655096307,8.80290404741688,6.926948247949772,0,3.812498225333564,5.6806056662025926,9.871443254934135,6.459923362676901,3.8298495598446904,5.157447996274298,5.537296067090842,3.5147534984397533,3.2249663650002742,5.699884728697476,3.035623909730721,6.914444931746792,5.338780943894699,9.013601832683456,7.578712331048165,3.3405622690264134,2.3950627995175777,7.221587121264805,7.78959885005233,2.2234225499349374,2.8439838440483265,2.950468414150123,5.833649127594984,5.802968651005583,4.618238655595454,7.581803148516525,4.948600847493355,5.416164164733128,5.012121672712217,2.9373443921502322,2.3923174227787602,5.259649120649155,7.119252412157343,5.452858964713811,2.5160151470036647,2.788685710613534,5.040892430646901,6.390942772802543,6.032541697270038,6.177519202597814,6.111865963867557,7.0083165637349865,6.074034394212556,4.471187460386985,9.507715369062607,6.152994605492435,4.169123281476757,4.706530552538117,3.0426443374084937,5.305605789585429,6.942279901705763,2.4724877714627436,11.02948049374083,7.127736443588829,3.594548549550354,3.8063240573900288,2.144046369616707,7.036942844798256,4.022367813028454,2.0214797274104517,8.589726041133547,6.218781167784069,3.685940148445977,7.566662962962611,6.652343078740216,4.210232990095849,9.47854718263086,5.991861930556323,7.500961352460057,5.5855634984556914,2.319039815562536,2.9049657186840263,1.9781956296816516,5.509062386361898,5.592756010441026,4.39780296186249,3.431622959713292,11.193297418854996,7.925940095994448,8.041385369486115,5.051807107013298,2.3673710656485296,5.208673319629471,5.532940288372874,4.598722499676621,2.903038270112912,6.098453246309274,2.967168607532628,4.919340082442012,7.971830914780502,4.189033824390017,3.10936055940423,4.053111336459563,3.944858445807539,5.71699089440494,4.2787282129389395,3.7246502717329673,4.568032104771279,6.400025517914023,3.0443941193584534,9.140190703191836,6.7998644108818525,3.3923174227787602,4.880685525261125,10.32396671481508,2.521050736900963,1.8836208162856714,6.547203024756931,5.63023071585969,5.915042791371977,5.713970692436053,8.823876711241038,2.6530600171045644,4.462706750670158,4.524189078449365,5.342341397431548,6.70362677086498,8.008596693255853,5.81326814176608,5.795715006501729,1.5897634869849773,2.5058909297299574,4.153805336079036,4.707082991771706,5.617651119427331,5.7615512324444795,6.115615931428109,5.51443791382931,8.873136468833458,4.8811751553492755,3.4276061727818994,7.013685569928653,3.37851162325373,4.390942772802543,4.544114402209474,8.617283787779035,5.39677601078753,4.096767854714689,3.169925001442312,2.881664619320345,3.389566811762726,5.949301468328278,5.9095331149468935,2.2172307162206693,5.687900522480746,8.295998461911388,4.963011647599427,5.648177795724818,3.8509993947164736,3.669026765509631,5.924575003392735,1.2570106182060237,8.1184220243132,6.3103401206121505,4.59215800212536,2.169925001442312,2.771885578515366,5.309976492370556,5.718909554379232,7.2325648297287755,3.720278465233327,3.403267722339301,4.138323004056484,8.321792835886914,8.384999544730041,6.675392543042777,7.848935855524565,8.135247549809566,7.474760499576783,2.01435529297707,4.661635602335959,2.899175630480513,6.950701690089008,3.4776773275653072,6.275752048828529,2.7782085763980877,8.645802514606062,8.927066807759255,5.377123749129487,8.785648628652076,3.554588851677637,5.8529975876133165,7.093391153253013,3.8639384504239715,6.390082950951663,3.0214797274104517,5.947198584262056,2.0565835283663674,8.560600329022595,4.708187236020708,3.1473066987802936,5.260778431893426,2.3978029618624896,5.496654082593496,8.356011721502778,5.307064162255372,5.485426827170242,8.478971805032943,2.414135532984451,2.776103988073164,4.7420062108667365,2.4828482830684653,6.543805175964397,8.5918962955207,4.361066488794321,5.294620748891627,6.553360503335328,7.088629255297176,5.1201860275318625,7.00943675560809,4.090853430451113,6.7014104362328135,7.415741768290091,5.348728154231077,6.705563274566321,5.445263208140443,4.552131108253784,3.4436066514756147,3.5631581304028077,6.678494507773734,4.478971805032942,6.1657109924849305,3.403267722339301,4.534186139090097,3.0214797274104517,6.238786859587116,1.8639384504239715,7.020924396158786,4.20163386116965,2.7548875021634687,2.604071323668861,7.128870759465125,2.5459683691052923,5.414473836430928,9.995102877826515,4.861459166361513,5.25285462595139,6.31922050252387,5.308156975114762,5.526068811667588,5.763145958149476,5.903279342056139,5.825531251189622,7.048214384608036,2.513490745588118,5.944155663440363,5.856985689782205,3.412781525338476,2.956056652412403,5.664482840364682,1.74416109557041,4.886062338345599,2.4646682670034443,2.4515408330178317,6.433126376206435,7.7755773609306535,6.705839705883682,8.330737650206952,4.727375938594897,5.777945670695148,11.69784504363786,7.7627474419531515,13.587327482778312,6.558114538598396,6.332349900373874,3.2265085298086795,2.875780063068488,6.70473366267406,4.223422549934937,3.2387868595871163,6.136683577697236,12.23806432823473,5.572889668420581,5.849999259466098,5.533563348214512,5.325530331567558,2.797012977836145,4.150559676575382,8.298429210890312,7.731454807698346,8.903490246967104,2.8094144442358986,4.7824085649273735,8.461602224618334,5.583158003874408,3.060047383669939,6.429950657403768,7.193278935521388,2.3589588258323295,3.7311832415722,2.899175630480513,2.298658315564515,4.235727059838059,2.3812833725037836,11.663771557482411,5.684257705058932,5.212569338850806,6.9787676510367875,4.044394119358453,6.092334311186202,9.582254908357664,2.5160151470036647,6.402415216824399,5.740387932468353,5.784503982929566,2.6064422281316078,4.362469888750209,5.164705840182799,3.4208865749755315,3.948600847493356,7.965841991309597,4.976821852360685,2.722466024471091,2.7398481026993275,4.528571318870758,7.677578712165378,3.41006969175638,2.4905701304462013,3.5147534984397533,7.5673476962842585,6.849624029924543,2.833902076669163,7.406162478777607,8.131085259578953,3.176322772640463,8.148324042480084,2.657640005207824,3.350497247084133,6.8203066288938246,4.483493351284395,1.713695814843359,5.872582544833223,2.722466024471091,5.7742599514327635,5.704318677760832,4.760220946466509,4.294253136444514,6.037162550127378,7.151777654732053,3.215678596607928,4.742545233866053,5.613531652917927,5.727375938594897,7.7780114016125745,8.009884588931843,6.549977142771825,2.0461417816447205,2.0565835283663674,6.66746640495737,4.62760683812965,7.884597920990064,8.571145863602528,2.3161457422933562,8.563272552819562,7.111970261358359,6.001577085389535,3.4369613378336026,6.435461914479276,3.5084286525318573,3.940166750482817,2.5459683691052923,1.6644828403646825,4.94345253855882,6.003827078111476,4.960697039304312,6.684117412839159,5.612352498752662,5.609991295212678,2.62993940943954,3.611172380044005,5.522620761042079,5.891419186846079,3.6530600171045644,3.4581194811745064,3.1667154449664223,4.581351247168778,6.764340846670181,8.066089190457772,6.614857050384495,5.767654798237347,3.1953475983222193,2.9726926540042644,5.279471295644468,7.5105663894487495,5.02724253597374,8.25176642434399,7.822411496360477,5.334496768390418,5.715893370547605,7.3049672876759635,5.998646839005795,6.9725777851765525,2.4594316186372973,5.666472568842073,3.8728287595348854,6.052024560482831,3.631104282365877,2.3015876466031866,2.37016428054021,4.3384244147835584,6.546122758857796],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Lasso()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_pred_logLasso"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression lasso\n","# réglage des paramètre pour la gridsearch\n","alphaslasso_log = np.linspace(0.1, 1, 5)\n","param_gridLasso_log = {'lasso__alpha': alphaslasso_log}\n","\n","GridLasso_log, \\\n","BestParametresLasso_log, \\\n","ScoresLasso_log, \\\n","TotalGHGEmissions_pred_logLasso_log, \\\n","figLasso_log = reg_modelGrid(model=Lasso(),\n","                            scaler=RobustScaler(quantile_range=(10, 90)),\n","                            X_train=BEBNumM_train,\n","                            X_test=BEBNumM_test,\n","                            y_train=TotalGHGEmissions_train_log,\n","                            y_test=TotalGHGEmissions_test_log,\n","                            y_test_name='TotalGHGEmissions_test_log',\n","                            y_pred_name='TotalGHGEmissions_pred_logLasso',\n","                            score=score,\n","                            param_grid=param_gridLasso_log)\n","\n","print(BestParametresLasso_log)\n","print(ScoresLasso_log)\n","figLasso_log.show()\n"]},{"cell_type":"code","execution_count":118,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[1.994798547321519,1.9661561087751742,1.9565178901727975,1.9963971588562281,2.0576288004073744]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[2.3421410233461466,2.124990554332758,2.0014873345441537,2.0291327262594088,2.092976668625442]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[1.6474560712968915,1.8073216632175908,1.9115484458014411,1.9636615914530475,2.022280932189307]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.8286132810682012,1.884035361541639,1.9370680790472927,1.9982927725253947,2.0502389058637225],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.8494613671990836,1.9280620500791068,1.9787222619932092,2.031983137738866,2.0789541744872824],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[2.686732036478909,2.2783186752126325,2.021960565820199,1.9855743509509138,2.0866027601547494],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.8456131762006707,1.9024273194420367,1.9586942636372753,2.0255863098491247,2.0807715946174246],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.763572875660731,1.8379371376004565,1.8861442803660113,1.9405492232168406,1.9915765669136936],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Lasso en fonction de alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Lasso pour tout les paramètres de GridSearchCV\n","FigRMSEGRidLasso_log = visuRMSEGrid(Lasso(), 'Lasso', alphaslasso_log, 'alpha',\n","                                    GridLasso_log, None, None)\n","FigRMSEGRidLasso_log.show()\n","if write_data is True:\n","    FigRMSEGRidLasso_log.write_image(\n","        './Figures/EmissionsGraphRMSELasso_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.4 Modèle ElasticNet"]},{"cell_type":"code","execution_count":119,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.546e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.900e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.064e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.901e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.889e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.185e+02, tolerance: 1.785e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.420e+02, tolerance: 1.785e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.809e+02, tolerance: 1.785e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.467e+02, tolerance: 1.785e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.408e+02, tolerance: 1.785e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.900e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.546e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.064e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.134e+01, tolerance: 1.785e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.901e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.889e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.739e+01, tolerance: 1.785e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.314e+02, tolerance: 1.785e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.065e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.900e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.170e+01, tolerance: 1.785e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.546e+01, tolerance: 1.785e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.902e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+01, tolerance: 1.785e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.791e+00, tolerance: 1.785e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.547e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.548e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.890e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.901e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.890e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.902e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.065e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.902e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.066e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.903e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.483e+00, tolerance: 1.785e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.549e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.891e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.551e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.903e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.905e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.893e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.067e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.905e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.069e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.906e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.553e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.556e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.894e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.897e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.071e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.907e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.908e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.911e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.074e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.912e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.565e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.560e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.904e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.900e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.078e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.915e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.921e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.083e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.916e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.921e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.572e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.582e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.910e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.918e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.929e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.940e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.100e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.929e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.090e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.939e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.595e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.612e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.929e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.955e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.943e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.974e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.131e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.113e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.953e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.971e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.665e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.635e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.000e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.986e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.962e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.033e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.995e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.184e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.154e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.026e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.075e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.755e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.704e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.128e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.065e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.059e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.272e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.113e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.018e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.223e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.898e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.819e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.193e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.173e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.172e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.109e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.407e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.272e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.333e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.111e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.245e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.364e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.334e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.995e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.595e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.329e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.469e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.494e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.247e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.426e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.395e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.587e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.535e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.245e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.548e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.715e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.835e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.435e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.709e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.655e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.730e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.851e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.801e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.558e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.990e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.108e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.784e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.670e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.969e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.917e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.072e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.129e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.903e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.263e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.069e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.052e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.382e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.184e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.935e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.247e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.388e+03, tolerance: 1.794e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.228e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.368e+03, tolerance: 1.785e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.199e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.309e+03, tolerance: 1.781e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.321e+03, tolerance: 1.780e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.508e+03, tolerance: 1.820e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"name":"stdout","output_type":"stream","text":["              paramètre  ElasticNet()\n","0     elasticnet__alpha       2.04336\n","1  elasticnet__l1_ratio       0.00000\n","                    R²      RMSE       MAE\n","ElasticNet()  0.190936  1.826576  1.513938\n"]},{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning:\n","\n","Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.631e+03, tolerance: 2.240e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_pred_logEN=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4.869559575669818,4.91120501196276,4.928979244763792,4.945121400923755,4.874954443766909,4.867027766068831,4.882033614778932,5.486248827969217,4.91160167421806,4.89093961186933,5.105546608029983,4.940251853168828,10.492548346416932,4.8784464113258155,4.991030182821262,4.9223463260724705,4.955714096998877,6.060084219428823,5.03862781557091,13.023211311402036,5.599860045948977,4.98077740795674,4.944977146376361,4.9055140940809645,5.632051256807976,5.0284722424087045,4.838540171341337,5.058623193624281,4.922715728486972,5.001005429588061,5.059456606123962,4.961147244347792,4.884588051919459,5.6008489115116395,4.944934580923126,4.939142279428765,4.900425390900893,4.864889072402235,4.904710902947452,4.850261633857677,4.877889788226964,5.3366463655656045,5.704916462100368,5.076260473025653,5.137412855314829,5.638669198857693,4.957507224250771,4.9801924207641814,5.190773602981942,5.121882974658023,4.943499515137169,4.921923475831737,5.215187482304881,4.887869372882071,5.0463201678994265,5.027736358385986,4.996549458678089,6.455002356769223,5.4885821036768885,4.879988290627792,4.8418817782756305,5.15240060834008,5.11977920221986,4.869071779629411,4.901198658284003,4.885445489415197,5.009936664202258,4.882891285291521,5.250985829415249,5.132340901763735,4.9967062395875645,4.933285762406853,4.918680871906572,4.845981813063453,4.9620261398128545,5.061528646974191,4.837884128499404,4.883094392625443,4.868022997174684,5.656417767659364,4.8502644009473554,5.043608430558372,4.911242160757239,4.913088933695212,4.827623680263752,4.961461682789289,5.097627139613724,4.927477342227942,4.824055051207875,4.866673515598932,4.974320727036325,4.8588312832286205,4.824162631355625,4.830773295301961,5.580028795456501,5.2619968117356075,4.891455171287512,4.8847851930518065,4.977014963157099,5.358231245136751,4.844177335248643,5.102816771844923,4.856503854265792,5.258699676457721,5.029456839093214,4.896324153520094,5.725213151346289,4.945600194692726,4.952522836497974,4.975168336274193,5.011983947747179,6.042588539434252,5.81305399209199,4.814547888892278,4.900055295299106,6.397016689834428,4.889138141140145,4.870255740942961,5.600686985853353,5.387105771203964,4.919120429795425,4.897413476328868,4.957390586570299,4.897877923515784,5.070094748244905,4.8962063777229075,6.72759125412785,4.834046398237916,5.009556043529952,4.89928831076079,4.930714835406915,4.901764142051919,5.085190465714404,5.537775513881658,6.226355552862562,5.453061826212679,5.088338089023045,5.095781079576157,4.89594169024345,4.846179474345679,5.059447898821058,4.876964747423585,5.869341920544627,4.955513627434671,4.8982613651175555,4.939825037557949,4.911121438818167,4.858864515547249,5.573724066593619,4.96600532882353,4.881741350561099,5.380121598797611,4.630101719578695,4.967553364870256,4.840701074847195,4.922787122894209,5.306550970623778,4.83315574039792,5.116688723785775,4.992575207209204,4.874649636809185,5.268631562643764,4.912880346062231,4.8558538275436245,4.93808068725584,6.462749295694329,5.212673779599345,4.8313941307139645,5.2897151432866005,5.342338718871457,5.047511837403028,4.970813112587683,6.922571729388358,4.87985106725391,4.892636822956875,5.736060994796485,4.931867716218316,5.136769063132952,4.929376223235302,4.889214019381056,5.07679416502154,5.15341397740173,5.011611073687154,4.928530217249929,4.922085445358861,5.227361997303331,4.942715606393443,4.834039598057274,4.991871512632895,4.831831133013298,4.852150388890753,4.837914912088404,4.8713413831058885,4.846967375250238,4.894442033253542,5.06546880469163,4.983975671483019,5.32698432063397,4.896420253283091,5.2699354727503955,4.9270689154122485,5.289955251518108,5.002936771786464,4.871507630201621,4.849340522763102,5.008590995800866,4.902698371052086,5.1221213223144915,4.961310344756008,4.925758720988441,4.984141376331345,4.899966725496698,5.518414239251186,4.876004512298375,4.9994343557759064,4.984655794706948,4.938311017238129,5.215086256952873,5.89900974337068,5.297423515424719,4.88358139766181,5.05865282096182,4.900402936038043,5.07829272960298,5.199137030523951,5.11419481420573,5.120635881292557,4.910112077581433,4.927382989589757,4.903944770785071,4.923950265580485,5.260210384302675,4.892129018056667,4.903828427324881,5.4102833489404745,4.956585692009709,4.907338242040961,4.8296151955930675,4.858930490202064,5.158983809685046,5.037033232340943,4.925116226991942,5.15293676234717,4.819436825370858,4.834090594894352,4.969556659400173,4.939337204545263,5.1239209119525535,6.152266121155863,4.852271827350139,5.106614951601654,4.871404883331488,4.947269971689149,5.009445434448658,5.018181010584052,4.892265200980101,5.47959958378393,5.083642674338849,5.1222688643052,4.882858842325511,4.8860137845316824,4.879700302860794,4.829285126591192,4.99520546015215,4.9647662694636026,5.389661538945822,4.957973841468239,5.251805979465046,4.86770968607183,5.1556063076248915,5.119839230812747,4.965463428739326,4.94602105530759,4.9459005379845875,7.173050825697338,5.093638480614328,4.960998761563656,5.114161833554347,4.906083212687647,5.6542179249507765,4.930953867463855,4.931693141130616,4.920024281030693,4.965485265595004,4.962031556199183,5.471553605451047,4.885033153030488,4.897836962145393,5.4909134324564945,4.896985199219707,4.969107349415906,4.871747566323759,4.879013193587632,4.920700178886572,4.899238174220502,4.916162847877327,6.224525228717487,5.230534061146729,4.893554607278342,4.835733649405363,4.860651595906283,5.257393526289603,5.09960279982652,4.876777215638955,4.94438353187785,5.068944052786473,4.911922317931444,4.8631646304851275,4.880697999013483,4.958579676057749,4.891422721116729,4.9133797580332566,5.478698945767933,5.262610237968094,4.892267135244266,4.904568561458008,5.014427767533138,4.898417483727682,4.846002605850024,4.828616652309029,4.96593538261533,4.889512160246979,4.915283445169476,4.948823097131644,5.175600277360786,4.872888693053522,4.934771354759305,4.932518896614698,5.248187299596361,5.333574043443474,4.948142664089175,4.887589971521711,5.038875138175253,5.2727253847285,4.926655076581143,4.896121621120216,4.88131257679876,4.929427344666285,4.897164286019737,4.845499331331303,5.520049887298547,4.976847064982556,5.319588842388625,4.9731397780010225,4.830987573332879,4.890413524970818,4.903685055463145,4.849871961501545,4.993611780088426,5.4624694644896605,4.865252191116573,5.191256007568193,4.876161056174026,4.9141920247940485,4.93570457778011,4.945064422812779,4.947278307017736,8.577398516800686,4.8996682130078355,4.962848468688347,5.713614555659703,4.860759787449194,5.381844789935569,4.920359082640094,4.999605813790726,4.8902930215798515,5.22436923799736,5.0038058081416645,4.868748718377644,4.920770411615487,4.890141759299853,4.889374794551021,4.894707094491925,4.837807454795856,5.094323247271322,4.841685362538618,5.271666653853435,4.9553780422090306,5.230861802884521,5.548633840009668,4.8204894209999445,5.082852139484414,4.8752494063984075,6.098912265445092,4.857875725750136,4.980178927642326,4.915415290193108,4.911766282008667,5.228862340663939,6.396758774972466,6.0523076300562675,5.216239932709412,5.857458179272672,4.881360372518099,5.114401112812545,5.279131922542889,4.9026075479181,4.841980595525485,4.835837264267909,4.843207878713297,4.92309731850892,5.242678964011194,4.967453769875995,5.130117793904458,4.889421380482183,5.244597646067064,4.951671267119472,4.962999017106156,5.259856191490297,4.87280132457197,4.90731083137684,4.8399213743812055,4.784206247045147,4.931776742598741,4.996687571130096,5.140183552427976,10.15405003526622,4.9802292238615244,4.835191958153333,5.017637705727881,4.831315613882617,4.878647101121356,5.460242649888439,4.830925689557492,4.8774093947693355,4.9128060866842675,4.938451963091596,4.954475037638949,4.877517637287456,4.8684565766781756,4.9273922045984655,4.939988768875982,5.44127238468122,4.8808443253796385,4.853299002051973,4.909244346715564,4.8846537935574235,4.84300777022732,5.089345005988747,4.907791971202884,4.963031683477078,5.192591460188488,4.960284770773037,4.919580525983828,4.874663899444462,4.850212606205858,4.940967996518387,4.987872947470847,4.9146476894049,4.903378018823401,4.963956719409192,4.8705103557202785,4.85111480406769,4.876646869541297,4.9864448520115205,5.0321375678206,6.623267001958934,5.117294790281796,4.997471267616672,4.8414816480128104,5.15694314640059,4.866251840657656,4.963199387560648,5.61305044199143,4.940892248088658,4.973185353546818,4.937511487455642,5.0840623545532715,4.833735255339388,4.853614856883113,4.89411882148565,4.8463353707379815,4.8970851854440545,5.0435241433680496,5.279896888737605,4.910002033343523,4.881605148293371,5.138076282692274,5.10647083450695,4.935917301643573,4.961825565293177,6.630889702560378,4.9905677361623875,4.819052972833756,4.865479741099528,4.939845513669581,4.95802860068525,5.095225736018582,4.903066138637005,4.934634119852815,4.980457442622809,4.825787762632882,4.858109154157541,4.888834420611158,5.128650895225976,4.980212725513401,5.8914515981201925,5.3188498682177325,4.821539901188646,4.900766949594959,5.089311568001866,4.874655634505077,5.004660087453072,5.07595936160833,4.904223213433651,5.128012159190262,4.872368635409628,4.9815753965567255,4.804691033383428,4.984704915425166,4.993981344604711,5.037092963459716,4.930634107212793,5.0599152037261375,5.1100349327632735,5.080192511264709,5.151812982644995,4.979872511143044,5.09624111651076,4.8123094038933525,4.920672330512854,4.876794129281509,4.838050093262829,4.909677568520297,4.891896380118023,5.108850894151398,5.5048496987604825,5.016103550498728,4.973130605229176,5.833930073990627,5.266432222933276,4.939650283880595,5.245944595276054,4.825137211032931,5.6438171286991485,5.293697827082343,4.826154210557666,4.875076773917116,5.023339329999858,5.06676680761437,4.915813883493647,4.90353533254923,4.820086244137942,4.909163853361715,4.950915989482062,5.048435494526281,5.065509478110836,4.8952768765505095,4.919272494247371,4.94944950836803,4.950280000700635,4.839408571382419,4.881740784202968,4.983431009825676,5.018863653228279,4.862226805404407,4.860858516414566,4.934136180304292,4.964126110344021,4.838452923906263,4.930319121664992,4.943811740703627,4.977652320223659,4.9795267428166845,5.0840320154466685,5.084176958282687,4.957935914871475,5.246460529263255,6.634615332576293,5.114721825767124,5.420498777341862,5.073435666249267,4.8663284169467005,4.995466421131452,4.890386235432447,5.332643127028969,4.939772782109657,4.902453315229463,4.927786804875954,4.977968672791989,4.9145834853849655,5.0862505934329825,4.909549066586942,5.016670914070106,5.812513056316073,5.053302423041461,4.893232475628849,4.976890806675577,5.527484282057005,4.967796116092518,4.9293590520559425,4.8921271556559915,8.579151805761,5.1187468363228,4.992031209902032,4.8957410043130025,4.990367752101579,4.8947011825366715,5.14833412479734,4.879338855832032,4.939442045787719,4.839587564232534,4.984288680338264,5.185286521468155,4.891464304159566,4.945045407822694,6.012908162170519,4.959994399056473,5.390412830622264,9.795819779910481,4.862775013268983,4.90301897139628,4.896626008884726,4.948138241957547,4.892324846603585,5.087335907789072,5.717254854376883,6.2237548905790465,5.172840082331103,5.029888560806016,4.9924150910060705,4.939315776566482,5.127930876894455,4.8698732863574365,4.8800298030413645,4.941132466213673,4.900169251521058,4.838691846723298,4.95658299426932,4.840174609593164,5.074983745399158,5.487171985309876,4.957970616595503,4.906917240709379,4.897203390902351,6.293532192623356,4.866899334631154,4.945548397028299,5.352877204366904,6.355348673140414,5.4042846009209615,4.854373468497347,4.892022794928963,4.8587941408096595,4.87740857691961,4.887506420100025,4.8429833358497305,4.893713767714008,4.971005038856234,5.4656571004488645,6.141846830506367,5.146064027046849,4.929411661620079,4.9004173684101024,5.177145009370018,5.0562088627418404,4.859964509118309,5.197941099824857,5.387616983124608,4.953006838218096,5.227874001100616,4.8629574360837795,4.925394101618766,5.0596276669695115,5.509950298485444,4.971845172728813,4.86920372893904,4.8653540225026415,5.9140381384817164,5.095481697539056,4.890447800806167,5.2793653436732555,4.9415204045478,4.515495852130242,4.905020289517559,4.8665832121529755,4.920051613403799,4.8498535587787375,4.934467889807185,4.965162254069968,4.97326508244942,4.8299007318790546,4.889999183135131,5.297287670034947,5.0761097835363085,5.0378370349831565,5.159959193100397,4.848983078547951,4.880416170450682,6.330874131611422,4.942046581003762,5.056178280487804,4.941991766287137,5.20253283846471,6.04348871686336,4.920960264331471,4.920888873290328,4.914236002609725,5.042441410107046,4.923307968154658,4.953910615987185,4.958816117591254,5.18428182668684,4.92870443201056,5.033205680916926,4.916122555973405,4.914470990392929,5.017293042464358,5.145272318501235,4.869705350872454,4.843909792050334,4.891416215579601,4.8661259834839266,5.23430641424241,5.469253949299804,5.281807358850477,4.862331532065671,4.827551707913045,4.89839914578208,5.004559228295699,5.20240211065318,4.964608147894459,4.838040863702141,4.836947269562588,5.299139284731397,4.923961299159384,4.868719078190895,7.2000602483851335,4.834269127312342,5.219541682102193,4.9991542026240054,5.167287698608989,4.94513476936202,4.9261434362014835,4.8270952218099845,5.068914425448934,4.9530626901053285,4.91533984197089,5.0567672550383485,4.9967965023811045,4.9597689151458555,4.983309549529933,4.8559799721158,5.156090888964538,4.84793706192432,5.484738852076851,6.850660771160606,4.845202112597463,5.38171928072308,5.005263498964147,5.123616810900621,4.931676074707207,4.910148836176165,4.878455174852804,4.88505442960873,5.0917925455814474,4.928666862960141,8.005686543123472,5.3174781853527655,4.915082522357086,4.918238985425275,4.911910691158857,5.127785050867621,4.892451031234172,4.890514123753694,4.889934001349636,5.065774276281934,4.943576994231506,5.177477880665612,5.118352809741846,5.391957697494124,4.896625194558313,4.9025871372920395,5.395939844813121,5.307211507743079,4.885803166647198,4.888096470999563,4.905612681980963,4.966280971026416,4.934500877134592,5.187143838731231,4.934001167443494,4.948752113358837,4.863019893604329,4.996174369667975,4.894196963918474,7.858070457410014,5.858212063076292,5.3370551434797875,4.8763301542781345,5.88116036629554,4.935273765578277,5.086503528255397,5.181184866427273,5.132174206482676,4.8920325823857524,4.984528453159452,6.202359412779574,5.053199080236217,4.824582848446391,5.842450003301606,4.8692007300910936,4.880623341917845,4.8484774954959775,4.952838652681437,5.051362493737307,4.91921435351727,4.8625974612288685,5.18208178870439,4.904753770868889,4.903775760709656,4.880751831998596,4.991103890977664,4.904897313677865,4.916335920592361,4.893153801172137,4.9303390574949475,4.934096225518328,5.056253537676083,4.838186263671325,4.907981879536644,4.974725950425511,4.925245618046469,5.023432434372488,4.9648838088886516,4.899388815747056,4.885067002246197,5.181993147572208,4.858427846366242,4.8510656974319275,4.904236144874917,4.914378374399313,5.213602241047162,5.019772300010555,4.890142299078564,5.095255376205331,4.892910669508321,4.823807458387402,5.287635114913081,4.883849000111444,5.97181526462715,4.8726104202058265,4.8652277388528224,4.944491642855071,5.286632905717975,4.8755615287141705,4.834201127195988,4.959369160532597,4.945814183140981,4.864776407468167,4.997812070064827,4.904992213344824,5.156627913588491,5.004452364819667,4.874530845510465,4.836306452820717,4.983182761174258,5.0393709802920466,5.668243324199548,4.961446989933507,5.225090885156268,4.998765863367711,5.100495634753471,4.948979082001584,4.893666985691368,4.865571102173887,4.8565197647461,4.886489088500445,5.03952804081526,4.966410445739988,4.903088526208603,4.855157706469002,4.899083668564565,4.869442665688141,4.864718266738065,4.818404862546604,4.887845730391215,4.926817828696646,4.967629150395322,5.5081230074265335,4.870187351065366,4.968598850321183,5.1533414631029295,5.2172985854727845,4.861170396600961,4.9313338537342295,4.857324644699513,4.829847840050911,4.9348435375373665,4.918492212471819,4.873779163972436,5.686729489034446,5.6993311979096175,5.439427738594588,4.980240932477344,4.94878139105399,4.8713923143246864,4.882365331766954,7.123702191478769,4.895582409799478,5.105080967884349,5.6489604306823455,4.937810093552863,4.950994307821373,4.8356905812388,5.962031520900375,5.1349910922450555,5.937601750322701,5.055434609830508,5.375551073000566,5.011541862179842,5.126688900061908,4.853965291765697,4.853577693126974,5.100616343296555,5.001854768987479,4.8612163983769685,4.9365742436844835,4.904582380846678,5.1112473294190846,5.75057807452375,5.52623944316113,4.891598866980731,4.989817974641337,4.886912159077697,4.836297177754853,5.471518600356047,4.840062901544235,4.9379184092001545,5.005424071653051,5.1890244665463205,4.965040563308262,5.253717988273368,6.0599560371392664,4.896749552923319,4.930339161712896,5.349762212755898,4.928160731266943,4.88219497125846,4.875277688962697,5.004291553790998,5.0730072884401824,4.916656917528797,4.925179643391655,4.910077032481211,4.912630860373141,4.890027729076861,5.026036589186876,4.8709764398340285,5.145282324069709,5.083217843180031,4.913670067334731,5.424948645805323,4.8616435368534,5.074697314129845,5.0996362378133995,4.852241919653769,4.941459315429909,5.08548759169467,4.880445655020078,8.962196874124258,5.73733454202774,4.871727133611117,4.975618662633334,4.875143731044174,6.111286792801748,4.991188902952495,4.85471667692547,4.951792671655901,4.881267671658682,5.007087734205758,4.998382126816177,5.369037383967408,4.864238451066687,4.910656710554407,4.873310547413247,7.169662641974481,5.140877198302286,4.8422154930599834,4.87662887645362,4.877168669083921,5.798988096610566,5.0113475077845395,5.109789840740842,5.129790503542474,5.947369913255466,5.921556999940161,5.694202466979655,4.816353195355839,4.932637557897123,5.174007415640277,4.861980318563263,5.431065217874769,4.946247312327177,4.904539735597672,4.938699264259153,4.925062309071331,5.641098344933713,5.140231165404958,4.941631556240891,5.230768661264281,4.949703167231821,4.841841397321163,5.048752102179432,5.039256002642884,4.884149987102476,4.944710050595892,4.955216918746353,6.0231993939951725,5.232793409819128,5.2072159576263735,4.9810840135311745,6.14031158183522,4.9571162113986516,4.883752810470111,6.461891783109383,5.0165928517233755,4.976010697387863,4.946291762444667,10.173020482966315,4.915233654025326,5.009761568402258,5.5312659635158,5.1027095051357545,4.852029676531179,6.392420401766638,4.8552296788197085,5.012612668406519,4.856327257167986,4.882459682705488,4.931676918518258,5.471499807593888,4.848502908985007,4.837645830099667,4.995716954306088,4.8651141146669525,5.862107963267593,4.94495084079394,4.963074808476271,5.633716256730127,4.993549170033613,5.211673112520241,4.853735906754623,5.864686129193646,4.93138126094422,4.893565788343714,4.985994657989467,4.945302710995002,4.938995663835214,4.87572255270703,4.842099719186018,4.929174731364111,5.032103002201333,7.593498592741351,4.836395310506137,4.958416752350552,5.183552970813529,5.065495884281884,4.9283650012199915,4.906679750412073,5.107546988112396,4.84046167363049,4.967487041922729,4.894742718240093,5.118359663401324,4.8306525166893275,5.12913147806569,5.009309362802094,5.072875035617149,4.976755666597756,4.935522951316328,9.31140830100988,6.508496408758826,4.918695036151842,5.215047909608963,5.08345135712963,5.158817509014944,4.851583079717233,5.037448945942535,4.82279206398114,6.516084887380961,4.886039494325378,4.855539165385817,4.876553891569972,6.085318345664274,5.480622200631842,4.907649131042601,6.668143365025921,4.873480075846453,4.950613880848236,4.935323142781391,5.086136659355073,4.882395661053999,4.873679405710619,4.920293265168176,4.841216921382144,6.515349061548443,4.694631025082925,5.007009636050924,4.841431138917402,4.920714300296534,4.844998423953977,4.837839996222893,4.86463706482667,4.845793246928452,5.230402736824919,4.844866474644348,4.9146964852984505,5.159658296652188,4.906497609547561,4.935397923872486,5.153808682920313,4.915472288636774,4.876503739166297,6.550953573359987,5.281841632833764,4.879400794303218,4.8657588669753675,4.879118053554468,4.949271141528832,5.230688440521218,5.2455948354329704,5.6047860639476195,4.959032893953222,4.996383547290902,4.861050151281235,5.03697110478072,5.259664190745004,4.962898804565174,5.002970573440284,4.979348701192808,4.973423195839764,4.926338013025268,5.03796340496127,4.882353910199603,6.853616271906051,4.8788469093154925,4.971693074314832,4.833980780661144,4.970284874756322,4.902928424648334,4.92011517052023,6.458890106325365,4.856477980214495,4.864986457138044,5.011226168819087,4.976153620186868,5.007322342402942,4.970824759686755,4.8966401887980435,5.129081641600509,8.18328878359093,4.889176289277015,4.937773905248044,5.180964775743541,5.053210879319317,4.89808944668243,5.281765262748244,4.833832675344706,4.981628120900498,4.926116986325595,4.9445064325738155,5.171936191482582,5.074988488175792,5.378138862731893,5.170163499283571,4.880896470097792,5.182747205626073,6.495297338645339,4.804652922542135,9.83297538892657,4.9747901662737934,4.944640808024422,4.941776082251867,4.911307787119341,5.70769150834204,5.064007991806084,5.059253691263066,4.892256855915192,8.321087841344788,5.033811264922214,4.886348956973391,4.930009202797683,4.882911287239126,4.941625558544998,5.058489966851481,5.932656647057246,5.2991337977506285,5.658674757701452,4.928822389139623,4.856837471565943,6.0956095774889265,4.885994511606187,4.957178513595824,4.94322429279299,5.338461095540903,4.894394284703254,4.86400454092689,4.946596523985584,4.794285286644545,4.880222891929042,4.964335152310459,10.86549742456525,5.252496560938259,4.88317630979857,5.039288225279704,4.941639738458316,5.309597218827468,6.3205828997867695,4.848632213338256,4.929311204651234,5.285842866511202,5.118790409775856,4.862430494047892,4.909493047475625,4.8228330961868995,4.889703272189117,4.92607411440601,4.952583443404225,4.836312156449993,4.9180008754720275,4.955893287398018,5.356086169518179,5.508502853775097,4.977733924420009,4.838800312298993,4.858163282940121,4.835355182716949,4.987138296807209,4.921841925627629,5.558085909788973,5.200062047076356,5.104092572187978,5.375550250090385,4.992350611670222,4.974930182524986,5.141262106149068,4.897009047095416,4.844123907169359,5.0798272331248455,4.8917391669046495,4.919578345262385,5.218702675727392,5.541557195340452,4.8719672103789495,4.888456565769947,5.0580692951098944,4.953896390057371,4.864634647288287,4.987421546839679,5.2084114439027385,5.662149984652087,6.384466201333204,5.090019876700844,4.882682528554418,4.87045670070089,5.660951010108285,4.875178248374113,6.274068316052228,5.019199750893246,4.903514523997976,4.924071691220461,5.821073545082403,4.835309727549643,4.967442692595355,4.909843008661888,4.909802713743104,5.175342768774873,4.94287751117321,4.855422419414673,4.904638667321368,4.9125318983909185,4.8714636318929285,4.944163513509687,4.994145985837276,4.96186299954639,4.927118242690486,4.849904539193822,4.997513860785988,4.880069717666939,4.985883137843322,4.955117641295183,5.010948530879356,5.14935619636521,4.988497551942771,5.086551704850306,5.131299934528094,5.072926611355378,4.974536937904333,4.948990496083806,5.070957320386502,4.825063950892297,4.850998885677303,4.960834376104378,4.880108935707089,4.908058528573435,4.963640697790402,4.893493375400162,4.887068447463609,4.876512264897658,4.886561293868002,4.897851464913996,4.9282471894157105,5.31438175397977,4.902671686344093,4.872803160800791,4.862058055593011,4.877043259477003,4.8997428372204315],"xaxis":"x","y":[3.137503523749935,2.4646682670034443,4.808385050656093,6.381629467033664,5.687900522480746,4.296457407371244,2.877744249949002,3.542258049766918,2.752748591407134,2.1984941536390834,5.6604951318885135,2.9763636357327616,8.749098443586298,2.1043366598147357,5.40973035386445,2.3305584000308026,2.3950627995175777,6.141391916170729,3.76659516172927,9.707290101958575,8.663593681954685,2.9126498648972037,6.819668183496455,1.6553518286125541,11.880218691860534,9.057883448899627,4.445594291341182,4.326249700837456,4.708739041359579,3.5570424152637194,7.3848269116030085,6.815319245777306,2.2600256559614555,8.198690585668917,4.72137265947682,2.6712933724815757,7.400708693037901,3.6229303509201767,5.137503523749935,5.359661722333877,4.571676809970931,6.199672344836364,8.475936042446387,7.905507349949873,3.8933622107638715,7.537839615916466,5.353323291162897,3.311793717753649,5.2712762591759255,4.778208576398088,6.628919373511362,3.1858665453113337,6.145473670317708,6.023698917266407,3.145677455195635,5.683696454306515,6.109569509370688,11.964503324303022,7.672213165334465,3.8787253414801053,4.8283269264148165,6.010108453474289,7.029563314432658,5.911212201558612,5.9590746011405145,4.0232553523003025,4.234961094839504,3.41006969175638,5.59215800212536,5.982537314399271,2.910732661902913,3.7687136570304847,7.183982678112831,1.0976107966264221,2.6016965164809576,5.511910748670928,5.6570683012015754,2.2868811477881614,6.999098033705607,6.746447139362815,4.83035674741922,3.411426245726465,4.77663042305105,2.550900664647523,5.942279901705763,7.626950122372515,6.862203399053224,3.4019034716079584,2.608809242675524,6.046141781644721,3.3936907641874536,5.390254956321868,4.541638734436141,6.063718640152248,8.226941966807852,6.203788453308596,5.530445354692906,3.2433644256936605,6.703349915166396,8.202417721575182,6.075318692553354,6.401732849522621,3.3305584000308026,6.606294160697713,5.5786368624717415,4.273515889702116,7.668814088419117,3.097610796626422,3.321928094887362,4.593353770980297,4.792334805706692,8.090535901003173,8.367589480412983,3.1358631653686793,2.7070829917717063,7.1593668097437355,4.614709844115208,5.946496941203293,6.783456654360239,8.491372117815171,7.273609132231856,5.944624223055033,3.070389327891398,2.2479275134435857,9.095924419998536,2.4489009511451276,7.048759311919856,6.058532970201611,7.216163813388774,4.46532151358055,4.871350840634026,2.611172380044005,5.783718557790366,7.3824943402931185,8.12401788912289,5.2787282129389395,6.099295204337775,6.1705259991768475,2.269033146455237,3.6392321632492775,4.547819956577703,6.003152448666922,7.816087658600206,6.762348815644129,4.940166750482817,3.01435529297707,5.487357715311547,2.333423733725192,10.642557179595622,2.503348735167504,2.4356285940520905,4.773468927905195,9.760270854333914,8.156942620908886,6.218781167784069,6.306517445249819,8.352882086522959,7.032211069322931,5.5963388642052,5.320845667645722,3.0089887832272546,5.943218087426284,7.3403842354553985,4.443606651475615,2.845991770664573,7.217618485351048,6.137913322088407,7.084383221650577,5.784242222057781,6.097610796626422,3.605257262939004,0,6.954079959320971,4.883620816285672,5.118941072723508,6.583308464847678,6.034963989749303,7.257764968571387,3.91169158187234,9.10522758436473,4.507794640198696,5.001802242633985,6.858478390423273,8.095608004811197,2.3103401206121505,7.6398833131998005,3.144046369616707,3.9420452599160467,4.142413437873741,3.925049964727359,1.835924074254375,6.111448698487674,3.363171077119244,3.127633279725874,3.24031432933371,4.160274831408593,3.5397791916984938,7.571221725048103,5.284292026394312,5.47800105583793,3.7398481026993275,7.1320626778543845,5.576522137920503,3.127633279725874,4.3110671022555955,4.375734538583156,6.4432751117658045,3.91169158187234,3.8042601156347384,6.4409521980296365,5.85872702377863,6.02724253597374,5.212958362728285,2.5058909297299574,7.561631630375841,4.92552476974757,3.5619370596224327,6.237448995639321,7.881113960675097,2.176322772640463,5.361768359419154,4.9941271140031285,2.584962500721156,6.166916251569974,9.4493756610678,6.0332027259372225,10.919861451442168,2.693765712217783,2.4724877714627436,2.7484612330040354,5.633721812641012,8.17707065762942,1.9634741239748859,2.7803100990433753,5.23687524536694,2.698218478224414,4.238022517825721,5.317231700222606,2.1176950426697547,5.604071323668861,3.2555007331483865,2.2387868595871168,3.881664619320345,4.493134922305505,5.357903838552492,4.9597701552114675,4.724650271732967,4.4046306842176115,8.46707564951531,7.096029876307993,7.341896820977107,5.339850002884624,2.859969548221026,6.582706526780788,9.101634319640722,4.129283016944966,7.151270288790165,4.58014548442338,4.628773595201645,3.070389327891398,6.034303767768035,6.339493737901733,5.174326515173915,3.357552004618084,4.809414444235898,7.880195728943098,5.632268215499513,4.79960542245267,2.077242998932461,3.5298209465286954,5.433960931121179,6.078951341394822,4.854494418154875,2.6484654430273142,8.345405246717796,3.4462562298895643,5.595145567990858,6.766065051474954,2.4222330006830477,7.366846735262642,2.9873208659292536,2.6507645591169022,5.940871478189196,3.552131108253784,2.4672794804599825,7.901409979999464,7.459185683748918,2.3868109464722167,8.668955876629637,3.1667154449664223,3.017921907997262,2.4982508675278257,1.7782085763980877,2.9088129077395473,2.1602748314085933,2.3074285251922473,8.316598324108684,5.603477988254004,5.669310286202474,7.991691898336964,1.0840642647884746,5.091276694365222,4.042644337408494,5.96254902292306,5.020591094773247,5.20163386116965,3.065227622775619,7.08533966935737,4.321206566969903,3.5298209465286954,6.083000565676004,5.04176864995163,9.435170178881494,6.4705368650925825,2.0531113364595623,4.078951341394822,5.525755692829486,5.9818526532897405,2.063502942306158,4.110196177754199,7.135657989368776,2.3812833725037836,3.693765712217783,3.00539998774259,5.589164236699772,5.36737106564853,6.38646609286931,2.3248106034204836,3.1811025507537978,8.27891401949994,2.430285272977781,2,4.867402305727543,6.299391206126829,2.6803243568440163,2.693765712217783,2.7949356628035362,6.605701738949982,9.159820892013935,4.255500733148386,5.555509427083257,6.871104373555458,5.632849830117706,4.966707314150316,2.526068811667588,6.09950561709114,2.927896453728821,4.28835856219366,3.1634987322828794,7.07692245939234,3.8429788317883253,6.341274183692174,5.926948247949772,2.538538163629804,6.73226919950145,2.788685710613534,5.508111681193465,9.398936038454162,2.3161457422933562,5.28835856219366,7.479861085016818,6.0925457415435655,5.747655932932393,3.9059284781731347,6.235152624217932,7.3334237337251915,8.870210577263595,3.460742563789644,2.974529312483882,2.6016965164809576,2.2387868595871168,2.7676547982373463,5.659639187015652,3.894332742277694,5.771093251699539,7.173327349650988,6.027463723408577,1.4222330006830475,5.794675787986858,4.720825666089835,5.342696959981934,5.562547724199122,2.62760683812965,6.426767932655541,7.593353770980297,2.5185351389821804,6.676239195086378,4.472487771462744,6.212180210044431,7.178117045706399,7.534497433738168,6.304876050044889,7.545505100691072,5.077670274232752,3.953265239014844,4.609991295212678,2.9126498648972037,7.174426393677253,3.6016965164809576,4.515384460636948,2.503348735167504,8.129128434194934,2.9726926540042644,6.030998118171895,2.2539892662307865,6.597829097826899,6.8209447918806925,2.9523335663696857,8.18769847638827,4.950468414150123,6.1409831048784245,4.674121632632394,6.830610274357907,4.4376272483189485,3.7037651787934096,5.95837871156634,9.296618003796326,3.351910961103077,4.143230134776893,4.707082991771706,3.560714954474479,3.9845893503624565,8.328001662149257,2.1667154449664223,3.1811025507537978,2.4854268271702415,2.931683057059806,4.788685710613533,5.659068274843228,5.738227400367242,4.6937657122177825,6.860962797858116,8.65474342118394,4.814550423461808,6.107687869314374,4.720825666089835,6.4775154361842375,5.619119511453218,4.7136958148433585,2.5533605033353277,4.412781525338476,3.72137265947682,3.460742563789644,3.0197019144425483,6.602587527342561,4.687060688339892,2.526068811667588,4.698218478224414,2.8698714061777126,2.144046369616707,5.394376944957055,4.649615459063409,5.80683958179819,4.443606651475615,5.450881315273369,6.6503337538459295,7.198494153639083,5.228049047884462,3.9231491804723087,1.3840498067951599,4.524815928357506,7.211012193485511,7.027021314622255,7.212958362728285,6.658497136656478,3.3881895371560837,3.140778655782796,4.37364821133469,3.319039815562536,7.33360262826828,5.175524601089875,3.2047667506546134,2.702657543390911,3.228049047884462,9.322806969423008,5.561631630375841,5.8755343515732354,5.032100843167024,3.9873208659292536,3.1842802944193824,3.277984747299765,7.444849247263915,3.6701605141266334,4.595145567990858,3.8369340113210915,3.575312330687437,2.788685710613534,5.4571345943180365,2.4276061727818994,8.563615765629766,4.712045448553693,6.260590274730996,0,2.4594316186372973,2.8257856274647914,3.049630767724601,8.097242070141174,7.431622959713292,4.030336078370959,4.964398632203184,4.020591094773247,2.211012193485512,5.462052318796433,3.3881895371560837,2.411426245726465,5.0430819818956625,3.6064422281316078,3.613531652917927,3.883620816285671,2.8318772411916733,5.081936081719432,5.575917361118149,4.958842675243241,6.716579420796345,6.531225485053606,6.050284014301706,7.73125113789623,2.950468414150123,5.645009884391095,5.4015622072560685,3.840966704487421,4.3298411765306275,0.8155754288625726,3.7070829917717063,5.848748114611959,4.303050084681673,8.796785917134267,5.497931651896279,4.67581593117227,8.189231548647284,6.9092930858238235,4.772941337831336,5.6082178530214595,7.304419775319909,6.588864518190148,10.665637927637095,4.672990993827748,2.8399595874895316,5.305605789585429,3.0908534304511135,2.4854268271702415,2.693765712217783,2.942983598187102,2.5897634869849773,4.631686366311998,3.0125686735030555,5.651051691178929,5.488964817004848,3.1858665453113337,0,6.703073006328523,4.255500733148386,5.9593064897596575,5.218781167784069,3.968090752045256,4.8594726668519375,3.460742563789644,5.341274183692174,3.0089887832272546,2.032100843167024,2.169925001442312,3.356143810225275,4.814037647539975,3.4168397419128294,5.653060017104565,6.074462620704536,3.2016338611696504,6.9656688644419456,6.816215687622865,4.330558400030802,7.16380060395775,5.6061460780657235,6.093179846765504,3.5716768099709313,5.342341397431548,4.228049047884462,6.310885391185277,4.751677945687579,7.502871574574138,3.953265239014844,3.975446765640962,2.6599245584023783,2.430285272977781,5.7303682363490065,7.689648608288815,4.371558862611963,3.215678596607928,6.492173659610827,7.480507489974544,4.436295119839363,4.804260115634738,4.405311682597445,13.396289023308677,3.8914191868460786,3.24031432933371,1.899175630480513,3.2942531364445142,8.382105211486744,4.143230134776893,4.92552476974757,2.8439838440483265,2.8399595874895316,6.101187815259738,5.0232553523003025,4.583158003874408,3.176322772640463,8.996303999184056,3.8257856274647914,4.7755773609306535,8.12401788912289,5.9618548076361595,2.454175893185802,6.208088008224774,2.673556423990145,2.8972404255747994,6.791683858152045,7.137810883416096,5.8052924556007115,7.338067797542616,5.198494153639083,2.851998837112446,5.726013748860151,6.567880042052731,2.8155754288625725,2.2387868595871168,3.6769443591069124,4.318316841334983,2.1795110502715103,5.6884601404244926,0.8073549220576041,7.244982170376553,8.642268351758643,6.836681593327116,2.73768676140986,2.4646682670034443,5.964860664245848,2.7420062108667365,2.411426245726465,7.279842693520348,7.446835177086041,6.531849285751203,3.8982083525087177,4.721919445551544,5.500164679492167,5.429615964201735,0,3.7311832415722,4.726831217032493,4.904002316283692,4.561937059622433,7.438459204423481,4.631686366311998,7.300764373248062,4.112700132749362,7.9329824099498465,5.1081062236861285,5.868143479666726,4.161081482277184,5.996162742681061,3.097610796626422,4.263785613890803,6.339493737901733,4.370861740085285,4.842978831788326,5.636334602442182,2.84197311892718,4.7713574089905615,4.257010618206024,6.916596067892185,6.015247773698937,4.595145567990858,6.7410624357490905,3.4168397419128294,3.823749360308273,4.279471295644468,4.7114949066500875,2.883620816285671,2.969012307516316,5.089159131911238,2.3362833878644325,8.080604371320732,1.8559896973084806,6.941341105485299,8.14639047567022,7.163498732282879,8.281651890258825,6.329482431025636,2.140778655782796,2.3103401206121505,9.500782139394977,4.674686619927999,7.249066033855867,3.1060132376221192,7.897179908573977,7.6546360285279675,2.944858445807539,4.94673086014031,2.711494906650088,6.054414387917307,2.788685710613534,4.198494153639083,2.9467308601403097,4.060047383669939,3.442280035252584,2.7070829917717063,5.9586107120558465,2.4982508675278257,3.6484654430273142,5.0870384567669475,5.130519083027338,1.8155754288625725,2.6064422281316078,5.673839055990439,6.37364821133469,5.609991295212678,6.076388068512841,4.695437110405369,7.459267666703387,5.211401637418471,3.063502942306158,5.931210274803932,6.819540460505722,0.918386234446348,4.878234879112339,6.098663781886574,4.098453246309274,3.4154882710497003,9.8361608416947,5.3103401206121505,4.20633064787105,6.436128517259249,4.381283372503783,5.0259146987507926,5.226123143125237,5.73903797911741,4.110196177754199,3.8042601156347384,4.503984703897613,6.067165427017112,3.7322691995014496,6.121844297876376,8.248591759535907,7.711013010084339,5.782670658677007,6.0774566524005875,7.592232766722112,6.917909073852982,4.423578170981798,4.901108243014512,3.795974694206668,6.907251224292033,3.189033824390017,2.4646682670034443,6.061128135118254,2.733354340613827,6.664198369291911,5.166715444966422,8.492293852480074,7.279749853014451,2.807354922057604,8.362689046726734,2.3248106034204836,4.826294245499635,2.321928094887362,4.238022517825721,4.232660756790275,5.51127826508303,3.7644735509926663,5.290940402403678,7.535353147842131,4.329123596291566,5.908092340818271,2.570462931026041,4.634012356402116,6.99491908871511,2.560714954474479,2.3589588258323295,2.8278190246173196,5.439623137557117,2.927896453728821,7.476543706214256,8.334094473911568,6.726695004499616,2.097610796626422,3.619413010597937,2.523561956057013,7.136683577697236,8.908992993250534,4.4950555283680185,6.970393537914677,8.089159131911238,2.996388746447621,3.7506065048355923,5.468583317006828,4.732811872082943,2.5360529002402097,2.729008870337863,7.712939631305699,4.038260575175349,4.514122260170708,6.948834425578686,2.0036022366801953,2.169925001442312,6.04023491675437,7.246788093844365,7.018923431842335,5.046141781644721,5.637784110291324,6.976363635732762,5.840463233869542,6.170125361839952,4.760753208062987,3.089159131911238,2.8399595874895316,2.3978029618624896,2.3305584000308026,3.0925457415435655,2.6599245584023783,7.9217814432688,5.963242904318827,2.4515408330178317,5.976821852360685,1.7865963618908067,3.221877081077034,3.0036022366801953,2.722466024471091,4.2024177215751815,5.9025560053446275,3.997292407636508,5.323730337521348,6.463851292611099,5.938756261492295,7.593951283948411,5.233044401265581,4.79025073883523,5.718361626138354,6.435295215647167,1.3504972470841332,4.12267271882414,6.361768359419154,6.8820316083521,2.0250287944915226,2.298658315564515,3.636914580355878,7.934634441094905,5.246408087246385,3.918386234446348,3.929790997718597,4.147306698780294,5.169925001442312,5.542258049766918,4.693208148910016,3.6971065744769747,5.707082991771706,3.363171077119244,5.035183996618336,4.274261661257048,3.321928094887362,5.9740703670240265,2.3757345385831563,10.174663577413781,2.8439838440483265,5.24260250592898,3.4581194811745064,2.063502942306158,3.1858665453113337,4.7692427951405385,4.301587646603187,5.986638471730853,5.058749412335524,4.556429415449573,2.5360529002402097,2.4489009511451276,4.974988111991927,6.248307120087241,4.346956889378885,2.641546029087524,2.8579809951275723,7.624539603781834,5.197314999473818,5.38404980679516,3.2387868595871163,4.464668267003444,3.955126781261366,6.258895755075436,3.1358631653686793,7.831180547126736,1.3785116232537298,4.934044647112246,4.503984703897613,4.100977647724821,4.86195536414487,8.439830883981392,7.851624127346895,6.091065077930535,7.190022174802645,5.992541859109112,2.983677694698067,7.161484638641659,2.6959938131099,8.21003812347289,7.461643148073906,4.848497755385146,3.192194165283345,1.6690267655096307,8.80290404741688,6.926948247949772,0,3.812498225333564,5.6806056662025926,9.871443254934135,6.459923362676901,3.8298495598446904,5.157447996274298,5.537296067090842,3.5147534984397533,3.2249663650002742,5.699884728697476,3.035623909730721,6.914444931746792,5.338780943894699,9.013601832683456,7.578712331048165,3.3405622690264134,2.3950627995175777,7.221587121264805,7.78959885005233,2.2234225499349374,2.8439838440483265,2.950468414150123,5.833649127594984,5.802968651005583,4.618238655595454,7.581803148516525,4.948600847493355,5.416164164733128,5.012121672712217,2.9373443921502322,2.3923174227787602,5.259649120649155,7.119252412157343,5.452858964713811,2.5160151470036647,2.788685710613534,5.040892430646901,6.390942772802543,6.032541697270038,6.177519202597814,6.111865963867557,7.0083165637349865,6.074034394212556,4.471187460386985,9.507715369062607,6.152994605492435,4.169123281476757,4.706530552538117,3.0426443374084937,5.305605789585429,6.942279901705763,2.4724877714627436,11.02948049374083,7.127736443588829,3.594548549550354,3.8063240573900288,2.144046369616707,7.036942844798256,4.022367813028454,2.0214797274104517,8.589726041133547,6.218781167784069,3.685940148445977,7.566662962962611,6.652343078740216,4.210232990095849,9.47854718263086,5.991861930556323,7.500961352460057,5.5855634984556914,2.319039815562536,2.9049657186840263,1.9781956296816516,5.509062386361898,5.592756010441026,4.39780296186249,3.431622959713292,11.193297418854996,7.925940095994448,8.041385369486115,5.051807107013298,2.3673710656485296,5.208673319629471,5.532940288372874,4.598722499676621,2.903038270112912,6.098453246309274,2.967168607532628,4.919340082442012,7.971830914780502,4.189033824390017,3.10936055940423,4.053111336459563,3.944858445807539,5.71699089440494,4.2787282129389395,3.7246502717329673,4.568032104771279,6.400025517914023,3.0443941193584534,9.140190703191836,6.7998644108818525,3.3923174227787602,4.880685525261125,10.32396671481508,2.521050736900963,1.8836208162856714,6.547203024756931,5.63023071585969,5.915042791371977,5.713970692436053,8.823876711241038,2.6530600171045644,4.462706750670158,4.524189078449365,5.342341397431548,6.70362677086498,8.008596693255853,5.81326814176608,5.795715006501729,1.5897634869849773,2.5058909297299574,4.153805336079036,4.707082991771706,5.617651119427331,5.7615512324444795,6.115615931428109,5.51443791382931,8.873136468833458,4.8811751553492755,3.4276061727818994,7.013685569928653,3.37851162325373,4.390942772802543,4.544114402209474,8.617283787779035,5.39677601078753,4.096767854714689,3.169925001442312,2.881664619320345,3.389566811762726,5.949301468328278,5.9095331149468935,2.2172307162206693,5.687900522480746,8.295998461911388,4.963011647599427,5.648177795724818,3.8509993947164736,3.669026765509631,5.924575003392735,1.2570106182060237,8.1184220243132,6.3103401206121505,4.59215800212536,2.169925001442312,2.771885578515366,5.309976492370556,5.718909554379232,7.2325648297287755,3.720278465233327,3.403267722339301,4.138323004056484,8.321792835886914,8.384999544730041,6.675392543042777,7.848935855524565,8.135247549809566,7.474760499576783,2.01435529297707,4.661635602335959,2.899175630480513,6.950701690089008,3.4776773275653072,6.275752048828529,2.7782085763980877,8.645802514606062,8.927066807759255,5.377123749129487,8.785648628652076,3.554588851677637,5.8529975876133165,7.093391153253013,3.8639384504239715,6.390082950951663,3.0214797274104517,5.947198584262056,2.0565835283663674,8.560600329022595,4.708187236020708,3.1473066987802936,5.260778431893426,2.3978029618624896,5.496654082593496,8.356011721502778,5.307064162255372,5.485426827170242,8.478971805032943,2.414135532984451,2.776103988073164,4.7420062108667365,2.4828482830684653,6.543805175964397,8.5918962955207,4.361066488794321,5.294620748891627,6.553360503335328,7.088629255297176,5.1201860275318625,7.00943675560809,4.090853430451113,6.7014104362328135,7.415741768290091,5.348728154231077,6.705563274566321,5.445263208140443,4.552131108253784,3.4436066514756147,3.5631581304028077,6.678494507773734,4.478971805032942,6.1657109924849305,3.403267722339301,4.534186139090097,3.0214797274104517,6.238786859587116,1.8639384504239715,7.020924396158786,4.20163386116965,2.7548875021634687,2.604071323668861,7.128870759465125,2.5459683691052923,5.414473836430928,9.995102877826515,4.861459166361513,5.25285462595139,6.31922050252387,5.308156975114762,5.526068811667588,5.763145958149476,5.903279342056139,5.825531251189622,7.048214384608036,2.513490745588118,5.944155663440363,5.856985689782205,3.412781525338476,2.956056652412403,5.664482840364682,1.74416109557041,4.886062338345599,2.4646682670034443,2.4515408330178317,6.433126376206435,7.7755773609306535,6.705839705883682,8.330737650206952,4.727375938594897,5.777945670695148,11.69784504363786,7.7627474419531515,13.587327482778312,6.558114538598396,6.332349900373874,3.2265085298086795,2.875780063068488,6.70473366267406,4.223422549934937,3.2387868595871163,6.136683577697236,12.23806432823473,5.572889668420581,5.849999259466098,5.533563348214512,5.325530331567558,2.797012977836145,4.150559676575382,8.298429210890312,7.731454807698346,8.903490246967104,2.8094144442358986,4.7824085649273735,8.461602224618334,5.583158003874408,3.060047383669939,6.429950657403768,7.193278935521388,2.3589588258323295,3.7311832415722,2.899175630480513,2.298658315564515,4.235727059838059,2.3812833725037836,11.663771557482411,5.684257705058932,5.212569338850806,6.9787676510367875,4.044394119358453,6.092334311186202,9.582254908357664,2.5160151470036647,6.402415216824399,5.740387932468353,5.784503982929566,2.6064422281316078,4.362469888750209,5.164705840182799,3.4208865749755315,3.948600847493356,7.965841991309597,4.976821852360685,2.722466024471091,2.7398481026993275,4.528571318870758,7.677578712165378,3.41006969175638,2.4905701304462013,3.5147534984397533,7.5673476962842585,6.849624029924543,2.833902076669163,7.406162478777607,8.131085259578953,3.176322772640463,8.148324042480084,2.657640005207824,3.350497247084133,6.8203066288938246,4.483493351284395,1.713695814843359,5.872582544833223,2.722466024471091,5.7742599514327635,5.704318677760832,4.760220946466509,4.294253136444514,6.037162550127378,7.151777654732053,3.215678596607928,4.742545233866053,5.613531652917927,5.727375938594897,7.7780114016125745,8.009884588931843,6.549977142771825,2.0461417816447205,2.0565835283663674,6.66746640495737,4.62760683812965,7.884597920990064,8.571145863602528,2.3161457422933562,8.563272552819562,7.111970261358359,6.001577085389535,3.4369613378336026,6.435461914479276,3.5084286525318573,3.940166750482817,2.5459683691052923,1.6644828403646825,4.94345253855882,6.003827078111476,4.960697039304312,6.684117412839159,5.612352498752662,5.609991295212678,2.62993940943954,3.611172380044005,5.522620761042079,5.891419186846079,3.6530600171045644,3.4581194811745064,3.1667154449664223,4.581351247168778,6.764340846670181,8.066089190457772,6.614857050384495,5.767654798237347,3.1953475983222193,2.9726926540042644,5.279471295644468,7.5105663894487495,5.02724253597374,8.25176642434399,7.822411496360477,5.334496768390418,5.715893370547605,7.3049672876759635,5.998646839005795,6.9725777851765525,2.4594316186372973,5.666472568842073,3.8728287595348854,6.052024560482831,3.631104282365877,2.3015876466031866,2.37016428054021,4.3384244147835584,6.546122758857796],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle ElasticNet()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_pred_logEN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression elasticnet\n","# réglage des paramètre pour la gridsearch\n","alphasEN_log = np.logspace(-3, 1, 30)\n","l1ratioEN_log = np.linspace(0, 1, 6)\n","param_gridEN_log = {\n","    'elasticnet__alpha': alphasEN_log,\n","    'elasticnet__l1_ratio': l1ratioEN_log\n","}\n","\n","GridEN_log, \\\n","BestParametresEN_log, \\\n","ScoresEN_log, \\\n","TotalGHGEmissions_pred_logEN, \\\n","figEN_log = reg_modelGrid(model=ElasticNet(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=TotalGHGEmissions_train_log,\n","                         y_test=TotalGHGEmissions_test_log,\n","                         y_test_name='TotalGHGEmissions_test_log',\n","                         y_pred_name='TotalGHGEmissions_pred_logEN',\n","                         score=score,\n","                         param_grid=param_gridEN_log)\n","\n","print(BestParametresEN_log)\n","print(ScoresEN_log)\n","figEN_log.show()\n"]},{"cell_type":"code","execution_count":120,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"y":[2.056769215506322,2.0566086728169926,2.0563881083966544,2.056087230029906,2.0556789087106067,2.055126012810906,2.054378290437526,2.053369765565688,2.052015493958006,2.050207014081483,2.047807137761386,2.044644651243798,2.040510012826579,2.0351549972768024,2.0283018597007634,2.019669897832422,2.0090272202880564,1.9962707022912454,1.98152645232858,1.9652489450119295,1.9482856659859933,1.9318726786307798,1.9175369281227792,1.9068991480843653,1.9013932759709384,1.9019483408492293,1.9087203569558924,1.9209904696384288,1.9373080606192232,1.9558424810836534]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"y":[2.552660292004693,2.552191705836738,2.5515488544368496,2.5506693261172857,2.5494692478372722,2.5478354562043464,2.5456163154238673,2.542611166746548,2.538557324014951,2.533114018938555,2.5258436406026985,2.516191253816232,2.5034656200066383,2.486830014720922,2.465318721055201,2.437902561817575,2.4036284503844954,2.3618463110665315,2.3125072941264846,2.2564746716661213,2.1957502462905873,2.133507894933205,2.0738592865714316,2.021356144714287,1.980339486189812,1.954321188261615,1.9452874679582908,1.951702034927463,1.9674015542130685,1.986457084399595]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"y":[1.5608781390079516,1.561025639797247,1.5612273623564592,1.561505133942526,1.561888569583941,1.5624165694174659,1.5631402654511843,1.5641283643848283,1.5654736639010611,1.567300009224411,1.5697706349200735,1.5730980486713637,1.5775544056465198,1.5834799798326826,1.591284998346326,1.6014372338472693,1.6144259901916174,1.6306950935159592,1.6505456105306755,1.674023218357738,1.700821085681399,1.7302374623283545,1.7612145696741266,1.7924421514544437,1.8224470657520648,1.8495754934368436,1.872153245953494,1.8902789043493946,1.907214567025378,1.9252278777677119]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"xaxis":"x","y":[1.8282846658174055,1.8282746040529332,1.8282616676020123,1.8282433510186062,1.8282165894888602,1.8281788131131749,1.8281282961681073,1.8280634970882663,1.8279828806363039,1.8278857939141921,1.8277736660532495,1.8276513867624644,1.8275291523728023,1.8274252492858631,1.8273706056731418,1.827416201424678,1.8276441866901518,1.8281824581819257,1.829220291603965,1.8310199599205061,1.8339179997436614,1.838311851291639,1.8446307315702826,1.8532875706409015,1.864603190279853,1.87870098476681,1.895399377580595,1.914156563117058,1.9341128966125136,1.9542312715291033],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"xaxis":"x","y":[1.8285135955276344,1.8285184162992092,1.8285248606629856,1.8285344775834027,1.8285493883015915,1.8285720115283712,1.828605322815226,1.8286539115085716,1.82872526629186,1.8288312237053943,1.8289902288093578,1.8292308566966164,1.8295969287144946,1.8301545372773154,1.83100094628729,1.8322744274801508,1.834162517537029,1.836904318311359,1.8407815015878857,1.846094387801136,1.8531250386155467,1.862097130743497,1.8731461996649545,1.886306317533658,1.901503178575256,1.9185362895108022,1.9370479811089054,1.9565022610637048,1.9762056256102407,1.9953844384202442],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"xaxis":"x","y":[3.046039873317801,3.0452640626529357,3.0442000955271333,3.0427434257670303,3.040753396438889,3.038040824625818,3.0343527827036265,3.0293540118332887,3.0226040469028446,3.0135295964081648,3.0013921580020675,2.985252231985718,2.9639354839428296,2.9360144513935724,2.899831900598129,2.853604563825257,2.7956492369771673,2.724754656494305,2.6406740919057325,2.544642087943533,2.439751759588415,2.331005265174363,2.224899726814095,2.12853504600964,2.0483870102730695,1.989020361539796,1.952104757597633,1.9361190932279257,1.9369563939757488,1.9492202314419085],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"xaxis":"x","y":[1.839407889718243,1.8393637253466328,1.8393005644219622,1.8392188547426638,1.8391197324562618,1.839000675794445,1.8388539361791616,1.8386706190701467,1.8384450068889289,1.8381757421503728,1.837866901526002,1.8375300594388866,1.8371868023065538,1.8368721040949207,1.8366396741658244,1.8365701696746486,1.8367817678673022,1.8374405373933196,1.8387667457342427,1.8410342623569422,1.8445639252859818,1.8497152955064486,1.8568779661583354,1.8664494603006283,1.8787734051093719,1.8940247319889671,1.9120740427004854,1.932402611675083,1.9541283545492085,1.9761475289790107],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"xaxis":"x","y":[1.7416000531505254,1.7416225557332516,1.7416533537691783,1.741696041037827,1.7417554368674302,1.7418377389927218,1.7419511143215094,1.7421067883281671,1.7423202690700923,1.7426127142292924,1.7430127344162536,1.7435587213353054,1.7443016967962133,1.74530864433234,1.7466661717794316,1.748484126757376,1.750898392368631,1.7540715410753174,1.7581896308110747,1.7634540270375285,1.7700696066963613,1.778233850437952,1.7881300164062275,1.799917345936999,1.8136995956171427,1.829459336439771,1.8469756257918437,1.865771819108373,1.8851370323484042,1.9042289350479997],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle EN pour le paramètre<br>elasticnet__l1_ratio=0.0<br>en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE ElasticNet pour tout le meilleur paramètre l1 ratio\n","FigRMSEGRidEN_log = visuRMSEGrid(ElasticNet(), 'EN', alphasEN_log, 'alpha',\n","                                 GridEN_log, BestParametresEN_log,\n","                                 'elasticnet__l1_ratio')\n","FigRMSEGRidEN_log.show()\n","if write_data is True:\n","    FigRMSEGRidEN_log.write_image('./Figures/EmissionsGraphRMSEEN_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.5 Modèle kNeighborsRegressor"]},{"cell_type":"code","execution_count":121,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                          paramètre  KNeighborsRegressor()\n","0  kneighborsregressor__n_neighbors                     35\n","                             R²     RMSE      MAE\n","KNeighborsRegressor()  0.416219  1.55157  1.25722\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_pred_logkNN=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[5.078848808243546,3.446225872498873,5.4795977825470015,4.104609357817115,4.31742924118592,3.6238853196995318,3.9126037858517253,5.926420553943785,3.680371199706875,3.2595616387402577,5.655615430131665,4.090807879601735,8.991879986413307,2.7258369332379586,4.545663185024637,3.578274654079002,3.256616683622807,7.527008567470206,4.996522771063884,9.03191726915845,7.884547912187112,3.62741915715301,4.231780920364385,5.32468580152749,7.701896423876427,5.222699959810584,4.711054784554862,5.416688363568362,5.375598731841453,4.930316548644388,5.430659543947817,4.485616406522956,2.75313938528356,7.480830648390848,4.394133734692858,3.8607964651272004,4.080031473863135,4.328115851355084,4.461934204128718,5.275405685795339,4.7187624337915794,6.948209344525291,7.37518032643019,5.457974497973948,6.355070857908861,8.088106076632911,4.905900447616405,3.957170091760342,6.618774389217685,6.02593176318848,5.816433836328898,4.653598148133077,5.290626883351426,3.9156883016830957,4.10476437342607,3.705621955843566,4.710662063074621,9.335396364937976,7.5833890032897235,4.946723161115563,4.569434276122344,6.267267580906726,4.635746784950948,4.317163818855893,5.3447884471016325,4.239761236647085,4.833271263765824,4.015626742666579,5.251954190877186,6.116445323635277,4.187531238487419,4.086286828274175,3.9047915018330954,4.545055676113698,4.376108047248365,4.9400461185142595,3.7205456944075115,4.135485374779464,4.046702258384559,7.320916981148394,3.886315786123844,4.0813464042294925,5.212670266621428,3.187990312893515,3.807624176620153,5.1501061867315325,6.099378742358116,4.575433964469764,4.054631972846838,3.886855123518405,4.824844859815054,4.09268999249449,4.722628486797343,4.602012566393484,6.890214530066554,6.265563521235693,5.135200857015077,4.234716844786524,4.4590195455273705,6.347919099903376,4.4860194702955924,4.846867336247294,4.207275549203311,6.675539288608978,4.8699128668616245,4.938091625239964,7.9409818697425,4.965922529374217,3.2040270097536343,4.420789033199608,4.648003728866227,8.109476290535058,7.339500006737302,4.697254331003025,3.5331834299503377,7.489388680849982,4.460387416801158,4.22390710286451,6.184463036967113,6.790219421495739,5.43717369634822,4.1594551514591895,5.052647605721552,3.9479299628258566,6.282085459632181,3.221845976534451,8.821143004545098,4.006281661617754,4.710859447646423,3.573011969136473,4.534321025081469,3.654023259821426,5.643685447487864,7.361428533455094,7.402707000963879,6.1570992035093415,5.674937889521231,5.4714016738709965,3.427070638486843,4.353634199077635,4.962824332004734,4.07443068248392,7.86321060500882,4.358831401459158,5.381675760294063,4.030980489454153,4.231621353341928,4.44790145424616,7.137173200369129,3.6226419625717248,4.017207609263725,7.073063367185111,7.719828715203164,4.976164113313412,3.630103575782691,5.689635353457253,6.463406881239651,3.9363412014380597,5.8954280906686645,4.801130577190487,3.935185730867536,6.841107489562368,5.32840980929564,4.77864015222819,3.7564970563853,7.570807501179506,6.489082116877719,4.849243794644988,6.061639594750208,6.997496581303888,4.288639739396051,4.994699496489253,8.589535196992955,4.189532322361878,3.3121562005588947,7.169063814304825,4.294201326138286,5.475167882360902,5.479597782547001,4.8693312543577925,4.413622063774997,5.785082970422246,4.60681938087969,5.696928376343744,3.3242006868754634,6.344593378439772,4.883910023379748,4.186408014356791,5.261456735309434,4.629992922830948,4.742266346439615,3.740709310874414,5.123427689871644,3.7442573322526673,3.837407331707769,4.51312800611682,3.924019290731643,6.28863803589152,4.824634355781241,5.485037490508456,3.8944303724739444,6.750095201826662,4.680295328971115,5.350749023003691,4.691549444503028,4.7774044852519495,4.960208815558327,5.645382230380673,4.965804937781688,3.3872038113643694,4.790847247186683,4.016999196628427,7.607347728988907,4.238944101564117,4.723407552086428,4.8077944346722035,3.8742292751494185,5.574163314306519,7.504612785496923,6.044390173737751,4.830516123689321,5.243080076199936,3.978053604884041,5.329305847779332,6.607540352210222,5.899633736049402,6.07547825697986,3.4033016084717382,4.217630104803824,3.4940231173476426,5.3935395312636585,5.526948316816332,2.8231390094561757,4.289251786573694,6.083025229080861,4.196398193058303,5.779611700953366,4.694585854303663,4.44790145424616,6.283881582752803,6.284317297253142,5.6682558581729285,5.80187007457645,4.081208914935554,3.9818928122691397,6.408514186207108,3.8607964651272013,5.756336032573686,7.822793841365071,4.731913442132624,5.878839481518139,4.173768459910729,4.809541526091628,5.366166842485127,5.188382277991494,5.164299719871322,6.2691628543045805,5.457464584766124,4.745685699151363,3.7005594545138307,4.2610551577945985,4.76226271323786,3.7834804302537464,4.594982293630409,3.604117400604378,6.914611131266281,4.406185449308197,6.825922672760335,3.761408121404084,4.934708645379874,5.929564144494571,5.0878368528692475,4.104609357817115,3.336412975439566,9.71993155707433,4.49350768621649,3.6078831867282073,6.15560430007168,3.5971757526261317,6.370378638091253,3.4207540066149282,3.6927000748159524,3.8499996208683473,5.0695099783527064,4.17776099400576,7.5358795854057545,5.117716214826424,3.3128955372405837,6.169366008059606,4.186387628401023,5.02685961310778,4.110653246505964,2.7258369332379586,3.787247057181456,3.5428934576945164,3.4378069585463975,7.766731401064206,5.588967934225832,3.336531136287696,4.1272960251171655,3.9264288649341665,5.819519887439263,6.352862417982034,4.743275045726757,4.635320946628381,5.5085755282292155,4.571718213554022,4.174504353003605,3.9953548587032115,5.5404706157918,4.872789416023203,4.576271759270123,7.139231185995652,5.477506710266342,3.0400400806328847,3.5835216105937002,4.400550642830941,5.388494030968766,5.221999241596618,4.153205114680055,4.95624479632287,3.2308913891926125,3.5612215220171892,4.362175805416295,5.621980229109591,3.444643197681642,4.644697256570369,2.987687957403748,6.8480750474988765,6.137251281679181,6.171710131045436,4.899258351625193,3.9517557915555415,6.153225535802467,3.2166096960569224,3.422442996210746,4.218321979185544,5.579045692436912,5.346407126538627,4.545055676113698,6.506708857598275,5.159393061859019,5.928170557106888,5.993180619732287,4.0109887844523096,5.135369511917858,3.4684952085055336,4.652992946453684,4.482958628675815,7.5787224682166165,3.702793493103138,6.208046104084301,4.112049291975112,3.437806958546398,5.393918411420033,4.104609357817114,4.121834255510178,8.372564642679809,3.3557128129648293,5.1501061867315325,7.818765582630762,4.0481021326184035,6.9299120043050335,3.6573399840684364,4.831695671436156,5.026280846595526,6.43808220309262,4.906711786972082,3.887673877188344,3.810822027875493,2.558022230656676,4.072566135893636,5.059103920641071,3.7263543213219132,5.505953585411319,4.8025376826830435,6.403323558831414,4.70476479088703,6.759739130490868,6.149116624667086,4.153486876265143,5.045358986276456,4.073299974047012,7.405883794121928,5.099409232595946,3.9359430115113097,3.998876758682041,5.806415577199522,6.31862212861451,7.430124691668993,7.511702101986483,6.519065971005782,6.815319935545232,4.84561742957369,5.629303053006025,6.191547396565208,3.5971757526261317,4.635859729135506,4.662116779413573,4.653385634281758,3.4914848526269515,5.407503005467896,4.730923316686498,6.166587451161882,2.5580222306566758,6.1822984786727195,4.812782299095357,4.6754500821936995,5.693783593150581,4.31956198241264,4.330208345030004,3.630103575782691,4.563157183956332,4.000912930530766,5.023930505875196,4.922788583660632,8.811305173934418,5.095034021931442,3.8461462210345556,4.926401617892916,4.0109887844523096,3.939143955242558,7.375159363596528,4.08457794954677,2.8416012247405793,3.402747718530359,3.8742292751494185,3.033644241258488,5.3452600581374305,4.607339596906679,4.609787818004089,4.334612611465924,7.1149322010277025,4.973184593955883,4.817947740683569,4.428952642727085,4.2346646873269,4.799231635014029,5.888899837367964,3.438086009707019,4.932030193190055,5.194118734727063,5.180049065271052,2.868554486129599,3.850979595459528,3.9021638178185722,3.9825126812088447,4.7778837881788245,3.5628127255323907,3.548290844274326,6.365498689895211,4.701999226280886,4.758040808714581,4.174801883299941,6.049781878451037,4.919391516509147,7.422342047243967,4.636603835864188,4.9015960800147695,4.686689200873997,4.864824935527501,4.878311095237894,3.798117910845922,6.252361504136692,6.507607521617811,5.390550443801932,4.910525107003325,5.661321092676469,4.11481950677674,5.133626763340611,5.279202559671316,4.499782867972586,3.9267667872246435,4.367191180369714,6.558469964951741,5.4647421946460595,4.010758160499105,4.881363252559499,5.751255859601478,4.409470278671893,4.6855844942605085,7.440589638465066,4.676799199771704,4.081208914935554,4.281284333022068,3.8478101297753913,4.5060975402358405,5.685494797578006,3.426464878131772,5.567134724430091,4.393681857387961,4.496608837072345,4.98379223347089,3.1112061945987106,5.916698950734927,4.421695789530945,7.896119714269082,6.324554407783587,4.632722559740573,3.9220095106216677,5.999349418344562,3.935185730867537,5.197859194322917,6.282085459632181,3.5183950911057185,5.839524369832582,4.691070291147861,4.752996277038496,4.389441691847959,3.5335031050242542,4.501875298649468,4.905023020156638,5.611897670073441,4.911270622266028,4.57725806164744,5.615826864569662,6.561010573487205,4.057987609912138,5.706413886285645,4.616835443140299,3.5063778991001278,4.844014391921485,3.7366630093952318,6.300658133085785,4.05923924054144,5.5573993300544196,7.358388719690764,4.344418070493399,4.401918265518242,7.852665882278505,6.763671724639205,3.8273750018919617,6.106090560579716,4.612167199949584,6.201788339255524,6.206447332077025,3.804565992668636,3.576761319917002,4.989895214666642,4.95905108794764,3.557043252133626,3.3605405484175783,4.567043927736861,3.798543047679568,4.492671094221007,4.942852016873686,6.118702282848349,4.386095051439685,4.674517344665464,5.578393642202004,4.5788802511112365,3.7366630093952318,4.031276870900647,5.982990843436869,4.669538900318738,4.766031698165321,3.938042871010981,5.539368544607815,3.4935975120928315,3.6568674295303936,2.9891654533332277,4.6814077097998235,3.74370797904544,5.316277135515818,5.389443448054801,6.024869809971169,4.224937240668439,6.215958753164369,9.025602365207284,4.73206097786177,7.056366050158352,6.40105310584804,4.597792084255178,4.870082462405995,2.612730153562612,6.10830585100625,4.661878060397479,3.429608311319055,5.323236108662758,4.014976619679847,3.602045626278649,6.099378742358116,3.437806958546398,5.084839386584227,6.98289579171757,5.320182220037874,5.418746179138012,4.362800540821367,7.670372088354655,6.358674810604389,4.035474548138356,3.303632758910612,10.069079524318123,5.730459923748734,4.35349687560768,2.8681670797459877,4.536219261618027,5.408133797560598,5.862541816121198,4.812654636845996,3.8144038723688123,4.645594442979989,5.982990843436869,5.138800132504383,4.34313267554992,4.988851818911605,8.056644247217044,4.7592394107057014,6.9088447891777545,8.925681781233894,3.494157830704499,3.5056535174581263,4.278519519530018,4.19113931864896,3.8365965609263655,6.272660320503721,7.184283377080452,7.572506809917019,6.425718367363207,4.405498654631542,3.8509081874962043,5.24877427800503,5.8448158489336794,3.9099960920128956,2.7258369332379595,3.9403147364805973,3.5331834299503373,3.7366630093952318,5.135807044117592,4.019835250471024,5.15359261563885,7.4742865062489665,3.6399356216918575,4.229677627790368,3.229491558984623,8.410327884677281,3.8879592002102834,3.0242451064987845,6.2974666693121755,8.72477822899772,7.544631618616916,3.9032711805080216,3.7287922768402253,5.181667856112413,4.179824603866732,4.2064097360328505,4.646983068379319,4.167854242542624,4.994699496489253,6.004195928267386,8.353928495867128,6.374195416357767,5.3908393578106155,3.5035010295483833,6.625156661741154,4.734228431490451,4.595606160367555,6.058993552620066,6.309007298395962,4.997153993522233,6.495172175821569,5.1999897370938895,4.425928676018807,5.648943179398127,6.108531410048007,3.6409186346649256,4.317163818855893,3.7155734302755086,7.510474234096075,6.03804194345236,3.2287570668431447,6.541051450377287,5.483315759162958,6.648443491602516,3.5814424082029452,4.119556873217763,3.3883772638298826,3.945788253714553,4.285532828947326,3.6391424506635506,5.070468112625973,4.6102632355905,5.1961286912350095,6.81391379386217,5.991489368489481,5.172510254529788,6.44274738521342,3.96297836013931,3.711228009229109,9.300104366347249,4.519255743341711,4.654486100316647,4.750540652400811,6.541051615093953,8.11550221823517,3.477483094756609,4.673627484594551,3.5346451775161776,5.047885499678691,3.713011873341533,4.517434628955159,4.974464792693079,5.780209689282867,4.674638214499745,4.307167027032923,4.333570773015715,3.6986844131055143,4.850885077551307,5.768352918766447,3.8978972474186997,4.098682660177474,3.9097462093736395,5.022520724323699,6.427892448165191,6.143543540143529,6.177256534581016,5.2047758146391905,3.807624176620153,3.440528869656325,4.323023965494638,5.409070032738116,4.395312603741497,3.740709310874414,3.7029831865885945,5.930407213101125,5.115051920292164,5.08369093133878,9.540987203228523,4.171290128579118,5.396429199865999,5.004932914402446,5.685871161583868,4.409857072573305,5.3170151693546455,4.575384137347977,5.341743819933127,4.458869953817706,3.437806958546398,5.93368067076329,4.584489245931356,4.773827270617008,4.333503080662217,5.062608418440276,5.862802407234869,4.741691232846844,6.612514537514175,7.784392520601882,4.594878022312308,6.94787790063015,4.376323230226291,5.624766172177624,4.20868881861437,3.437806958546398,3.939143955242557,4.318570215114349,6.099378742358116,5.752919947443439,8.535848173407167,6.394901198619447,3.561221522017189,5.781393740733165,3.558436404695828,4.938656566940127,3.211877421662083,4.900822536493101,2.5749090025175576,5.383438022863349,4.128258467325917,5.496079244219904,5.731498828654943,6.098269718804275,3.9267667872246435,3.5056535174581263,6.312896587979667,5.859634126827,4.080927648442233,4.237511780205309,4.44069547720472,3.2888772132954194,4.285532828947326,6.474780414592748,5.8952373389292205,5.534685341270594,3.653347929271539,4.648695331256619,3.7752211890104963,8.216339202056304,8.21996541084418,6.497667859548592,4.974230899139214,8.349826035337934,3.8072245118682604,5.087643185058024,5.512023606553825,5.954124997880758,3.9888774827609894,3.8737160702393307,8.399654702464188,4.834320758956575,4.01807912639191,7.978951555621435,4.317163818855893,3.831915204370484,4.6355145984425645,3.959796114312852,5.159979290878003,3.4378069585463975,3.5739934955047397,6.293540090187823,5.372001351515991,3.8157388903053664,5.480347030762523,4.068491672557061,3.5971757526261317,3.9274718896562155,3.4839067727195747,4.606791973891636,3.7937534598096296,5.329105911605029,4.2478426173085015,3.486179199375755,4.98705342740223,3.3127317324034538,4.328337613331362,4.576139132314461,3.5282527066695746,4.302290198739967,6.293540090187824,4.863746511053774,4.659523984957573,3.5663512556959533,4.653956088992833,4.8392247100446815,4.994818093736734,4.504642632569528,5.4594856999959065,4.2100292194878195,4.705454665956486,6.023592447082613,4.25253812351132,8.132493279606054,3.859211468344899,4.477698285449913,4.140112307435103,5.48742840171913,4.491411521196799,4.073889788776301,4.482544726937467,4.24492601902021,4.778635565527787,4.469715409659065,3.5694329801606925,5.690349263847314,4.4223779840649575,5.160049047100087,3.8563607295324807,4.573165211141251,5.057721111696874,6.449466842080399,5.576523691867441,5.572684098513383,3.6405716196518694,4.9081802200586715,4.984354243169772,2.803150112280471,4.8080416204196865,4.993054145441744,4.321248235411427,5.9851052590133165,4.669401668904511,4.2786034699545334,4.144701258749169,3.301033272969977,4.859833530922129,3.5907446908271807,4.090085174415065,4.340008160453324,3.988185659599145,5.323734095047944,7.656308191117437,4.201545991179053,5.343624925462893,6.515149362353484,5.29586396702517,3.99086935307806,4.07933561156666,5.106547477317723,4.694539913866731,4.810757077785757,3.3254459579161515,3.730938621606879,7.034650507359149,8.006438347206833,7.4799031167486145,4.693665601749083,4.809541526091628,3.613981771020292,2.712380481568694,7.785562684436386,4.141094407925064,6.073591195432747,7.812197681011672,4.0592814111702635,5.0889823547403745,4.1272960251171655,7.454260164027723,5.629536191801023,7.442694105082757,4.79491051467345,7.120730402247554,4.89017956267801,6.4268651394763046,4.2696721908315665,4.3400570571489645,6.026359209552514,4.831695671436156,4.957555956580167,5.364630830351978,4.510286866261305,6.5542188500796525,7.339992313418624,7.210565542314042,5.170734087324806,5.496442053503347,3.070249337392061,4.213814738391453,6.7815003247156715,4.808490049712724,3.0195587155200956,4.795384456754727,4.638596640942581,4.398410887886752,6.2739732209635,7.572119012962072,4.223648343610094,5.455041472578723,6.482965099331244,3.930432864752492,2.6988759235373694,2.9540568722038754,4.798669893052523,4.979856560701952,3.495853098002778,3.3127317324034533,3.601656267277584,4.238590595623038,3.909468719503739,4.956871149902065,4.594781180626609,6.043589097187747,5.363988931529919,5.217516323322589,6.43517629559237,4.835925062265506,5.96325077302737,6.067749272443393,3.8954803639927253,4.869823165914601,6.24450925724925,4.297508474424554,8.842230157471723,7.833085700816394,5.350749023003691,4.995995384002209,2.77181723827983,7.2445027284495085,4.276760784463022,4.09474867907595,5.769727318662147,4.66782035632398,4.3740407873295,4.887030745190718,7.031336215037203,3.4628844296048467,6.065700242216951,4.729285813219404,8.032081021912557,5.8583919113486,3.7827083234804606,4.17480188329994,4.07443068248392,7.288805117427172,4.340232283245146,5.276197584293046,4.76291722739762,8.297790593782095,8.189865835921601,7.718000279365019,4.563730264982021,4.127263598455549,4.447653490769875,3.942960288054792,6.08143803719612,3.0716067509429905,5.145811685544746,4.314381116490044,5.652252697597123,7.953939678553224,5.255817652524367,4.22878587098509,5.041011025876139,4.891920198636257,4.12517976032044,5.007857496334296,4.376063889410817,4.245199968513746,4.819132870300922,4.909373809772186,8.221531895854245,5.771498380986501,5.399776249032841,4.505246977150127,8.764695770425554,4.628726807088,3.2388078321788782,9.007349787465754,6.244821087216971,4.89721291219286,4.965922529374217,8.898319607922323,2.9150848461782957,5.04020501436711,5.966678304181279,5.677826172769659,4.787629049778729,7.427374221085931,4.203951797795767,5.60227946852461,4.020386153398045,4.249818356051049,3.059198016109629,6.653743548373984,5.2627152255953025,4.3764503534127215,5.285954409467489,3.7906293328293876,7.662513827188881,4.365140643728707,5.050683074548525,7.800889680645473,4.391893416410206,6.495644521476129,4.121338600316224,8.181097156661153,3.6409987661493686,2.8370618938651244,3.756232498453916,4.024798731800183,4.917832611503777,4.491411521196799,4.613071757719256,3.5460479302019356,5.2020910147885315,8.947346738550294,4.255824050518852,3.1669943874385225,6.657264149760722,4.337406797250335,4.764050555827244,3.387405229426824,6.439871925251257,4.340905227310593,4.35055222686029,2.7600629430913797,5.815506697057842,4.021101140442118,5.892241219879337,5.192531988473314,4.89305562119166,4.552378400518953,4.353631010685826,9.094798029019985,7.512277083997141,3.3416319376439176,5.850242363073918,5.260199703716036,5.485456079225155,3.7869061643832636,4.303491660293079,4.626067855060655,7.507132691698138,2.9263855082640298,4.826823804450602,3.966485968199916,8.118889796878152,6.375853811838192,3.4828791623760025,8.039010696917453,3.7770309312455375,6.195627843156056,4.64469725657037,4.486551144402688,4.002795881379977,4.780414281790033,5.482532300361568,4.138950641935447,7.548973788710134,6.327670760333335,4.080001116459903,4.4237731113055405,3.442169595469066,3.9076502234510007,3.7366630093952318,4.269104032273214,4.15587295993911,6.6046724309908145,3.9076502234510007,3.8456781970630143,6.031391242891847,3.095843956485958,3.6415421977856512,6.2158108824558225,3.8794031997669003,4.3187729404267,7.356430150571581,6.282938471850708,4.628516395857176,3.699214036667928,5.305990222584472,3.788004162718588,6.4957900047621076,6.182702007290844,7.060978480200096,4.896743255192095,4.599340340556848,5.074977341671316,5.1021530876170536,6.486876325892636,4.398067017568727,4.820286991947682,4.4285812913657345,5.8780275457308955,3.9393086811613145,4.4463286475130035,2.6988759235373685,7.573471597457592,4.762163345241516,3.5455806677421804,3.8300771557364044,5.94494012859252,3.5971757526261317,4.674517344665464,7.534952528415147,4.3660010610362825,2.821410554684318,4.8502719077258165,5.993180619732288,4.675871045989405,4.322167615706849,3.9267667872246435,5.856717338644561,8.184295380062752,3.1038476555487087,5.412115106480243,6.373463833996178,5.308322426015057,4.138453568251186,5.458953565783213,4.076197186273917,5.559768937042909,3.553092013553776,4.395860933451671,5.854131010417565,6.068353162394341,6.220931146629742,6.3084598840896104,2.635086461000355,6.435613073153448,9.44619177639992,4.703491707338991,9.541714111043621,4.870120680410953,4.301275869402506,4.457524670758406,3.680371199706875,6.268858654758585,4.456311852467181,5.388483935730099,3.303632758910612,8.360322432473682,4.392619542386394,3.990846259840788,3.9931643390625937,3.087111233696616,4.22878587098509,5.135915283536306,7.073696361970376,7.058775375374181,6.4228200437250695,3.541276563582653,4.794348741166149,8.137654233064106,2.923067620917845,5.466948822285862,4.738217878628932,6.979854598197226,3.332648495754655,3.542418167751633,4.020422587070285,4.7627748568887345,4.8625257480431525,3.4526310824537436,9.613539474005352,5.074839207564945,4.451320107093289,5.313552514341952,4.065049792934121,6.471427520740022,9.314889788126662,3.962978360139309,5.504947574599231,6.891833492737547,5.948380145724019,5.204775814639191,5.4684962316721375,5.2797916808012655,2.604748969813339,3.9393086811613136,4.963145293805927,3.786897723361266,3.3692895028077947,4.7524106913275475,6.116870162112865,6.860401015948253,4.408814600539348,3.702525979797795,4.44790145424616,4.569434276122344,4.388636732898007,3.492729272554148,6.423715808187978,6.330183307285187,5.8673972569493165,7.218013479789297,4.413918006702719,4.324348664140639,5.535166301374907,3.9267667872246435,4.068458676056858,5.465565562558726,4.123203929216141,4.68626364650599,5.775981326852248,6.147337546311429,4.168878548048618,3.127065485461676,4.6263061772107,4.942595083849209,5.242000019605583,5.063116750637191,5.676671366717337,7.355593192374386,7.675927791292109,6.284304795886583,4.663504275064824,3.211759129593438,7.175601386874591,4.246008182125149,7.596280521485198,4.383539045334319,3.8139922432677524,5.775817653850641,6.933918718157618,4.113811570030552,5.366694866153045,4.606729166554834,5.208255263231923,5.690595287281871,4.014022472566121,4.257602344922742,4.44745512497612,4.238590595623038,4.997022959870557,5.530911367092881,4.545663185024637,4.31824293811944,3.5055517795400695,3.945788253714553,4.660662816206619,4.733882527753155,5.063116750637191,4.980849772689997,4.3429321673783585,6.19521659173715,4.799174357542925,5.55929607540667,6.395388172544516,5.24760825249927,5.085268662372152,4.407404331927399,5.771006734952781,4.430689402842003,4.688631053834361,4.735690680144946,4.839247547687782,5.510695266319012,5.585245580554344,5.41874617913801,4.940505619235329,4.518873233809714,3.071064132010626,2.8093124223080843,5.3093295785778745,5.953278958733001,4.2786034699545334,4.070935268651234,4.1078170045768685,3.838516534083135,5.535303841886987],"xaxis":"x","y":[3.137503523749935,2.4646682670034443,4.808385050656093,6.381629467033664,5.687900522480746,4.296457407371244,2.877744249949002,3.542258049766918,2.752748591407134,2.1984941536390834,5.6604951318885135,2.9763636357327616,8.749098443586298,2.1043366598147357,5.40973035386445,2.3305584000308026,2.3950627995175777,6.141391916170729,3.76659516172927,9.707290101958575,8.663593681954685,2.9126498648972037,6.819668183496455,1.6553518286125541,11.880218691860534,9.057883448899627,4.445594291341182,4.326249700837456,4.708739041359579,3.5570424152637194,7.3848269116030085,6.815319245777306,2.2600256559614555,8.198690585668917,4.72137265947682,2.6712933724815757,7.400708693037901,3.6229303509201767,5.137503523749935,5.359661722333877,4.571676809970931,6.199672344836364,8.475936042446387,7.905507349949873,3.8933622107638715,7.537839615916466,5.353323291162897,3.311793717753649,5.2712762591759255,4.778208576398088,6.628919373511362,3.1858665453113337,6.145473670317708,6.023698917266407,3.145677455195635,5.683696454306515,6.109569509370688,11.964503324303022,7.672213165334465,3.8787253414801053,4.8283269264148165,6.010108453474289,7.029563314432658,5.911212201558612,5.9590746011405145,4.0232553523003025,4.234961094839504,3.41006969175638,5.59215800212536,5.982537314399271,2.910732661902913,3.7687136570304847,7.183982678112831,1.0976107966264221,2.6016965164809576,5.511910748670928,5.6570683012015754,2.2868811477881614,6.999098033705607,6.746447139362815,4.83035674741922,3.411426245726465,4.77663042305105,2.550900664647523,5.942279901705763,7.626950122372515,6.862203399053224,3.4019034716079584,2.608809242675524,6.046141781644721,3.3936907641874536,5.390254956321868,4.541638734436141,6.063718640152248,8.226941966807852,6.203788453308596,5.530445354692906,3.2433644256936605,6.703349915166396,8.202417721575182,6.075318692553354,6.401732849522621,3.3305584000308026,6.606294160697713,5.5786368624717415,4.273515889702116,7.668814088419117,3.097610796626422,3.321928094887362,4.593353770980297,4.792334805706692,8.090535901003173,8.367589480412983,3.1358631653686793,2.7070829917717063,7.1593668097437355,4.614709844115208,5.946496941203293,6.783456654360239,8.491372117815171,7.273609132231856,5.944624223055033,3.070389327891398,2.2479275134435857,9.095924419998536,2.4489009511451276,7.048759311919856,6.058532970201611,7.216163813388774,4.46532151358055,4.871350840634026,2.611172380044005,5.783718557790366,7.3824943402931185,8.12401788912289,5.2787282129389395,6.099295204337775,6.1705259991768475,2.269033146455237,3.6392321632492775,4.547819956577703,6.003152448666922,7.816087658600206,6.762348815644129,4.940166750482817,3.01435529297707,5.487357715311547,2.333423733725192,10.642557179595622,2.503348735167504,2.4356285940520905,4.773468927905195,9.760270854333914,8.156942620908886,6.218781167784069,6.306517445249819,8.352882086522959,7.032211069322931,5.5963388642052,5.320845667645722,3.0089887832272546,5.943218087426284,7.3403842354553985,4.443606651475615,2.845991770664573,7.217618485351048,6.137913322088407,7.084383221650577,5.784242222057781,6.097610796626422,3.605257262939004,0,6.954079959320971,4.883620816285672,5.118941072723508,6.583308464847678,6.034963989749303,7.257764968571387,3.91169158187234,9.10522758436473,4.507794640198696,5.001802242633985,6.858478390423273,8.095608004811197,2.3103401206121505,7.6398833131998005,3.144046369616707,3.9420452599160467,4.142413437873741,3.925049964727359,1.835924074254375,6.111448698487674,3.363171077119244,3.127633279725874,3.24031432933371,4.160274831408593,3.5397791916984938,7.571221725048103,5.284292026394312,5.47800105583793,3.7398481026993275,7.1320626778543845,5.576522137920503,3.127633279725874,4.3110671022555955,4.375734538583156,6.4432751117658045,3.91169158187234,3.8042601156347384,6.4409521980296365,5.85872702377863,6.02724253597374,5.212958362728285,2.5058909297299574,7.561631630375841,4.92552476974757,3.5619370596224327,6.237448995639321,7.881113960675097,2.176322772640463,5.361768359419154,4.9941271140031285,2.584962500721156,6.166916251569974,9.4493756610678,6.0332027259372225,10.919861451442168,2.693765712217783,2.4724877714627436,2.7484612330040354,5.633721812641012,8.17707065762942,1.9634741239748859,2.7803100990433753,5.23687524536694,2.698218478224414,4.238022517825721,5.317231700222606,2.1176950426697547,5.604071323668861,3.2555007331483865,2.2387868595871168,3.881664619320345,4.493134922305505,5.357903838552492,4.9597701552114675,4.724650271732967,4.4046306842176115,8.46707564951531,7.096029876307993,7.341896820977107,5.339850002884624,2.859969548221026,6.582706526780788,9.101634319640722,4.129283016944966,7.151270288790165,4.58014548442338,4.628773595201645,3.070389327891398,6.034303767768035,6.339493737901733,5.174326515173915,3.357552004618084,4.809414444235898,7.880195728943098,5.632268215499513,4.79960542245267,2.077242998932461,3.5298209465286954,5.433960931121179,6.078951341394822,4.854494418154875,2.6484654430273142,8.345405246717796,3.4462562298895643,5.595145567990858,6.766065051474954,2.4222330006830477,7.366846735262642,2.9873208659292536,2.6507645591169022,5.940871478189196,3.552131108253784,2.4672794804599825,7.901409979999464,7.459185683748918,2.3868109464722167,8.668955876629637,3.1667154449664223,3.017921907997262,2.4982508675278257,1.7782085763980877,2.9088129077395473,2.1602748314085933,2.3074285251922473,8.316598324108684,5.603477988254004,5.669310286202474,7.991691898336964,1.0840642647884746,5.091276694365222,4.042644337408494,5.96254902292306,5.020591094773247,5.20163386116965,3.065227622775619,7.08533966935737,4.321206566969903,3.5298209465286954,6.083000565676004,5.04176864995163,9.435170178881494,6.4705368650925825,2.0531113364595623,4.078951341394822,5.525755692829486,5.9818526532897405,2.063502942306158,4.110196177754199,7.135657989368776,2.3812833725037836,3.693765712217783,3.00539998774259,5.589164236699772,5.36737106564853,6.38646609286931,2.3248106034204836,3.1811025507537978,8.27891401949994,2.430285272977781,2,4.867402305727543,6.299391206126829,2.6803243568440163,2.693765712217783,2.7949356628035362,6.605701738949982,9.159820892013935,4.255500733148386,5.555509427083257,6.871104373555458,5.632849830117706,4.966707314150316,2.526068811667588,6.09950561709114,2.927896453728821,4.28835856219366,3.1634987322828794,7.07692245939234,3.8429788317883253,6.341274183692174,5.926948247949772,2.538538163629804,6.73226919950145,2.788685710613534,5.508111681193465,9.398936038454162,2.3161457422933562,5.28835856219366,7.479861085016818,6.0925457415435655,5.747655932932393,3.9059284781731347,6.235152624217932,7.3334237337251915,8.870210577263595,3.460742563789644,2.974529312483882,2.6016965164809576,2.2387868595871168,2.7676547982373463,5.659639187015652,3.894332742277694,5.771093251699539,7.173327349650988,6.027463723408577,1.4222330006830475,5.794675787986858,4.720825666089835,5.342696959981934,5.562547724199122,2.62760683812965,6.426767932655541,7.593353770980297,2.5185351389821804,6.676239195086378,4.472487771462744,6.212180210044431,7.178117045706399,7.534497433738168,6.304876050044889,7.545505100691072,5.077670274232752,3.953265239014844,4.609991295212678,2.9126498648972037,7.174426393677253,3.6016965164809576,4.515384460636948,2.503348735167504,8.129128434194934,2.9726926540042644,6.030998118171895,2.2539892662307865,6.597829097826899,6.8209447918806925,2.9523335663696857,8.18769847638827,4.950468414150123,6.1409831048784245,4.674121632632394,6.830610274357907,4.4376272483189485,3.7037651787934096,5.95837871156634,9.296618003796326,3.351910961103077,4.143230134776893,4.707082991771706,3.560714954474479,3.9845893503624565,8.328001662149257,2.1667154449664223,3.1811025507537978,2.4854268271702415,2.931683057059806,4.788685710613533,5.659068274843228,5.738227400367242,4.6937657122177825,6.860962797858116,8.65474342118394,4.814550423461808,6.107687869314374,4.720825666089835,6.4775154361842375,5.619119511453218,4.7136958148433585,2.5533605033353277,4.412781525338476,3.72137265947682,3.460742563789644,3.0197019144425483,6.602587527342561,4.687060688339892,2.526068811667588,4.698218478224414,2.8698714061777126,2.144046369616707,5.394376944957055,4.649615459063409,5.80683958179819,4.443606651475615,5.450881315273369,6.6503337538459295,7.198494153639083,5.228049047884462,3.9231491804723087,1.3840498067951599,4.524815928357506,7.211012193485511,7.027021314622255,7.212958362728285,6.658497136656478,3.3881895371560837,3.140778655782796,4.37364821133469,3.319039815562536,7.33360262826828,5.175524601089875,3.2047667506546134,2.702657543390911,3.228049047884462,9.322806969423008,5.561631630375841,5.8755343515732354,5.032100843167024,3.9873208659292536,3.1842802944193824,3.277984747299765,7.444849247263915,3.6701605141266334,4.595145567990858,3.8369340113210915,3.575312330687437,2.788685710613534,5.4571345943180365,2.4276061727818994,8.563615765629766,4.712045448553693,6.260590274730996,0,2.4594316186372973,2.8257856274647914,3.049630767724601,8.097242070141174,7.431622959713292,4.030336078370959,4.964398632203184,4.020591094773247,2.211012193485512,5.462052318796433,3.3881895371560837,2.411426245726465,5.0430819818956625,3.6064422281316078,3.613531652917927,3.883620816285671,2.8318772411916733,5.081936081719432,5.575917361118149,4.958842675243241,6.716579420796345,6.531225485053606,6.050284014301706,7.73125113789623,2.950468414150123,5.645009884391095,5.4015622072560685,3.840966704487421,4.3298411765306275,0.8155754288625726,3.7070829917717063,5.848748114611959,4.303050084681673,8.796785917134267,5.497931651896279,4.67581593117227,8.189231548647284,6.9092930858238235,4.772941337831336,5.6082178530214595,7.304419775319909,6.588864518190148,10.665637927637095,4.672990993827748,2.8399595874895316,5.305605789585429,3.0908534304511135,2.4854268271702415,2.693765712217783,2.942983598187102,2.5897634869849773,4.631686366311998,3.0125686735030555,5.651051691178929,5.488964817004848,3.1858665453113337,0,6.703073006328523,4.255500733148386,5.9593064897596575,5.218781167784069,3.968090752045256,4.8594726668519375,3.460742563789644,5.341274183692174,3.0089887832272546,2.032100843167024,2.169925001442312,3.356143810225275,4.814037647539975,3.4168397419128294,5.653060017104565,6.074462620704536,3.2016338611696504,6.9656688644419456,6.816215687622865,4.330558400030802,7.16380060395775,5.6061460780657235,6.093179846765504,3.5716768099709313,5.342341397431548,4.228049047884462,6.310885391185277,4.751677945687579,7.502871574574138,3.953265239014844,3.975446765640962,2.6599245584023783,2.430285272977781,5.7303682363490065,7.689648608288815,4.371558862611963,3.215678596607928,6.492173659610827,7.480507489974544,4.436295119839363,4.804260115634738,4.405311682597445,13.396289023308677,3.8914191868460786,3.24031432933371,1.899175630480513,3.2942531364445142,8.382105211486744,4.143230134776893,4.92552476974757,2.8439838440483265,2.8399595874895316,6.101187815259738,5.0232553523003025,4.583158003874408,3.176322772640463,8.996303999184056,3.8257856274647914,4.7755773609306535,8.12401788912289,5.9618548076361595,2.454175893185802,6.208088008224774,2.673556423990145,2.8972404255747994,6.791683858152045,7.137810883416096,5.8052924556007115,7.338067797542616,5.198494153639083,2.851998837112446,5.726013748860151,6.567880042052731,2.8155754288625725,2.2387868595871168,3.6769443591069124,4.318316841334983,2.1795110502715103,5.6884601404244926,0.8073549220576041,7.244982170376553,8.642268351758643,6.836681593327116,2.73768676140986,2.4646682670034443,5.964860664245848,2.7420062108667365,2.411426245726465,7.279842693520348,7.446835177086041,6.531849285751203,3.8982083525087177,4.721919445551544,5.500164679492167,5.429615964201735,0,3.7311832415722,4.726831217032493,4.904002316283692,4.561937059622433,7.438459204423481,4.631686366311998,7.300764373248062,4.112700132749362,7.9329824099498465,5.1081062236861285,5.868143479666726,4.161081482277184,5.996162742681061,3.097610796626422,4.263785613890803,6.339493737901733,4.370861740085285,4.842978831788326,5.636334602442182,2.84197311892718,4.7713574089905615,4.257010618206024,6.916596067892185,6.015247773698937,4.595145567990858,6.7410624357490905,3.4168397419128294,3.823749360308273,4.279471295644468,4.7114949066500875,2.883620816285671,2.969012307516316,5.089159131911238,2.3362833878644325,8.080604371320732,1.8559896973084806,6.941341105485299,8.14639047567022,7.163498732282879,8.281651890258825,6.329482431025636,2.140778655782796,2.3103401206121505,9.500782139394977,4.674686619927999,7.249066033855867,3.1060132376221192,7.897179908573977,7.6546360285279675,2.944858445807539,4.94673086014031,2.711494906650088,6.054414387917307,2.788685710613534,4.198494153639083,2.9467308601403097,4.060047383669939,3.442280035252584,2.7070829917717063,5.9586107120558465,2.4982508675278257,3.6484654430273142,5.0870384567669475,5.130519083027338,1.8155754288625725,2.6064422281316078,5.673839055990439,6.37364821133469,5.609991295212678,6.076388068512841,4.695437110405369,7.459267666703387,5.211401637418471,3.063502942306158,5.931210274803932,6.819540460505722,0.918386234446348,4.878234879112339,6.098663781886574,4.098453246309274,3.4154882710497003,9.8361608416947,5.3103401206121505,4.20633064787105,6.436128517259249,4.381283372503783,5.0259146987507926,5.226123143125237,5.73903797911741,4.110196177754199,3.8042601156347384,4.503984703897613,6.067165427017112,3.7322691995014496,6.121844297876376,8.248591759535907,7.711013010084339,5.782670658677007,6.0774566524005875,7.592232766722112,6.917909073852982,4.423578170981798,4.901108243014512,3.795974694206668,6.907251224292033,3.189033824390017,2.4646682670034443,6.061128135118254,2.733354340613827,6.664198369291911,5.166715444966422,8.492293852480074,7.279749853014451,2.807354922057604,8.362689046726734,2.3248106034204836,4.826294245499635,2.321928094887362,4.238022517825721,4.232660756790275,5.51127826508303,3.7644735509926663,5.290940402403678,7.535353147842131,4.329123596291566,5.908092340818271,2.570462931026041,4.634012356402116,6.99491908871511,2.560714954474479,2.3589588258323295,2.8278190246173196,5.439623137557117,2.927896453728821,7.476543706214256,8.334094473911568,6.726695004499616,2.097610796626422,3.619413010597937,2.523561956057013,7.136683577697236,8.908992993250534,4.4950555283680185,6.970393537914677,8.089159131911238,2.996388746447621,3.7506065048355923,5.468583317006828,4.732811872082943,2.5360529002402097,2.729008870337863,7.712939631305699,4.038260575175349,4.514122260170708,6.948834425578686,2.0036022366801953,2.169925001442312,6.04023491675437,7.246788093844365,7.018923431842335,5.046141781644721,5.637784110291324,6.976363635732762,5.840463233869542,6.170125361839952,4.760753208062987,3.089159131911238,2.8399595874895316,2.3978029618624896,2.3305584000308026,3.0925457415435655,2.6599245584023783,7.9217814432688,5.963242904318827,2.4515408330178317,5.976821852360685,1.7865963618908067,3.221877081077034,3.0036022366801953,2.722466024471091,4.2024177215751815,5.9025560053446275,3.997292407636508,5.323730337521348,6.463851292611099,5.938756261492295,7.593951283948411,5.233044401265581,4.79025073883523,5.718361626138354,6.435295215647167,1.3504972470841332,4.12267271882414,6.361768359419154,6.8820316083521,2.0250287944915226,2.298658315564515,3.636914580355878,7.934634441094905,5.246408087246385,3.918386234446348,3.929790997718597,4.147306698780294,5.169925001442312,5.542258049766918,4.693208148910016,3.6971065744769747,5.707082991771706,3.363171077119244,5.035183996618336,4.274261661257048,3.321928094887362,5.9740703670240265,2.3757345385831563,10.174663577413781,2.8439838440483265,5.24260250592898,3.4581194811745064,2.063502942306158,3.1858665453113337,4.7692427951405385,4.301587646603187,5.986638471730853,5.058749412335524,4.556429415449573,2.5360529002402097,2.4489009511451276,4.974988111991927,6.248307120087241,4.346956889378885,2.641546029087524,2.8579809951275723,7.624539603781834,5.197314999473818,5.38404980679516,3.2387868595871163,4.464668267003444,3.955126781261366,6.258895755075436,3.1358631653686793,7.831180547126736,1.3785116232537298,4.934044647112246,4.503984703897613,4.100977647724821,4.86195536414487,8.439830883981392,7.851624127346895,6.091065077930535,7.190022174802645,5.992541859109112,2.983677694698067,7.161484638641659,2.6959938131099,8.21003812347289,7.461643148073906,4.848497755385146,3.192194165283345,1.6690267655096307,8.80290404741688,6.926948247949772,0,3.812498225333564,5.6806056662025926,9.871443254934135,6.459923362676901,3.8298495598446904,5.157447996274298,5.537296067090842,3.5147534984397533,3.2249663650002742,5.699884728697476,3.035623909730721,6.914444931746792,5.338780943894699,9.013601832683456,7.578712331048165,3.3405622690264134,2.3950627995175777,7.221587121264805,7.78959885005233,2.2234225499349374,2.8439838440483265,2.950468414150123,5.833649127594984,5.802968651005583,4.618238655595454,7.581803148516525,4.948600847493355,5.416164164733128,5.012121672712217,2.9373443921502322,2.3923174227787602,5.259649120649155,7.119252412157343,5.452858964713811,2.5160151470036647,2.788685710613534,5.040892430646901,6.390942772802543,6.032541697270038,6.177519202597814,6.111865963867557,7.0083165637349865,6.074034394212556,4.471187460386985,9.507715369062607,6.152994605492435,4.169123281476757,4.706530552538117,3.0426443374084937,5.305605789585429,6.942279901705763,2.4724877714627436,11.02948049374083,7.127736443588829,3.594548549550354,3.8063240573900288,2.144046369616707,7.036942844798256,4.022367813028454,2.0214797274104517,8.589726041133547,6.218781167784069,3.685940148445977,7.566662962962611,6.652343078740216,4.210232990095849,9.47854718263086,5.991861930556323,7.500961352460057,5.5855634984556914,2.319039815562536,2.9049657186840263,1.9781956296816516,5.509062386361898,5.592756010441026,4.39780296186249,3.431622959713292,11.193297418854996,7.925940095994448,8.041385369486115,5.051807107013298,2.3673710656485296,5.208673319629471,5.532940288372874,4.598722499676621,2.903038270112912,6.098453246309274,2.967168607532628,4.919340082442012,7.971830914780502,4.189033824390017,3.10936055940423,4.053111336459563,3.944858445807539,5.71699089440494,4.2787282129389395,3.7246502717329673,4.568032104771279,6.400025517914023,3.0443941193584534,9.140190703191836,6.7998644108818525,3.3923174227787602,4.880685525261125,10.32396671481508,2.521050736900963,1.8836208162856714,6.547203024756931,5.63023071585969,5.915042791371977,5.713970692436053,8.823876711241038,2.6530600171045644,4.462706750670158,4.524189078449365,5.342341397431548,6.70362677086498,8.008596693255853,5.81326814176608,5.795715006501729,1.5897634869849773,2.5058909297299574,4.153805336079036,4.707082991771706,5.617651119427331,5.7615512324444795,6.115615931428109,5.51443791382931,8.873136468833458,4.8811751553492755,3.4276061727818994,7.013685569928653,3.37851162325373,4.390942772802543,4.544114402209474,8.617283787779035,5.39677601078753,4.096767854714689,3.169925001442312,2.881664619320345,3.389566811762726,5.949301468328278,5.9095331149468935,2.2172307162206693,5.687900522480746,8.295998461911388,4.963011647599427,5.648177795724818,3.8509993947164736,3.669026765509631,5.924575003392735,1.2570106182060237,8.1184220243132,6.3103401206121505,4.59215800212536,2.169925001442312,2.771885578515366,5.309976492370556,5.718909554379232,7.2325648297287755,3.720278465233327,3.403267722339301,4.138323004056484,8.321792835886914,8.384999544730041,6.675392543042777,7.848935855524565,8.135247549809566,7.474760499576783,2.01435529297707,4.661635602335959,2.899175630480513,6.950701690089008,3.4776773275653072,6.275752048828529,2.7782085763980877,8.645802514606062,8.927066807759255,5.377123749129487,8.785648628652076,3.554588851677637,5.8529975876133165,7.093391153253013,3.8639384504239715,6.390082950951663,3.0214797274104517,5.947198584262056,2.0565835283663674,8.560600329022595,4.708187236020708,3.1473066987802936,5.260778431893426,2.3978029618624896,5.496654082593496,8.356011721502778,5.307064162255372,5.485426827170242,8.478971805032943,2.414135532984451,2.776103988073164,4.7420062108667365,2.4828482830684653,6.543805175964397,8.5918962955207,4.361066488794321,5.294620748891627,6.553360503335328,7.088629255297176,5.1201860275318625,7.00943675560809,4.090853430451113,6.7014104362328135,7.415741768290091,5.348728154231077,6.705563274566321,5.445263208140443,4.552131108253784,3.4436066514756147,3.5631581304028077,6.678494507773734,4.478971805032942,6.1657109924849305,3.403267722339301,4.534186139090097,3.0214797274104517,6.238786859587116,1.8639384504239715,7.020924396158786,4.20163386116965,2.7548875021634687,2.604071323668861,7.128870759465125,2.5459683691052923,5.414473836430928,9.995102877826515,4.861459166361513,5.25285462595139,6.31922050252387,5.308156975114762,5.526068811667588,5.763145958149476,5.903279342056139,5.825531251189622,7.048214384608036,2.513490745588118,5.944155663440363,5.856985689782205,3.412781525338476,2.956056652412403,5.664482840364682,1.74416109557041,4.886062338345599,2.4646682670034443,2.4515408330178317,6.433126376206435,7.7755773609306535,6.705839705883682,8.330737650206952,4.727375938594897,5.777945670695148,11.69784504363786,7.7627474419531515,13.587327482778312,6.558114538598396,6.332349900373874,3.2265085298086795,2.875780063068488,6.70473366267406,4.223422549934937,3.2387868595871163,6.136683577697236,12.23806432823473,5.572889668420581,5.849999259466098,5.533563348214512,5.325530331567558,2.797012977836145,4.150559676575382,8.298429210890312,7.731454807698346,8.903490246967104,2.8094144442358986,4.7824085649273735,8.461602224618334,5.583158003874408,3.060047383669939,6.429950657403768,7.193278935521388,2.3589588258323295,3.7311832415722,2.899175630480513,2.298658315564515,4.235727059838059,2.3812833725037836,11.663771557482411,5.684257705058932,5.212569338850806,6.9787676510367875,4.044394119358453,6.092334311186202,9.582254908357664,2.5160151470036647,6.402415216824399,5.740387932468353,5.784503982929566,2.6064422281316078,4.362469888750209,5.164705840182799,3.4208865749755315,3.948600847493356,7.965841991309597,4.976821852360685,2.722466024471091,2.7398481026993275,4.528571318870758,7.677578712165378,3.41006969175638,2.4905701304462013,3.5147534984397533,7.5673476962842585,6.849624029924543,2.833902076669163,7.406162478777607,8.131085259578953,3.176322772640463,8.148324042480084,2.657640005207824,3.350497247084133,6.8203066288938246,4.483493351284395,1.713695814843359,5.872582544833223,2.722466024471091,5.7742599514327635,5.704318677760832,4.760220946466509,4.294253136444514,6.037162550127378,7.151777654732053,3.215678596607928,4.742545233866053,5.613531652917927,5.727375938594897,7.7780114016125745,8.009884588931843,6.549977142771825,2.0461417816447205,2.0565835283663674,6.66746640495737,4.62760683812965,7.884597920990064,8.571145863602528,2.3161457422933562,8.563272552819562,7.111970261358359,6.001577085389535,3.4369613378336026,6.435461914479276,3.5084286525318573,3.940166750482817,2.5459683691052923,1.6644828403646825,4.94345253855882,6.003827078111476,4.960697039304312,6.684117412839159,5.612352498752662,5.609991295212678,2.62993940943954,3.611172380044005,5.522620761042079,5.891419186846079,3.6530600171045644,3.4581194811745064,3.1667154449664223,4.581351247168778,6.764340846670181,8.066089190457772,6.614857050384495,5.767654798237347,3.1953475983222193,2.9726926540042644,5.279471295644468,7.5105663894487495,5.02724253597374,8.25176642434399,7.822411496360477,5.334496768390418,5.715893370547605,7.3049672876759635,5.998646839005795,6.9725777851765525,2.4594316186372973,5.666472568842073,3.8728287595348854,6.052024560482831,3.631104282365877,2.3015876466031866,2.37016428054021,4.3384244147835584,6.546122758857796],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle KNeighborsRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_pred_logkNN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle kNN\n","# réglage des paramètre pour la gridsearch\n","n_neighbors_log = np.linspace(1, 100, dtype=int)\n","param_gridkNN_log = {'kneighborsregressor__n_neighbors': n_neighbors_log}\n","\n","\n","GridkNN_log, \\\n","BestParametreskNN_log, \\\n","ScoreskNN_log, \\\n","TotalGHGEmissions_pred_logkNN_log, \\\n","figkNN_log = reg_modelGrid(model=KNeighborsRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=TotalGHGEmissions_train_log,\n","                         y_test=TotalGHGEmissions_test_log,\n","                         y_test_name='TotalGHGEmissions_test_log',\n","                         y_pred_name='TotalGHGEmissions_pred_logkNN',\n","                         score=score,\n","                         param_grid=param_gridkNN_log)\n","\n","print(BestParametreskNN_log)\n","print(ScoreskNN_log)\n","figkNN_log.show()\n"]},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[2.114889475062169,1.7515039464707756,1.6444782423135131,1.6116203091241865,1.5910406545139988,1.5755493800239377,1.570964542081293,1.562835562513166,1.5580492269013089,1.555515619473612,1.5543391114374567,1.5546679882292493,1.553585593572316,1.5521821597511851,1.551959317563258,1.5523229455531697,1.552491386190886,1.5515997660329397,1.551862998677581,1.5518058396514123,1.5520083940626421,1.5522582476105728,1.5524270245809721,1.5518384183796723,1.552269540266916,1.5528179233504988,1.553119499820146,1.553066066591073,1.5538498095599165,1.5541441440493382,1.5553124472027577,1.5553360271488887,1.5561829727953889,1.556783279059145,1.5575557213278641,1.5576214941616189,1.5576681766806248,1.557647799172121,1.5572366907330972,1.5574645955306785,1.5585016697254293,1.558902700210107,1.5587362182431495,1.5594494494731725,1.5608311154386365,1.561701750214057,1.561588251411521,1.5622370154131806,1.5625815627231594,1.5624376006987506]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[2.165156886349609,1.7728682528652122,1.6491712258060427,1.6231381921862003,1.6022660674250517,1.5922518673548387,1.5946886152507687,1.5894965662744076,1.581355378469761,1.5776910426997905,1.5769127602378625,1.5759429171709094,1.5740649186574704,1.5733838898005492,1.572746533897584,1.5706720144570212,1.5713153504076525,1.5704731562551317,1.5702048415713188,1.570177744276237,1.5695474756592553,1.5701078599768914,1.5699244845643177,1.5680115485696977,1.56729331745034,1.568514312248456,1.5686643844919905,1.5687771538544286,1.5702778046586592,1.5705040251990372,1.5715675599842214,1.5710665039800313,1.571132786594423,1.5713754987624249,1.5725654463241194,1.572695988582798,1.5729868114324768,1.5734201805373327,1.5731631957233014,1.5732157557763526,1.5740028378262256,1.5745249777161339,1.5749243294407653,1.5758360817012198,1.5769771443492302,1.5776412528709203,1.577770078371754,1.5781952576198686,1.578633039020274,1.578154092218726]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[2.064622063774729,1.730139640076339,1.6397852588209836,1.6001024260621728,1.5798152416029458,1.5588468926930368,1.5472404689118173,1.5361745587519244,1.5347430753328568,1.5333401962474333,1.531765462637051,1.5333930592875893,1.5331062684871615,1.530980429701821,1.531172101228932,1.5339738766493183,1.5336674219741195,1.5327263758107477,1.533521155783843,1.5334339350265875,1.534469312466029,1.5344086352442543,1.5349295645976266,1.5356652881896469,1.537245763083492,1.5371215344525417,1.5375746151483016,1.5373549793277173,1.5374218144611738,1.5377842628996392,1.539057334421294,1.5396055503177462,1.5412331589963548,1.542191059355865,1.5425459963316088,1.5425469997404397,1.542349541928773,1.5418754178069092,1.541310185742893,1.5417134352850044,1.5430005016246329,1.5432804227040802,1.5425481070455336,1.5430628172451253,1.5446850865280428,1.545762247557194,1.5454064244512877,1.5462787732064927,1.5465300864260447,1.5467211091787751]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[2.092110631673962,1.7492055639323854,1.6491485128351595,1.6100186498346847,1.5890568513622483,1.5611813408234452,1.5498247457613532,1.5410668929748546,1.5427215394800322,1.5351534666107087,1.5339931863534448,1.5360769711663678,1.534880963049033,1.5313735454780346,1.534476716561232,1.5369936553347083,1.5357455108630207,1.533749592241164,1.5376071219247585,1.5363230353992832,1.5360557085513684,1.5355323931263372,1.5397463519511745,1.5406535436672821,1.542736750276518,1.541696017563739,1.542655390133572,1.543031818339634,1.5424939614391868,1.5413265620214096,1.5411050745885329,1.5420030407527416,1.542840524182036,1.5431269411017716,1.5433202679810603,1.543587379737429,1.5442592619722832,1.5435761568516388,1.542267607057662,1.542836041778483,1.5444596239311057,1.5444323558353754,1.542761753178114,1.5424585750552082,1.5439737966587785,1.545025042062323,1.5435559167031243,1.545100230955686,1.5459007687434954,1.5455646117458797],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[2.097017199907938,1.7453762781411823,1.6503501121388153,1.627434628978155,1.6031828874048,1.5980872664720847,1.6075181716933655,1.6019660078801663,1.5914351763484913,1.5820958709553414,1.5823486877183672,1.5823706033462006,1.5792005466164143,1.5764067724467792,1.5766626968188635,1.5739920767568538,1.5759201240785141,1.577060171206481,1.5770028121228137,1.5759998688540606,1.5723283549236287,1.5714024716663304,1.5701451987421196,1.5679712457376238,1.5651845158345967,1.5649116446138256,1.5652306307014254,1.5660193637602577,1.5663084907708693,1.5661441254703046,1.5667610694949317,1.5653971175507189,1.5653312089427678,1.565716487796331,1.5661212542408596,1.5659024725318254,1.5646305264238018,1.564186802076511,1.563371871673879,1.5637384472482672,1.5646454649948562,1.5650199887407112,1.5649915799630547,1.5656796342033845,1.5677958455238938,1.568057095490214,1.5692728918736207,1.5696778143035235,1.570754685492172,1.5699701720662098],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[2.1119830942776665,1.7312195667928836,1.642823141849766,1.6130021639953216,1.591518972630923,1.569858299095858,1.5577774621928797,1.54602009016948,1.54037103468995,1.5417009822860612,1.5392349846459814,1.5402757985600486,1.5378544624193033,1.5350910801065367,1.5347990289396625,1.5380058244538266,1.5387864895185561,1.54044544047646,1.538514825220984,1.538796156093525,1.541214031272818,1.54085457862433,1.5381545623220652,1.536822749205818,1.538613581983505,1.5394024179854113,1.5394180404950997,1.5380155589986115,1.5403122140981405,1.5423367020234933,1.5449830539511693,1.5458784175917863,1.5474857008795089,1.549251045099816,1.5505383097274932,1.5509677910629425,1.5497356837520282,1.550138276847443,1.5495249551941808,1.550350367434067,1.5511361878369652,1.551804783407496,1.5510766285985627,1.550927316213891,1.5523170143181577,1.5544402099002912,1.5556283390358514,1.555781152478214,1.5551638374955197,1.5548062852374454],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[2.2102844401246173,1.7924649561045747,1.6424431546241476,1.6158052496084543,1.6002460746822873,1.592268393170818,1.5905238662327825,1.5877225930614043,1.5808127666185945,1.5829562452238575,1.5814424637106894,1.578860253323124,1.5780540752817873,1.5795395391844318,1.5781513616371332,1.575574306172035,1.575106059576897,1.5718578088470436,1.5713388255039256,1.5724493367383032,1.57437312491731,1.5764533884812408,1.5771657244237405,1.5748490864419804,1.5751777282876156,1.577808488904239,1.5777431619268445,1.5773747856214646,1.5801251435607133,1.5805915487163027,1.581804914321711,1.5815474283322708,1.5812973488404336,1.5811665773578347,1.582882464987971,1.5831825265301604,1.5845899945049142,1.5855838764006565,1.5856366551815575,1.5854090000328187,1.58608052448929,1.5866746199604556,1.587503946617932,1.5885403233805782,1.5891250542792938,1.5897873266447002,1.5891913381590885,1.58970589367742,1.5900553969879545,1.5895157546540426],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[2.0630520093266616,1.7392533673828512,1.6376262901196772,1.591840853204316,1.5711984864897353,1.5563516005574842,1.5491784645260833,1.5374022284799242,1.5349056173694764,1.5356715322920913,1.5346762347588008,1.5357563147505062,1.5379379204950412,1.5384998615401433,1.5357067838593983,1.5370488650484266,1.5368987469174424,1.5348858173935496,1.5348514086154235,1.535460801171889,1.5360707506480853,1.5370484061546261,1.5369232854657615,1.538895466845657,1.5396351249523452,1.54027104768528,1.5405502758437892,1.5408888062353967,1.5400092379306727,1.5403217820151816,1.5419081236574441,1.5418541315169267,1.5439600811321978,1.5446553439399713,1.5449163097019365,1.544467300945737,1.5451254167500963,1.5447538836843555,1.545382364558207,1.5449891211597575,1.5461865473749288,1.5465817531064956,1.5473471828580843,1.5496413985128,1.5509438664130588,1.5511990769727577,1.5502927712859196,1.550919985651059,1.5510331248966558,1.5523311797901755],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle kNN en fonction de n neighbors"},"xaxis":{"title":{"text":"n neighbors"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE kNN pour les paramètres de GridSearchCV\n","FigRMSEGRidkNN_log = visuRMSEGrid(KNeighborsRegressor(), 'kNN',\n","                                  n_neighbors_log, 'n neighbors', GridkNN_log)\n","FigRMSEGRidkNN_log.show()\n","if write_data is True:\n","    FigRMSEGRidkNN_log.write_image('./Figures/EmissionsGraphRMSEkNN_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.6 Modèle RandomForestRegressor"]},{"cell_type":"code","execution_count":123,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                             paramètre RandomForestRegressor()\n","0  randomforestregressor__n_estimators                    1000\n","1  randomforestregressor__max_features                    auto\n","                               R²     RMSE       MAE\n","RandomForestRegressor()  0.658611  1.18651  0.870374\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_pred_log_logRF=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4.3721082859438365,3.6960431267617375,5.17057885506433,5.055588395709279,5.18346799833815,4.057864518848092,4.9051727371732525,4.2960887367498835,2.772259178800248,2.4360776567737075,5.710706486360555,3.5168937887982685,8.51103962116616,2.334579354202988,4.860653314284448,2.88955506148259,2.420866528766606,6.720138961091206,4.162896827401142,9.849855753192266,8.59506812732708,3.7428670684897236,4.814600625454892,3.17858108788743,10.854662267427202,4.484109838834468,3.6785600485616774,5.109604597417012,5.2142999646302,4.044090340781223,5.54334338393602,5.808077779282951,3.2556799646525136,7.549304478001165,4.810902904150202,2.9528561752077467,5.765102620353967,4.2280815570550825,5.647031583142329,5.092431578668385,4.819771335562534,7.024892439806504,7.986169381622957,5.924841223486936,5.4438921414926496,7.242990707263696,3.028672704959074,4.129255975237269,6.598646863839929,5.453439378000211,6.6260573396993845,4.726654779788692,5.893070408521316,5.238822144117295,4.196961173689108,4.33095889701787,5.112581768867135,11.095430426391347,7.362817093518746,4.1930899948193465,4.876893080361682,6.008319789738701,5.275777014154263,3.536387861735357,6.341636047057067,4.552663966701061,4.1542991733656915,3.2972165611123665,5.801286849943077,5.948092904958715,3.772698525728282,3.3842302878820902,3.770401984876172,4.087643442531914,3.406677105838881,5.091200259608892,4.914366555363692,3.0316181213591613,5.691303034459707,6.721121956882582,4.38302585928539,4.512391085981438,4.582404862390705,3.4251189645478095,2.8469297933395037,6.261757902185666,5.763528320140565,4.294310826316438,2.3912748951319034,4.445472311616729,4.895764908136583,4.2746388748832205,4.387264812153121,5.011853500217374,8.045581967269921,7.3262758903109715,4.933105676214216,3.259354137247873,6.660728468031622,7.544695693065843,5.3705080377268315,6.348979349409281,4.180777068607769,6.44086405854584,5.811991897763231,4.357638135643446,7.541858303697192,4.137896543333632,3.6782073788162406,4.334522711450622,5.556921021462233,8.22769711899136,8.079037175963236,3.5083680086738656,4.702393087183479,6.889942605574551,5.4455731012739506,4.719102087643008,6.188530722196268,7.028968302762599,3.8442127106601713,5.219161944175313,5.417673777868431,4.96536219290055,7.583436399413065,3.081598586232574,7.988597679714492,4.460225064173751,4.768847574278101,3.2803827041008202,3.919725948036987,2.824509636791295,6.913465870350135,7.543254128983912,7.305453855457124,5.2305627099733805,5.8832572561017695,5.926066865996088,2.738911129204279,4.306540804872476,5.174810597305953,4.421635819482087,7.32796479564443,5.2845201029892115,4.711265192095534,3.489063442019487,4.925007945524841,4.086978685788609,9.332406927130178,2.4349266036408697,3.5615406246213523,7.046868373793345,10.590812307767218,6.330294031944758,5.212708717926096,6.014533993167443,7.554581393174426,5.627975404330586,5.748377413658679,4.4647502587324315,2.5839499616287296,6.139711789361061,6.829298113225783,4.432836960897058,3.898065301598972,7.100144797593312,6.704006450917722,5.128671264516867,5.958425489058398,6.519273812610274,3.6883435892187837,3.4473850310971295,8.221885241511616,4.355901376182805,5.202332108787081,6.795190096912534,5.465171923794893,6.539964898115086,4.559988217530617,7.969446022334581,4.612604853237739,5.940945504545359,5.582636514173156,6.4437442771740425,2.6055930319743306,6.990212241746852,4.9900754003556465,3.2877797446267953,4.5840530523986045,4.068423609333479,3.027611701563243,2.869822564942472,5.2498521979416335,3.5958486922026274,3.975674003065347,4.186006862411655,4.070954056118059,6.381264635516159,4.581980944674693,6.303877145945882,3.736043181756769,6.370620788311288,4.975622135992873,5.359582054651703,4.158413397947448,4.426903773764677,6.176795103404771,5.210986176766999,4.384999340767437,5.062895074113453,5.614315435466234,5.898074964136853,7.0835695184672804,3.1119055395112585,6.364714290160797,3.84958861433385,4.08893146478708,5.9179266836366295,7.317312136243375,4.06835267734507,5.109717582649321,5.181467369647824,4.27523254660665,5.886974760435426,7.329927610349308,6.424337864265021,7.357020664640988,2.6364050683164035,3.1605065538379544,3.388445955296049,5.86296967894795,6.17983168435944,3.356924645482983,3.903075585325721,4.978790365309042,4.295654583032707,4.638660004127835,4.918844957806785,4.021136237624778,5.775896598695573,4.785301316873591,4.0202560345494005,6.132028890138901,3.8132194870946967,4.3756125131581705,5.317388436607992,3.021383432557482,4.824828503908975,8.317766297338542,5.336292870050924,6.405907576765922,5.192756711454201,2.6667599838879057,6.955256608290515,4.498726841853092,4.730527616773662,6.144563830109211,6.019257533184257,4.630945928204286,3.367739524390517,4.870500684975095,5.715224409855986,2.846974171730145,4.988817778756723,3.5996250725863392,6.877591813823355,4.62320906969498,6.014820266643393,2.6525296655052752,4.192514087273028,5.561785689356687,4.8677579690804125,4.6438089102266,2.803693921499225,10.086322948784307,4.354736840536713,3.8146862488504314,6.5513715137438675,2.808138290852313,6.447943930160528,3.9023453584017855,2.6848454179247954,5.705599214093669,3.913547924361254,2.5650533945783067,8.201955076654095,6.67894506206194,2.6927263629394096,5.936126186891813,3.861984274395185,5.790523285733686,3.4407293639262644,2.4600329311619897,3.393467295051341,3.5788536152424695,2.594065329869977,8.18558316188873,4.261308740159442,3.8453494632884873,6.248305363970508,2.3641698548728542,5.7405700547467475,7.144153901638417,4.777201901471961,4.876954157691164,5.265829402718214,4.407229072356456,2.874839223760369,3.972743341941099,4.982206442613333,5.350766597901778,3.342433993934222,7.403741814434295,5.573298585727112,2.825926777881625,2.896830363300165,4.598491660247916,5.129120521062814,3.0094703174401105,3.9910001797052335,5.990824968846403,2.4829903701566955,3.480902411797654,3.6372765827394375,7.0431772986085734,3.959059530843614,5.893961762793457,2.6579900763218323,5.564849326391309,7.541513265875038,3.7939698146109295,3.0255065327699935,3.939401797916346,6.3419308538619426,3.386495919427488,2.7842922413043696,3.933257992744645,5.39534574158908,7.5368351298157865,5.230976420618897,6.5238865768801375,4.640527488767057,6.823006419797153,3.5476981900544216,5.460926548906583,5.999979590702735,2.814228677859684,4.2986485359707896,4.259930306516301,6.886384895778479,3.8501112095610623,4.744056336317434,4.900479779435666,2.4708209045953415,5.9382592926558475,3.4725019034759383,4.116065990222003,9.368514948596054,2.881342536707881,4.50764600222756,7.416460614399419,5.444548980078777,6.515163192973556,4.197782042654909,5.392155654237266,7.173999597495194,7.61752240666555,4.108915044596038,4.200808079829915,3.058897956809412,2.6184553312876533,2.869590156246455,5.40775604235617,4.232922637067826,5.071717300305434,5.189421658718917,5.67138817010648,2.3321534969000104,6.383477903894174,5.926593119726485,5.509847223350194,5.551059479296402,2.7439859051947444,6.705582131253856,6.568421521257405,3.6316321493812933,4.56491063693892,5.190248383413368,7.463630015072019,6.930333335457494,7.660088295344813,6.352391014262723,8.046952435005872,5.212119105667918,4.331539953143308,5.214806964474116,3.026862384820325,5.181862634470127,3.86090503588779,3.2685842753311043,2.7543767092553617,6.562898947771824,4.405613410189166,6.006696784040424,2.3624821098911064,6.867967449184876,4.824346747170508,4.03371459533097,6.897354571922546,4.6994277402548725,5.287097917279048,3.421038744780234,5.551348841186388,4.640920968019125,4.052025529479114,5.609006119721865,9.201211957320796,3.9566019736747657,2.9894033494932333,4.900040001043395,4.19290673026271,4.114643105028639,7.868622411740806,4.372591409497309,3.5450897238799306,2.763984404215124,4.182398217835899,3.5002996433065725,5.405729935635032,4.92653900482175,3.9670629730696025,4.677976691989146,7.898010245174918,4.9339509656579486,4.605604109217822,4.455879208910781,6.05521061330572,4.234783849862335,6.414953515590226,2.6036280788645687,4.9874368678623675,4.365918585486619,4.498728256142296,2.240957216715671,5.291465503410192,3.387868064502528,3.3831866066168796,4.295747948662563,3.1573514880767903,2.8340394990348603,6.179107136897973,5.171704202209822,4.405711068056246,3.917072290862449,6.128780102737151,5.692991962447114,7.299983599016977,4.8548564316919744,4.26907815271696,3.0020564266937826,4.677420448866894,6.047133757305147,4.168821712823835,6.644781192905299,5.938960069903705,4.304632461918059,3.6593041244338016,6.0430012195820515,3.285792831980758,5.9251713338594705,5.487658375708981,3.7796169008759084,3.4809831756543597,4.670056766090281,8.847638947196982,5.015591985229959,3.4478932644382194,4.89392657664753,4.9655757565188186,4.8714337780893295,4.297737258073728,7.7335604871836665,4.305795386538332,4.157188094621042,4.31416832769771,3.4346824007473966,3.7526548619395586,6.074835766389051,3.7166774061024386,6.694195878161421,4.417203277882694,5.919055175100816,5.063872260802461,2.3572079388621514,6.447715120186466,3.9371327596089625,8.35001834302573,6.484783334100688,4.06075913087396,4.754013219788891,7.200227892072013,2.4196058391778448,6.3544076998367505,6.93416725958466,2.8161575254605204,5.576796375318583,4.569923771217283,3.7847885919125774,4.720823889876493,3.441060496436488,4.730800866938384,5.074021983141317,6.021594892121467,6.045543179075497,5.906303375452557,6.150305314001125,6.739921343350493,4.080857076056361,5.7454251007892205,5.096086911812043,3.803801064246519,4.230476624315932,1.2127800318512463,4.5569228811767415,3.402233542950377,4.239410611940539,8.060223601094231,5.879428129905802,5.129885934134736,7.787292627914754,7.141241114271289,4.239675567447955,5.633259969830148,6.279665094215221,6.954679826321015,9.117698075927189,4.673542859771442,3.497218868009565,5.628466164841558,3.756594848130602,3.146937041538923,2.810410219035864,3.4535986005172665,2.7885957742069283,5.0358040745235115,3.804512891113304,5.622438704846077,4.835923909996439,4.007306082569312,4.075008874409479,4.398644707255458,3.6204529367838765,5.311078640623544,3.312330286751563,4.42701246239106,4.791624434089759,4.544402862474812,5.298561269947287,3.091223228725141,3.2332245700844977,3.146662767798511,5.706713654105506,3.156275483986182,5.60089625679262,5.137153307084883,6.189704308297323,3.9894016528011185,6.5959495482774155,7.857299023517747,4.81766140526524,7.10117895739051,5.614192533715009,5.3832565703975,4.020167935219949,4.014774013462812,5.826237312749019,5.957687837186055,4.402373096804151,5.918180024436172,3.5280728640144874,3.4710821429759795,6.957659651727786,3.0301225254628004,5.094185447866201,6.73945594756162,4.886323196495838,4.744258114030329,5.719253625817321,7.558129415535655,5.37070359861362,4.25385289332753,3.8596094749480465,10.996597132062409,5.057813990325366,3.4934143232374373,2.80401946953824,4.428664583276396,7.275350422596265,6.0301186110281595,4.714268951982835,5.169702355896414,3.8936459099995933,3.75908695858415,4.903392270636383,4.835231449407143,3.7385140080354016,9.343329376394665,4.473642978234052,6.996224837437908,8.47960654169406,5.0725748048112,3.8274795731530267,3.680685268854719,3.7204248801527244,4.887824614040465,5.7504042814511065,7.636167401255842,7.915759087024002,7.15019510285393,5.523715924942348,3.7032725330656993,5.3660601921473505,6.191941538697167,3.669905776344101,2.366038437785174,3.3127938857419745,4.4787359128130335,4.392812444251643,5.88115662586619,1.8811593285298305,6.162146495302265,7.70018673021987,3.92747829648955,4.6668421115080685,2.342861580741779,7.380853671929076,2.938644520881588,2.620823771887208,6.394700884310011,7.518067691102726,6.820607497666494,4.128557477412202,3.274175139974274,5.626066238115045,4.413609815506365,4.898092036483387,3.7062819268453797,4.499984874104375,3.3668696610225575,4.928206947483589,7.895988591879786,4.691260380292013,4.146276447135345,3.3510405580103058,6.542849943161429,4.674918424599439,4.689735191277441,4.552097210217833,5.4461196230055755,4.948043270298072,4.898438221055683,6.042126244397944,3.7318432303739106,5.074604290486935,5.887835302029767,3.1040315759882877,3.732136955882375,4.222453608450556,7.648894770258077,6.955312522173099,3.7755379276193146,7.550309370058049,4.253044117354964,4.533504047638851,3.645738297644094,4.174909844390328,3.3372844168065634,3.6051997484673652,3.8490463453739876,2.7258202351849046,6.190415163528925,3.0924444792159425,7.256233407991435,7.496153494431911,6.9897178902683885,6.462312114446028,5.959723663152492,4.3872930960170144,2.552665903081807,11.04273329770463,4.650595753019315,6.486040825681546,5.31770727824374,7.132012055697122,7.760353949244794,2.900089587215288,4.893963070437995,3.466960751640374,5.04484497961453,2.7242111845727868,4.167782258475196,5.989613493279513,5.406378463473734,4.6256495398544,3.3577342075373067,4.799161055643987,2.932823518283285,4.269157065249285,5.189876421883067,4.790628341650659,3.031046753068232,3.3128224071035937,5.539992087161773,7.223263989059268,5.755611320451346,5.627299417836939,4.282981910511386,3.3613365538402507,4.6001862245702005,3.5413917799135786,6.188242334394048,6.311818637336811,2.806979382939572,4.396284049627083,6.430587361884645,4.68429159312727,4.557396184949331,9.781747087063817,4.769719407878859,5.901253824256103,6.9500908062312385,5.786488217304363,3.720240140391233,5.53366390500818,5.256827180832496,5.221387127629281,4.022804741759287,3.733738610467068,6.073148394612905,4.222939785258355,5.934393897451801,6.841294365405268,6.2948116510001055,5.173198220872889,4.885485703887666,4.93809036689735,7.4057978847584796,4.946826964419773,6.331241180033086,3.8326307834093116,5.435896439408908,3.886442178645698,2.5153098406890186,5.989369965869252,2.9385799674491833,6.34818411619952,5.688488976235436,8.541721043713492,6.666807610980991,3.2208350373288983,6.63789236649192,2.925624275395311,4.822489286707726,3.5330539419837472,5.838656292905892,2.7093402501775556,5.7629890060903675,3.4764411409078626,5.115801043767923,5.5690549320763685,5.770087192974148,4.661596482488902,2.6367842479419297,5.736259407236551,6.0765413957472125,3.710493030267699,2.875870091153013,4.436522577032467,3.7896897447831503,3.2741764013703087,7.333309252376081,6.866285519501759,5.582836150031881,4.6456177907279015,4.05297211702512,3.021151069566271,7.816405844925751,9.061816380053799,5.79349411560063,5.791229737936103,8.382105142487937,3.258745226711464,5.235305264368507,5.561944658929152,5.514731409341916,3.442363200093167,4.53161433163035,8.133950709742555,4.436900880253492,4.254352664612663,7.418369485173138,3.726095958415057,2.480591994750854,3.9163162244894396,6.519696543391659,6.368023406199968,4.164926883954949,3.9332140321328586,6.179747053700745,4.95168586311788,5.01156533760355,5.321021676832337,5.5169203315287385,2.400554414294403,3.563185121587292,2.7292240163277106,3.9940358417059696,4.639331367670873,6.722589271697377,4.08392343371313,4.101765609663114,5.272716733630128,2.444358724902239,4.993368982020224,3.1280828416910085,3.3407687893294065,3.5567641892190456,5.5554321589412785,4.586878916970968,4.767628364665187,2.9272833986540445,5.866733898564138,6.362458227982224,5.0332509884068495,4.504132331324243,5.613622377355815,5.360596032192511,2.8980686433782945,5.917512252824263,5.312055067766766,7.985923339529377,2.327247907841884,4.57231833683753,3.6408755880969896,6.037886414594055,4.801912644517614,3.208296130738633,3.9805642174913043,4.072741863574659,3.608302976255776,4.508738144913338,3.32911264832071,4.0704941410755096,5.953240945988674,3.9255607789112363,4.1137558953982145,4.068730976944012,4.910304960250702,6.440282759179169,5.3692299963142345,8.249923520759097,3.966535557157424,6.056128162197795,4.241369428863391,2.2164358780418776,3.476877658746957,4.311051139970727,4.42926714482222,6.282158676419265,4.6181177761203935,3.199371232397644,3.793895843315777,2.5104641074282235,5.37092887134658,4.441051131499438,4.706570425513986,3.385305332732786,3.3422103375325727,5.9174742627712,7.009052886004228,5.679464249446206,3.930894436925693,5.387482647297727,4.502384524036494,5.7628129018756304,3.4811972163187876,6.693412165436491,3.0069739120782213,3.8697323527815293,2.956718028536854,3.3586873917413334,5.6727924516184265,8.630983518692814,7.36788788737786,5.156954542383218,6.4976356462928,5.129342563837877,3.3206303459337723,7.996010253678541,3.333831004694083,7.421989823726073,7.25132009315556,4.736374998022215,3.313705002187927,4.117007623318077,8.54392278120293,5.210831213569738,8.620896906040656,4.219289892851135,7.121007014554211,7.289809339032149,6.4715967023772905,4.279734247016708,4.839598872063071,5.688115006930676,3.6311930933502827,5.249583821598374,5.449084715669527,3.5195211337696026,6.681702839097759,8.366609159951576,8.225095412549518,7.188155474418674,5.73103912619118,2.3518207803857605,6.135363521581779,7.065229385638946,3.681429766231701,2.5636332996155184,3.5746633078554,4.732581719028025,2.2428127963521876,5.005244575870157,7.598060067555463,4.705696511731299,4.458568472257945,5.401270931682391,3.681195416789094,2.351471558865488,3.534392439025388,6.16724218566817,5.3457861762539265,3.150166455600169,2.6666873154372928,4.620869298643272,5.081520850104681,4.755488224807652,5.324180477028014,6.383760358083269,5.1912685140310515,4.990018395401721,4.464840459732612,8.068281018658862,6.10557458307121,5.030493653273659,6.484326123816673,3.7826817409209106,5.173292448480461,6.7229498494986695,3.531599960441825,9.259213550643402,7.402061919078765,4.5880036411650815,4.179848713341021,2.898494586838452,7.495652850323621,3.955759109051734,3.130554716844178,7.098731451608743,6.419775892218131,4.230366027088742,6.153109647000442,7.32854836290369,3.424408226622665,7.391645228133141,5.718946737395029,7.752280402053882,5.8617259558529025,2.9420897528167163,3.555771990574588,2.991865679466776,8.391954516504669,5.463991288712184,5.9943677888485976,4.055820064720414,8.038580728906302,9.15258793396159,8.002079953730275,3.823567477726073,2.9847375482734715,4.336134847468216,4.563143056344429,6.221691151725722,2.8750823743943377,6.212037421969025,5.788479830334533,4.631463436114348,7.661863416613791,4.702260173698627,3.2624132950081064,5.082761405434796,4.442839575216926,4.446741002348899,4.742087476502394,3.5372583437233702,4.764996753175641,5.910225860285733,3.790297297545963,9.346521011339965,5.762811978223144,5.434290617931529,4.332880887721744,10.079742572692739,3.857538420777924,2.2319543850280614,7.681777529537665,5.755248110877504,6.0424983973707675,5.78277526042514,8.638726338112214,2.4334263477558298,4.74922909439316,6.6937599568318795,5.26897524573256,5.934340103546099,7.7371528338811215,5.058328843550416,5.896227621080902,2.4952279158629884,3.664744014185877,2.6362603633355777,5.679810430615554,5.606064888072143,4.811544231264269,6.000468526989951,4.271943899530426,7.5052213451448795,4.893538989128197,6.262388624254174,8.02357168559753,5.324966738637369,5.372449049100871,4.4124947059568695,9.151154723006858,4.097980711274473,3.1888072524407702,3.5420638037423444,4.826042284082442,4.706873475150915,4.785903751225375,5.323184239530932,2.7229254329931445,5.937063422265643,8.224365660658114,4.557695237057761,2.78207883953747,4.3211348114638675,5.035346497995605,5.484771027176599,2.3978532179026906,6.2390730767818905,5.443827557115911,5.156938569182495,2.874205713091152,6.532703797009404,3.9258032432318077,5.770704756731396,6.601162509635121,4.761673857820018,4.4034739072342175,4.032151022628982,8.615030877538851,7.906435935649032,4.914750627744144,6.3580021934526005,6.662411580812605,5.260502426291192,3.824682019390737,4.54781393451458,3.1710112787448965,6.899854284541517,4.017644218381078,5.527347154581672,2.772827873542497,8.68884419115193,5.888047390709083,4.717124218653999,9.50609903726561,3.4970055786970997,6.180907314501505,5.628594634154479,4.870588389142755,4.837014053311395,3.7831832249720407,5.212128317015757,4.370496363646189,7.277949335932832,5.546955986151104,2.955432628475273,5.257478288569939,3.962999577422958,5.305704121434788,6.653652209100501,4.422853530807745,4.863507759754019,7.852936185153853,3.5767806735619208,2.8351825636047723,5.348669197386687,3.0985069365712063,4.374362838135372,7.881362331635558,4.545605018317826,4.803979473216183,7.0184871034280345,7.002947871433591,5.136058990129382,5.177219132220184,4.643061235217994,2.5300356394549834,7.159661491907847,5.559056598883901,7.317791790306274,4.193108214994491,4.2614541295528,5.126107904675879,5.215313990360715,6.963685090801726,4.390455304867067,5.632772613584898,3.750047941298858,7.985643282361824,3.322532731023966,5.849459055256567,2.0798165176494066,7.331560038781987,5.3483957078605,3.08754353308562,4.350706875461529,6.533792864576346,2.7089197860090724,5.1168092585278915,7.624330865507514,4.660652546555601,3.647917061898792,4.772899781333472,6.136755341957176,4.616594657358185,5.254498703944227,4.661596482488902,5.906511146013141,7.5513558326592305,3.6632510524988198,5.670972754257857,5.44539599604468,3.9696053566879836,4.678484324481003,5.592946079934768,3.802060696694123,5.201804714225362,4.8239360037072965,3.025808712667764,5.788740416685124,7.140881212822861,6.413976575765049,7.928995918179404,2.1465765807884236,5.935029685870647,11.265732638088632,5.112226841208495,12.00379840899197,6.1485825977909645,5.702842468566461,3.921145503044779,2.903395036336932,6.454063810256954,4.2973096104168755,6.024770767510727,5.069112882886674,10.587815164821722,5.215725567542375,4.946568909346517,3.7533748958844066,4.502430523197901,3.1868709879119548,4.552486469071231,7.724717523281735,7.4423742831674975,7.776641274723408,3.587545779588227,4.911011080639553,8.721118778548071,4.093474446327898,4.614185151538344,5.97433622745236,7.276977209765143,2.8811434764099904,4.421569899789889,3.2039076016368218,4.454904576629263,5.343191020281989,3.202710596930441,10.30875009804628,5.614855143568741,5.132795166981424,5.12688936336836,3.7533709538422775,6.3371870481180315,11.042668250072659,2.0731684933503507,5.746938202574038,6.360133867002622,5.806594514470918,3.355718832267953,5.173976358370443,5.787728354367858,3.2946284671568797,3.738362790855855,6.413447467623493,4.215748287655274,2.882196918672383,4.340039819705627,5.774979701714033,6.977769378121281,3.7765902522208825,2.918337759192832,3.207087813983078,3.841559084864358,4.419517622672882,2.7750028405262186,7.295731016764381,7.325623636930534,3.9366875140940945,7.4483874156815295,4.480755428872615,3.3376707102298826,6.198691275809457,4.174892024029259,3.8653722607573573,6.158352215480298,4.001978208001591,5.259542792928897,5.191347018332057,6.7442420461555885,5.399690409778108,5.521917779677304,6.234406478193586,4.121845540519712,4.755111952852182,5.618496332152727,5.204130192614001,7.643088116241997,7.703300710071289,6.080198181496718,4.007031087927357,3.722244701444762,7.021774057382307,4.292539947813843,7.733026518007811,5.905452246691413,4.947832849610417,5.604953172200901,7.098962184725261,4.578724078549928,3.932286373584778,5.3244258891761085,4.221144364087062,5.525600960423871,5.482694313639995,2.514875326875056,4.499452224377911,4.998617851791563,6.001894451372308,5.493590275007419,4.925869708480675,4.928446432005186,3.449751146104893,3.4278290591518363,5.3156257537452625,5.188248973689123,4.0146416351756775,3.914895489631693,3.7220218780984795,5.450532009032174,5.920610732097709,5.971060379921733,7.008115282092683,4.912408094965615,4.0501321818828036,5.857800585079153,6.539653096244475,3.7601675376632038,4.583020537168141,7.039333832190832,5.719699935210574,5.251095239547351,5.44753619967571,6.579133750433443,4.940332940080514,6.1509104808617945,2.5678456172848207,4.579039972827681,4.7727327306899925,5.838319216377542,3.4454827909485277,2.9575859115801912,2.7819545725319474,4.122158714647931,6.548217616538388],"xaxis":"x","y":[3.137503523749935,2.4646682670034443,4.808385050656093,6.381629467033664,5.687900522480746,4.296457407371244,2.877744249949002,3.542258049766918,2.752748591407134,2.1984941536390834,5.6604951318885135,2.9763636357327616,8.749098443586298,2.1043366598147357,5.40973035386445,2.3305584000308026,2.3950627995175777,6.141391916170729,3.76659516172927,9.707290101958575,8.663593681954685,2.9126498648972037,6.819668183496455,1.6553518286125541,11.880218691860534,9.057883448899627,4.445594291341182,4.326249700837456,4.708739041359579,3.5570424152637194,7.3848269116030085,6.815319245777306,2.2600256559614555,8.198690585668917,4.72137265947682,2.6712933724815757,7.400708693037901,3.6229303509201767,5.137503523749935,5.359661722333877,4.571676809970931,6.199672344836364,8.475936042446387,7.905507349949873,3.8933622107638715,7.537839615916466,5.353323291162897,3.311793717753649,5.2712762591759255,4.778208576398088,6.628919373511362,3.1858665453113337,6.145473670317708,6.023698917266407,3.145677455195635,5.683696454306515,6.109569509370688,11.964503324303022,7.672213165334465,3.8787253414801053,4.8283269264148165,6.010108453474289,7.029563314432658,5.911212201558612,5.9590746011405145,4.0232553523003025,4.234961094839504,3.41006969175638,5.59215800212536,5.982537314399271,2.910732661902913,3.7687136570304847,7.183982678112831,1.0976107966264221,2.6016965164809576,5.511910748670928,5.6570683012015754,2.2868811477881614,6.999098033705607,6.746447139362815,4.83035674741922,3.411426245726465,4.77663042305105,2.550900664647523,5.942279901705763,7.626950122372515,6.862203399053224,3.4019034716079584,2.608809242675524,6.046141781644721,3.3936907641874536,5.390254956321868,4.541638734436141,6.063718640152248,8.226941966807852,6.203788453308596,5.530445354692906,3.2433644256936605,6.703349915166396,8.202417721575182,6.075318692553354,6.401732849522621,3.3305584000308026,6.606294160697713,5.5786368624717415,4.273515889702116,7.668814088419117,3.097610796626422,3.321928094887362,4.593353770980297,4.792334805706692,8.090535901003173,8.367589480412983,3.1358631653686793,2.7070829917717063,7.1593668097437355,4.614709844115208,5.946496941203293,6.783456654360239,8.491372117815171,7.273609132231856,5.944624223055033,3.070389327891398,2.2479275134435857,9.095924419998536,2.4489009511451276,7.048759311919856,6.058532970201611,7.216163813388774,4.46532151358055,4.871350840634026,2.611172380044005,5.783718557790366,7.3824943402931185,8.12401788912289,5.2787282129389395,6.099295204337775,6.1705259991768475,2.269033146455237,3.6392321632492775,4.547819956577703,6.003152448666922,7.816087658600206,6.762348815644129,4.940166750482817,3.01435529297707,5.487357715311547,2.333423733725192,10.642557179595622,2.503348735167504,2.4356285940520905,4.773468927905195,9.760270854333914,8.156942620908886,6.218781167784069,6.306517445249819,8.352882086522959,7.032211069322931,5.5963388642052,5.320845667645722,3.0089887832272546,5.943218087426284,7.3403842354553985,4.443606651475615,2.845991770664573,7.217618485351048,6.137913322088407,7.084383221650577,5.784242222057781,6.097610796626422,3.605257262939004,0,6.954079959320971,4.883620816285672,5.118941072723508,6.583308464847678,6.034963989749303,7.257764968571387,3.91169158187234,9.10522758436473,4.507794640198696,5.001802242633985,6.858478390423273,8.095608004811197,2.3103401206121505,7.6398833131998005,3.144046369616707,3.9420452599160467,4.142413437873741,3.925049964727359,1.835924074254375,6.111448698487674,3.363171077119244,3.127633279725874,3.24031432933371,4.160274831408593,3.5397791916984938,7.571221725048103,5.284292026394312,5.47800105583793,3.7398481026993275,7.1320626778543845,5.576522137920503,3.127633279725874,4.3110671022555955,4.375734538583156,6.4432751117658045,3.91169158187234,3.8042601156347384,6.4409521980296365,5.85872702377863,6.02724253597374,5.212958362728285,2.5058909297299574,7.561631630375841,4.92552476974757,3.5619370596224327,6.237448995639321,7.881113960675097,2.176322772640463,5.361768359419154,4.9941271140031285,2.584962500721156,6.166916251569974,9.4493756610678,6.0332027259372225,10.919861451442168,2.693765712217783,2.4724877714627436,2.7484612330040354,5.633721812641012,8.17707065762942,1.9634741239748859,2.7803100990433753,5.23687524536694,2.698218478224414,4.238022517825721,5.317231700222606,2.1176950426697547,5.604071323668861,3.2555007331483865,2.2387868595871168,3.881664619320345,4.493134922305505,5.357903838552492,4.9597701552114675,4.724650271732967,4.4046306842176115,8.46707564951531,7.096029876307993,7.341896820977107,5.339850002884624,2.859969548221026,6.582706526780788,9.101634319640722,4.129283016944966,7.151270288790165,4.58014548442338,4.628773595201645,3.070389327891398,6.034303767768035,6.339493737901733,5.174326515173915,3.357552004618084,4.809414444235898,7.880195728943098,5.632268215499513,4.79960542245267,2.077242998932461,3.5298209465286954,5.433960931121179,6.078951341394822,4.854494418154875,2.6484654430273142,8.345405246717796,3.4462562298895643,5.595145567990858,6.766065051474954,2.4222330006830477,7.366846735262642,2.9873208659292536,2.6507645591169022,5.940871478189196,3.552131108253784,2.4672794804599825,7.901409979999464,7.459185683748918,2.3868109464722167,8.668955876629637,3.1667154449664223,3.017921907997262,2.4982508675278257,1.7782085763980877,2.9088129077395473,2.1602748314085933,2.3074285251922473,8.316598324108684,5.603477988254004,5.669310286202474,7.991691898336964,1.0840642647884746,5.091276694365222,4.042644337408494,5.96254902292306,5.020591094773247,5.20163386116965,3.065227622775619,7.08533966935737,4.321206566969903,3.5298209465286954,6.083000565676004,5.04176864995163,9.435170178881494,6.4705368650925825,2.0531113364595623,4.078951341394822,5.525755692829486,5.9818526532897405,2.063502942306158,4.110196177754199,7.135657989368776,2.3812833725037836,3.693765712217783,3.00539998774259,5.589164236699772,5.36737106564853,6.38646609286931,2.3248106034204836,3.1811025507537978,8.27891401949994,2.430285272977781,2,4.867402305727543,6.299391206126829,2.6803243568440163,2.693765712217783,2.7949356628035362,6.605701738949982,9.159820892013935,4.255500733148386,5.555509427083257,6.871104373555458,5.632849830117706,4.966707314150316,2.526068811667588,6.09950561709114,2.927896453728821,4.28835856219366,3.1634987322828794,7.07692245939234,3.8429788317883253,6.341274183692174,5.926948247949772,2.538538163629804,6.73226919950145,2.788685710613534,5.508111681193465,9.398936038454162,2.3161457422933562,5.28835856219366,7.479861085016818,6.0925457415435655,5.747655932932393,3.9059284781731347,6.235152624217932,7.3334237337251915,8.870210577263595,3.460742563789644,2.974529312483882,2.6016965164809576,2.2387868595871168,2.7676547982373463,5.659639187015652,3.894332742277694,5.771093251699539,7.173327349650988,6.027463723408577,1.4222330006830475,5.794675787986858,4.720825666089835,5.342696959981934,5.562547724199122,2.62760683812965,6.426767932655541,7.593353770980297,2.5185351389821804,6.676239195086378,4.472487771462744,6.212180210044431,7.178117045706399,7.534497433738168,6.304876050044889,7.545505100691072,5.077670274232752,3.953265239014844,4.609991295212678,2.9126498648972037,7.174426393677253,3.6016965164809576,4.515384460636948,2.503348735167504,8.129128434194934,2.9726926540042644,6.030998118171895,2.2539892662307865,6.597829097826899,6.8209447918806925,2.9523335663696857,8.18769847638827,4.950468414150123,6.1409831048784245,4.674121632632394,6.830610274357907,4.4376272483189485,3.7037651787934096,5.95837871156634,9.296618003796326,3.351910961103077,4.143230134776893,4.707082991771706,3.560714954474479,3.9845893503624565,8.328001662149257,2.1667154449664223,3.1811025507537978,2.4854268271702415,2.931683057059806,4.788685710613533,5.659068274843228,5.738227400367242,4.6937657122177825,6.860962797858116,8.65474342118394,4.814550423461808,6.107687869314374,4.720825666089835,6.4775154361842375,5.619119511453218,4.7136958148433585,2.5533605033353277,4.412781525338476,3.72137265947682,3.460742563789644,3.0197019144425483,6.602587527342561,4.687060688339892,2.526068811667588,4.698218478224414,2.8698714061777126,2.144046369616707,5.394376944957055,4.649615459063409,5.80683958179819,4.443606651475615,5.450881315273369,6.6503337538459295,7.198494153639083,5.228049047884462,3.9231491804723087,1.3840498067951599,4.524815928357506,7.211012193485511,7.027021314622255,7.212958362728285,6.658497136656478,3.3881895371560837,3.140778655782796,4.37364821133469,3.319039815562536,7.33360262826828,5.175524601089875,3.2047667506546134,2.702657543390911,3.228049047884462,9.322806969423008,5.561631630375841,5.8755343515732354,5.032100843167024,3.9873208659292536,3.1842802944193824,3.277984747299765,7.444849247263915,3.6701605141266334,4.595145567990858,3.8369340113210915,3.575312330687437,2.788685710613534,5.4571345943180365,2.4276061727818994,8.563615765629766,4.712045448553693,6.260590274730996,0,2.4594316186372973,2.8257856274647914,3.049630767724601,8.097242070141174,7.431622959713292,4.030336078370959,4.964398632203184,4.020591094773247,2.211012193485512,5.462052318796433,3.3881895371560837,2.411426245726465,5.0430819818956625,3.6064422281316078,3.613531652917927,3.883620816285671,2.8318772411916733,5.081936081719432,5.575917361118149,4.958842675243241,6.716579420796345,6.531225485053606,6.050284014301706,7.73125113789623,2.950468414150123,5.645009884391095,5.4015622072560685,3.840966704487421,4.3298411765306275,0.8155754288625726,3.7070829917717063,5.848748114611959,4.303050084681673,8.796785917134267,5.497931651896279,4.67581593117227,8.189231548647284,6.9092930858238235,4.772941337831336,5.6082178530214595,7.304419775319909,6.588864518190148,10.665637927637095,4.672990993827748,2.8399595874895316,5.305605789585429,3.0908534304511135,2.4854268271702415,2.693765712217783,2.942983598187102,2.5897634869849773,4.631686366311998,3.0125686735030555,5.651051691178929,5.488964817004848,3.1858665453113337,0,6.703073006328523,4.255500733148386,5.9593064897596575,5.218781167784069,3.968090752045256,4.8594726668519375,3.460742563789644,5.341274183692174,3.0089887832272546,2.032100843167024,2.169925001442312,3.356143810225275,4.814037647539975,3.4168397419128294,5.653060017104565,6.074462620704536,3.2016338611696504,6.9656688644419456,6.816215687622865,4.330558400030802,7.16380060395775,5.6061460780657235,6.093179846765504,3.5716768099709313,5.342341397431548,4.228049047884462,6.310885391185277,4.751677945687579,7.502871574574138,3.953265239014844,3.975446765640962,2.6599245584023783,2.430285272977781,5.7303682363490065,7.689648608288815,4.371558862611963,3.215678596607928,6.492173659610827,7.480507489974544,4.436295119839363,4.804260115634738,4.405311682597445,13.396289023308677,3.8914191868460786,3.24031432933371,1.899175630480513,3.2942531364445142,8.382105211486744,4.143230134776893,4.92552476974757,2.8439838440483265,2.8399595874895316,6.101187815259738,5.0232553523003025,4.583158003874408,3.176322772640463,8.996303999184056,3.8257856274647914,4.7755773609306535,8.12401788912289,5.9618548076361595,2.454175893185802,6.208088008224774,2.673556423990145,2.8972404255747994,6.791683858152045,7.137810883416096,5.8052924556007115,7.338067797542616,5.198494153639083,2.851998837112446,5.726013748860151,6.567880042052731,2.8155754288625725,2.2387868595871168,3.6769443591069124,4.318316841334983,2.1795110502715103,5.6884601404244926,0.8073549220576041,7.244982170376553,8.642268351758643,6.836681593327116,2.73768676140986,2.4646682670034443,5.964860664245848,2.7420062108667365,2.411426245726465,7.279842693520348,7.446835177086041,6.531849285751203,3.8982083525087177,4.721919445551544,5.500164679492167,5.429615964201735,0,3.7311832415722,4.726831217032493,4.904002316283692,4.561937059622433,7.438459204423481,4.631686366311998,7.300764373248062,4.112700132749362,7.9329824099498465,5.1081062236861285,5.868143479666726,4.161081482277184,5.996162742681061,3.097610796626422,4.263785613890803,6.339493737901733,4.370861740085285,4.842978831788326,5.636334602442182,2.84197311892718,4.7713574089905615,4.257010618206024,6.916596067892185,6.015247773698937,4.595145567990858,6.7410624357490905,3.4168397419128294,3.823749360308273,4.279471295644468,4.7114949066500875,2.883620816285671,2.969012307516316,5.089159131911238,2.3362833878644325,8.080604371320732,1.8559896973084806,6.941341105485299,8.14639047567022,7.163498732282879,8.281651890258825,6.329482431025636,2.140778655782796,2.3103401206121505,9.500782139394977,4.674686619927999,7.249066033855867,3.1060132376221192,7.897179908573977,7.6546360285279675,2.944858445807539,4.94673086014031,2.711494906650088,6.054414387917307,2.788685710613534,4.198494153639083,2.9467308601403097,4.060047383669939,3.442280035252584,2.7070829917717063,5.9586107120558465,2.4982508675278257,3.6484654430273142,5.0870384567669475,5.130519083027338,1.8155754288625725,2.6064422281316078,5.673839055990439,6.37364821133469,5.609991295212678,6.076388068512841,4.695437110405369,7.459267666703387,5.211401637418471,3.063502942306158,5.931210274803932,6.819540460505722,0.918386234446348,4.878234879112339,6.098663781886574,4.098453246309274,3.4154882710497003,9.8361608416947,5.3103401206121505,4.20633064787105,6.436128517259249,4.381283372503783,5.0259146987507926,5.226123143125237,5.73903797911741,4.110196177754199,3.8042601156347384,4.503984703897613,6.067165427017112,3.7322691995014496,6.121844297876376,8.248591759535907,7.711013010084339,5.782670658677007,6.0774566524005875,7.592232766722112,6.917909073852982,4.423578170981798,4.901108243014512,3.795974694206668,6.907251224292033,3.189033824390017,2.4646682670034443,6.061128135118254,2.733354340613827,6.664198369291911,5.166715444966422,8.492293852480074,7.279749853014451,2.807354922057604,8.362689046726734,2.3248106034204836,4.826294245499635,2.321928094887362,4.238022517825721,4.232660756790275,5.51127826508303,3.7644735509926663,5.290940402403678,7.535353147842131,4.329123596291566,5.908092340818271,2.570462931026041,4.634012356402116,6.99491908871511,2.560714954474479,2.3589588258323295,2.8278190246173196,5.439623137557117,2.927896453728821,7.476543706214256,8.334094473911568,6.726695004499616,2.097610796626422,3.619413010597937,2.523561956057013,7.136683577697236,8.908992993250534,4.4950555283680185,6.970393537914677,8.089159131911238,2.996388746447621,3.7506065048355923,5.468583317006828,4.732811872082943,2.5360529002402097,2.729008870337863,7.712939631305699,4.038260575175349,4.514122260170708,6.948834425578686,2.0036022366801953,2.169925001442312,6.04023491675437,7.246788093844365,7.018923431842335,5.046141781644721,5.637784110291324,6.976363635732762,5.840463233869542,6.170125361839952,4.760753208062987,3.089159131911238,2.8399595874895316,2.3978029618624896,2.3305584000308026,3.0925457415435655,2.6599245584023783,7.9217814432688,5.963242904318827,2.4515408330178317,5.976821852360685,1.7865963618908067,3.221877081077034,3.0036022366801953,2.722466024471091,4.2024177215751815,5.9025560053446275,3.997292407636508,5.323730337521348,6.463851292611099,5.938756261492295,7.593951283948411,5.233044401265581,4.79025073883523,5.718361626138354,6.435295215647167,1.3504972470841332,4.12267271882414,6.361768359419154,6.8820316083521,2.0250287944915226,2.298658315564515,3.636914580355878,7.934634441094905,5.246408087246385,3.918386234446348,3.929790997718597,4.147306698780294,5.169925001442312,5.542258049766918,4.693208148910016,3.6971065744769747,5.707082991771706,3.363171077119244,5.035183996618336,4.274261661257048,3.321928094887362,5.9740703670240265,2.3757345385831563,10.174663577413781,2.8439838440483265,5.24260250592898,3.4581194811745064,2.063502942306158,3.1858665453113337,4.7692427951405385,4.301587646603187,5.986638471730853,5.058749412335524,4.556429415449573,2.5360529002402097,2.4489009511451276,4.974988111991927,6.248307120087241,4.346956889378885,2.641546029087524,2.8579809951275723,7.624539603781834,5.197314999473818,5.38404980679516,3.2387868595871163,4.464668267003444,3.955126781261366,6.258895755075436,3.1358631653686793,7.831180547126736,1.3785116232537298,4.934044647112246,4.503984703897613,4.100977647724821,4.86195536414487,8.439830883981392,7.851624127346895,6.091065077930535,7.190022174802645,5.992541859109112,2.983677694698067,7.161484638641659,2.6959938131099,8.21003812347289,7.461643148073906,4.848497755385146,3.192194165283345,1.6690267655096307,8.80290404741688,6.926948247949772,0,3.812498225333564,5.6806056662025926,9.871443254934135,6.459923362676901,3.8298495598446904,5.157447996274298,5.537296067090842,3.5147534984397533,3.2249663650002742,5.699884728697476,3.035623909730721,6.914444931746792,5.338780943894699,9.013601832683456,7.578712331048165,3.3405622690264134,2.3950627995175777,7.221587121264805,7.78959885005233,2.2234225499349374,2.8439838440483265,2.950468414150123,5.833649127594984,5.802968651005583,4.618238655595454,7.581803148516525,4.948600847493355,5.416164164733128,5.012121672712217,2.9373443921502322,2.3923174227787602,5.259649120649155,7.119252412157343,5.452858964713811,2.5160151470036647,2.788685710613534,5.040892430646901,6.390942772802543,6.032541697270038,6.177519202597814,6.111865963867557,7.0083165637349865,6.074034394212556,4.471187460386985,9.507715369062607,6.152994605492435,4.169123281476757,4.706530552538117,3.0426443374084937,5.305605789585429,6.942279901705763,2.4724877714627436,11.02948049374083,7.127736443588829,3.594548549550354,3.8063240573900288,2.144046369616707,7.036942844798256,4.022367813028454,2.0214797274104517,8.589726041133547,6.218781167784069,3.685940148445977,7.566662962962611,6.652343078740216,4.210232990095849,9.47854718263086,5.991861930556323,7.500961352460057,5.5855634984556914,2.319039815562536,2.9049657186840263,1.9781956296816516,5.509062386361898,5.592756010441026,4.39780296186249,3.431622959713292,11.193297418854996,7.925940095994448,8.041385369486115,5.051807107013298,2.3673710656485296,5.208673319629471,5.532940288372874,4.598722499676621,2.903038270112912,6.098453246309274,2.967168607532628,4.919340082442012,7.971830914780502,4.189033824390017,3.10936055940423,4.053111336459563,3.944858445807539,5.71699089440494,4.2787282129389395,3.7246502717329673,4.568032104771279,6.400025517914023,3.0443941193584534,9.140190703191836,6.7998644108818525,3.3923174227787602,4.880685525261125,10.32396671481508,2.521050736900963,1.8836208162856714,6.547203024756931,5.63023071585969,5.915042791371977,5.713970692436053,8.823876711241038,2.6530600171045644,4.462706750670158,4.524189078449365,5.342341397431548,6.70362677086498,8.008596693255853,5.81326814176608,5.795715006501729,1.5897634869849773,2.5058909297299574,4.153805336079036,4.707082991771706,5.617651119427331,5.7615512324444795,6.115615931428109,5.51443791382931,8.873136468833458,4.8811751553492755,3.4276061727818994,7.013685569928653,3.37851162325373,4.390942772802543,4.544114402209474,8.617283787779035,5.39677601078753,4.096767854714689,3.169925001442312,2.881664619320345,3.389566811762726,5.949301468328278,5.9095331149468935,2.2172307162206693,5.687900522480746,8.295998461911388,4.963011647599427,5.648177795724818,3.8509993947164736,3.669026765509631,5.924575003392735,1.2570106182060237,8.1184220243132,6.3103401206121505,4.59215800212536,2.169925001442312,2.771885578515366,5.309976492370556,5.718909554379232,7.2325648297287755,3.720278465233327,3.403267722339301,4.138323004056484,8.321792835886914,8.384999544730041,6.675392543042777,7.848935855524565,8.135247549809566,7.474760499576783,2.01435529297707,4.661635602335959,2.899175630480513,6.950701690089008,3.4776773275653072,6.275752048828529,2.7782085763980877,8.645802514606062,8.927066807759255,5.377123749129487,8.785648628652076,3.554588851677637,5.8529975876133165,7.093391153253013,3.8639384504239715,6.390082950951663,3.0214797274104517,5.947198584262056,2.0565835283663674,8.560600329022595,4.708187236020708,3.1473066987802936,5.260778431893426,2.3978029618624896,5.496654082593496,8.356011721502778,5.307064162255372,5.485426827170242,8.478971805032943,2.414135532984451,2.776103988073164,4.7420062108667365,2.4828482830684653,6.543805175964397,8.5918962955207,4.361066488794321,5.294620748891627,6.553360503335328,7.088629255297176,5.1201860275318625,7.00943675560809,4.090853430451113,6.7014104362328135,7.415741768290091,5.348728154231077,6.705563274566321,5.445263208140443,4.552131108253784,3.4436066514756147,3.5631581304028077,6.678494507773734,4.478971805032942,6.1657109924849305,3.403267722339301,4.534186139090097,3.0214797274104517,6.238786859587116,1.8639384504239715,7.020924396158786,4.20163386116965,2.7548875021634687,2.604071323668861,7.128870759465125,2.5459683691052923,5.414473836430928,9.995102877826515,4.861459166361513,5.25285462595139,6.31922050252387,5.308156975114762,5.526068811667588,5.763145958149476,5.903279342056139,5.825531251189622,7.048214384608036,2.513490745588118,5.944155663440363,5.856985689782205,3.412781525338476,2.956056652412403,5.664482840364682,1.74416109557041,4.886062338345599,2.4646682670034443,2.4515408330178317,6.433126376206435,7.7755773609306535,6.705839705883682,8.330737650206952,4.727375938594897,5.777945670695148,11.69784504363786,7.7627474419531515,13.587327482778312,6.558114538598396,6.332349900373874,3.2265085298086795,2.875780063068488,6.70473366267406,4.223422549934937,3.2387868595871163,6.136683577697236,12.23806432823473,5.572889668420581,5.849999259466098,5.533563348214512,5.325530331567558,2.797012977836145,4.150559676575382,8.298429210890312,7.731454807698346,8.903490246967104,2.8094144442358986,4.7824085649273735,8.461602224618334,5.583158003874408,3.060047383669939,6.429950657403768,7.193278935521388,2.3589588258323295,3.7311832415722,2.899175630480513,2.298658315564515,4.235727059838059,2.3812833725037836,11.663771557482411,5.684257705058932,5.212569338850806,6.9787676510367875,4.044394119358453,6.092334311186202,9.582254908357664,2.5160151470036647,6.402415216824399,5.740387932468353,5.784503982929566,2.6064422281316078,4.362469888750209,5.164705840182799,3.4208865749755315,3.948600847493356,7.965841991309597,4.976821852360685,2.722466024471091,2.7398481026993275,4.528571318870758,7.677578712165378,3.41006969175638,2.4905701304462013,3.5147534984397533,7.5673476962842585,6.849624029924543,2.833902076669163,7.406162478777607,8.131085259578953,3.176322772640463,8.148324042480084,2.657640005207824,3.350497247084133,6.8203066288938246,4.483493351284395,1.713695814843359,5.872582544833223,2.722466024471091,5.7742599514327635,5.704318677760832,4.760220946466509,4.294253136444514,6.037162550127378,7.151777654732053,3.215678596607928,4.742545233866053,5.613531652917927,5.727375938594897,7.7780114016125745,8.009884588931843,6.549977142771825,2.0461417816447205,2.0565835283663674,6.66746640495737,4.62760683812965,7.884597920990064,8.571145863602528,2.3161457422933562,8.563272552819562,7.111970261358359,6.001577085389535,3.4369613378336026,6.435461914479276,3.5084286525318573,3.940166750482817,2.5459683691052923,1.6644828403646825,4.94345253855882,6.003827078111476,4.960697039304312,6.684117412839159,5.612352498752662,5.609991295212678,2.62993940943954,3.611172380044005,5.522620761042079,5.891419186846079,3.6530600171045644,3.4581194811745064,3.1667154449664223,4.581351247168778,6.764340846670181,8.066089190457772,6.614857050384495,5.767654798237347,3.1953475983222193,2.9726926540042644,5.279471295644468,7.5105663894487495,5.02724253597374,8.25176642434399,7.822411496360477,5.334496768390418,5.715893370547605,7.3049672876759635,5.998646839005795,6.9725777851765525,2.4594316186372973,5.666472568842073,3.8728287595348854,6.052024560482831,3.631104282365877,2.3015876466031866,2.37016428054021,4.3384244147835584,6.546122758857796],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle RandomForestRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_pred_log_logRF"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle RandomForestRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsRF_log = np.logspace(0, 3, 10, dtype=int)\n","param_gridRF_log = {\n","    'randomforestregressor__n_estimators': n_estimatorsRF_log,\n","    'randomforestregressor__max_features': ['auto', 'sqrt', 'log2'],\n","}\n","\n","GridRF_log, \\\n","BestParametresRF_log, \\\n","ScoresRF_log, \\\n","TotalGHGEmissions_pred_logRF_log, \\\n","figRF_log = reg_modelGrid(model=RandomForestRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=TotalGHGEmissions_train_log.ravel(),\n","                         y_test=TotalGHGEmissions_test_log,\n","                         y_test_name='TotalGHGEmissions_test_log',\n","                         y_pred_name='TotalGHGEmissions_pred_log_logRF',\n","                         score=score,\n","                         param_grid=param_gridRF)\n","\n","print(BestParametresRF_log)\n","print(ScoresRF_log)\n","figRF_log.show()\n"]},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[1.8430408794713125,1.57919346988352,1.4466333608323523,1.3467308241018805,1.3254349004242638,1.301199667938657,1.302919216784804,1.2974430250614035,1.2957766245835878,1.2916037529795432]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[1.8691647165283851,1.6036104214398075,1.4827747759334116,1.3666109175795627,1.3565004777609306,1.3278813750057927,1.3326575514188814,1.3281975698490582,1.3250903911178853,1.3202287349901731]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[1.8169170424142398,1.5547765183272324,1.410491945731293,1.3268507306241983,1.294369323087597,1.2745179608715214,1.2731808821507267,1.2666884802737488,1.2664628580492903,1.2629787709689133]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.8469968398423238,1.5778499895642084,1.4920802861542453,1.35938577028,1.3526505517787097,1.3268649577711986,1.329492099619271,1.3209798326334978,1.3196072272984851,1.3124613004344488],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.8090357411394,1.6016867133848727,1.464952763921647,1.374020581073861,1.357102320995852,1.325899291561888,1.3367680518171727,1.3337605599853133,1.3290593472626153,1.3252254306321845],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.887422733139372,1.6040574454617482,1.4683741745644476,1.3515972626210697,1.3375703841241238,1.3124833389635284,1.3052164209541819,1.308444467128966,1.303982883665039,1.3031511777736096],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.8267144781349687,1.5761407644175536,1.4047706366285957,1.3289895137837866,1.3042913054313894,1.2818469434798878,1.2888495253020449,1.2726208539059507,1.2776113233911623,1.2690400982039853],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.8450346051004962,1.5362324365892166,1.4029889428928253,1.3196609927506862,1.2755599397912438,1.2589038079167827,1.254269986231349,1.251409411653291,1.248622341300637,1.2481407578534878],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle RF pour le paramètre<br>randomforestregressor__max_features=auto<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE RandomForestRegressor\n","# pour le meilleur paramètre max features\n","FigRMSEGRidRF_log = visuRMSEGrid(RandomForestRegressor(), 'RF',\n","                                 n_estimatorsRF_log, 'n estimators',\n","                                 GridRF_log, BestParametresRF_log,\n","                                 'randomforestregressor__max_features')\n","FigRMSEGRidRF_log.show()\n","if write_data is True:\n","    FigRMSEGRidRF_log.write_image('./Figures/EmissionsGraphRMSERF_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.7 Modèle AdaboostRegressor"]},{"cell_type":"code","execution_count":125,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                         paramètre AdaBoostRegressor()\n","0  adaboostregressor__n_estimators                   6\n","1          adaboostregressor__loss         exponential\n","                          R²      RMSE       MAE\n","AdaBoostRegressor()  0.39957  1.573539  1.292772\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predAB_log=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4.547286940274507,3.8483783443236588,5.293668555451877,3.8483783443236588,3.8483783443236588,3.8483783443236588,3.8483783443236588,6.170293938275902,3.8483783443236588,3.7556403250442747,5.293668555451877,4.48578950242225,9.08791474633755,3.7556403250442747,5.065124785683751,3.8251022477096437,3.7556403250442747,7.146518417819807,5.293668555451877,9.100946160258701,7.586480325758329,3.8483783443236588,4.48578950242225,5.096201618632806,9.08791474633755,5.293668555451877,4.513992217328446,5.840089060035568,4.724413102786124,5.068335693283613,5.840089060035568,4.48578950242225,3.7556403250442747,7.703737654554672,3.7556403250442747,4.48578950242225,4.090865746436358,4.513992217328446,4.626320085866708,4.513992217328446,4.626320085866708,6.672806262801639,7.586480325758329,5.840089060035568,6.170293938275902,7.586480325758329,5.065124785683751,3.8483783443236588,6.170293938275902,6.170293938275902,5.293668555451877,4.626320085866708,5.293668555451877,4.090865746436358,5.068335693283613,5.065124785683751,5.096201618632806,9.08791474633755,7.703737654554672,4.513992217328446,4.513992217328446,6.19555112244501,5.536067135919687,3.8251022477096437,4.626961551547263,3.8483783443236588,5.293668555451877,3.8483783443236588,6.170293938275902,5.840089060035568,4.724413102786124,3.8483783443236588,4.48578950242225,4.513992217328446,4.48578950242225,5.096201618632806,4.513992217328446,3.8483783443236588,3.8251022477096437,7.3018266087463735,4.513992217328446,4.724413102786124,5.096201618632806,3.8483783443236588,4.513992217328446,5.293668555451877,6.170293938275902,4.626320085866708,4.513992217328446,3.8251022477096437,5.068335693283613,4.513992217328446,4.513992217328446,4.513992217328446,7.586480325758329,6.208448158424733,4.626320085866708,3.8483783443236588,5.068335693283613,7.096180585885039,4.513992217328446,5.536067135919687,3.8483783443236588,7.096180585885039,5.068335693283613,4.626320085866708,7.703737654554672,5.068335693283613,3.8483783443236588,4.724413102786124,5.096201618632806,8.119394708328175,7.586480325758329,4.513992217328446,3.7556403250442747,7.703737654554672,4.626320085866708,4.090865746436358,6.19555112244501,7.096180585885039,4.724413102786124,4.48578950242225,5.065124785683751,3.8483783443236588,6.170293938275902,3.8483783443236588,8.867028823377424,4.513992217328446,5.096201618632806,3.8483783443236588,4.626961551547263,3.8483783443236588,5.18092665909789,7.586480325758329,7.096180585885039,6.208448158424733,5.840089060035568,5.293668555451877,3.8483783443236588,4.513992217328446,5.18092665909789,3.8251022477096437,8.119394708328175,4.48578950242225,5.096201618632806,4.48578950242225,4.626320085866708,3.8251022477096437,7.146518417819807,3.7556403250442747,3.8483783443236588,7.146518417819807,9.08791474633755,5.068335693283613,4.513992217328446,5.293668555451877,6.208448158424733,4.513992217328446,5.840089060035568,4.724413102786124,3.8251022477096437,7.096180585885039,5.18092665909789,4.626320085866708,4.48578950242225,7.586480325758329,6.170293938275902,4.626320085866708,6.170293938275902,7.146518417819807,5.065124785683751,5.068335693283613,8.867028823377424,4.513992217328446,3.8483783443236588,7.3018266087463735,4.48578950242225,5.068335693283613,5.18092665909789,5.096201618632806,5.096201618632806,6.170293938275902,5.068335693283613,5.293668555451877,3.7556403250442747,6.170293938275902,5.068335693283613,4.513992217328446,5.068335693283613,4.513992217328446,4.513992217328446,4.513992217328446,4.626320085866708,4.513992217328446,4.090865746436358,4.724413102786124,5.065124785683751,6.170293938275902,4.626320085866708,5.840089060035568,4.48578950242225,7.096180585885039,5.096201618632806,4.626320085866708,4.513992217328446,5.293668555451877,4.626320085866708,5.840089060035568,5.096201618632806,4.48578950242225,5.096201618632806,4.626320085866708,7.703737654554672,3.8483783443236588,4.724413102786124,5.096201618632806,3.8483783443236588,6.170293938275902,7.586480325758329,6.170293938275902,4.547286940274507,5.293668555451877,4.48578950242225,5.18092665909789,6.208448158424733,5.840089060035568,5.18092665909789,3.8483783443236588,3.8483783443236588,3.8483783443236588,5.293668555451877,6.170293938275902,3.7556403250442747,4.48578950242225,6.170293938275902,4.48578950242225,5.293668555451877,4.513992217328446,3.8251022477096437,6.19555112244501,6.170293938275902,5.293668555451877,6.170293938275902,4.513992217328446,4.513992217328446,7.146518417819807,4.48578950242225,5.293668555451877,8.119394708328175,4.513992217328446,5.293668555451877,3.8251022477096437,5.065124785683751,5.536067135919687,5.293668555451877,5.096201618632806,7.096180585885039,5.18092665909789,5.068335693283613,4.513992217328446,5.096201618632806,4.626320085866708,4.513992217328446,5.068335693283613,3.8483783443236588,7.146518417819807,4.48578950242225,6.208448158424733,3.8251022477096437,5.293668555451877,6.170293938275902,4.724413102786124,3.8483783443236588,3.8483783443236588,9.08791474633755,5.18092665909789,3.8483783443236588,6.170293938275902,3.7556403250442747,7.096180585885039,4.48578950242225,4.48578950242225,3.8483783443236588,5.096201618632806,3.8483783443236588,7.3018266087463735,4.626961551547263,3.8483783443236588,7.096180585885039,4.48578950242225,5.096201618632806,3.8251022477096437,3.7556403250442747,4.48578950242225,3.8483783443236588,3.7556403250442747,8.119394708328175,5.065124785683751,3.8483783443236588,4.513992217328446,4.513992217328446,7.096180585885039,6.170293938275902,4.626320085866708,4.48578950242225,5.293668555451877,4.626320085866708,4.513992217328446,3.8483783443236588,5.293668555451877,4.626320085866708,4.626320085866708,7.3018266087463735,6.170293938275902,3.7556403250442747,3.8483783443236588,5.068335693283613,5.293668555451877,4.513992217328446,4.513992217328446,5.096201618632806,3.7556403250442747,3.8483783443236588,4.48578950242225,6.170293938275902,3.8251022477096437,5.068335693283613,3.7556403250442747,7.146518417819807,6.170293938275902,5.293668555451877,5.096201618632806,5.096201618632806,5.096201618632806,4.48578950242225,3.8483783443236588,3.8483783443236588,5.293668555451877,5.293668555451877,4.513992217328446,7.096180585885039,5.096201618632806,6.208448158424733,5.750640065034668,4.513992217328446,5.096201618632806,3.8483783443236588,4.513992217328446,5.068335693283613,7.3018266087463735,3.8251022477096437,6.170293938275902,3.8251022477096437,3.7556403250442747,5.293668555451877,3.8483783443236588,4.48578950242225,9.08791474633755,3.8483783443236588,5.293668555451877,7.703737654554672,3.8251022477096437,7.146518417819807,3.8483783443236588,5.096201618632806,5.096201618632806,7.096180585885039,5.068335693283613,3.8251022477096437,3.7556403250442747,3.7556403250442747,4.090865746436358,5.096201618632806,4.513992217328446,5.293668555451877,4.626320085866708,7.096180585885039,5.065124785683751,7.096180585885039,7.096180585885039,4.513992217328446,5.293668555451877,3.8251022477096437,7.146518417819807,4.513992217328446,3.7556403250442747,3.8483783443236588,4.626961551547263,6.208448158424733,7.703737654554672,7.096180585885039,5.840089060035568,7.586480325758329,4.626320085866708,5.840089060035568,6.19555112244501,3.7556403250442747,4.513992217328446,4.513992217328446,4.513992217328446,3.8483783443236588,6.170293938275902,5.068335693283613,5.293668555451877,3.7556403250442747,7.096180585885039,4.724413102786124,4.48578950242225,5.840089060035568,4.626320085866708,4.626320085866708,4.513992217328446,4.513992217328446,3.8483783443236588,5.068335693283613,5.18092665909789,9.08791474633755,5.068335693283613,4.513992217328446,5.293668555451877,4.513992217328446,3.8251022477096437,7.703737654554672,4.513992217328446,3.7556403250442747,3.8483783443236588,3.8483783443236588,3.8483783443236588,4.626961551547263,4.626320085866708,4.626320085866708,5.065124785683751,7.3018266087463735,4.513992217328446,4.513992217328446,4.626320085866708,3.8483783443236588,4.513992217328446,6.170293938275902,3.8483783443236588,5.096201618632806,5.293668555451877,5.096201618632806,3.7556403250442747,3.8251022477096437,4.513992217328446,3.8483783443236588,4.48578950242225,3.8483783443236588,3.8483783443236588,5.750640065034668,4.513992217328446,4.513992217328446,3.8251022477096437,5.750640065034668,5.840089060035568,8.867028823377424,5.18092665909789,5.096201618632806,4.513992217328446,5.293668555451877,4.626320085866708,4.48578950242225,7.096180585885039,5.840089060035568,5.068335693283613,5.065124785683751,5.840089060035568,4.513992217328446,5.293668555451877,4.626961551547263,4.513992217328446,4.090865746436358,5.096201618632806,7.703737654554672,5.293668555451877,3.8483783443236588,5.293668555451877,5.840089060035568,4.513992217328446,5.068335693283613,7.096180585885039,4.48578950242225,4.513992217328446,4.513992217328446,4.48578950242225,4.724413102786124,5.840089060035568,3.8483783443236588,5.293668555451877,4.48578950242225,4.513992217328446,4.513992217328446,3.7556403250442747,6.170293938275902,4.48578950242225,8.119394708328175,6.170293938275902,4.513992217328446,3.8483783443236588,6.170293938275902,3.8251022477096437,5.840089060035568,6.170293938275902,3.8251022477096437,6.170293938275902,4.513992217328446,5.068335693283613,4.513992217328446,4.626961551547263,4.48578950242225,5.068335693283613,5.293668555451877,5.840089060035568,5.293668555451877,5.840089060035568,6.170293938275902,4.48578950242225,5.293668555451877,4.513992217328446,4.48578950242225,4.547286940274507,4.513992217328446,6.170293938275902,3.8483783443236588,5.293668555451877,7.703737654554672,5.068335693283613,5.065124785683751,7.703737654554672,7.096180585885039,4.626961551547263,6.170293938275902,4.513992217328446,7.586480325758329,7.096180585885039,4.513992217328446,3.8251022477096437,5.840089060035568,5.065124785683751,3.8483783443236588,3.8483783443236588,4.513992217328446,3.8251022477096437,4.48578950242225,5.096201618632806,6.170293938275902,4.626320085866708,4.626320085866708,5.293668555451877,5.065124785683751,4.513992217328446,4.090865746436358,5.750640065034668,5.096201618632806,4.513992217328446,4.513992217328446,5.293668555451877,3.7556403250442747,4.513992217328446,3.7556403250442747,5.068335693283613,3.7556403250442747,5.536067135919687,5.293668555451877,6.170293938275902,4.48578950242225,7.096180585885039,8.239381390958782,5.18092665909789,7.146518417819807,6.170293938275902,4.626320085866708,5.096201618632806,3.7556403250442747,6.170293938275902,4.724413102786124,3.8483783443236588,5.18092665909789,3.8483783443236588,4.48578950242225,6.170293938275902,3.7556403250442747,5.096201618632806,7.146518417819807,5.840089060035568,5.293668555451877,4.48578950242225,7.586480325758329,5.750640065034668,4.626961551547263,3.7556403250442747,9.100946160258701,5.840089060035568,4.724413102786124,3.7556403250442747,5.096201618632806,5.293668555451877,6.170293938275902,4.547286940274507,4.48578950242225,4.513992217328446,5.750640065034668,5.293668555451877,4.48578950242225,5.068335693283613,8.119394708328175,5.068335693283613,7.146518417819807,9.08791474633755,3.8251022477096437,3.8483783443236588,4.626320085866708,4.48578950242225,3.8483783443236588,6.170293938275902,7.586480325758329,7.703737654554672,6.170293938275902,5.096201618632806,3.8483783443236588,5.18092665909789,5.18092665909789,3.8251022477096437,3.7556403250442747,4.48578950242225,3.7556403250442747,4.513992217328446,5.068335693283613,4.513992217328446,5.293668555451877,7.096180585885039,4.48578950242225,4.626320085866708,3.7556403250442747,8.119394708328175,3.8251022477096437,3.7556403250442747,7.096180585885039,8.239381390958782,7.3018266087463735,4.513992217328446,3.8483783443236588,4.513992217328446,3.8251022477096437,3.8483783443236588,4.513992217328446,4.626320085866708,5.068335693283613,6.170293938275902,8.867028823377424,5.293668555451877,4.724413102786124,3.8251022477096437,5.293668555451877,5.068335693283613,4.090865746436358,5.18092665909789,5.840089060035568,5.068335693283613,6.19555112244501,4.626320085866708,3.7556403250442747,5.18092665909789,7.096180585885039,3.7556403250442747,3.8251022477096437,3.8251022477096437,7.096180585885039,5.18092665909789,3.7556403250442747,6.672806262801639,5.18092665909789,5.096201618632806,3.8483783443236588,3.8483783443236588,3.8483783443236588,4.513992217328446,4.48578950242225,3.7556403250442747,5.096201618632806,4.513992217328446,5.293668555451877,7.146518417819807,6.170293938275902,5.293668555451877,5.293668555451877,4.513992217328446,3.8251022477096437,9.08791474633755,3.7556403250442747,5.096201618632806,5.068335693283613,6.208448158424733,8.119394708328175,3.8483783443236588,4.626320085866708,3.8483783443236588,5.840089060035568,4.48578950242225,3.8483783443236588,5.096201618632806,6.208448158424733,5.065124785683751,4.48578950242225,4.626320085866708,3.8483783443236588,5.18092665909789,6.170293938275902,3.8251022477096437,4.513992217328446,3.8483783443236588,4.513992217328446,7.096180585885039,7.096180585885039,7.096180585885039,4.513992217328446,4.513992217328446,4.48578950242225,4.626961551547263,6.170293938275902,4.48578950242225,4.513992217328446,4.513992217328446,6.170293938275902,4.626961551547263,4.626320085866708,9.08791474633755,4.513992217328446,6.170293938275902,5.536067135919687,6.170293938275902,4.626961551547263,5.18092665909789,4.513992217328446,5.840089060035568,3.8483783443236588,3.7556403250442747,6.170293938275902,4.724413102786124,5.068335693283613,4.547286940274507,4.513992217328446,6.170293938275902,4.513992217328446,7.096180585885039,7.586480325758329,4.513992217328446,7.146518417819807,3.8483783443236588,6.170293938275902,4.48578950242225,3.7556403250442747,3.8251022477096437,3.8483783443236588,6.170293938275902,5.293668555451877,9.08791474633755,7.096180585885039,3.8483783443236588,5.293668555451877,3.8483783443236588,5.293668555451877,3.8483783443236588,4.626320085866708,3.7556403250442747,5.840089060035568,3.8483783443236588,5.840089060035568,6.170293938275902,6.208448158424733,4.090865746436358,3.8483783443236588,6.19555112244501,6.19555112244501,4.090865746436358,3.8483783443236588,4.48578950242225,3.8483783443236588,4.48578950242225,6.208448158424733,5.293668555451877,5.293668555451877,3.7556403250442747,5.096201618632806,3.8251022477096437,9.08791474633755,8.119394708328175,7.096180585885039,4.626320085866708,8.119394708328175,4.48578950242225,5.293668555451877,5.840089060035568,6.170293938275902,3.8483783443236588,4.48578950242225,8.867028823377424,5.068335693283613,4.513992217328446,8.119394708328175,3.8251022477096437,3.8251022477096437,4.513992217328446,4.626961551547263,4.48578950242225,3.7556403250442747,3.8251022477096437,6.170293938275902,4.724413102786124,4.48578950242225,4.547286940274507,5.065124785683751,3.7556403250442747,3.8483783443236588,3.7556403250442747,5.068335693283613,3.8483783443236588,5.068335693283613,4.513992217328446,3.8483783443236588,5.068335693283613,3.8483783443236588,5.068335693283613,4.48578950242225,3.8483783443236588,3.8483783443236588,6.170293938275902,4.626320085866708,4.513992217328446,3.8251022477096437,4.626320085866708,5.840089060035568,5.096201618632806,4.513992217328446,5.293668555451877,4.48578950242225,4.513992217328446,6.170293938275902,4.513992217328446,8.119394708328175,3.8251022477096437,4.48578950242225,3.8483783443236588,6.170293938275902,5.096201618632806,4.513992217328446,4.48578950242225,3.8483783443236588,4.626320085866708,4.626961551547263,3.8483783443236588,5.096201618632806,5.068335693283613,4.626320085866708,4.513992217328446,4.724413102786124,4.724413102786124,7.096180585885039,5.293668555451877,6.170293938275902,4.48578950242225,5.18092665909789,5.068335693283613,3.7556403250442747,4.626320085866708,4.513992217328446,3.8483783443236588,6.170293938275902,4.48578950242225,4.626320085866708,4.513992217328446,3.7556403250442747,4.626961551547263,3.8251022477096437,4.513992217328446,4.626320085866708,4.48578950242225,5.096201618632806,7.703737654554672,3.8251022477096437,5.068335693283613,6.170293938275902,6.170293938275902,4.513992217328446,3.8483783443236588,4.513992217328446,4.513992217328446,4.626961551547263,3.8483783443236588,3.8251022477096437,7.3018266087463735,8.119394708328175,7.146518417819807,3.8483783443236588,5.065124785683751,3.8483783443236588,3.7556403250442747,8.867028823377424,4.48578950242225,6.170293938275902,7.586480325758329,3.8483783443236588,5.068335693283613,4.513992217328446,7.096180585885039,5.293668555451877,7.586480325758329,5.096201618632806,7.096180585885039,5.293668555451877,6.170293938275902,4.513992217328446,4.513992217328446,5.840089060035568,5.096201618632806,4.626320085866708,5.293668555451877,4.626320085866708,7.146518417819807,7.096180585885039,7.146518417819807,5.096201618632806,5.536067135919687,3.7556403250442747,4.513992217328446,7.096180585885039,4.513992217328446,3.7556403250442747,4.48578950242225,5.293668555451877,5.065124785683751,6.170293938275902,8.119394708328175,4.48578950242225,5.18092665909789,6.170293938275902,3.8483783443236588,3.7556403250442747,3.7556403250442747,4.48578950242225,5.293668555451877,3.8483783443236588,3.8483783443236588,3.8483783443236588,4.626320085866708,3.8483783443236588,5.293668555451877,4.626320085866708,5.293668555451877,6.170293938275902,4.626961551547263,7.096180585885039,4.626320085866708,6.170293938275902,6.170293938275902,4.513992217328446,4.724413102786124,6.170293938275902,3.8483783443236588,9.08791474633755,7.703737654554672,4.626320085866708,5.096201618632806,3.7556403250442747,8.119394708328175,3.8483783443236588,4.513992217328446,5.293668555451877,4.626320085866708,4.724413102786124,5.068335693283613,7.146518417819807,3.8251022477096437,5.293668555451877,4.547286940274507,8.867028823377424,6.170293938275902,4.513992217328446,3.8251022477096437,3.8251022477096437,7.096180585885039,5.068335693283613,5.750640065034668,5.096201618632806,7.586480325758329,9.08791474633755,7.703737654554672,4.513992217328446,3.8251022477096437,5.293668555451877,3.8251022477096437,6.208448158424733,3.7556403250442747,4.626320085866708,5.065124785683751,5.18092665909789,7.146518417819807,5.840089060035568,4.48578950242225,5.840089060035568,5.068335693283613,4.513992217328446,5.293668555451877,5.065124785683751,3.8483783443236588,5.068335693283613,5.068335693283613,8.119394708328175,6.170293938275902,6.170293938275902,5.068335693283613,9.08791474633755,3.8483783443236588,3.7556403250442747,7.703737654554672,6.170293938275902,5.096201618632806,5.068335693283613,9.08791474633755,3.7556403250442747,5.096201618632806,7.3018266087463735,5.293668555451877,4.513992217328446,7.703737654554672,4.513992217328446,5.750640065034668,4.513992217328446,3.8483783443236588,3.7556403250442747,7.096180585885039,4.513992217328446,4.513992217328446,5.750640065034668,3.8251022477096437,8.119394708328175,4.48578950242225,5.293668555451877,8.119394708328175,4.724413102786124,7.096180585885039,4.513992217328446,8.119394708328175,4.48578950242225,3.7556403250442747,4.626961551547263,4.48578950242225,5.065124785683751,5.096201618632806,4.513992217328446,3.8483783443236588,4.626320085866708,9.08791474633755,4.513992217328446,3.8483783443236588,5.293668555451877,5.293668555451877,5.065124785683751,3.8483783443236588,6.170293938275902,4.513992217328446,4.48578950242225,3.7556403250442747,6.170293938275902,4.513992217328446,5.840089060035568,5.293668555451877,5.293668555451877,4.48578950242225,3.8483783443236588,9.08791474633755,7.586480325758329,3.8483783443236588,6.170293938275902,5.293668555451877,6.170293938275902,4.513992217328446,4.626961551547263,4.513992217328446,7.703737654554672,3.7556403250442747,4.513992217328446,3.8251022477096437,8.239381390958782,7.096180585885039,3.8483783443236588,9.08791474633755,3.8251022477096437,5.293668555451877,5.068335693283613,5.096201618632806,3.8251022477096437,4.547286940274507,5.293668555451877,4.513992217328446,8.119394708328175,5.750640065034668,3.7556403250442747,4.513992217328446,4.48578950242225,4.513992217328446,4.513992217328446,4.513992217328446,4.513992217328446,6.672806262801639,4.513992217328446,3.8483783443236588,6.170293938275902,3.8483783443236588,4.48578950242225,5.840089060035568,4.48578950242225,3.8483783443236588,7.586480325758329,6.208448158424733,5.096201618632806,3.8251022477096437,4.626961551547263,3.8483783443236588,7.096180585885039,6.19555112244501,7.3018266087463735,4.724413102786124,4.626961551547263,4.626320085866708,5.18092665909789,6.672806262801639,4.48578950242225,5.096201618632806,4.48578950242225,5.750640065034668,4.48578950242225,4.626961551547263,3.7556403250442747,8.239381390958782,4.626320085866708,3.8483783443236588,4.513992217328446,5.750640065034668,3.7556403250442747,4.626320085866708,7.146518417819807,4.513992217328446,3.7556403250442747,5.293668555451877,5.750640065034668,4.626961551547263,4.48578950242225,4.090865746436358,5.293668555451877,8.119394708328175,3.7556403250442747,5.096201618632806,6.170293938275902,5.293668555451877,4.626320085866708,6.170293938275902,4.513992217328446,5.293668555451877,4.48578950242225,3.8483783443236588,6.170293938275902,6.170293938275902,6.19555112244501,6.19555112244501,3.7556403250442747,5.840089060035568,9.08791474633755,4.513992217328446,9.100946160258701,5.096201618632806,3.8483783443236588,5.065124785683751,3.8483783443236588,7.096180585885039,4.626961551547263,5.068335693283613,3.8483783443236588,9.08791474633755,5.096201618632806,4.090865746436358,4.48578950242225,3.8483783443236588,4.48578950242225,5.293668555451877,7.146518417819807,7.146518417819807,7.096180585885039,3.8483783443236588,4.547286940274507,8.239381390958782,3.7556403250442747,5.293668555451877,5.068335693283613,6.672806262801639,3.8483783443236588,3.8251022477096437,4.724413102786124,4.513992217328446,4.626320085866708,3.8483783443236588,9.684796246735674,5.840089060035568,4.48578950242225,5.096201618632806,3.8483783443236588,6.19555112244501,9.08791474633755,4.513992217328446,5.18092665909789,6.672806262801639,5.293668555451877,4.513992217328446,5.293668555451877,5.293668555451877,3.7556403250442747,4.48578950242225,5.068335693283613,4.513992217328446,3.8483783443236588,4.724413102786124,6.170293938275902,7.146518417819807,5.068335693283613,4.513992217328446,3.7556403250442747,4.513992217328446,5.096201618632806,3.8483783443236588,7.096180585885039,6.19555112244501,5.068335693283613,7.096180585885039,4.626961551547263,4.48578950242225,5.840089060035568,4.090865746436358,4.513992217328446,5.840089060035568,4.48578950242225,4.626320085866708,6.170293938275902,7.3018266087463735,4.090865746436358,3.7556403250442747,4.724413102786124,4.724413102786124,4.547286940274507,5.096201618632806,6.170293938275902,7.586480325758329,8.867028823377424,6.170293938275902,5.096201618632806,3.7556403250442747,7.3018266087463735,3.8483783443236588,7.586480325758329,4.626320085866708,3.8483783443236588,4.724413102786124,7.3018266087463735,4.513992217328446,5.068335693283613,4.513992217328446,5.096201618632806,5.840089060035568,4.48578950242225,4.513992217328446,4.626320085866708,4.626320085866708,5.065124785683751,5.293668555451877,5.065124785683751,4.48578950242225,4.48578950242225,4.513992217328446,5.096201618632806,4.626320085866708,5.096201618632806,5.068335693283613,5.068335693283613,5.840089060035568,5.096201618632806,5.840089060035568,6.170293938275902,6.170293938275902,5.096201618632806,5.065124785683751,5.068335693283613,4.513992217328446,4.513992217328446,5.065124785683751,4.626320085866708,5.293668555451877,5.293668555451877,5.293668555451877,4.626320085866708,4.090865746436358,3.7556403250442747,3.7556403250442747,5.293668555451877,6.170293938275902,4.626320085866708,3.8483783443236588,4.513992217328446,3.8251022477096437,4.547286940274507],"xaxis":"x","y":[3.137503523749935,2.4646682670034443,4.808385050656093,6.381629467033664,5.687900522480746,4.296457407371244,2.877744249949002,3.542258049766918,2.752748591407134,2.1984941536390834,5.6604951318885135,2.9763636357327616,8.749098443586298,2.1043366598147357,5.40973035386445,2.3305584000308026,2.3950627995175777,6.141391916170729,3.76659516172927,9.707290101958575,8.663593681954685,2.9126498648972037,6.819668183496455,1.6553518286125541,11.880218691860534,9.057883448899627,4.445594291341182,4.326249700837456,4.708739041359579,3.5570424152637194,7.3848269116030085,6.815319245777306,2.2600256559614555,8.198690585668917,4.72137265947682,2.6712933724815757,7.400708693037901,3.6229303509201767,5.137503523749935,5.359661722333877,4.571676809970931,6.199672344836364,8.475936042446387,7.905507349949873,3.8933622107638715,7.537839615916466,5.353323291162897,3.311793717753649,5.2712762591759255,4.778208576398088,6.628919373511362,3.1858665453113337,6.145473670317708,6.023698917266407,3.145677455195635,5.683696454306515,6.109569509370688,11.964503324303022,7.672213165334465,3.8787253414801053,4.8283269264148165,6.010108453474289,7.029563314432658,5.911212201558612,5.9590746011405145,4.0232553523003025,4.234961094839504,3.41006969175638,5.59215800212536,5.982537314399271,2.910732661902913,3.7687136570304847,7.183982678112831,1.0976107966264221,2.6016965164809576,5.511910748670928,5.6570683012015754,2.2868811477881614,6.999098033705607,6.746447139362815,4.83035674741922,3.411426245726465,4.77663042305105,2.550900664647523,5.942279901705763,7.626950122372515,6.862203399053224,3.4019034716079584,2.608809242675524,6.046141781644721,3.3936907641874536,5.390254956321868,4.541638734436141,6.063718640152248,8.226941966807852,6.203788453308596,5.530445354692906,3.2433644256936605,6.703349915166396,8.202417721575182,6.075318692553354,6.401732849522621,3.3305584000308026,6.606294160697713,5.5786368624717415,4.273515889702116,7.668814088419117,3.097610796626422,3.321928094887362,4.593353770980297,4.792334805706692,8.090535901003173,8.367589480412983,3.1358631653686793,2.7070829917717063,7.1593668097437355,4.614709844115208,5.946496941203293,6.783456654360239,8.491372117815171,7.273609132231856,5.944624223055033,3.070389327891398,2.2479275134435857,9.095924419998536,2.4489009511451276,7.048759311919856,6.058532970201611,7.216163813388774,4.46532151358055,4.871350840634026,2.611172380044005,5.783718557790366,7.3824943402931185,8.12401788912289,5.2787282129389395,6.099295204337775,6.1705259991768475,2.269033146455237,3.6392321632492775,4.547819956577703,6.003152448666922,7.816087658600206,6.762348815644129,4.940166750482817,3.01435529297707,5.487357715311547,2.333423733725192,10.642557179595622,2.503348735167504,2.4356285940520905,4.773468927905195,9.760270854333914,8.156942620908886,6.218781167784069,6.306517445249819,8.352882086522959,7.032211069322931,5.5963388642052,5.320845667645722,3.0089887832272546,5.943218087426284,7.3403842354553985,4.443606651475615,2.845991770664573,7.217618485351048,6.137913322088407,7.084383221650577,5.784242222057781,6.097610796626422,3.605257262939004,0,6.954079959320971,4.883620816285672,5.118941072723508,6.583308464847678,6.034963989749303,7.257764968571387,3.91169158187234,9.10522758436473,4.507794640198696,5.001802242633985,6.858478390423273,8.095608004811197,2.3103401206121505,7.6398833131998005,3.144046369616707,3.9420452599160467,4.142413437873741,3.925049964727359,1.835924074254375,6.111448698487674,3.363171077119244,3.127633279725874,3.24031432933371,4.160274831408593,3.5397791916984938,7.571221725048103,5.284292026394312,5.47800105583793,3.7398481026993275,7.1320626778543845,5.576522137920503,3.127633279725874,4.3110671022555955,4.375734538583156,6.4432751117658045,3.91169158187234,3.8042601156347384,6.4409521980296365,5.85872702377863,6.02724253597374,5.212958362728285,2.5058909297299574,7.561631630375841,4.92552476974757,3.5619370596224327,6.237448995639321,7.881113960675097,2.176322772640463,5.361768359419154,4.9941271140031285,2.584962500721156,6.166916251569974,9.4493756610678,6.0332027259372225,10.919861451442168,2.693765712217783,2.4724877714627436,2.7484612330040354,5.633721812641012,8.17707065762942,1.9634741239748859,2.7803100990433753,5.23687524536694,2.698218478224414,4.238022517825721,5.317231700222606,2.1176950426697547,5.604071323668861,3.2555007331483865,2.2387868595871168,3.881664619320345,4.493134922305505,5.357903838552492,4.9597701552114675,4.724650271732967,4.4046306842176115,8.46707564951531,7.096029876307993,7.341896820977107,5.339850002884624,2.859969548221026,6.582706526780788,9.101634319640722,4.129283016944966,7.151270288790165,4.58014548442338,4.628773595201645,3.070389327891398,6.034303767768035,6.339493737901733,5.174326515173915,3.357552004618084,4.809414444235898,7.880195728943098,5.632268215499513,4.79960542245267,2.077242998932461,3.5298209465286954,5.433960931121179,6.078951341394822,4.854494418154875,2.6484654430273142,8.345405246717796,3.4462562298895643,5.595145567990858,6.766065051474954,2.4222330006830477,7.366846735262642,2.9873208659292536,2.6507645591169022,5.940871478189196,3.552131108253784,2.4672794804599825,7.901409979999464,7.459185683748918,2.3868109464722167,8.668955876629637,3.1667154449664223,3.017921907997262,2.4982508675278257,1.7782085763980877,2.9088129077395473,2.1602748314085933,2.3074285251922473,8.316598324108684,5.603477988254004,5.669310286202474,7.991691898336964,1.0840642647884746,5.091276694365222,4.042644337408494,5.96254902292306,5.020591094773247,5.20163386116965,3.065227622775619,7.08533966935737,4.321206566969903,3.5298209465286954,6.083000565676004,5.04176864995163,9.435170178881494,6.4705368650925825,2.0531113364595623,4.078951341394822,5.525755692829486,5.9818526532897405,2.063502942306158,4.110196177754199,7.135657989368776,2.3812833725037836,3.693765712217783,3.00539998774259,5.589164236699772,5.36737106564853,6.38646609286931,2.3248106034204836,3.1811025507537978,8.27891401949994,2.430285272977781,2,4.867402305727543,6.299391206126829,2.6803243568440163,2.693765712217783,2.7949356628035362,6.605701738949982,9.159820892013935,4.255500733148386,5.555509427083257,6.871104373555458,5.632849830117706,4.966707314150316,2.526068811667588,6.09950561709114,2.927896453728821,4.28835856219366,3.1634987322828794,7.07692245939234,3.8429788317883253,6.341274183692174,5.926948247949772,2.538538163629804,6.73226919950145,2.788685710613534,5.508111681193465,9.398936038454162,2.3161457422933562,5.28835856219366,7.479861085016818,6.0925457415435655,5.747655932932393,3.9059284781731347,6.235152624217932,7.3334237337251915,8.870210577263595,3.460742563789644,2.974529312483882,2.6016965164809576,2.2387868595871168,2.7676547982373463,5.659639187015652,3.894332742277694,5.771093251699539,7.173327349650988,6.027463723408577,1.4222330006830475,5.794675787986858,4.720825666089835,5.342696959981934,5.562547724199122,2.62760683812965,6.426767932655541,7.593353770980297,2.5185351389821804,6.676239195086378,4.472487771462744,6.212180210044431,7.178117045706399,7.534497433738168,6.304876050044889,7.545505100691072,5.077670274232752,3.953265239014844,4.609991295212678,2.9126498648972037,7.174426393677253,3.6016965164809576,4.515384460636948,2.503348735167504,8.129128434194934,2.9726926540042644,6.030998118171895,2.2539892662307865,6.597829097826899,6.8209447918806925,2.9523335663696857,8.18769847638827,4.950468414150123,6.1409831048784245,4.674121632632394,6.830610274357907,4.4376272483189485,3.7037651787934096,5.95837871156634,9.296618003796326,3.351910961103077,4.143230134776893,4.707082991771706,3.560714954474479,3.9845893503624565,8.328001662149257,2.1667154449664223,3.1811025507537978,2.4854268271702415,2.931683057059806,4.788685710613533,5.659068274843228,5.738227400367242,4.6937657122177825,6.860962797858116,8.65474342118394,4.814550423461808,6.107687869314374,4.720825666089835,6.4775154361842375,5.619119511453218,4.7136958148433585,2.5533605033353277,4.412781525338476,3.72137265947682,3.460742563789644,3.0197019144425483,6.602587527342561,4.687060688339892,2.526068811667588,4.698218478224414,2.8698714061777126,2.144046369616707,5.394376944957055,4.649615459063409,5.80683958179819,4.443606651475615,5.450881315273369,6.6503337538459295,7.198494153639083,5.228049047884462,3.9231491804723087,1.3840498067951599,4.524815928357506,7.211012193485511,7.027021314622255,7.212958362728285,6.658497136656478,3.3881895371560837,3.140778655782796,4.37364821133469,3.319039815562536,7.33360262826828,5.175524601089875,3.2047667506546134,2.702657543390911,3.228049047884462,9.322806969423008,5.561631630375841,5.8755343515732354,5.032100843167024,3.9873208659292536,3.1842802944193824,3.277984747299765,7.444849247263915,3.6701605141266334,4.595145567990858,3.8369340113210915,3.575312330687437,2.788685710613534,5.4571345943180365,2.4276061727818994,8.563615765629766,4.712045448553693,6.260590274730996,0,2.4594316186372973,2.8257856274647914,3.049630767724601,8.097242070141174,7.431622959713292,4.030336078370959,4.964398632203184,4.020591094773247,2.211012193485512,5.462052318796433,3.3881895371560837,2.411426245726465,5.0430819818956625,3.6064422281316078,3.613531652917927,3.883620816285671,2.8318772411916733,5.081936081719432,5.575917361118149,4.958842675243241,6.716579420796345,6.531225485053606,6.050284014301706,7.73125113789623,2.950468414150123,5.645009884391095,5.4015622072560685,3.840966704487421,4.3298411765306275,0.8155754288625726,3.7070829917717063,5.848748114611959,4.303050084681673,8.796785917134267,5.497931651896279,4.67581593117227,8.189231548647284,6.9092930858238235,4.772941337831336,5.6082178530214595,7.304419775319909,6.588864518190148,10.665637927637095,4.672990993827748,2.8399595874895316,5.305605789585429,3.0908534304511135,2.4854268271702415,2.693765712217783,2.942983598187102,2.5897634869849773,4.631686366311998,3.0125686735030555,5.651051691178929,5.488964817004848,3.1858665453113337,0,6.703073006328523,4.255500733148386,5.9593064897596575,5.218781167784069,3.968090752045256,4.8594726668519375,3.460742563789644,5.341274183692174,3.0089887832272546,2.032100843167024,2.169925001442312,3.356143810225275,4.814037647539975,3.4168397419128294,5.653060017104565,6.074462620704536,3.2016338611696504,6.9656688644419456,6.816215687622865,4.330558400030802,7.16380060395775,5.6061460780657235,6.093179846765504,3.5716768099709313,5.342341397431548,4.228049047884462,6.310885391185277,4.751677945687579,7.502871574574138,3.953265239014844,3.975446765640962,2.6599245584023783,2.430285272977781,5.7303682363490065,7.689648608288815,4.371558862611963,3.215678596607928,6.492173659610827,7.480507489974544,4.436295119839363,4.804260115634738,4.405311682597445,13.396289023308677,3.8914191868460786,3.24031432933371,1.899175630480513,3.2942531364445142,8.382105211486744,4.143230134776893,4.92552476974757,2.8439838440483265,2.8399595874895316,6.101187815259738,5.0232553523003025,4.583158003874408,3.176322772640463,8.996303999184056,3.8257856274647914,4.7755773609306535,8.12401788912289,5.9618548076361595,2.454175893185802,6.208088008224774,2.673556423990145,2.8972404255747994,6.791683858152045,7.137810883416096,5.8052924556007115,7.338067797542616,5.198494153639083,2.851998837112446,5.726013748860151,6.567880042052731,2.8155754288625725,2.2387868595871168,3.6769443591069124,4.318316841334983,2.1795110502715103,5.6884601404244926,0.8073549220576041,7.244982170376553,8.642268351758643,6.836681593327116,2.73768676140986,2.4646682670034443,5.964860664245848,2.7420062108667365,2.411426245726465,7.279842693520348,7.446835177086041,6.531849285751203,3.8982083525087177,4.721919445551544,5.500164679492167,5.429615964201735,0,3.7311832415722,4.726831217032493,4.904002316283692,4.561937059622433,7.438459204423481,4.631686366311998,7.300764373248062,4.112700132749362,7.9329824099498465,5.1081062236861285,5.868143479666726,4.161081482277184,5.996162742681061,3.097610796626422,4.263785613890803,6.339493737901733,4.370861740085285,4.842978831788326,5.636334602442182,2.84197311892718,4.7713574089905615,4.257010618206024,6.916596067892185,6.015247773698937,4.595145567990858,6.7410624357490905,3.4168397419128294,3.823749360308273,4.279471295644468,4.7114949066500875,2.883620816285671,2.969012307516316,5.089159131911238,2.3362833878644325,8.080604371320732,1.8559896973084806,6.941341105485299,8.14639047567022,7.163498732282879,8.281651890258825,6.329482431025636,2.140778655782796,2.3103401206121505,9.500782139394977,4.674686619927999,7.249066033855867,3.1060132376221192,7.897179908573977,7.6546360285279675,2.944858445807539,4.94673086014031,2.711494906650088,6.054414387917307,2.788685710613534,4.198494153639083,2.9467308601403097,4.060047383669939,3.442280035252584,2.7070829917717063,5.9586107120558465,2.4982508675278257,3.6484654430273142,5.0870384567669475,5.130519083027338,1.8155754288625725,2.6064422281316078,5.673839055990439,6.37364821133469,5.609991295212678,6.076388068512841,4.695437110405369,7.459267666703387,5.211401637418471,3.063502942306158,5.931210274803932,6.819540460505722,0.918386234446348,4.878234879112339,6.098663781886574,4.098453246309274,3.4154882710497003,9.8361608416947,5.3103401206121505,4.20633064787105,6.436128517259249,4.381283372503783,5.0259146987507926,5.226123143125237,5.73903797911741,4.110196177754199,3.8042601156347384,4.503984703897613,6.067165427017112,3.7322691995014496,6.121844297876376,8.248591759535907,7.711013010084339,5.782670658677007,6.0774566524005875,7.592232766722112,6.917909073852982,4.423578170981798,4.901108243014512,3.795974694206668,6.907251224292033,3.189033824390017,2.4646682670034443,6.061128135118254,2.733354340613827,6.664198369291911,5.166715444966422,8.492293852480074,7.279749853014451,2.807354922057604,8.362689046726734,2.3248106034204836,4.826294245499635,2.321928094887362,4.238022517825721,4.232660756790275,5.51127826508303,3.7644735509926663,5.290940402403678,7.535353147842131,4.329123596291566,5.908092340818271,2.570462931026041,4.634012356402116,6.99491908871511,2.560714954474479,2.3589588258323295,2.8278190246173196,5.439623137557117,2.927896453728821,7.476543706214256,8.334094473911568,6.726695004499616,2.097610796626422,3.619413010597937,2.523561956057013,7.136683577697236,8.908992993250534,4.4950555283680185,6.970393537914677,8.089159131911238,2.996388746447621,3.7506065048355923,5.468583317006828,4.732811872082943,2.5360529002402097,2.729008870337863,7.712939631305699,4.038260575175349,4.514122260170708,6.948834425578686,2.0036022366801953,2.169925001442312,6.04023491675437,7.246788093844365,7.018923431842335,5.046141781644721,5.637784110291324,6.976363635732762,5.840463233869542,6.170125361839952,4.760753208062987,3.089159131911238,2.8399595874895316,2.3978029618624896,2.3305584000308026,3.0925457415435655,2.6599245584023783,7.9217814432688,5.963242904318827,2.4515408330178317,5.976821852360685,1.7865963618908067,3.221877081077034,3.0036022366801953,2.722466024471091,4.2024177215751815,5.9025560053446275,3.997292407636508,5.323730337521348,6.463851292611099,5.938756261492295,7.593951283948411,5.233044401265581,4.79025073883523,5.718361626138354,6.435295215647167,1.3504972470841332,4.12267271882414,6.361768359419154,6.8820316083521,2.0250287944915226,2.298658315564515,3.636914580355878,7.934634441094905,5.246408087246385,3.918386234446348,3.929790997718597,4.147306698780294,5.169925001442312,5.542258049766918,4.693208148910016,3.6971065744769747,5.707082991771706,3.363171077119244,5.035183996618336,4.274261661257048,3.321928094887362,5.9740703670240265,2.3757345385831563,10.174663577413781,2.8439838440483265,5.24260250592898,3.4581194811745064,2.063502942306158,3.1858665453113337,4.7692427951405385,4.301587646603187,5.986638471730853,5.058749412335524,4.556429415449573,2.5360529002402097,2.4489009511451276,4.974988111991927,6.248307120087241,4.346956889378885,2.641546029087524,2.8579809951275723,7.624539603781834,5.197314999473818,5.38404980679516,3.2387868595871163,4.464668267003444,3.955126781261366,6.258895755075436,3.1358631653686793,7.831180547126736,1.3785116232537298,4.934044647112246,4.503984703897613,4.100977647724821,4.86195536414487,8.439830883981392,7.851624127346895,6.091065077930535,7.190022174802645,5.992541859109112,2.983677694698067,7.161484638641659,2.6959938131099,8.21003812347289,7.461643148073906,4.848497755385146,3.192194165283345,1.6690267655096307,8.80290404741688,6.926948247949772,0,3.812498225333564,5.6806056662025926,9.871443254934135,6.459923362676901,3.8298495598446904,5.157447996274298,5.537296067090842,3.5147534984397533,3.2249663650002742,5.699884728697476,3.035623909730721,6.914444931746792,5.338780943894699,9.013601832683456,7.578712331048165,3.3405622690264134,2.3950627995175777,7.221587121264805,7.78959885005233,2.2234225499349374,2.8439838440483265,2.950468414150123,5.833649127594984,5.802968651005583,4.618238655595454,7.581803148516525,4.948600847493355,5.416164164733128,5.012121672712217,2.9373443921502322,2.3923174227787602,5.259649120649155,7.119252412157343,5.452858964713811,2.5160151470036647,2.788685710613534,5.040892430646901,6.390942772802543,6.032541697270038,6.177519202597814,6.111865963867557,7.0083165637349865,6.074034394212556,4.471187460386985,9.507715369062607,6.152994605492435,4.169123281476757,4.706530552538117,3.0426443374084937,5.305605789585429,6.942279901705763,2.4724877714627436,11.02948049374083,7.127736443588829,3.594548549550354,3.8063240573900288,2.144046369616707,7.036942844798256,4.022367813028454,2.0214797274104517,8.589726041133547,6.218781167784069,3.685940148445977,7.566662962962611,6.652343078740216,4.210232990095849,9.47854718263086,5.991861930556323,7.500961352460057,5.5855634984556914,2.319039815562536,2.9049657186840263,1.9781956296816516,5.509062386361898,5.592756010441026,4.39780296186249,3.431622959713292,11.193297418854996,7.925940095994448,8.041385369486115,5.051807107013298,2.3673710656485296,5.208673319629471,5.532940288372874,4.598722499676621,2.903038270112912,6.098453246309274,2.967168607532628,4.919340082442012,7.971830914780502,4.189033824390017,3.10936055940423,4.053111336459563,3.944858445807539,5.71699089440494,4.2787282129389395,3.7246502717329673,4.568032104771279,6.400025517914023,3.0443941193584534,9.140190703191836,6.7998644108818525,3.3923174227787602,4.880685525261125,10.32396671481508,2.521050736900963,1.8836208162856714,6.547203024756931,5.63023071585969,5.915042791371977,5.713970692436053,8.823876711241038,2.6530600171045644,4.462706750670158,4.524189078449365,5.342341397431548,6.70362677086498,8.008596693255853,5.81326814176608,5.795715006501729,1.5897634869849773,2.5058909297299574,4.153805336079036,4.707082991771706,5.617651119427331,5.7615512324444795,6.115615931428109,5.51443791382931,8.873136468833458,4.8811751553492755,3.4276061727818994,7.013685569928653,3.37851162325373,4.390942772802543,4.544114402209474,8.617283787779035,5.39677601078753,4.096767854714689,3.169925001442312,2.881664619320345,3.389566811762726,5.949301468328278,5.9095331149468935,2.2172307162206693,5.687900522480746,8.295998461911388,4.963011647599427,5.648177795724818,3.8509993947164736,3.669026765509631,5.924575003392735,1.2570106182060237,8.1184220243132,6.3103401206121505,4.59215800212536,2.169925001442312,2.771885578515366,5.309976492370556,5.718909554379232,7.2325648297287755,3.720278465233327,3.403267722339301,4.138323004056484,8.321792835886914,8.384999544730041,6.675392543042777,7.848935855524565,8.135247549809566,7.474760499576783,2.01435529297707,4.661635602335959,2.899175630480513,6.950701690089008,3.4776773275653072,6.275752048828529,2.7782085763980877,8.645802514606062,8.927066807759255,5.377123749129487,8.785648628652076,3.554588851677637,5.8529975876133165,7.093391153253013,3.8639384504239715,6.390082950951663,3.0214797274104517,5.947198584262056,2.0565835283663674,8.560600329022595,4.708187236020708,3.1473066987802936,5.260778431893426,2.3978029618624896,5.496654082593496,8.356011721502778,5.307064162255372,5.485426827170242,8.478971805032943,2.414135532984451,2.776103988073164,4.7420062108667365,2.4828482830684653,6.543805175964397,8.5918962955207,4.361066488794321,5.294620748891627,6.553360503335328,7.088629255297176,5.1201860275318625,7.00943675560809,4.090853430451113,6.7014104362328135,7.415741768290091,5.348728154231077,6.705563274566321,5.445263208140443,4.552131108253784,3.4436066514756147,3.5631581304028077,6.678494507773734,4.478971805032942,6.1657109924849305,3.403267722339301,4.534186139090097,3.0214797274104517,6.238786859587116,1.8639384504239715,7.020924396158786,4.20163386116965,2.7548875021634687,2.604071323668861,7.128870759465125,2.5459683691052923,5.414473836430928,9.995102877826515,4.861459166361513,5.25285462595139,6.31922050252387,5.308156975114762,5.526068811667588,5.763145958149476,5.903279342056139,5.825531251189622,7.048214384608036,2.513490745588118,5.944155663440363,5.856985689782205,3.412781525338476,2.956056652412403,5.664482840364682,1.74416109557041,4.886062338345599,2.4646682670034443,2.4515408330178317,6.433126376206435,7.7755773609306535,6.705839705883682,8.330737650206952,4.727375938594897,5.777945670695148,11.69784504363786,7.7627474419531515,13.587327482778312,6.558114538598396,6.332349900373874,3.2265085298086795,2.875780063068488,6.70473366267406,4.223422549934937,3.2387868595871163,6.136683577697236,12.23806432823473,5.572889668420581,5.849999259466098,5.533563348214512,5.325530331567558,2.797012977836145,4.150559676575382,8.298429210890312,7.731454807698346,8.903490246967104,2.8094144442358986,4.7824085649273735,8.461602224618334,5.583158003874408,3.060047383669939,6.429950657403768,7.193278935521388,2.3589588258323295,3.7311832415722,2.899175630480513,2.298658315564515,4.235727059838059,2.3812833725037836,11.663771557482411,5.684257705058932,5.212569338850806,6.9787676510367875,4.044394119358453,6.092334311186202,9.582254908357664,2.5160151470036647,6.402415216824399,5.740387932468353,5.784503982929566,2.6064422281316078,4.362469888750209,5.164705840182799,3.4208865749755315,3.948600847493356,7.965841991309597,4.976821852360685,2.722466024471091,2.7398481026993275,4.528571318870758,7.677578712165378,3.41006969175638,2.4905701304462013,3.5147534984397533,7.5673476962842585,6.849624029924543,2.833902076669163,7.406162478777607,8.131085259578953,3.176322772640463,8.148324042480084,2.657640005207824,3.350497247084133,6.8203066288938246,4.483493351284395,1.713695814843359,5.872582544833223,2.722466024471091,5.7742599514327635,5.704318677760832,4.760220946466509,4.294253136444514,6.037162550127378,7.151777654732053,3.215678596607928,4.742545233866053,5.613531652917927,5.727375938594897,7.7780114016125745,8.009884588931843,6.549977142771825,2.0461417816447205,2.0565835283663674,6.66746640495737,4.62760683812965,7.884597920990064,8.571145863602528,2.3161457422933562,8.563272552819562,7.111970261358359,6.001577085389535,3.4369613378336026,6.435461914479276,3.5084286525318573,3.940166750482817,2.5459683691052923,1.6644828403646825,4.94345253855882,6.003827078111476,4.960697039304312,6.684117412839159,5.612352498752662,5.609991295212678,2.62993940943954,3.611172380044005,5.522620761042079,5.891419186846079,3.6530600171045644,3.4581194811745064,3.1667154449664223,4.581351247168778,6.764340846670181,8.066089190457772,6.614857050384495,5.767654798237347,3.1953475983222193,2.9726926540042644,5.279471295644468,7.5105663894487495,5.02724253597374,8.25176642434399,7.822411496360477,5.334496768390418,5.715893370547605,7.3049672876759635,5.998646839005795,6.9725777851765525,2.4594316186372973,5.666472568842073,3.8728287595348854,6.052024560482831,3.631104282365877,2.3015876466031866,2.37016428054021,4.3384244147835584,6.546122758857796],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle AdaBoostRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predAB_log"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle AdaBoostRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsAB_log = np.logspace(0, 2, 30, dtype=int)\n","param_gridAB_log = {\n","    'adaboostregressor__n_estimators': n_estimatorsAB_log,\n","    'adaboostregressor__loss': ['linear', 'square', 'exponential']\n","}\n","\n","GridAB_log, \\\n","BestParametresAB_log, \\\n","ScoresAB_log, \\\n","TotalGHGEmissions_pred_logAB, \\\n","figAB_log = reg_modelGrid(model=AdaBoostRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=TotalGHGEmissions_train_log.ravel(),\n","                         y_test=TotalGHGEmissions_test_log,\n","                         y_test_name='TotalGHGEmissions_test_log',\n","                         y_pred_name='TotalGHGEmissions_predAB_log',\n","                         score=score,\n","                         param_grid=param_gridAB_log)\n","\n","print(BestParametresAB_log)\n","print(ScoresAB_log)\n","figAB_log.show()\n"]},{"cell_type":"code","execution_count":126,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[1.5872226051439673,1.5918180330998535,1.5903874263092754,1.5884418308757808,1.5965871727108785,1.5827840229184782,1.5846877610000325,1.5701811586038183,1.5677653621613499,1.5728923795249519,1.5702648723369905,1.5637221193098916,1.559386033254713,1.5775346955111724,1.5756458557350324,1.579529799306012,1.581603779176952,1.581873819624029,1.5901749959920752,1.5970244188673086,1.6006547076328097,1.6056147043593518,1.600121249927906,1.6058784944636622,1.6020074096351375,1.6130494842943346,1.6139524827850678,1.615520385911751,1.6192422585585007,1.621629779425641]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[1.5967218645310113,1.6063773960612888,1.5989073340774826,1.6013571879487385,1.6149228058416465,1.591286297099643,1.6103413442235535,1.5853601017164278,1.5763194343538725,1.5847530374909438,1.5798799223947275,1.5804098700685731,1.5662709954176393,1.598784591238449,1.5905078655139506,1.595744894932294,1.60150620400773,1.600477526427812,1.6102825345277252,1.6137196033661794,1.616942136463866,1.6240354061459035,1.6146438969024857,1.6291331731394978,1.6204936726573844,1.633870611627039,1.632545441131073,1.6394151955188325,1.6399607254952706,1.6382338188052985]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[1.5777233457569233,1.5772586701384181,1.5818675185410682,1.5755264738028232,1.5782515395801104,1.5742817487373133,1.5590341777765115,1.5550022154912089,1.5592112899688273,1.56103172155896,1.5606498222792535,1.54703436855121,1.5525010710917866,1.5562847997838958,1.560783845956114,1.56331470367973,1.561701354346174,1.563270112820246,1.5700674574564253,1.5803292343684379,1.5843672788017533,1.5871940025728002,1.5855986029533264,1.5826238157878265,1.5835211466128907,1.5922283569616302,1.5953595244390626,1.5916255763046696,1.5985237916217307,1.6050257400459833]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.5842495006891584,1.571210733341604,1.5816030311719624,1.569399467254809,1.5830651973456815,1.5692828453546412,1.567765304012918,1.5447089433097054,1.554855900170997,1.5523879292000256,1.5609670848952042,1.5517328644789228,1.5487426894596752,1.542810713516982,1.5481135712303458,1.5546317098717513,1.5515906029365973,1.549258532602516,1.5580773146201952,1.5713987742370783,1.5802465347460282,1.579755218896868,1.580453519397027,1.5729594434802088,1.5765296227070333,1.5920377064879214,1.5971233362322201,1.5846210347276912,1.5961607754798905,1.6053727993077147],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.6028268374396244,1.6061318568757463,1.5921552858710237,1.6022241957925276,1.585208013037549,1.5767266659968788,1.5645201585610835,1.5853627851199004,1.5740078393251749,1.5852549935431905,1.5860970961911383,1.5735867676547468,1.5602652584646721,1.5714544113594688,1.5831319345898822,1.5721179013764626,1.5789571394028288,1.583965941042162,1.5915549047193607,1.5996478404947303,1.5918076063218505,1.6210014866895417,1.6033404388695378,1.6165796658189286,1.6122799196889364,1.6241344207269963,1.6085848594623324,1.6277933363067354,1.6308607726426174,1.6162309807130526],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.5814046566398774,1.5994586587992428,1.5910968907286496,1.576917740763039,1.588910504151847,1.5894461995108815,1.5740169461587423,1.5610349162311348,1.568555733819864,1.5692076610248733,1.5603901196963894,1.5416154474196258,1.5605394761090141,1.595110653059227,1.5853885169716047,1.5948357401697892,1.609617923603231,1.5919124072690456,1.6167714951901182,1.6200561305563104,1.619869900605548,1.6128912627529683,1.617877488149423,1.6320371817103336,1.6209817462819118,1.6413858350318018,1.6411833339228148,1.6430694665557155,1.6365174482401517,1.643781318834849],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.5922359322727428,1.6047650736375332,1.6049921693234515,1.5988099288012272,1.6326125918708423,1.591964173897397,1.6344816413403374,1.5801613778128016,1.579140079321881,1.5836325854919437,1.5686147760468458,1.5894221105365327,1.5701935790961281,1.603920006117455,1.5892334931897174,1.5994544955803203,1.5956984818664537,1.6053783370303814,1.603930375450117,1.6070004138709773,1.6200276523237302,1.626358131779372,1.6125290734619568,1.6240783597991748,1.617253833547653,1.6216045480813974,1.6296325395281037,1.6328610580884177,1.6404526000396005,1.6386187869214928],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.5753960986784328,1.577523842845141,1.5820897544512904,1.5948578217673015,1.5931395571484717,1.586500229832592,1.5826547549270806,1.5796377705455489,1.5622672581688328,1.573978728364727,1.575255284855375,1.5622534064596294,1.557189163144075,1.5743776935027285,1.5723617626936104,1.5766091495317363,1.5721547480756481,1.57885388017604,1.5805408899805842,1.587018935177448,1.5913218441668924,1.5880674216780104,1.586405729761586,1.583737821509665,1.5829919259501521,1.5860849111435553,1.5932383447798686,1.5892570338801941,1.5922196963902442,1.6041450113510962],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle AB pour le paramètre<br>adaboostregressor__loss=exponential<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE AdaBoostRegressor\n","# pour le meilleur paramètre loss\n","FigRMSEGRidAB_log = visuRMSEGrid(AdaBoostRegressor(), 'AB', n_estimatorsAB_log,\n","                                 'n estimators', GridAB_log,\n","                                 BestParametresAB_log,\n","                                 'adaboostregressor__loss')\n","FigRMSEGRidAB_log.show()\n","if write_data is True:\n","    FigRMSEGRidAB_log.write_image('./Figures/EmissionsGraphRMSEAB_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.8 Modèle GradientBoostRegressor"]},{"cell_type":"code","execution_count":128,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                                 paramètre GradientBoostingRegressor()\n","0  gradientboostingregressor__n_estimators                        3593\n","1          gradientboostingregressor__loss               squared_error\n","                                   R²      RMSE       MAE\n","GradientBoostingRegressor()  0.672942  1.161339  0.799445\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predGB_log=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[3.626464252565117,3.13450271244382,5.150196919017444,5.889441756205647,4.178251895377189,4.335153624862098,4.183369028495245,3.5154105335869126,3.0183376680656875,2.3846466883869946,6.657960910503496,3.8078391785043277,8.664863421563876,1.9967650992967632,4.846075929515938,3.00612550262841,2.966427922147438,6.350229307746936,5.007708643093584,8.978342333129406,8.873923863833474,2.447229345526698,4.638770963300596,2.9988208715800178,11.337199148958504,3.8691345559318693,3.8261639347788017,5.757799126980582,4.6422075264507034,3.495108169477715,5.543622315253983,6.365637352439039,2.2267330526670635,7.880294254869321,4.62792491097233,2.6907393816076444,6.374495349064656,4.424521823542013,5.437013020347759,5.484340395529509,4.506733177410591,8.279923203126664,8.00015019108059,5.0921072007311805,4.946385217334185,7.114652381197515,3.8559480065608476,3.4499534278874195,6.343979466994132,5.376358254490079,6.922710054356886,4.075021094872692,5.893529119607486,5.157130641786521,4.392257383967739,5.295877874339602,5.122254123411834,11.096558033476443,7.966206326540464,4.427248116562137,4.7791163834190264,6.105562040101504,5.5325494908056445,5.623376480540699,5.655560195591937,3.9391145438582464,5.181792698079809,3.7455853002124138,4.990282455856595,5.649588164707978,3.512494142820117,2.5066832662513265,4.798562162065916,4.71536275661286,3.7875037242487926,5.6962378913829115,5.223639342434966,3.115928756203794,6.750253138906622,7.2038627827378985,4.79584545480293,3.604294328510293,4.688751182312802,3.0055044019724724,4.057963494859672,6.74725047385346,5.579507412394253,3.9857551394450637,2.297847333911313,2.5607876352982273,4.492058695786614,4.095667087534104,4.358356595625088,5.429509110843708,8.118898994746159,8.06234337202849,5.07290649403209,3.303395066488914,6.933739639690669,8.017320963349135,5.606677865863301,6.158825799497909,3.4273351403275107,6.579074324391901,4.901589529191255,4.475095648910262,7.328814815089664,4.077233028514109,3.518968348913212,4.300279560163778,5.165388904783657,8.499714181144615,8.236290958575658,3.2866690750446343,3.98370063184134,7.276059229933064,5.346611397651463,0.8235221472101677,6.517515769541116,7.1754573895797655,3.0012334969784575,5.593567108201074,5.046273556941214,3.593731243612891,8.085642215828678,2.8194940149826437,7.580477311155871,4.776438227481396,5.504475731484524,4.311476698990754,3.7321986668807066,2.485255620288935,6.76287636951597,7.378782186986058,7.471381246946006,4.961811519694742,5.710252689489527,5.947809131968454,2.5107445492061364,4.2828367625604145,5.074491040224517,2.525359731359378,7.060408157011768,4.533421720620326,4.908810130294141,3.039554400186647,4.577022991747896,2.925003637287217,10.0023920002563,2.9869785823871755,4.3850030349218265,6.10519662659212,10.55412442923048,5.856498648266064,5.838947958010701,5.859636984620966,7.068437942753511,6.131897900924846,5.36166876686879,4.600451695676722,2.665998580075316,6.422938901000735,6.288063781434621,4.791337152226549,3.3609452004784135,7.60155110577278,6.546263170634044,6.029901221975558,6.240202955541054,6.4629924462410715,3.758893158660834,3.756109318161126,7.50740937005833,5.431272396839854,4.565062675422548,6.836614984533408,5.0648328639251625,6.953095527525955,3.9022003857201994,8.123831852623452,4.201614641511907,5.661917407052541,7.576207764266823,5.284946684510323,2.766683500876593,7.2709102311950335,4.348924298968831,3.5737396048304215,4.303408019587988,5.270199530441671,3.612877129322135,3.9555046493896238,4.625504394832635,3.2244142504175843,4.593249377270104,4.037038416619247,3.7293644090295324,6.706114034161627,5.088025140895457,6.495763739700077,3.9798770983589216,6.711906082833818,5.070386557254736,5.093448713821585,3.9423666846406076,4.719416546587844,6.499292071628274,4.87188398998228,4.124988229452614,5.1072261470031055,5.571087433500394,6.064226743088039,7.847285618625252,3.432254610511305,6.49486791472106,4.053409666359105,3.284053283688066,5.876460260319584,7.265450374156521,3.742270583093546,5.069828378966777,5.350836722713017,4.234136344900577,6.131838198989312,9.02056372438307,5.567024363692746,7.424954169292612,2.686767511375766,3.584780463054887,3.631522508773446,5.360510717972833,6.300383670725523,2.7748861714846242,3.194815946305211,5.045999501211102,3.376785247084121,4.636293952382218,5.1218715036427716,2.943468970983991,5.914588628155794,3.9729385587321664,3.623503967752574,6.124000927522381,3.0642143454880073,4.697881960721879,5.021729298602733,3.563079789760698,3.7157705487740635,8.92496949314558,5.829926734630796,7.1939765093038925,4.824402873240916,3.1788224860492846,5.962723129467957,3.896167590198725,4.374644046849497,5.7247346926525555,5.8634273390402445,4.56810649239023,2.883479041508367,4.9373988462805976,5.424995480958835,3.0285258941634194,3.671455216039478,2.7001120418311495,6.819215688836605,5.301139756349481,5.807972077621137,3.0209975282043997,4.229426107683098,5.702464989187032,5.531003793308377,4.817143943819424,3.88399416572039,9.993311302260429,4.0663210320771315,5.427302397174947,6.713039908003232,3.16142448967037,6.896876967480713,2.922585499906311,2.7709645337308833,6.108723857417066,3.862233246764793,2.347419004250173,8.037911169198974,7.203178340122286,2.8067122934757385,5.469614590763208,3.6051768700591245,5.363940103037401,3.5735682654175247,2.172579224370668,3.303058400399964,3.4932794546732095,2.4073853063123547,8.386956910816615,5.571557146387512,5.032548158643353,7.357974320833443,1.9464087240887715,5.560047371846062,6.447359139017814,4.873413704344354,4.163592964448883,5.365102554798755,3.239859635815498,3.6123871208727283,4.075151701450599,5.27240455661994,5.409542609369731,3.195548354356885,8.022240314292356,7.034582058942908,2.5060268537773243,3.1514119960358444,4.7190791300493515,5.955493569614832,1.8539150224749479,3.5358480199915516,6.074135352847085,2.783227464447036,4.075993638742139,3.443077628071297,6.569014619985073,4.405163232050596,5.62353695122331,2.272047199200219,4.414201398231484,7.523546133783114,4.257705820629167,3.353311487103009,3.723003048821298,6.7003312602121685,3.8939274693917416,2.8790113487118387,4.025798092294625,5.508613863119602,8.281354016883592,4.473524207456932,5.686643498116355,4.086818655194647,6.596144502028413,1.7353958431320657,4.890645215583597,5.676391799801904,2.094922558899809,4.404767419755322,5.4437188880662335,7.482137688611463,3.835293029951573,5.281194976576141,4.41809273208502,3.4613214458396317,5.708857826803833,2.6215189083230497,3.1964536296521873,9.249114770634492,2.9983598807812477,5.571235987598636,7.1697707541301465,5.6117036922088595,6.148297460807588,4.266023225880041,5.784085542437001,7.054923069530423,8.506218688364681,3.6502884926515566,4.187253402187659,3.1177177579555444,2.5269476151771277,3.7853377412443496,5.51188064299825,4.2677445027725,5.0524740938686445,5.921962767291336,5.497030264315811,1.999814138943078,6.066930632024382,6.115224299775423,5.585654336333561,5.581305240814102,3.1415329648826926,6.461457094166064,6.562101802758624,2.5596175006808712,4.589074044630574,4.8832413271585695,8.075351951909063,7.6777716088098495,6.7358301239280225,6.332993824698238,7.527752168767826,5.8347378445871305,4.744160926232245,5.79577675028222,3.4084747480735107,5.825784877033307,3.7392389071966456,3.639776734602816,3.0706236225653587,6.135254196580702,4.2603633042240405,6.073787796284124,2.3741226911739406,6.898846664619736,4.508734522360901,5.266762885850376,6.63918538509221,5.418542923440831,5.367829155995764,4.673787266673878,5.758709508439316,4.374311626477371,4.075388871880871,5.572114444291215,9.364622513860692,3.5626067575773126,3.3278771787628885,4.384090078321502,5.095764625713214,3.953139981251383,7.830428205265669,4.0429660371631035,3.1356250867751827,3.0811867977661707,2.547775087768599,2.679561381591412,5.472762398958995,5.236094435046183,4.359112313574729,4.227694855142824,8.263885715955672,4.753773621095429,4.293989370874932,4.706936452597877,5.7342082598527035,4.277762865347454,6.3242922116742335,2.8053996183723964,4.828762805541552,3.9096507109738994,4.3566438334771584,2.32371422888909,5.792726499884415,4.705178246619517,3.0642405610530816,4.588327267798956,2.809782652609976,2.2887789031505643,6.502517517285189,5.030361880678909,4.416118031288532,3.4625378953436945,5.086336270554942,6.035508340264546,7.057109450285655,4.927845584530915,4.441328983770258,2.9445208036585298,4.849287376314675,5.088124774911811,5.375709729413411,6.301233899557734,5.431453179302181,4.0214534774512085,3.6917853479032408,6.047668353877637,3.539797531088782,6.905238130507097,5.288843023143861,3.2970113118255058,3.1498158006744634,4.761836460045637,9.123543131515117,5.169003095935035,3.9626096307612495,5.008134784542888,4.5461347285949945,3.3641637604573984,3.9779005691061697,7.916820512119844,4.455414500136283,3.5676651629214793,4.259256805059907,3.5723617789263065,2.519018817543737,5.475797840473157,2.9105833304283517,6.729650643417362,4.065398800496352,6.289533265273388,4.978644312967315,2.4416828415146563,6.533642093793646,4.112401807766793,8.632648002188214,6.925562959772686,5.282658596220804,4.983094727664598,6.490023007438181,2.665998580075316,5.805578832813509,5.773835854460931,2.8782515035359486,5.725909811908213,4.025465081277942,4.0122387638933095,4.434545633325142,3.1625671557114083,4.869717945839169,4.9420177228409745,6.0126732000533805,5.835254067292138,6.246418105817757,6.186318468084906,6.587608325392964,2.885627834783346,5.670114960479395,4.735412036896328,4.404160370824961,4.643126846251525,0.9964551641656313,4.611660259097189,3.9639581952618497,4.438801816405167,8.266575932946523,5.586975331977393,5.483436575054256,7.536390806768498,7.404573592172002,6.359669428233592,5.9215863903015356,6.867648782276225,7.266861244698348,10.106883380732032,5.210033199075371,3.0643037416550474,5.417368461864614,4.610990822919152,2.043307458537274,2.989685269636123,3.11567522634084,2.574526100931162,4.933631653920565,3.8822522644445203,5.532114549040202,5.354312124220535,3.043031947002385,4.555260281629312,4.243888668243967,3.473363662221993,4.8785817534071105,1.723573851220528,5.091461426749313,4.67839896737406,4.319081190699637,5.253967815993559,2.440688860950891,3.0167379799200593,1.983355391470417,5.7895659653142255,3.0249955561653907,5.6525600593566905,5.0382082617829065,6.288426590989264,5.074459110837515,7.28643543410421,8.215689212372231,4.836983070691622,6.845421966384048,5.404855770684573,5.5531686906188416,3.7661904742991097,5.097486460705968,6.177497552120686,5.51578058757894,4.57224044714216,6.602910051677677,3.973115046698301,3.6285753788597974,5.725504413711483,2.212906495866272,5.111216352517343,7.1375872499720705,4.574433363601061,5.374802379583989,5.81609657069028,7.4041512622112045,4.943000949508785,6.32601750686423,4.706281467806882,11.253984355725379,4.985018704970062,4.086553132196631,2.4898101678320135,3.849514148223451,7.49696521965834,6.444979250697534,4.066906249673992,4.456808840016693,3.840031895489278,4.6295742760674115,4.614085576839276,4.241222085091474,4.2008682181942705,8.500213092301758,3.951579881394323,6.079827551366973,8.3922527703049,5.289012939789517,2.617670977163384,4.265765179258528,3.6245870408770884,4.196802240925835,5.627838853143702,7.365370545595557,7.700791553683205,6.9589225888116,5.415763740937339,4.007225378055973,5.467397907752312,6.26512243051691,3.176047044573885,2.412263374741022,3.487471492820139,4.136137896463177,3.3271618611252984,6.33390313753893,1.5513967391237033,6.440344912006219,8.265198775055522,3.5726658764569303,4.384647198522659,2.800666040676485,6.8452105646926205,2.779433518399862,0.9026819733762376,6.613797103309666,7.81509900936904,7.151583427294761,4.2562762472090485,3.324444129885991,4.216919884038689,4.776481545256366,5.055254465904019,3.4498913274923466,4.806159030845941,4.852588321361522,4.297474284786168,8.16441551483955,4.945278693262481,2.924575142321648,4.138370358103182,6.987743478449865,5.31957964140194,0.789911205139404,4.834041321844011,5.394230175460754,4.365288251063771,5.073630264295725,6.551902588332279,4.22796551311816,5.0900359393127985,5.840328951662186,2.1423225144702434,4.519857537941669,4.046809364432204,6.935288543736984,6.777142201601708,3.3162038735472206,6.317077670252053,3.877472797464101,2.753287120370503,3.7983008225664516,4.508611342091094,3.793193845419111,3.7916417318465574,4.432529921189943,2.2184221725134288,7.171482007506461,2.196166259010864,6.419449582128753,7.480523388352418,7.361179726135469,7.307627794539004,6.5370916788329545,3.358552841070937,2.830980695622878,10.065170822298224,4.3759325715258806,7.267481605574666,4.318376358001413,7.147361364327683,7.60193241717228,2.8401050192562276,5.090902126416087,3.6449557212040355,7.015475248055516,2.9725143213440792,4.122536931293226,5.347576150942461,5.347338684215295,4.334975084797902,2.9020988505134557,5.604132233394011,3.2890796842757046,5.089579148772451,5.680810361535681,3.583401793838992,2.520295544634692,3.972955076707615,5.404980540466459,7.078575828039824,5.626028881654996,5.471661189090664,4.564347926866034,5.139969579059078,4.771107919699579,3.5855277859111685,6.3782216994853,6.028328533558361,1.8334313174595467,4.225617794707748,6.344963722503447,4.378364882900753,5.0959470710865835,8.652739291499241,5.402414535958822,4.811420174550282,5.940992363980864,5.284203200232581,4.834610447815074,5.113927315266662,4.9378544593681815,5.768789042585805,3.867624953987084,4.135838516257178,6.374275320111229,5.074278863382388,5.962346317927404,7.649315122717178,7.1142616990755245,5.046051328416972,4.341135072672235,4.317014420311243,7.205939646984416,4.398167101940576,6.377590755342389,3.5308859756801145,5.179947654307944,3.3558746624316758,2.677043791397099,5.974073480896769,2.8543439512937714,6.605003788924151,5.216549502645748,8.506772853010693,6.661983995335032,3.3022298404859187,5.29467055641266,2.7182030665997474,4.951982163862872,2.9921583972244807,4.271848677368434,4.4719942648383295,5.48191411984121,2.5228770793524706,5.430782078962293,5.608391400880963,5.155003818536369,5.511299710508312,2.6892643968180745,5.578347142608745,6.177861213681195,3.656652876541836,2.9728249786506904,3.380602385948394,3.4893688136861396,3.564710299056195,7.165265142192121,8.159121998917408,5.937852342222168,5.701586303426463,4.003904373871176,2.835306492881985,7.512745830568129,10.167364680960834,5.85373963873798,5.740997253943921,8.693950474056239,3.1313940437511416,5.092107261434302,5.115930746553012,5.377494215698911,4.386351599422427,2.5486943503595336,8.273322505451281,4.38972593496345,4.350738716966134,7.1229274037332395,4.519857537941669,2.7783770787563213,1.6880508349188623,6.986230767811994,6.488454462167999,4.0684736504853625,4.4589393203514405,6.478975477205706,4.461705940762809,5.442608264896317,4.919288764553503,2.141984197921192,3.6348390357683513,3.2883164772997686,2.5059274062014305,4.327493968737926,4.016990391356069,6.782348374450545,1.67873395878318,2.71799327223689,5.677900553068773,2.157283156556413,4.293472224819049,3.3084187580121163,3.9148334035438386,3.709807979410653,6.068123472036559,5.076959964888552,4.169468861072687,3.29702815343844,4.713345033160424,6.901639400421461,5.154036374352624,5.541225944462033,6.643695078417758,5.044614125287185,1.7145597367445946,6.826552633927029,6.372105973312604,8.102208020381603,2.7519978642370715,2.172504845699349,3.363877348150116,6.183672832630793,5.155576367943045,3.3728845394773233,4.526099410228834,3.607204134001204,4.790441567772514,5.446923140029321,4.062560426311548,3.8493719354550078,5.455955452825931,3.6206234648110507,4.275496899923169,4.143663054358915,5.591567651441213,6.204780938811364,4.819914835776391,8.85884471107135,4.24094812427533,6.215753716611009,3.484918744983029,2.3638585579030105,3.7488000476832,4.357890960852361,4.105376802988194,8.18892881378267,4.300105206540382,3.1793545412557407,3.4247429535178107,2.6180903091465053,4.870080894502041,3.042702875827874,4.480238174296359,3.4782581989352708,3.4447404505943053,6.222802155163842,7.8726546938503965,5.530772662027915,3.7074160463756782,5.4363120026595695,4.204091551913323,6.30482376396761,3.226584395044896,6.943775420901647,1.740742771371729,4.833091744320678,3.056021635123741,4.488403012222248,5.811920981770952,8.592252995682848,7.369822181549743,5.481917000048172,6.515045044639201,5.035015283070897,3.1543993234190295,8.055756863070808,2.9439425469947915,7.846683024429322,7.089283305972368,4.239889472279147,3.749486181163417,4.706045880477176,8.646173051325631,6.575685692315573,9.156278872871594,4.13691249639907,6.87602428101565,8.603402238623357,6.520220318955774,2.5087267840236867,4.171990023679309,6.512271782126353,4.083374333920137,5.074461607623554,5.369247685809031,3.7606241006326186,6.930716194742305,8.864816098146811,8.365809385287836,7.267156161113071,5.6742908248437836,2.9237920766799874,6.217729279305824,7.275089038690001,3.099604771372295,1.3474805595620933,3.927839306099661,5.393433776625992,3.865354361085104,4.734419475752987,7.268128077580051,5.549394757387587,5.191295368077173,5.496298246449041,2.5951685871923305,1.8385092180887348,4.190936443267241,6.523451140890373,5.581612391159067,2.919676830114952,2.3934872597587242,4.415568355581971,4.983439577260676,4.703958476264942,5.983999005452945,6.684341556534417,6.6635568972733035,5.6188078391203495,4.4197846541735535,8.392847607305251,6.649089410999054,5.386773443621658,6.302048594922061,3.6207414293054683,4.555107612554857,6.43077281180025,2.797911429058357,10.060391359572419,7.612988232981923,4.806207778841205,3.85492013208166,2.183661424716992,7.047326393395437,3.886931227856221,2.66393523404093,6.680641703761322,6.703328662732448,5.090642815477328,6.308248403085804,7.485096356332103,3.852947265145896,7.551617002531563,5.7836324949180735,7.635852271805596,5.723948659047158,2.5629202212920967,3.4625378953436945,2.245974121355154,8.769842319904372,5.258145726685116,5.31898246028145,3.4054383187696486,9.309974216421057,9.932974554329721,8.054169655068726,3.185662373619276,3.0695960757152756,4.560904190564704,5.085131161957899,5.955341418977082,2.759476248642453,5.178201265527184,4.760709420012867,4.881776452528474,7.950455696147668,4.513998285044553,3.2253700963110177,4.020699504930123,3.903082711424417,5.1357703000517025,4.942235909164416,3.6702991873222146,4.617012222609427,5.974674335809855,3.7518024980200786,8.470884240618254,5.810184419666218,5.460668861694449,4.2339761560957045,10.310262519153925,3.108015791431982,2.8537119568579223,6.583408692733924,6.87938169054225,6.090499304942025,6.351911674902602,8.801293449845904,2.1663497529249076,4.872577278153218,6.759611374216624,5.18303484062538,6.617161747138667,7.54258626913738,5.043388155679932,5.63275614005889,3.568943654162903,3.5584677106188476,2.6864886973109914,5.0365582095551975,4.236043230924015,4.331818196536551,5.714250403203087,4.368221357495161,7.1876776838529315,4.0272933965801,5.903981469007654,8.323837131736363,4.8995918654526145,4.669150567953522,4.550588357226565,8.173436473873885,4.389141148870369,3.33252046841436,2.360812928825995,4.913373423754987,4.383472254767807,4.918411740082566,5.644547996579672,2.8216372617676453,4.143695313665212,8.120654275460039,4.875213410681338,3.6795524966149507,4.248349015966265,4.863709826960606,5.717025441674459,1.5410368504278544,6.063994408015297,6.253496429563287,4.6544486800576506,2.282178202216033,6.576305962214013,4.213358908514122,5.303294100432329,6.318650543982579,3.5059617156529406,4.349242762540668,3.60068055096241,8.120426632960168,8.458237406544884,4.3161810769750435,6.871401160453595,6.800525143840561,5.851831272082484,3.4947818327139792,5.279070921435187,2.507590034624658,6.907114476425763,3.9022451313801882,6.12688659520184,2.8570070160106433,8.640936413656496,5.477682688451966,5.582226694168931,8.600406880152219,3.5291709882504203,5.985279516125618,5.896881467781962,4.532404289358156,5.392646115129653,3.689011641158084,5.1881603411268635,4.052282913298788,7.046416568795131,4.840761054368213,3.072188183704361,5.131596335896411,3.235879967816171,4.494739189724146,7.723590136228006,4.860359250255958,5.24720617910039,7.835337031939071,3.074149096166851,2.5519198339306595,5.657574680946213,2.826017598021438,3.9660931176043985,8.569838254603061,4.489105737354286,5.510170072445378,7.0432839419231605,7.711860702448295,4.8767414804600016,6.348344493872661,4.138676532860407,1.7976089458717666,7.278972406659287,5.878373447628488,8.54533058608635,4.155572989500668,4.150055773845683,4.607522626191336,4.247604898406592,6.39191694028942,4.379676338327687,5.553378521238754,3.8268537842038026,7.004822609426679,2.945586874608785,5.86191418621585,2.016432610126668,7.488130084588609,5.2953209345846695,2.9338548749845255,4.515615931036821,7.359277196261938,2.7168972447513973,5.053343209655106,8.110927257440258,4.853846267558573,4.173870800770439,5.548106181753321,5.098158262466481,4.889778712126746,5.504424648355245,5.511299710508312,6.550903744905879,7.087595499575885,2.7389746474866707,5.93865467384947,5.3065640518012875,4.031011723069639,3.49374860332286,6.492287763727512,3.765456502898792,5.4885104878915225,4.290060302702591,3.205287018675843,5.287203760684641,7.2467424390810775,6.493835952992837,7.336973973870308,1.5254670323714015,6.248170270457121,11.797959155330796,6.611697090858296,14.798224633388216,5.387856291160954,5.829992068734903,4.01851326014001,3.092178232813075,6.519785587868346,4.099565785932836,3.739151247119463,5.702752403700402,11.904170321678679,5.085509741703141,5.497866498077722,4.679633694579677,5.19519145085102,3.2253700963110177,4.997308282292633,8.004929541434361,7.640771863324278,8.190123982420758,3.5419193354078886,4.475801488490238,8.633563494462146,5.088763502641255,4.316156849463078,5.735549242841912,7.042348703078729,3.207262187236908,3.7443871305125236,3.2408790911486194,3.2559684053764553,4.248189930865935,3.9961374763884185,11.778220362993094,5.782413723068615,5.3667492748856445,5.615045941997825,3.2236197006402443,6.3821035012068155,10.160026051564882,2.221093891151321,5.797082774602651,6.1792385608414415,6.534539792810939,3.246811923019333,4.723312555333354,6.018252334527505,3.9628821400273306,3.677953179370398,7.2583027372596485,4.61105079711674,3.303383519370985,5.599772736077975,5.458077530892943,7.0005264540996714,5.692008424134386,2.5700555886502947,3.5807064561676776,4.778046532781112,4.041286652189086,3.0345314811668316,5.858252537466295,6.943416591784192,3.65043598709932,7.769293815233885,4.507161453344933,2.959342190213292,6.50205536650252,4.239468715643119,3.8060157535837833,5.670258734452597,3.649078436821475,4.536625871587096,5.337485346984682,6.731123024859669,5.616951820836855,5.43352062593885,6.784322796559279,4.175726919697498,4.336581524069975,5.517263353877313,5.36284823786866,7.615435062262023,7.357607810032356,6.5658672934741,3.6513786528261343,3.629356013799597,6.961692566945359,4.003871140870471,7.956471802106363,3.2135144682979395,4.286816342014476,5.928676931754462,7.077943813289496,4.901263992393729,5.675644472039447,6.033221919953017,5.319469386930483,4.379374454534422,4.378109261586071,2.97353521398245,4.545765614114941,4.983439577260676,6.326136711881297,2.737389589489237,5.312515738226379,5.100250147281024,3.6854078204628804,4.0515868046551695,4.870079918647021,6.397385812541001,3.967404233884372,3.5699306342856376,4.4922274606723915,5.3194716720064426,5.950876654511856,5.071961703988211,7.233575227064499,5.617671877911518,3.807248966143651,4.77690323311401,3.861026969650129,4.854174522039699,4.444077262846167,7.110613168098834,6.572783139786218,5.379040147112933,5.637096614436421,6.560278804767925,4.99976065700984,6.544922656276957,2.5259635650358008,5.908156433506111,5.668608465196578,5.879671706106293,3.232453616028245,3.1094051731650003,2.88721026646415,3.618705228742056,5.470060404571919],"xaxis":"x","y":[3.137503523749935,2.4646682670034443,4.808385050656093,6.381629467033664,5.687900522480746,4.296457407371244,2.877744249949002,3.542258049766918,2.752748591407134,2.1984941536390834,5.6604951318885135,2.9763636357327616,8.749098443586298,2.1043366598147357,5.40973035386445,2.3305584000308026,2.3950627995175777,6.141391916170729,3.76659516172927,9.707290101958575,8.663593681954685,2.9126498648972037,6.819668183496455,1.6553518286125541,11.880218691860534,9.057883448899627,4.445594291341182,4.326249700837456,4.708739041359579,3.5570424152637194,7.3848269116030085,6.815319245777306,2.2600256559614555,8.198690585668917,4.72137265947682,2.6712933724815757,7.400708693037901,3.6229303509201767,5.137503523749935,5.359661722333877,4.571676809970931,6.199672344836364,8.475936042446387,7.905507349949873,3.8933622107638715,7.537839615916466,5.353323291162897,3.311793717753649,5.2712762591759255,4.778208576398088,6.628919373511362,3.1858665453113337,6.145473670317708,6.023698917266407,3.145677455195635,5.683696454306515,6.109569509370688,11.964503324303022,7.672213165334465,3.8787253414801053,4.8283269264148165,6.010108453474289,7.029563314432658,5.911212201558612,5.9590746011405145,4.0232553523003025,4.234961094839504,3.41006969175638,5.59215800212536,5.982537314399271,2.910732661902913,3.7687136570304847,7.183982678112831,1.0976107966264221,2.6016965164809576,5.511910748670928,5.6570683012015754,2.2868811477881614,6.999098033705607,6.746447139362815,4.83035674741922,3.411426245726465,4.77663042305105,2.550900664647523,5.942279901705763,7.626950122372515,6.862203399053224,3.4019034716079584,2.608809242675524,6.046141781644721,3.3936907641874536,5.390254956321868,4.541638734436141,6.063718640152248,8.226941966807852,6.203788453308596,5.530445354692906,3.2433644256936605,6.703349915166396,8.202417721575182,6.075318692553354,6.401732849522621,3.3305584000308026,6.606294160697713,5.5786368624717415,4.273515889702116,7.668814088419117,3.097610796626422,3.321928094887362,4.593353770980297,4.792334805706692,8.090535901003173,8.367589480412983,3.1358631653686793,2.7070829917717063,7.1593668097437355,4.614709844115208,5.946496941203293,6.783456654360239,8.491372117815171,7.273609132231856,5.944624223055033,3.070389327891398,2.2479275134435857,9.095924419998536,2.4489009511451276,7.048759311919856,6.058532970201611,7.216163813388774,4.46532151358055,4.871350840634026,2.611172380044005,5.783718557790366,7.3824943402931185,8.12401788912289,5.2787282129389395,6.099295204337775,6.1705259991768475,2.269033146455237,3.6392321632492775,4.547819956577703,6.003152448666922,7.816087658600206,6.762348815644129,4.940166750482817,3.01435529297707,5.487357715311547,2.333423733725192,10.642557179595622,2.503348735167504,2.4356285940520905,4.773468927905195,9.760270854333914,8.156942620908886,6.218781167784069,6.306517445249819,8.352882086522959,7.032211069322931,5.5963388642052,5.320845667645722,3.0089887832272546,5.943218087426284,7.3403842354553985,4.443606651475615,2.845991770664573,7.217618485351048,6.137913322088407,7.084383221650577,5.784242222057781,6.097610796626422,3.605257262939004,0,6.954079959320971,4.883620816285672,5.118941072723508,6.583308464847678,6.034963989749303,7.257764968571387,3.91169158187234,9.10522758436473,4.507794640198696,5.001802242633985,6.858478390423273,8.095608004811197,2.3103401206121505,7.6398833131998005,3.144046369616707,3.9420452599160467,4.142413437873741,3.925049964727359,1.835924074254375,6.111448698487674,3.363171077119244,3.127633279725874,3.24031432933371,4.160274831408593,3.5397791916984938,7.571221725048103,5.284292026394312,5.47800105583793,3.7398481026993275,7.1320626778543845,5.576522137920503,3.127633279725874,4.3110671022555955,4.375734538583156,6.4432751117658045,3.91169158187234,3.8042601156347384,6.4409521980296365,5.85872702377863,6.02724253597374,5.212958362728285,2.5058909297299574,7.561631630375841,4.92552476974757,3.5619370596224327,6.237448995639321,7.881113960675097,2.176322772640463,5.361768359419154,4.9941271140031285,2.584962500721156,6.166916251569974,9.4493756610678,6.0332027259372225,10.919861451442168,2.693765712217783,2.4724877714627436,2.7484612330040354,5.633721812641012,8.17707065762942,1.9634741239748859,2.7803100990433753,5.23687524536694,2.698218478224414,4.238022517825721,5.317231700222606,2.1176950426697547,5.604071323668861,3.2555007331483865,2.2387868595871168,3.881664619320345,4.493134922305505,5.357903838552492,4.9597701552114675,4.724650271732967,4.4046306842176115,8.46707564951531,7.096029876307993,7.341896820977107,5.339850002884624,2.859969548221026,6.582706526780788,9.101634319640722,4.129283016944966,7.151270288790165,4.58014548442338,4.628773595201645,3.070389327891398,6.034303767768035,6.339493737901733,5.174326515173915,3.357552004618084,4.809414444235898,7.880195728943098,5.632268215499513,4.79960542245267,2.077242998932461,3.5298209465286954,5.433960931121179,6.078951341394822,4.854494418154875,2.6484654430273142,8.345405246717796,3.4462562298895643,5.595145567990858,6.766065051474954,2.4222330006830477,7.366846735262642,2.9873208659292536,2.6507645591169022,5.940871478189196,3.552131108253784,2.4672794804599825,7.901409979999464,7.459185683748918,2.3868109464722167,8.668955876629637,3.1667154449664223,3.017921907997262,2.4982508675278257,1.7782085763980877,2.9088129077395473,2.1602748314085933,2.3074285251922473,8.316598324108684,5.603477988254004,5.669310286202474,7.991691898336964,1.0840642647884746,5.091276694365222,4.042644337408494,5.96254902292306,5.020591094773247,5.20163386116965,3.065227622775619,7.08533966935737,4.321206566969903,3.5298209465286954,6.083000565676004,5.04176864995163,9.435170178881494,6.4705368650925825,2.0531113364595623,4.078951341394822,5.525755692829486,5.9818526532897405,2.063502942306158,4.110196177754199,7.135657989368776,2.3812833725037836,3.693765712217783,3.00539998774259,5.589164236699772,5.36737106564853,6.38646609286931,2.3248106034204836,3.1811025507537978,8.27891401949994,2.430285272977781,2,4.867402305727543,6.299391206126829,2.6803243568440163,2.693765712217783,2.7949356628035362,6.605701738949982,9.159820892013935,4.255500733148386,5.555509427083257,6.871104373555458,5.632849830117706,4.966707314150316,2.526068811667588,6.09950561709114,2.927896453728821,4.28835856219366,3.1634987322828794,7.07692245939234,3.8429788317883253,6.341274183692174,5.926948247949772,2.538538163629804,6.73226919950145,2.788685710613534,5.508111681193465,9.398936038454162,2.3161457422933562,5.28835856219366,7.479861085016818,6.0925457415435655,5.747655932932393,3.9059284781731347,6.235152624217932,7.3334237337251915,8.870210577263595,3.460742563789644,2.974529312483882,2.6016965164809576,2.2387868595871168,2.7676547982373463,5.659639187015652,3.894332742277694,5.771093251699539,7.173327349650988,6.027463723408577,1.4222330006830475,5.794675787986858,4.720825666089835,5.342696959981934,5.562547724199122,2.62760683812965,6.426767932655541,7.593353770980297,2.5185351389821804,6.676239195086378,4.472487771462744,6.212180210044431,7.178117045706399,7.534497433738168,6.304876050044889,7.545505100691072,5.077670274232752,3.953265239014844,4.609991295212678,2.9126498648972037,7.174426393677253,3.6016965164809576,4.515384460636948,2.503348735167504,8.129128434194934,2.9726926540042644,6.030998118171895,2.2539892662307865,6.597829097826899,6.8209447918806925,2.9523335663696857,8.18769847638827,4.950468414150123,6.1409831048784245,4.674121632632394,6.830610274357907,4.4376272483189485,3.7037651787934096,5.95837871156634,9.296618003796326,3.351910961103077,4.143230134776893,4.707082991771706,3.560714954474479,3.9845893503624565,8.328001662149257,2.1667154449664223,3.1811025507537978,2.4854268271702415,2.931683057059806,4.788685710613533,5.659068274843228,5.738227400367242,4.6937657122177825,6.860962797858116,8.65474342118394,4.814550423461808,6.107687869314374,4.720825666089835,6.4775154361842375,5.619119511453218,4.7136958148433585,2.5533605033353277,4.412781525338476,3.72137265947682,3.460742563789644,3.0197019144425483,6.602587527342561,4.687060688339892,2.526068811667588,4.698218478224414,2.8698714061777126,2.144046369616707,5.394376944957055,4.649615459063409,5.80683958179819,4.443606651475615,5.450881315273369,6.6503337538459295,7.198494153639083,5.228049047884462,3.9231491804723087,1.3840498067951599,4.524815928357506,7.211012193485511,7.027021314622255,7.212958362728285,6.658497136656478,3.3881895371560837,3.140778655782796,4.37364821133469,3.319039815562536,7.33360262826828,5.175524601089875,3.2047667506546134,2.702657543390911,3.228049047884462,9.322806969423008,5.561631630375841,5.8755343515732354,5.032100843167024,3.9873208659292536,3.1842802944193824,3.277984747299765,7.444849247263915,3.6701605141266334,4.595145567990858,3.8369340113210915,3.575312330687437,2.788685710613534,5.4571345943180365,2.4276061727818994,8.563615765629766,4.712045448553693,6.260590274730996,0,2.4594316186372973,2.8257856274647914,3.049630767724601,8.097242070141174,7.431622959713292,4.030336078370959,4.964398632203184,4.020591094773247,2.211012193485512,5.462052318796433,3.3881895371560837,2.411426245726465,5.0430819818956625,3.6064422281316078,3.613531652917927,3.883620816285671,2.8318772411916733,5.081936081719432,5.575917361118149,4.958842675243241,6.716579420796345,6.531225485053606,6.050284014301706,7.73125113789623,2.950468414150123,5.645009884391095,5.4015622072560685,3.840966704487421,4.3298411765306275,0.8155754288625726,3.7070829917717063,5.848748114611959,4.303050084681673,8.796785917134267,5.497931651896279,4.67581593117227,8.189231548647284,6.9092930858238235,4.772941337831336,5.6082178530214595,7.304419775319909,6.588864518190148,10.665637927637095,4.672990993827748,2.8399595874895316,5.305605789585429,3.0908534304511135,2.4854268271702415,2.693765712217783,2.942983598187102,2.5897634869849773,4.631686366311998,3.0125686735030555,5.651051691178929,5.488964817004848,3.1858665453113337,0,6.703073006328523,4.255500733148386,5.9593064897596575,5.218781167784069,3.968090752045256,4.8594726668519375,3.460742563789644,5.341274183692174,3.0089887832272546,2.032100843167024,2.169925001442312,3.356143810225275,4.814037647539975,3.4168397419128294,5.653060017104565,6.074462620704536,3.2016338611696504,6.9656688644419456,6.816215687622865,4.330558400030802,7.16380060395775,5.6061460780657235,6.093179846765504,3.5716768099709313,5.342341397431548,4.228049047884462,6.310885391185277,4.751677945687579,7.502871574574138,3.953265239014844,3.975446765640962,2.6599245584023783,2.430285272977781,5.7303682363490065,7.689648608288815,4.371558862611963,3.215678596607928,6.492173659610827,7.480507489974544,4.436295119839363,4.804260115634738,4.405311682597445,13.396289023308677,3.8914191868460786,3.24031432933371,1.899175630480513,3.2942531364445142,8.382105211486744,4.143230134776893,4.92552476974757,2.8439838440483265,2.8399595874895316,6.101187815259738,5.0232553523003025,4.583158003874408,3.176322772640463,8.996303999184056,3.8257856274647914,4.7755773609306535,8.12401788912289,5.9618548076361595,2.454175893185802,6.208088008224774,2.673556423990145,2.8972404255747994,6.791683858152045,7.137810883416096,5.8052924556007115,7.338067797542616,5.198494153639083,2.851998837112446,5.726013748860151,6.567880042052731,2.8155754288625725,2.2387868595871168,3.6769443591069124,4.318316841334983,2.1795110502715103,5.6884601404244926,0.8073549220576041,7.244982170376553,8.642268351758643,6.836681593327116,2.73768676140986,2.4646682670034443,5.964860664245848,2.7420062108667365,2.411426245726465,7.279842693520348,7.446835177086041,6.531849285751203,3.8982083525087177,4.721919445551544,5.500164679492167,5.429615964201735,0,3.7311832415722,4.726831217032493,4.904002316283692,4.561937059622433,7.438459204423481,4.631686366311998,7.300764373248062,4.112700132749362,7.9329824099498465,5.1081062236861285,5.868143479666726,4.161081482277184,5.996162742681061,3.097610796626422,4.263785613890803,6.339493737901733,4.370861740085285,4.842978831788326,5.636334602442182,2.84197311892718,4.7713574089905615,4.257010618206024,6.916596067892185,6.015247773698937,4.595145567990858,6.7410624357490905,3.4168397419128294,3.823749360308273,4.279471295644468,4.7114949066500875,2.883620816285671,2.969012307516316,5.089159131911238,2.3362833878644325,8.080604371320732,1.8559896973084806,6.941341105485299,8.14639047567022,7.163498732282879,8.281651890258825,6.329482431025636,2.140778655782796,2.3103401206121505,9.500782139394977,4.674686619927999,7.249066033855867,3.1060132376221192,7.897179908573977,7.6546360285279675,2.944858445807539,4.94673086014031,2.711494906650088,6.054414387917307,2.788685710613534,4.198494153639083,2.9467308601403097,4.060047383669939,3.442280035252584,2.7070829917717063,5.9586107120558465,2.4982508675278257,3.6484654430273142,5.0870384567669475,5.130519083027338,1.8155754288625725,2.6064422281316078,5.673839055990439,6.37364821133469,5.609991295212678,6.076388068512841,4.695437110405369,7.459267666703387,5.211401637418471,3.063502942306158,5.931210274803932,6.819540460505722,0.918386234446348,4.878234879112339,6.098663781886574,4.098453246309274,3.4154882710497003,9.8361608416947,5.3103401206121505,4.20633064787105,6.436128517259249,4.381283372503783,5.0259146987507926,5.226123143125237,5.73903797911741,4.110196177754199,3.8042601156347384,4.503984703897613,6.067165427017112,3.7322691995014496,6.121844297876376,8.248591759535907,7.711013010084339,5.782670658677007,6.0774566524005875,7.592232766722112,6.917909073852982,4.423578170981798,4.901108243014512,3.795974694206668,6.907251224292033,3.189033824390017,2.4646682670034443,6.061128135118254,2.733354340613827,6.664198369291911,5.166715444966422,8.492293852480074,7.279749853014451,2.807354922057604,8.362689046726734,2.3248106034204836,4.826294245499635,2.321928094887362,4.238022517825721,4.232660756790275,5.51127826508303,3.7644735509926663,5.290940402403678,7.535353147842131,4.329123596291566,5.908092340818271,2.570462931026041,4.634012356402116,6.99491908871511,2.560714954474479,2.3589588258323295,2.8278190246173196,5.439623137557117,2.927896453728821,7.476543706214256,8.334094473911568,6.726695004499616,2.097610796626422,3.619413010597937,2.523561956057013,7.136683577697236,8.908992993250534,4.4950555283680185,6.970393537914677,8.089159131911238,2.996388746447621,3.7506065048355923,5.468583317006828,4.732811872082943,2.5360529002402097,2.729008870337863,7.712939631305699,4.038260575175349,4.514122260170708,6.948834425578686,2.0036022366801953,2.169925001442312,6.04023491675437,7.246788093844365,7.018923431842335,5.046141781644721,5.637784110291324,6.976363635732762,5.840463233869542,6.170125361839952,4.760753208062987,3.089159131911238,2.8399595874895316,2.3978029618624896,2.3305584000308026,3.0925457415435655,2.6599245584023783,7.9217814432688,5.963242904318827,2.4515408330178317,5.976821852360685,1.7865963618908067,3.221877081077034,3.0036022366801953,2.722466024471091,4.2024177215751815,5.9025560053446275,3.997292407636508,5.323730337521348,6.463851292611099,5.938756261492295,7.593951283948411,5.233044401265581,4.79025073883523,5.718361626138354,6.435295215647167,1.3504972470841332,4.12267271882414,6.361768359419154,6.8820316083521,2.0250287944915226,2.298658315564515,3.636914580355878,7.934634441094905,5.246408087246385,3.918386234446348,3.929790997718597,4.147306698780294,5.169925001442312,5.542258049766918,4.693208148910016,3.6971065744769747,5.707082991771706,3.363171077119244,5.035183996618336,4.274261661257048,3.321928094887362,5.9740703670240265,2.3757345385831563,10.174663577413781,2.8439838440483265,5.24260250592898,3.4581194811745064,2.063502942306158,3.1858665453113337,4.7692427951405385,4.301587646603187,5.986638471730853,5.058749412335524,4.556429415449573,2.5360529002402097,2.4489009511451276,4.974988111991927,6.248307120087241,4.346956889378885,2.641546029087524,2.8579809951275723,7.624539603781834,5.197314999473818,5.38404980679516,3.2387868595871163,4.464668267003444,3.955126781261366,6.258895755075436,3.1358631653686793,7.831180547126736,1.3785116232537298,4.934044647112246,4.503984703897613,4.100977647724821,4.86195536414487,8.439830883981392,7.851624127346895,6.091065077930535,7.190022174802645,5.992541859109112,2.983677694698067,7.161484638641659,2.6959938131099,8.21003812347289,7.461643148073906,4.848497755385146,3.192194165283345,1.6690267655096307,8.80290404741688,6.926948247949772,0,3.812498225333564,5.6806056662025926,9.871443254934135,6.459923362676901,3.8298495598446904,5.157447996274298,5.537296067090842,3.5147534984397533,3.2249663650002742,5.699884728697476,3.035623909730721,6.914444931746792,5.338780943894699,9.013601832683456,7.578712331048165,3.3405622690264134,2.3950627995175777,7.221587121264805,7.78959885005233,2.2234225499349374,2.8439838440483265,2.950468414150123,5.833649127594984,5.802968651005583,4.618238655595454,7.581803148516525,4.948600847493355,5.416164164733128,5.012121672712217,2.9373443921502322,2.3923174227787602,5.259649120649155,7.119252412157343,5.452858964713811,2.5160151470036647,2.788685710613534,5.040892430646901,6.390942772802543,6.032541697270038,6.177519202597814,6.111865963867557,7.0083165637349865,6.074034394212556,4.471187460386985,9.507715369062607,6.152994605492435,4.169123281476757,4.706530552538117,3.0426443374084937,5.305605789585429,6.942279901705763,2.4724877714627436,11.02948049374083,7.127736443588829,3.594548549550354,3.8063240573900288,2.144046369616707,7.036942844798256,4.022367813028454,2.0214797274104517,8.589726041133547,6.218781167784069,3.685940148445977,7.566662962962611,6.652343078740216,4.210232990095849,9.47854718263086,5.991861930556323,7.500961352460057,5.5855634984556914,2.319039815562536,2.9049657186840263,1.9781956296816516,5.509062386361898,5.592756010441026,4.39780296186249,3.431622959713292,11.193297418854996,7.925940095994448,8.041385369486115,5.051807107013298,2.3673710656485296,5.208673319629471,5.532940288372874,4.598722499676621,2.903038270112912,6.098453246309274,2.967168607532628,4.919340082442012,7.971830914780502,4.189033824390017,3.10936055940423,4.053111336459563,3.944858445807539,5.71699089440494,4.2787282129389395,3.7246502717329673,4.568032104771279,6.400025517914023,3.0443941193584534,9.140190703191836,6.7998644108818525,3.3923174227787602,4.880685525261125,10.32396671481508,2.521050736900963,1.8836208162856714,6.547203024756931,5.63023071585969,5.915042791371977,5.713970692436053,8.823876711241038,2.6530600171045644,4.462706750670158,4.524189078449365,5.342341397431548,6.70362677086498,8.008596693255853,5.81326814176608,5.795715006501729,1.5897634869849773,2.5058909297299574,4.153805336079036,4.707082991771706,5.617651119427331,5.7615512324444795,6.115615931428109,5.51443791382931,8.873136468833458,4.8811751553492755,3.4276061727818994,7.013685569928653,3.37851162325373,4.390942772802543,4.544114402209474,8.617283787779035,5.39677601078753,4.096767854714689,3.169925001442312,2.881664619320345,3.389566811762726,5.949301468328278,5.9095331149468935,2.2172307162206693,5.687900522480746,8.295998461911388,4.963011647599427,5.648177795724818,3.8509993947164736,3.669026765509631,5.924575003392735,1.2570106182060237,8.1184220243132,6.3103401206121505,4.59215800212536,2.169925001442312,2.771885578515366,5.309976492370556,5.718909554379232,7.2325648297287755,3.720278465233327,3.403267722339301,4.138323004056484,8.321792835886914,8.384999544730041,6.675392543042777,7.848935855524565,8.135247549809566,7.474760499576783,2.01435529297707,4.661635602335959,2.899175630480513,6.950701690089008,3.4776773275653072,6.275752048828529,2.7782085763980877,8.645802514606062,8.927066807759255,5.377123749129487,8.785648628652076,3.554588851677637,5.8529975876133165,7.093391153253013,3.8639384504239715,6.390082950951663,3.0214797274104517,5.947198584262056,2.0565835283663674,8.560600329022595,4.708187236020708,3.1473066987802936,5.260778431893426,2.3978029618624896,5.496654082593496,8.356011721502778,5.307064162255372,5.485426827170242,8.478971805032943,2.414135532984451,2.776103988073164,4.7420062108667365,2.4828482830684653,6.543805175964397,8.5918962955207,4.361066488794321,5.294620748891627,6.553360503335328,7.088629255297176,5.1201860275318625,7.00943675560809,4.090853430451113,6.7014104362328135,7.415741768290091,5.348728154231077,6.705563274566321,5.445263208140443,4.552131108253784,3.4436066514756147,3.5631581304028077,6.678494507773734,4.478971805032942,6.1657109924849305,3.403267722339301,4.534186139090097,3.0214797274104517,6.238786859587116,1.8639384504239715,7.020924396158786,4.20163386116965,2.7548875021634687,2.604071323668861,7.128870759465125,2.5459683691052923,5.414473836430928,9.995102877826515,4.861459166361513,5.25285462595139,6.31922050252387,5.308156975114762,5.526068811667588,5.763145958149476,5.903279342056139,5.825531251189622,7.048214384608036,2.513490745588118,5.944155663440363,5.856985689782205,3.412781525338476,2.956056652412403,5.664482840364682,1.74416109557041,4.886062338345599,2.4646682670034443,2.4515408330178317,6.433126376206435,7.7755773609306535,6.705839705883682,8.330737650206952,4.727375938594897,5.777945670695148,11.69784504363786,7.7627474419531515,13.587327482778312,6.558114538598396,6.332349900373874,3.2265085298086795,2.875780063068488,6.70473366267406,4.223422549934937,3.2387868595871163,6.136683577697236,12.23806432823473,5.572889668420581,5.849999259466098,5.533563348214512,5.325530331567558,2.797012977836145,4.150559676575382,8.298429210890312,7.731454807698346,8.903490246967104,2.8094144442358986,4.7824085649273735,8.461602224618334,5.583158003874408,3.060047383669939,6.429950657403768,7.193278935521388,2.3589588258323295,3.7311832415722,2.899175630480513,2.298658315564515,4.235727059838059,2.3812833725037836,11.663771557482411,5.684257705058932,5.212569338850806,6.9787676510367875,4.044394119358453,6.092334311186202,9.582254908357664,2.5160151470036647,6.402415216824399,5.740387932468353,5.784503982929566,2.6064422281316078,4.362469888750209,5.164705840182799,3.4208865749755315,3.948600847493356,7.965841991309597,4.976821852360685,2.722466024471091,2.7398481026993275,4.528571318870758,7.677578712165378,3.41006969175638,2.4905701304462013,3.5147534984397533,7.5673476962842585,6.849624029924543,2.833902076669163,7.406162478777607,8.131085259578953,3.176322772640463,8.148324042480084,2.657640005207824,3.350497247084133,6.8203066288938246,4.483493351284395,1.713695814843359,5.872582544833223,2.722466024471091,5.7742599514327635,5.704318677760832,4.760220946466509,4.294253136444514,6.037162550127378,7.151777654732053,3.215678596607928,4.742545233866053,5.613531652917927,5.727375938594897,7.7780114016125745,8.009884588931843,6.549977142771825,2.0461417816447205,2.0565835283663674,6.66746640495737,4.62760683812965,7.884597920990064,8.571145863602528,2.3161457422933562,8.563272552819562,7.111970261358359,6.001577085389535,3.4369613378336026,6.435461914479276,3.5084286525318573,3.940166750482817,2.5459683691052923,1.6644828403646825,4.94345253855882,6.003827078111476,4.960697039304312,6.684117412839159,5.612352498752662,5.609991295212678,2.62993940943954,3.611172380044005,5.522620761042079,5.891419186846079,3.6530600171045644,3.4581194811745064,3.1667154449664223,4.581351247168778,6.764340846670181,8.066089190457772,6.614857050384495,5.767654798237347,3.1953475983222193,2.9726926540042644,5.279471295644468,7.5105663894487495,5.02724253597374,8.25176642434399,7.822411496360477,5.334496768390418,5.715893370547605,7.3049672876759635,5.998646839005795,6.9725777851765525,2.4594316186372973,5.666472568842073,3.8728287595348854,6.052024560482831,3.631104282365877,2.3015876466031866,2.37016428054021,4.3384244147835584,6.546122758857796],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle GradientBoostingRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predGB_log"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle GradientBoostRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsGB_log = np.logspace(0, 4, 10, dtype=int)\n","param_gridGB_log = {\n","    'gradientboostingregressor__n_estimators':\n","    n_estimatorsGB_log,\n","    'gradientboostingregressor__loss':\n","    ['squared_error', 'absolute_error', 'huber', 'quantile']\n","}\n","\n","GridGB_log, \\\n","BestParametresGB_log, \\\n","ScoresGB_log, \\\n","TotalGHGEmissions_pred_logGB, \\\n","figGB_log = reg_modelGrid(model=GradientBoostingRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=TotalGHGEmissions_train_log.ravel(),\n","                         y_test=TotalGHGEmissions_test_log,\n","                         y_test_name='TotalGHGEmissions_test_log',\n","                         y_pred_name='TotalGHGEmissions_predGB_log',\n","                         score=score,\n","                         param_grid=param_gridGB_log)\n","\n","print(BestParametresGB_log)\n","print(ScoresGB_log)\n","figGB_log.show()\n"]},{"cell_type":"code","execution_count":129,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,2,7,21,59,166,464,1291,3593,10000],"y":[1.9845137164573299,1.912311254299124,1.6889316292147747,1.5358952331347067,1.5055562094320463,1.4819672571502231,1.4259395176137855,1.33913685330568,1.2685758316673823,1.2812880045322272]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,2,7,21,59,166,464,1291,3593,10000],"y":[2.01642014077587,1.9415710818527798,1.7095294119070534,1.5498904905075441,1.5184944144485508,1.4944179634104984,1.4362597022481847,1.3534876592359701,1.2924323309682288,1.3114878427231205]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,2,7,21,59,166,464,1291,3593,10000],"y":[1.9526072921387894,1.8830514267454683,1.668333846522496,1.5218999757618692,1.4926180044155417,1.469516550889948,1.4156193329793862,1.3247860473753899,1.2447193323665358,1.251088166341334]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,7,21,59,166,464,1291,3593,10000],"xaxis":"x","y":[1.9779286124102207,1.904951555092851,1.68212117639995,1.526468127115086,1.49644266455793,1.471066138126888,1.4338490455524084,1.3523612285063389,1.2884153264396783,1.2909440784548853],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,7,21,59,166,464,1291,3593,10000],"xaxis":"x","y":[2.011525530150511,1.9389369964126282,1.7154458708845874,1.5534838512285534,1.5204537888848464,1.498340522377455,1.434033002777658,1.354999004749141,1.2706144455118895,1.2694031970278818],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,7,21,59,166,464,1291,3593,10000],"xaxis":"x","y":[1.995232979380953,1.9209107301851178,1.6840331159575728,1.5204543269107045,1.491883448720507,1.4666351652157747,1.4122158268172018,1.342525419228277,1.2944266779677622,1.3312174418849871],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,7,21,59,166,464,1291,3593,10000],"xaxis":"x","y":[2.0120519786372713,1.9374642830647397,1.7064123438370218,1.5521085127478198,1.5220291555397818,1.4941256431797074,1.435132537041478,1.3285754856999612,1.2626083996638424,1.2760794797603947],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,7,21,59,166,464,1291,3593,10000],"xaxis":"x","y":[1.9258294817076935,1.8592927067402834,1.6566456389947413,1.5269613476713697,1.496971989457166,1.4796688168512908,1.4144671758801812,1.3172231283446825,1.226814308753739,1.2387958255329867],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle GB pour le paramètre<br>gradientboostingregressor__loss=squared_error<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["FigRMSEGRidGB_log = visuRMSEGrid(GradientBoostingRegressor(), 'GB',\n","                                 n_estimatorsGB_log, 'n estimators',\n","                                 GridGB_log, BestParametresGB_log,\n","                                 'gradientboostingregressor__loss')\n","FigRMSEGRidGB_log.show()\n","if write_data is True:\n","    FigRMSEGRidGB_log.write_image('./Figures/EmissionsGraphRMSEGB_log.pdf')"]},{"cell_type":"code","execution_count":135,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>R²</th>\n","      <th>RMSE</th>\n","      <th>MAE</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Lasso()</th>\n","      <td>0.370266</td>\n","      <td>423.474821</td>\n","      <td>112.829957</td>\n","    </tr>\n","    <tr>\n","      <th>Ridge()</th>\n","      <td>0.261748</td>\n","      <td>458.512778</td>\n","      <td>104.953918</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNet()</th>\n","      <td>0.273076</td>\n","      <td>454.981512</td>\n","      <td>105.313051</td>\n","    </tr>\n","    <tr>\n","      <th>KNeighborsRegressor()</th>\n","      <td>0.501066</td>\n","      <td>376.938808</td>\n","      <td>96.126155</td>\n","    </tr>\n","    <tr>\n","      <th>RandomForestRegressor()</th>\n","      <td>0.667030</td>\n","      <td>307.929601</td>\n","      <td>77.676890</td>\n","    </tr>\n","    <tr>\n","      <th>AdaBoostRegressor()</th>\n","      <td>0.549957</td>\n","      <td>357.994164</td>\n","      <td>95.686241</td>\n","    </tr>\n","    <tr>\n","      <th>GradientBoostingRegressor()</th>\n","      <td>0.787747</td>\n","      <td>245.853425</td>\n","      <td>68.741149</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                   R²        RMSE         MAE\n","Lasso()                      0.370266  423.474821  112.829957\n","Ridge()                      0.261748  458.512778  104.953918\n","ElasticNet()                 0.273076  454.981512  105.313051\n","KNeighborsRegressor()        0.501066  376.938808   96.126155\n","RandomForestRegressor()      0.667030  307.929601   77.676890\n","AdaBoostRegressor()          0.549957  357.994164   95.686241\n","GradientBoostingRegressor()  0.787747  245.853425   68.741149"]},"execution_count":135,"metadata":{},"output_type":"execute_result"}],"source":["Scores = ScoresLasso.append(\n","    [ScoresRidge, ScoresEN, ScoreskNN, ScoresRF, ScoresAB, ScoresGB])\n","Scores\n"]},{"cell_type":"code","execution_count":136,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>R²</th>\n","      <th>RMSE</th>\n","      <th>MAE</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Lasso()_log</th>\n","      <td>0.134489</td>\n","      <td>1.889220</td>\n","      <td>1.566199</td>\n","    </tr>\n","    <tr>\n","      <th>Ridge()_log</th>\n","      <td>0.193604</td>\n","      <td>1.823562</td>\n","      <td>1.511262</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNet()_log</th>\n","      <td>0.190936</td>\n","      <td>1.826576</td>\n","      <td>1.513938</td>\n","    </tr>\n","    <tr>\n","      <th>KNeighborsRegressor()_log</th>\n","      <td>0.416219</td>\n","      <td>1.551570</td>\n","      <td>1.257220</td>\n","    </tr>\n","    <tr>\n","      <th>RandomForestRegressor()_log</th>\n","      <td>0.658611</td>\n","      <td>1.186510</td>\n","      <td>0.870374</td>\n","    </tr>\n","    <tr>\n","      <th>AdaBoostRegressor()_log</th>\n","      <td>0.399570</td>\n","      <td>1.573539</td>\n","      <td>1.292772</td>\n","    </tr>\n","    <tr>\n","      <th>GradientBoostingRegressor()_log</th>\n","      <td>0.672942</td>\n","      <td>1.161339</td>\n","      <td>0.799445</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                       R²      RMSE       MAE\n","Lasso()_log                      0.134489  1.889220  1.566199\n","Ridge()_log                      0.193604  1.823562  1.511262\n","ElasticNet()_log                 0.190936  1.826576  1.513938\n","KNeighborsRegressor()_log        0.416219  1.551570  1.257220\n","RandomForestRegressor()_log      0.658611  1.186510  0.870374\n","AdaBoostRegressor()_log          0.399570  1.573539  1.292772\n","GradientBoostingRegressor()_log  0.672942  1.161339  0.799445"]},"execution_count":136,"metadata":{},"output_type":"execute_result"}],"source":["ScoresLog = ScoresLasso_log.append(\n","    [ScoresRidge_log, ScoresEN_log, ScoreskNN_log, ScoresRF_log,\n","     ScoresAB_log, ScoresGB_log]).rename('{}_log'.format)\n","ScoresLog\n"]},{"cell_type":"code","execution_count":137,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>R²</th>\n","      <th>RMSE</th>\n","      <th>MAE</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Lasso()</th>\n","      <td>0.370266</td>\n","      <td>423.474821</td>\n","      <td>112.829957</td>\n","    </tr>\n","    <tr>\n","      <th>Ridge()</th>\n","      <td>0.261748</td>\n","      <td>458.512778</td>\n","      <td>104.953918</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNet()</th>\n","      <td>0.273076</td>\n","      <td>454.981512</td>\n","      <td>105.313051</td>\n","    </tr>\n","    <tr>\n","      <th>KNeighborsRegressor()</th>\n","      <td>0.501066</td>\n","      <td>376.938808</td>\n","      <td>96.126155</td>\n","    </tr>\n","    <tr>\n","      <th>RandomForestRegressor()</th>\n","      <td>0.667030</td>\n","      <td>307.929601</td>\n","      <td>77.676890</td>\n","    </tr>\n","    <tr>\n","      <th>AdaBoostRegressor()</th>\n","      <td>0.549957</td>\n","      <td>357.994164</td>\n","      <td>95.686241</td>\n","    </tr>\n","    <tr>\n","      <th>GradientBoostingRegressor()</th>\n","      <td>0.787747</td>\n","      <td>245.853425</td>\n","      <td>68.741149</td>\n","    </tr>\n","    <tr>\n","      <th>Lasso()_log</th>\n","      <td>0.134489</td>\n","      <td>1.889220</td>\n","      <td>1.566199</td>\n","    </tr>\n","    <tr>\n","      <th>Ridge()_log</th>\n","      <td>0.193604</td>\n","      <td>1.823562</td>\n","      <td>1.511262</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNet()_log</th>\n","      <td>0.190936</td>\n","      <td>1.826576</td>\n","      <td>1.513938</td>\n","    </tr>\n","    <tr>\n","      <th>KNeighborsRegressor()_log</th>\n","      <td>0.416219</td>\n","      <td>1.551570</td>\n","      <td>1.257220</td>\n","    </tr>\n","    <tr>\n","      <th>RandomForestRegressor()_log</th>\n","      <td>0.658611</td>\n","      <td>1.186510</td>\n","      <td>0.870374</td>\n","    </tr>\n","    <tr>\n","      <th>AdaBoostRegressor()_log</th>\n","      <td>0.399570</td>\n","      <td>1.573539</td>\n","      <td>1.292772</td>\n","    </tr>\n","    <tr>\n","      <th>GradientBoostingRegressor()_log</th>\n","      <td>0.672942</td>\n","      <td>1.161339</td>\n","      <td>0.799445</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                       R²        RMSE         MAE\n","Lasso()                          0.370266  423.474821  112.829957\n","Ridge()                          0.261748  458.512778  104.953918\n","ElasticNet()                     0.273076  454.981512  105.313051\n","KNeighborsRegressor()            0.501066  376.938808   96.126155\n","RandomForestRegressor()          0.667030  307.929601   77.676890\n","AdaBoostRegressor()              0.549957  357.994164   95.686241\n","GradientBoostingRegressor()      0.787747  245.853425   68.741149\n","Lasso()_log                      0.134489    1.889220    1.566199\n","Ridge()_log                      0.193604    1.823562    1.511262\n","ElasticNet()_log                 0.190936    1.826576    1.513938\n","KNeighborsRegressor()_log        0.416219    1.551570    1.257220\n","RandomForestRegressor()_log      0.658611    1.186510    0.870374\n","AdaBoostRegressor()_log          0.399570    1.573539    1.292772\n","GradientBoostingRegressor()_log  0.672942    1.161339    0.799445"]},"execution_count":137,"metadata":{},"output_type":"execute_result"}],"source":["CompareScores = Scores.append(ScoresLog)\n","if write_data is True:\n","    CompareScores.to_latex('./Tableaux/EmmisionsScoresModèles.tex')\n","CompareScores\n"]},{"cell_type":"code","execution_count":138,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"type":"bar","x":["Lasso()","Ridge()","ElasticNet()","KNeighborsRegressor()","RandomForestRegressor()","AdaBoostRegressor()","GradientBoostingRegressor()"],"xaxis":"x","y":[0.37026631852202396,0.2617480161205493,0.2730756178405732,0.5010656080363056,0.6670303776542779,0.5499573938723342,0.787746792562566],"yaxis":"y"},{"type":"bar","x":["Lasso()","Ridge()","ElasticNet()","KNeighborsRegressor()","RandomForestRegressor()","AdaBoostRegressor()","GradientBoostingRegressor()"],"xaxis":"x3","y":[423.47482099121487,458.51277807310174,454.9815123616429,376.9388077818161,307.9296006333622,357.9941644834814,245.85342499481504],"yaxis":"y3"},{"type":"bar","x":["Lasso()","Ridge()","ElasticNet()","KNeighborsRegressor()","RandomForestRegressor()","AdaBoostRegressor()","GradientBoostingRegressor()"],"xaxis":"x5","y":[112.82995684088895,104.95391816907696,105.31305132778532,96.12615482556659,77.67689021062971,95.68624098702296,68.74114877844693],"yaxis":"y5"},{"type":"bar","x":["Lasso()_log","Ridge()_log","ElasticNet()_log","KNeighborsRegressor()_log","RandomForestRegressor()_log","AdaBoostRegressor()_log","GradientBoostingRegressor()_log"],"xaxis":"x2","y":[0.13448866938106063,0.19360372895118627,0.1909360173185055,0.41621850529091653,0.6586106645579864,0.3995697646770705,0.6729415891460011],"yaxis":"y2"},{"type":"bar","x":["Lasso()_log","Ridge()_log","ElasticNet()_log","KNeighborsRegressor()_log","RandomForestRegressor()_log","AdaBoostRegressor()_log","GradientBoostingRegressor()_log"],"xaxis":"x4","y":[1.889220464850197,1.8235619341248772,1.8265757877550182,1.5515701815345089,1.1865095486808057,1.5735391013261562,1.1613387671551019],"yaxis":"y4"},{"type":"bar","x":["Lasso()_log","Ridge()_log","ElasticNet()_log","KNeighborsRegressor()_log","RandomForestRegressor()_log","AdaBoostRegressor()_log","GradientBoostingRegressor()_log"],"xaxis":"x6","y":[1.5661989354964698,1.5112619240650316,1.5139380128419642,1.2572200113515213,0.8703736522424925,1.2927720815791421,0.7994451188712236],"yaxis":"y6"}],"layout":{"annotations":[{"font":{"size":16},"showarrow":false,"text":"Émissions brutes","x":0.22,"xanchor":"center","xref":"paper","y":1,"yanchor":"bottom","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"Émissions log2","x":0.76,"xanchor":"center","xref":"paper","y":1,"yanchor":"bottom","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"R²","textangle":90,"x":0.98,"xanchor":"left","xref":"paper","y":0.8666666666666667,"yanchor":"middle","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"RMSE","textangle":90,"x":0.98,"xanchor":"left","xref":"paper","y":0.5,"yanchor":"middle","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"MAE","textangle":90,"x":0.98,"xanchor":"left","xref":"paper","y":0.13333333333333333,"yanchor":"middle","yref":"paper"}],"showlegend":false,"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Comparaison des scores des modèles d'émissions"},"xaxis":{"anchor":"y","domain":[0,0.44],"matches":"x5","showticklabels":false},"xaxis2":{"anchor":"y2","domain":[0.54,0.98],"matches":"x6","showticklabels":false},"xaxis3":{"anchor":"y3","domain":[0,0.44],"matches":"x5","showticklabels":false},"xaxis4":{"anchor":"y4","domain":[0.54,0.98],"matches":"x6","showticklabels":false},"xaxis5":{"anchor":"y5","domain":[0,0.44]},"xaxis6":{"anchor":"y6","domain":[0.54,0.98]},"yaxis":{"anchor":"x","domain":[0.7333333333333333,1]},"yaxis2":{"anchor":"x2","domain":[0.7333333333333333,1]},"yaxis3":{"anchor":"x3","domain":[0.36666666666666664,0.6333333333333333]},"yaxis4":{"anchor":"x4","domain":[0.36666666666666664,0.6333333333333333]},"yaxis5":{"anchor":"x5","domain":[0,0.26666666666666666]},"yaxis6":{"anchor":"x6","domain":[0,0.26666666666666666]}}}},"metadata":{},"output_type":"display_data"}],"source":["fig = make_subplots(3,\n","                    2,\n","                    column_titles=(\"Émissions brutes\", \"Émissions log2\"),\n","                    row_titles=('R²', 'RMSE', 'MAE'),\n","                    shared_xaxes=True)\n","fig.add_trace(go.Bar(x=Scores.index, y=Scores['R²']), row=1, col=1)\n","fig.add_trace(go.Bar(x=Scores.index, y=Scores['RMSE']), row=2, col=1)\n","fig.add_trace(go.Bar(x=Scores.index, y=Scores['MAE']), row=3, col=1)\n","fig.add_trace(go.Bar(x=ScoresLog.index, y=ScoresLog['R²']), row=1, col=2)\n","fig.add_trace(go.Bar(x=ScoresLog.index, y=ScoresLog['RMSE']), row=2, col=2)\n","fig.add_trace(go.Bar(x=ScoresLog.index, y=ScoresLog['MAE']), row=3, col=2)\n","fig.update_layout(title_text=\"Comparaison des scores des modèles d'émissions\",\n","                  showlegend=False)\n","fig.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Modèle de prédiction sur la consommation énergétique (SiteEnergyUse) avec les données catégorielles"]},{"cell_type":"code","execution_count":139,"metadata":{},"outputs":[],"source":["BEBCat = pd.read_csv('BEBCat.csv')"]}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
