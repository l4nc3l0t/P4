{"cells":[{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["import os\n","import pandas as pd\n","\n","pd.options.plotting.backend = 'plotly'\n","import numpy as np\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from sklearn import metrics\n","from sklearn.preprocessing import RobustScaler, StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n","from sklearn.pipeline import make_pipeline\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","\n","from Pélec_04_fonctions import *\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 17] File exists: './Figures/'\n","[Errno 17] File exists: './Tableaux/'\n"]}],"source":["write_data = True\n","\n","if write_data is True:\n","    try:\n","        os.mkdir(\"./Figures/\")\n","    except OSError as error:\n","        print(error)\n","    try:\n","        os.mkdir(\"./Tableaux/\")\n","    except OSError as error:\n","        print(error)\n","else:\n","    print(\"\"\"Visualisation uniquement dans le notebook\n","    pas de création de figures ni de tableaux\"\"\")\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["BEB = pd.read_csv('BEB.csv')\n","\n","BEBM = BEB.drop(columns=['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'])\n","SiteEnergyUse = np.array(BEB['SiteEnergyUse(kBtu)']).reshape(-1, 1)\n","TotalGHGEmissions = np.array(BEB.TotalGHGEmissions).reshape(-1, 1)\n","\n","BEBM_train, BEBM_test, SiteEnergyUse_train, SiteEnergyUse_test = train_test_split(\n","    BEBM, SiteEnergyUse, test_size=.2)\n","\n","score = 'neg_root_mean_squared_error'\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# Scaler moins sensible aux outlier d'après la doc\n","scaler = RobustScaler(quantile_range=(10, 90))\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# ACP sur toutes les colonnes\n","numPCA = BEBM.select_dtypes('number').drop(columns='DataYear').dropna().values\n","RobPCA = make_pipeline(StandardScaler(), PCA())\n","components = RobPCA.fit_transform(numPCA)\n","pca = RobPCA.named_steps['pca']\n","loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"Composantes=%{x}<br>Variance expliquée cumulée=%{y}<extra></extra>","legendgroup":"","line":{"color":"#636efa"},"marker":{"symbol":"circle"},"mode":"lines","name":"","orientation":"v","showlegend":false,"stackgroup":"1","type":"scatter","x":[1,2,3,4,5,6],"xaxis":"x","y":[0.639683279146419,0.8551499910170413,0.9483422002934013,0.9944143074603773,0.9999999999999998,0.9999999999999998],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"margin":{"t":60},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Scree plot"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"Composantes"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"Variance expliquée cumulée"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# visualisation de la variance expliquée de chaque composante (cumulée)\n","exp_var_cum = np.cumsum(pca.explained_variance_ratio_)\n","fig = px.area(x=range(1, exp_var_cum.shape[0] + 1),\n","              y=exp_var_cum,\n","              labels={\n","                  'x': 'Composantes',\n","                  'y': 'Variance expliquée cumulée'\n","              })\n","fig.update_layout(title='Scree plot')\n","fig.show()\n","if write_data is True:\n","    fig.write_image('./Figures/ScreePlot.pdf', height=300)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# création des graphiques\n","for a1, a2 in [[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]:\n","    fig = visuPCA(\n","        BEBM.select_dtypes('number').drop(columns='DataYear').dropna(),\n","        pca,\n","        components,\n","        loadings, [(a1, a2)],\n","        color=None)\n","    fig.show('browser')\n","    if write_data is True:\n","        fig.write_image('./Figures/PCAF{}F{}.pdf'.format(a1 + 1, a2 + 1),\n","                        width=1100,\n","                        height=1100)\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["r2 : -0.8952163699487132\n","rmse : 15371459.343786843\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predLR=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[6441303.648489429,909382.8378210517,2460954.7430728124,3106837.7790948674,4984036.369491796,3915347.879835359,188998.05374057125,1516527.6782688312,1774991.4731513492,9766458.714701153,9726045.725739976,14300978.540744755,6616345.384744622,17648688.51195205,1303974.671474041,3337593.986489023,870523.7857168918,4256589.219322566,1039049.356455694,5827368.350602628,1841722.8151873439,926989.4353756313,4389961.461845875,851701.988780047,557549.2327131429,7729234.85420418,3356524.936801469,3808941.9787933333,6737678.139544112,23924606.363313496,6431553.764339445,801118.6715526609,1890354.8196456495,1277774.3469489063,4162712.505787734,4411681.218850913,2050215.0789892424,508872.78520693164,4146782.2926048255,1484994.8302443535,1069306.4533053965,1631231.6336644136,2235362.541221025,2842361.070815706,3137935.3922207174,27732638.309716545,1111332.3295270733,2634854.7567415265,1202988.2040459975,10230546.331837349,942365.1567877324,3102795.593249604,4674658.29233484,2238048.8240108946,923279.3639712222,1572631.716300903,1428237.4042165629,7401920.596029294,43875297.676518165,1363616.8326122076,1718102.3484098706,3019919.8649043497,1211456.4795292194,21109601.513290368,3269062.785903683,1939635.8810078762,3843020.1002524197,7737803.286324005,1987709.798415353,1814575.0467049996,2722283.3634625124,791443.9343261933,8029879.414449516,2024873.7834290587,1216855.17456749,1232822.3727712773,4380212.361028711,1312413.7261465965,2162832.955846038,3656827.12712025,3439004.8368341792,15939596.046430217,1073602.7927205404,1219707.731820739,7173941.328589717,908212.4078262236,1123399.0344084324,2337214.9371226793,-113204.98717619944,2237424.118535689,627028.3860343939,3161075.5170208905,3809158.918018833,822142.3411634895,310489.6644745746,801255.8694185552,2892745.2824606528,113811.90062399115,5997509.509878289,3835899.0445741634,2598960.6893329695,4018929.2927819816,1246356.9849541872,1685783.3966480687,7456634.588802352,5671535.605663007,19390942.21721627,2757345.4736924507,292902.05594944046,2218292.772880908,998801.2568257661,675724.6768180237,2757283.115925486,1026848.1693508513,421674.7899629134,6788082.6816465,640028.9652293255,102918.59191140765,1877292.2775472086,2172084.36646702,3134537.698660295,18950406.907469988,777242.1619097881,481744.7025871584,4434429.179198944,1729627.0814194693,4836012.734104928,2912372.612017032,4462895.486228747,3041999.9328797646,2857844.6259387084,11521641.067996994,3147534.1313467682,106385.46735633537,14580340.793984847,544152.6576891392,-424403.1543837129,1636918.8445361452,1409603.8810476223,3484445.7661373727,2064583.3532034107,2744387.96785181,1545870.4595065096,1285343.7705954975,1032280.3879940871,4261075.3829448,5040436.68504997,8804987.87413422,10022247.022748293,1150238.6304836196,9478577.025607549,4873532.724218716,721423.5733184866,399387.7912631687,6740972.7117730705,228427.54766999697,479370.15704066097,4729439.062647501,8445652.14621837,664535.1797622873,3550008.9722688436,3588378.9083723817,1018056.748025903,1957126.0984518924,18949572.075567022,1444707.7671893022,4949480.102962647,1529809.8913032555,15210926.056202903,630903.9789478094,590267.9961617817,925702.1266673726,1682515.2095339901,700903.5096041795,2251677.531839713,3042409.3886430045,1712088.3624889576,1578401.315460661,759617.6327331373,7949065.46128803,2604662.2842988493,899759.1771339653,1402534.9188758135,1941112.9533535817,2677965.621689964,314999.0816883766,2688753.3145059575,903529.7307706617,8592295.254588315,21179281.782557957,14301008.226923484,3444519.033663556,3925193.6480681417,1762326.7768352972,1621107.5821786686,1151942.9395670153,588775.1884512191,1010799.667734196,14626186.60203828,6907761.705603008,1566965.4120179145,3011670.5027677426,1257959.6173050373,400079.3178872969,3165657.3042536285,3078218.063940633,2638378.595932695,103374.33256348781,1519007.912540362,5621355.554549709,1715408.468560014,653687.4234516937,844113.7070303769,8714331.20913925,2753788.6971796397,3364924.9853827734,6701094.486268911,8340099.690149661,602954.2790569321,1492862.900466823,18789335.18458148,2835319.084012311,1135696.2686155457,3469700.6953826295,1653125.9779835371,28984574.531518094,1372029.1600782725,1855375.9036300285,2831905.3745358326,984792.9277990314,8697606.598255899,4850974.975551436,1591008.711611924,2650514.447189625,846559.4447631838,5656707.098760884,21660094.141522147,10705629.07796688,5897458.144256557,700145.0211370317,2564086.7156721256,2613227.171856164,2919496.243971439,18108602.660028767,483033.4501069912,1518240.6315463926,1311859.9655610686,2516084.960987715,94305604.29925033,837520.3155296994,1221727.6273786514,2565613.38495033,3031537.4631086783,5943208.427449073,11282956.90968113,15784619.250524923,893670.8174828526,4285415.2014886495,1162735.5839389304,828043.2824733749,897985.6434855198,2950210.3855979443,1977279.002544492,2825268.2245230637,71046.13423619792,2508914.399381455,681736.9759812853,15501158.95501647,813629.3498547594,1192065.6899172375,639863.1188229877,5741663.396270402,4616245.644348326,6404156.388474298,1793446.1104664863,456613.7516989042,-131521.00572960684,1206965.323485011,11386980.904705051,-116437.60407635802,628310.6187972701,1096503.595209288,22372347.65845611,14683507.292142706,670388.2097136166,31761045.36736487,1942706.9458961661,9398182.162790459,200129.95320837013,2214429.9694570974,1274776.646915914,4331977.903913278,88021407.31844613,960564.7930661512,8658587.019316483,1498286.4130294803,2739083.4796470674,7627275.757931945,1620404.6592380428,28336068.55943136,1568628.9340838245,11047003.819324285,859999.559391595,5167427.102668475,3560693.5656540697,1903725.1930710129,3503244.69798173,3440994.7344442746,15795411.279228665,5879624.17918942,36893033.915877044,1523938.003248847,5322.19433772983,1688675.7115872842,5106153.717496296,2596478.7607706324,7486560.504744516,5135712.336667342,7075470.534547484,925577.1123498327,715844.4025069547,1757161.0380361693,1146202.4342115363,571048.0968882844,317203.58954204223,16210411.091579879,3832472.1514901044,6254372.373548195,1948138.3094735695,4901160.641146541,4956744.568877834,15109484.60375832,1341131.4164718536,1059556.6043728553,7217277.1052046735,9065316.702794304,1702425.5471271519,2347633.454114432,703329.3221338443,3547163.8706749287,27667882.89764926,1147951.5043950942,-940408.7898290865,-185852.24227345595,7634659.334226018,933952.2054829784,4767519.867149947,444507.7430090641,11782754.854167106,7519072.799025031,3222106.1168020163,4371375.410665714,940057.5451489903,732084.7286652597,551024.1051701191,3306939.366764934,-287875.2338530151,433034.777797743,56151899.08331535,13007416.471711382,1936519.3701502618,797021.8640747468,15815374.554879416,4852820.3521812055,911955.2419909462,28148222.02025225,3327305.9290592163,13793225.233577674,1424728.6482633774,896106.9758633608,3631805.038640155,403266.3603498959,1063400.7822342517,780208.9192278348,1212340.929367491,1918418.483071033,994824.9280302674,5614113.660485616,2202102.2212330867,3254019.3754047477,1392073.4795948132,1458081.9232094456,5625516.093842431,623656.9532438533,1940464.443851031,1024370.2380808499,1488682.3602369789,6468633.768279713,2059029.3881925535,4168913.364920618,928236.6540388982,1009560.3618661431,6026742.064870991,138096.87580415746,10300901.734151408,921961.4514766182,7838035.300723318,4226382.722239884,5408742.555853068,1022286.8847483299,1026811.3858439121,8845029.431933368,2760761.176167081,591748.5135521863,1129537.7612438027,167100.60946788406,9580257.705062283,3154085.938360365,736563.1517969484,1076273.777918994,3175593.4071780005,6629614.560176877,8654347.64039683,1314622.5204744823,61323907.90770178,1106021.8417148134,1942913.4824865991,3162199.2907300005,26733851.552599467,971148.683326601,4385162.127319301,2060316.4314445546,4324062.1499916585,5624969.2764953505,1490562.148330669,1631184.4878128122,5595905.976557092,6149601.166697003,3517839.318541361,4023457.913622643,10283087.287204549,4125561.6075323746,5933233.160753579,25667758.124944523,233245.4600990722,1910037.282102518,3358954.901346422,1488073.633174458,1930347.9596224735,2375677.9000232015,26520984.98970721,7437229.246234319,-572576.555716475,633301.465904732,22546185.043572463,967295.2435608304,10081163.99910525,3224632.0466528144,666597.0022880037,16650247.69022228,299192.1477610264,2653315.4254441797,35260406.27597385,3018770.0766043128,1356280.5069011012,7213751.170379064,6483514.013029884,3495526.074878108,18612854.26991792,1212948.638939129,1773889.9942862266,1360059.3296877542,4946018.412100823,1731699.318359745,2588105.172998592,4694760.9232723415,3380679.9510284485,15147737.460725976,2341984.544025463,7989761.786898186,558492.6058601718,17043211.193110168,5856147.195172496,207773.05092887837,2608742.7064185417,5236152.677082186,1098662.3220903676,1090509.324906107,6279365.010131583,807664.1613184409,1603378.2963860133,2599375.673206645,11087967.745447174,246252.62476061843,1555343.2806317,3410181.6574044707,1924598.5435325985,732562.6841658482,1724707.6793540062,2640191.0111282067,1179249.083838082,3128955.4481543615,172508.4405474416,2104503.8748491025,7428220.826976935,1687493.7480373974,2039556.996832654,584360.8507029843,11161085.455282912,9504723.690687738,8780482.326601155,8401655.97704162,8547297.948115248,1396658.0190055582,3464928.6028964836,2360097.170440101,969730.6162877472,1284372.830676,8429159.134358268,2078262.0855434875,980861.3718307742,11839028.513051098,454190.5643524199,2614328.6507212864,1593969.6801425156,14013260.647344464,10933243.937688747,2826587.2470669523,1209420.0308232405,2714177.36622269,42636668.99983305,10468647.291473426,1090803.7040705818,1290133.0378878624,6514429.4926847,2531452.922376032,791531.5361986286,2423194.151830554,1447810.8523160713,1082496.438310894,530717.3882940728,2446350.798262079,1685672.8416973352,4357660.239824918,86648.90042295959,43561888.97008531,2378079.014727558,6169319.927924101,8893171.654079378,6403329.621166706,721516.8738003967,16754444.181385802,9938275.594521815,2188108.112370732,14045030.418456113,1436132.1841951073,8137643.376839218,2605877.5861607026,1253510.1793463256,4502296.048411133,2119800.75428081,1334213.1447815504,11235834.124010555,563324.411943306,2153744.377884249,1513534.354814562,7400675.164178901,3771481.724201092,5240642.482775427,61241032.17935653,918760.1348644563,2508406.5283230934,4840422.763317417,7584228.719777934,4896867.01793628,2419205.3973858156,2506069.8801758084,384110.7389540938,5682794.78341311,1051613.9787986234,16570365.162247634,1162569.540616913,1334203.8049128258,13498662.47538154,-140377.06667251838,13580264.76688294,662482.5691602635,924283.7214002362,1055672.146156372,668948.4402185429,3800712.678282158,1043440.5214114059,2464146.4991315836,682195.0553352265,390326.8853851857,5824539.124615656,1218643.2674896535,2934828.6713420404,20219049.59837342,3527638.5880549457,1605004.0374169294,1404202.542243137,2245850.0201790645,4831875.646659431,758376.6064608521,855776.2672691497,31102047.35969603,743467.3874583018,589969.9677982517,619328.6768850139,1561315.2426592312,3106332.7234694096,1995386.3571982328,3478563.5927168047,3177833.093517863,1027287.453429078,1476615.8008173055,12094575.899840364,613593.1166393275,3236961.6667056195,1089406.7162781805,2415443.5113885067,2451991.8545035017,6767301.574589541,1750126.4237892139,2110941.608100905,7545148.001416457,9384995.604876317,2908977.6148995766,4747700.030846927,402914.58618020196,1006115.2345261434,3280333.9816469275,9394559.551944891,15972916.483508509,6307734.188494518,1137382.0220830936,2797497.843613581,824806.5495809021,879225.6309435512,4343951.111290054,1340136.1275940617,750703.2277128748,-655452.2840617294,1300290.7058042057,1336592.4821086447,779922.9754761714,813898.1937342102,1620643.2352426872,2709936.0718020494,915140.2741730383,1194374.127206699,1000236.355981058,1054042.150408642,2183746.926705793,3273649.208456214,1798374.179939783,882267.5618257085,824806.5495809021,4209766.891385412,5715238.567079108,1097729.3172712657,665265.5984886703,4339464.94766782,3833735.0065724296,1290946.077296298,36386957.18879424,21952652.33969432,13615414.754889162,-117431.19607110927,6909080.0481052175,1250064.103821582,1084257.062855321,1235738.856934226,718324.5769137072,681344.7444477882,369604.88859120896,3243066.860574301,8952140.201250624,3368768.4320588904,1669566.5912662044,11539164.555772178,1451421.498473385,12420383.084648168,3814270.9909050735,2412467.7373393546,-342356.58584344154,1029490.2255671895,833578.9560581292,625291.9570444333,402264.7605617463,9490290.01209953,12368338.572188687,369301.51228352357,1624417.0869829338,1920304.6378559205,8237164.986950224,2336293.214628942,1896589.4613868478,1219560.8885428854,108241956.17462905,1746104.6552077779,3124133.624067356,922055.4185409765,2740717.8331195014,1330089.920271014,458129.6133679508,801226.4280666381,327727.3574622867,16973772.99578031,47308479.730881825,1228611.0352881732,12772505.14568313,4397565.502262621,338386.6234456622,1419070.1325707613,816988.167203526,23628321.017787635,3533257.156014074,3152286.4136182545,496991.93641452654,2428936.4485345134,2358685.3302873857,509797.9998444489,1576273.7274555452,10476940.113126323,419490.978001653,3656882.5878671384,10976671.771586172,868329.4523506602,1554805.877670591,36469832.91713949,1143760.1412337862,1884590.0879986202,17056648.724125564,1298797.898093643,13480673.950964388,1874250.370106638,2037532.9869406703,2729274.8444346315,4593194.887015384,834443.9184836312,3037398.7784722275,5303982.127430889,1605581.9672261903,35217904.21282631,1277112.3257503677,742311.9668170719,16040130.316118738,1531569.784662403,4475461.7634566,5855659.963670908,2181803.1649189126,466246.7552941367,5358472.683968326,2403905.388438355,1316717.6329263311,10720628.862902526,3546231.990957991,1558473.8718416307,1755559.9696508728,4162170.829814072,13225775.734082028,5691440.998665017,4189058.701030108,7423483.132678578,855030.7347042775,-169388.35334411962,2055553.3285409352,15582137.731008573,935181.0196806486,1745887.7651013054,1430177.0612350465,1373866.149624107,3910628.7190488195,8626244.66488636,902245.949566354,1850833.3984403135,4822028.900813662,885389.9803125008,21193458.341377594,1599403.406614086,786646.6524796374,9407414.283754276,1635226.6200646162,1824608.4714244874,2096458.606704629,1813524.5721802546,599319.3269899718,10085794.257067842,104899.09549735254,2561585.938209722,4676686.733713131,5688618.107927572,1130844.2148581683,435396.0077366419,1415715.1888253738,10981631.450573076,602383.5711373324,2929317.1199559965,1518433.2748161461,6598205.009488195,3484899.2842275864,19839955.466480225,11702012.258592708,1945171.5521555226,10746485.181003513,1157479.9282974172,765119.1415673976,1030708.0232807049,8762153.703588113,10660185.247954018,53724874.918863125,965510.4497926703,886005.6234080491,1480516.2941397333,3663763.9303469183,1124133.3536518477,16293286.819925133,4085430.36897429,1478439.5143139767,10708842.14540185,879281.9464045826,4757547.034972162,431957.27517412347,1107245.9664261045,9349385.637962636,2113227.067024647,4169027.866483623,1023192.0900707261,1170744.7569754086,754363.6367236816,983255.6682734047,1037177.3045115571,1098569.0216084574,8251656.206015072,7403684.776399261,1207767.2664670867,215645.87986390665,22666380.358226694,11974425.660480758,1336385.9076915802,1489058.3835088187,1565985.577875394,2814924.6868281793,5992900.801243812,1280404.887360191,3468265.1512329257,3360894.2065575565,1705212.270995295,337977.45077071455,472447.6401193023,2673294.555288522,14937650.363823308,980658.8630650351,107400.9619221054,12397513.302309206,3809066.640793234,4548286.285479985,2919538.4219975835,12855380.874028385,1704642.178605792,5531237.9321403615,3108058.438159273,8613898.018756427,4918888.462450182,87277038.88628505,1977919.2901207174,691219.1175885997,-361093.9616377619,10026496.725996234,3721655.5634293463,3496460.5486665843,3449064.617111941,2117492.9716604864,2183233.850801165,2553495.2838936527,16239382.504544955,15651812.509436255,310851.43277640594,11598945.15666536,684388.2260326496,1798888.0031685084,48944036.2966898,1685969.9976118891,5919109.414289163,1390990.85883277,1303836.1241167928,846819.4210057722,2287752.0636266344,4406461.909697547,6132428.049026348,1114815.3900726954,865034.1371146205,19608484.174311824,3139230.388456762,5132054.239679434,-67182.59465462063,2609483.0679678903,2924652.095860488,984188.6730925066,1389822.1834171026,12376229.528412439,9913715.624922816,4580319.420511778,26493355.96244993,7601562.402059587,1207257.309542608,1655192.9231933954,20635497.29033097,5660962.587016393,4026452.8120600316,1835542.7547257785,24577452.217890322,4590140.87152136,3253596.066716472,22629060.771917716,3475674.075247557,80548373.88681048,7943645.403939713,788904.7221485609,215622.25449910434,839440.4955038412,4208437.335877629,2066732.238671098,6196356.338586016,1049480.883511835,5665935.706709642,1719004.4105140269,3156440.4200622123,585815.5976106217,4132325.8656293233,4498456.709333316,10898755.722227821,4824815.752749203,1125445.6027045231,4478367.737067057,3419980.331645149,7373758.860457097,769386.0633262536,4415580.980988062,3211237.973196846,7539070.345153427,1452264.3750387474,4944271.614353171,3437420.3930969927,2787127.185524878,911395.439099485,1678879.8921719943,2996126.307309621,5340157.571688394,3109422.908659793,880625.1381722039,6466903.624747664,1219766.347609785,2743711.518721698,1683214.2944434104,1678542.9440799532,306462.1118173734,-137363.20023711678,3109290.0544623574,4276030.908561431,4038941.7030168893,21767716.765228838,10969316.771020047,3812272.750132563,7613163.0124019785,-46225.174097999,4058334.1390985856,574395.8222946862,1570004.2658705343,1591917.0695404916,1506088.374652905,907682.2779261568,3086715.032938475,-292285.03699571546,1226512.8319877828,27907576.09726789,238814.67980333697,949377.0035179604,1027927.7410053033,1363280.6157054454,1503125.953476943,2273234.173469525,3912235.469378908,984822.9183627991,2468630.473738384,898423.1730544893,1884532.281219837,897232.6239522165,1014878.4893350236,11703667.548851289,1015693.2932632028,1223308.983934294,8553182.410478448,1135270.2737451906,977259.537022141,76037699.90240966,1173563.7509611654,2165907.724078413,2514354.057280766,1224511.8563900583,353289.81696907664,3131847.565773991,1866082.2349749247,1057353.5130648722,2905915.8563931477,999805.8368931713,5035493.781660024,967376.4594308741,1054506.293737839,4187449.949291078,1448078.6691499352,1988842.2199954214,3130086.941229564,2619593.8538267184,4126400.084086988,1993103.0994483442,1292074.2210901766,2135417.0445356537,845360.9256936437,14127906.146801367,3797913.6638248526,12130854.135074597,2332906.09920037,1079693.8122716583,6748455.780186987,1442834.984059886,8705413.627206689,666009.1293697711,362005.0665511172,1432373.2454740792,2078446.8621118795,1674285.3096909146,3940582.1852773884,154544.28139601415,2025789.2108318538,108851.46235628519,-64971.88020872045,1065148.590981533,4106769.201662503,3186747.484883412,13880424.516583472,2672517.9399035745,2693260.858417301,1647629.836162432,963637.6666632541,1983063.632422923,1190515.6158924,1938833.5609577335,1600696.1031870588,6268553.4521333035,2285010.1659742035,7158346.262892738,780016.2759580812,-282843.2450898262,1363000.7142597153,446332.27480932325,13938430.157288445,1769870.3763732328,1289094.6479749838,4183407.240266824,7621946.073498681,5590228.00593133,4626249.400450508,4171367.274244068,4036161.6668831883,2643811.603004904,897232.6239522165,2580248.142873359,2460073.8059397144,2839221.989000204,1915855.6461855266,4543373.672105254,13916329.726240309,5451820.954648547,3055039.342267726,1182850.5176643203,-108698.53772747144,594466.5178477401,4540959.525674053,6369816.621243151,4979742.746281534,1094818.7920969795,4153008.6031740685,821074.5303044948,26485879.186507873,2788315.7410215586,40066021.24041105,963707.1678174469,824272.3445108628,10349401.52644027,1156975.1232842319,669532.5582918338,-31722.085884241387,982915.8021395057,2496629.8382593663,4006148.375408404,5440592.891050476,1654028.1648503009,7682171.7543154955,4950438.650702212,4415214.552551635,1108949.56790881,1211994.6870129602,754665.7915085456,4253492.699665219,2239911.793986179,1194236.597405113,1341566.7065637743,1998893.7716327088,2271001.0042096134,1948415.112985958,3949649.241315448,2881137.0862694634,2683667.938436853,1905492.942404408,5100115.263336485,409784.6797873308,1294599.3764076848,13397798.222619133,13415786.747036286,2633439.0573027064,4655014.917978603,2844148.3120387876,5999921.080430757,37635341.13855655,2090184.6702676793,881533.2425822932,2380324.2685993356,10302870.03226984,870962.8034587102,9531568.661227694,9608065.461799193,6085933.46632013,852662.8603741298,1210668.2231403305,3713892.95695059,63061404.49462783,902345.2923541972,15190194.491115764,219338.29168603453,863551.14998264,9137804.975380158,2567671.6362749822,2245708.6841912926,1680221.9481101716,1780139.134929243,874834.4659878393,2767420.5251514916,1125659.9449511592,4269897.413304839,747937.4404631068,5722722.905072072,1079859.855593676,1841781.8024018942,2990782.655642677,24602908.866054513,5635498.21778819,3250551.7478830637,3630039.599020183,3390310.4734062096,1134489.707143878,3984952.7555060023,3450692.2208155487,17796898.159348965,1480547.236854679,6433166.504836689,1952949.7401705687,1413072.1303305377,25251941.23515372,2076969.8887192444,5757187.049151345,156879.53253732482,2070868.6495389945,1418272.7259392203,2772354.7666681237,1371559.4581864397,4482167.313542266,1231627.5934242446,929110.4460505533,1347600.0924386017,4172945.3472681874,4498152.982739424,2898298.332635004,2386947.360242048,4932273.007500721,682243.4132221432,5777491.533437946,2665077.3393892134,1144286.257744447,10959098.85235902,3137359.383646804,3764767.3209664077,1178854.645898491,3245580.853222504,550335.3899042236,1879271.3133876538,2386855.302333048,1042646.741334354,11785437.85124405,671047.355392921,6383572.922188927,734112.4388582716,4719612.071231654,5904014.785606179,1881763.7315137628,1206225.693120942,5429108.164402691,-23644.50808614446,7130875.44203381,8814346.925595222,2163579.335827088,1225824.1054427344,3477817.837308815,16151824.43124622,40148896.9687563,770955.4713477157,12803548.031062448,904040.204811349,5049178.51133418,2197685.7081278074,2268243.04316702,15031956.031790923,3880789.392170107,1595166.0761528271,2240627.2331262818,3885592.6787819713,5723015.844351189,2663123.8712186133,1595667.2157346988,6000651.51473357,4489970.785046937,644125.0469200211,2761512.2614670224,1529442.7316815478,6057003.270223863,1233375.4021715259,523863.2159463747,3329103.815292906,2902697.0740363183,33355705.018546432,1136229.4091797115,10588120.671224315,2740587.309099066,3690315.9050558317,934148.672073703,10994069.15037184,8491891.851441093,-102976.51392820152,494684.0013883845,1794565.7162494084,3427553.777968239,753535.8927264814,8358279.519039777,5335865.143380225,7593277.448979103,1634921.544864878,7896305.312326411,1837444.1045684027,3166057.5578611265,470072.07505245414,2618261.122978636,1357440.873888336,4042636.52852834,3258594.650520173,3147479.538663231,2609097.050243104,169139.11345338542,1573037.1020238912,838816.2069543204,1705256.521010413,3030580.7901162035,2138010.8324667243,3322932.9950319785,4908268.360954402,1425754.2777707994,3566678.1399792796,700810.2091222694,2241271.568579349,2826890.022576047,4091236.0871145134,50596.75887299515,2590418.826943267,132746.5261538499,414004.48647982813,513328.23416725383,1703348.7876561952,2182975.6224414995,952666.0657987194,3278265.737947637,48909057.809089735,6490992.862938383,553441.7731730677,911949.1996850129,1432984.439825309,1131157.3597177805,7975821.985586859,3026414.326117103,2098836.714676314,677105.8444111496,2681814.011672665,27034533.46686796,3217413.4270055494,1324933.996199545,2787121.143218945,9980681.952881161,4881502.701869469,7068536.804578436,2542949.534284969,1095408.8898353777,869522.3808248919,1327565.527598856,713158.8607489632,3682873.6196881887,790604.4453080343,5028894.140446077,1361161.9020526677,46571781.25882957],"xaxis":"x","y":[3195230,730765,1234149,1069095,629036,1328920,1677877,1700057.375,1116040,3466465,5722325.5,5674450,3098638,19459304,1418400,2378077,521341.9063,6135173,1066894,4660015,1713381.75,739711,6779778,760008,593042,2887392,2643948,2668769,1989387,10498344,37951708,1441900.5,804237,860505.625,2014500.375,2349950,1034827.313,965651.125,808968.1875,2074152,1832031.5,687953.6875,932598.6875,1207700,2390284,21346100,1750099.75,1355558,1435730,7166015,1385975.75,3155391,2075744.5,1073156.375,1394408.25,1247361,782483.125,11026945,37430780,1594504.875,2277316,3742915,1399062.75,33556756,1440662,1795462.75,991918,1250669,2545030.75,695828,3220827,1038815.188,6578370,1863468,745575,741850,10343027,659265,1107290.625,14172606,14585813,4926695,3343550,683307.375,2539565,974278,297715,4664399,1533245.75,12204922,515025,3043767,6118300.5,1764409.5,628609,4857581.5,3918038,858932,2313679,1984067.625,4971699,8611054,1344512,682632.3125,39061360,9560965,5696695,1813718,552007.5,3129811,675772.875,609218,1303247,2788860,1331469.75,8058913.5,1297872,1323038.75,1349970.5,993738,1827817.25,11813861,827211,520738,5049446.5,871355.5,3059758,2203026.5,3528086.75,3078672,416885.5938,3644982.25,1900006.25,615894,53166156,511180.6875,1970956,1290284,791052.875,65225380,872114.5,1241917,1019183,161363.4063,395517,4590246.5,2022990.625,14465798,3588718.75,1242666.25,8138616.5,1848656,572611,6293359,12783255,1109851,712057,4135280.25,3944933.5,520405,6807227,2564004,393493,1497886,19854012,2120807,2711776.5,638772,7080118,1164052.75,534110,1169932.75,928409,653175.8125,10608788,415975,230365,1365644,1022614.313,2882489.75,920068.875,1962435,770333,775728.8125,2116209,635698.6875,1403869,781217.875,1976920,27787286,10791309,2476833,238969.7031,312563,1239315.375,796471,551219,924068,15660628,5169726,1280690,0,838394.8125,2094525,1070085.375,12525174,1834290,345399,9191826,4141785.5,2065156,520765.8125,727374.6875,7946961,3647194,2136086,620483,5792926,3275779.25,1003551,14204281,3045139,1094343,284857.3125,523614.0938,10564729,682249.625,798322.375,816508,652425,21795830,1662054.125,664539,10420096,3778355,1049748.625,21169502,4524791,41092076,770082.125,1210798.375,400054,1810353,19344782,1392299.75,563237,1008665,4877959,78272488,1469870,677536.3125,1011667.5,2503804,3471845,6206083.5,6310254,1813864,3625434,1605182,611536,547084.375,2454812,1429196,2826828,5136604.5,3258819.5,979952,16701055,791542,1360340,90558.70313,11935719,1688612.5,2278160,659652.625,3537977,967835,918943.875,3822102.5,636266,642133,608359.6875,9722237,602245,746858.1875,18848780,4560492,7015692,4133811,2563985.5,898115.375,1967434.5,61576184,638507.125,6235947,4291789,6454983,5155090.5,703298,32381836,3176347,3093944,1896626.375,2006647,6781960,2912454.75,2601369,1417778,20311228,6525887,8374301,207572,826416.5,2823184,614768,1868497.125,11208498,1363191.625,5453438.5,855665.125,532670,3468606,905926.875,281191,2147968.25,6219841.5,1539066.375,4934334,982332.8125,605971.1875,4471029,6462938,650205.8125,860364.1875,2613607,3567977,4520838,15324777,2720238.25,1938613.125,25307744,1929118,7798464,747456.875,6859198,934980.875,2150120,662701.1875,6456026,4423958,2037693,1045413,1663324,542998,1193790,4573777,1247972,1208035.375,48479988,23262244,919180,987989,17457560,11924933,576174.125,16849234,4166306.25,4558824,847097.8125,1055624,11521140,1423463,415453,740230.375,603376,1314716,666930,2133372,818836.125,1993273,941366.5,644121.6875,25478086,2225265.5,5348309,907758.5,1007650,8136665.5,1026921,5457975,733249,652449,16917964,890977.8125,5231373,1523508,6960904.5,2966309,7456330,947518,1989287,2634166,2094724.625,947884,1870457.75,1840259,29949248,4103800.75,508057,2621041,2746152,5662234,8381834.5,1021798,25475474,2071670.25,987288.8125,924073,13093350,1960538,2082754.75,832168,1742351.25,408513,1976856,1605220.875,3046719,3366510,8688676,4399828,6344171,1924355.875,4826430,8747819,876237,3347567,2996958,878927.8125,3234871,1433267,11110405,6289567,580688,635697.3125,13567774,665216,3063969.75,14830411,536642.875,6069855,1125724,2923484,14856602,1848878.5,717197.8125,12187231,10328702,2666634,7564604,683274,1136111,847298.8125,5827595.5,719905.1875,1857889,1318096,1190733.5,10317852,842593,11047668,4193371,51168308,2172841,954352.6875,2547582,9290214,823460,1147582,5481995,1605669,2926089.5,1022957.688,6723971.5,2807583,1162226.375,4329966,1547556,5437110.5,2076289.625,821046,711118.375,1108310.875,1210229,1385697,6333706,1160989,2119806.25,691647,9162048,8773058,23612824,7243389,2992693,1811669,1375366,2254220,1431784,1072855,3404992,2046914,436848,11799105,501897.6875,226375,352136,6711975,14431245,906103,1917399,533742.8125,36367960,6482380,764008,1666221,2019254,241478.5938,720170.6875,2842760,132105,676032.625,510517,710275.375,756267,2474457,2023858.75,64547116,1228113.5,6361236.5,14332778,2330242,647906.1875,15036617,12719042,5903142,22394416,9546165,2924362,1375978.875,1022114.5,3637973,1137504.75,831855.875,15640994,2696136,957460,850667,11325513,2962476,1988334.5,26689658,1358519,744615,12704014,2712983.75,2153603.5,2080866.75,2791291,1000180,6182350.5,711594,14169301,3137206,564988.8125,6802622,673834,6335315.5,492481,603420,1208029.25,1067153.75,1618779.125,628023,1856230.625,2361088,793436,12087498,867456.875,1218114,8320719,6797528,829929,684739.6875,1757194,325513,1666762.125,739869.6875,50899248,529263,650008,1072487.875,6879291,2148565,1509292,3377209.75,3292049,666436,400770.9063,7762038,472532,4673827,811907.625,33268410,1834204,6010407,1403820,1603017.5,4750287,465533.3125,1073507,516216,585443,1038667,436252.5,3024478.5,2290681,1381590.125,1494252,1242311.5,685310.875,1000599,3986851,879235.8125,1372213.375,592739.3125,737935,2345314,800626.6875,2817261,2446842,1476969,646994,1241432,1327591,2043592.625,1111858,2443141.5,1216543,779204.875,1224685.125,1396191,2773647.5,730672,1054995,5948007,2569106.5,898845,39403320,15026352,42584028,938908,2270531,789209.8125,866825.375,1991234,768755.125,1900577,2531070.25,1912719,3091348.25,1272821,794724.625,4382240,43943,7405070,719734,5424718,936336,612158.1875,1585441,3076813.25,867184.375,13116256,4998015,721983.5,1097572,1647746,8575114,2179078,3303553,615955.625,57514116,1820814.75,1425599,2051501,14084606,1576953,498649.3125,722891,685764,4545148,44984468,885865.875,13424079,2212058.5,549834,4801472,578150,32675250,3632613.75,1546484.875,607373,918177,2569602,1401315.875,4467292,7319546,941688.875,2003948.375,8669367,1862752,1140620,48981052,794715.6875,9393108,4222682,965786,13315803,804620.5,928084,3473109,4188660.5,445943.8125,3988729,1310236.625,1191803,17463744,742565,670779,6399127.5,216115,1842562.75,1857329,3475804.25,562338.3125,4108523,1651120,714898,4748137,3528230,1700276,1639849.25,1650279.25,49102164,7802205.5,6606832.5,4626443,3158151,964923.8125,2531163,4249212,440419.1875,1000552,405823,538064.875,14329739,3534690.75,659368.625,2114860.25,1811506.375,632517,7370743,1795049,594848.625,12726789,2442220,757167.3125,1076693,1896858,2603931.25,2127077.25,944638.1875,3157579,4486477,3180819.25,557863.6875,930439.375,1826203,19018762,466904,3055176,2347893,9956396,1436773.875,29747868,6657348.5,1056099,1987934.625,1119679,151798,735907.8125,2664502.5,12729858,157606480,1620721,614985,917194.625,19061780,646691,5889882,2299216,7768057.5,9097980,1406687.5,5481549,1027394.313,875789,4337107.5,689082.3125,1634334,425574,1440554.25,836437.6875,729535,592805,634751,2057288,13491170,1509775,961749,37785552,4628171,984054,1038045.313,859137.125,1142949,2411402.25,5833800,7338511,3611538,966812.125,616749,730292,1120197,10053489,1044220.625,492807.6875,9308471,2240549,3241917,1433573.25,13146645,1208407.625,2131817,1424157.25,11353672,2755578,163945984,861643.6875,718362,784652,2757588,2759449,1765813.625,2162760,2227908.75,6458021.5,2087998,22325364,6481807.5,2503240,9085924,785144.8125,2458760.5,61984596,900973,1859571.875,945063,715728,509171.4063,2504814.5,1878442.25,2434208,572139,520011.8125,24017770,1814947,2231695,1725773,53401,1383950,728876,1046995.188,6843855,9155568,2885740.5,7220201,4424263,1694930.375,550324.1875,11356990,1742870,1094622,966677.125,24224888,1602030,3888945,13609040,5981430,16644664,1252323.875,479358,2630101,7004126,1911591,1357553,12184124,896172,2830674.75,1143826.625,4767227,2108012,3548858.25,1986624,19036416,5448687,1602143,2505271,1817513,40613740,1361427.75,2345752.5,4020660,4391348,5976246,3075425,13883385,1984823,677642.125,1456808,1189432,1271021,1696009,609494,41399504,767720,950695.625,1993766.375,702385,2188093,735810,2504797,17186624,1885314,8728584,11951945,2495819,1000796.875,1539176.625,1730542.25,1223296,1392153,847514.5,991513.875,1122837,2098850,637053.1875,819314,25672598,653672,1358652,845961.375,5807879,911366,846274,7879628,1433504.375,2279063,755269,1138520,133880,524479,10110827,624269.625,1890502.5,3902177,563298,919048.6875,85357952,503744.6875,2485521,10642754,1686405,2189124,1427294.375,1353079.5,862945.125,1956439.375,866806,1972491,627698,1553208.375,1929828,1151027,1222509,1404702.25,1725558,8354235,911088.8125,708928,3441643.25,723482.1875,23364636,1566177,7657568,1262255.75,3444270,5328051.5,313528.0938,2567976.75,634056,488181,915286.1875,2559461.75,3494929.5,4116871.5,695803,989257,1152896.25,719471.1875,639407.375,1755264,3619497,13349651,1393073,4258124,1897180,669117,1769137,541943.3125,946138,2214604.25,5229981,966764,6211441,639566.875,602618,523133,749207,4321141.5,2222097.5,648022,5177270.5,4785692,1933811.625,2184912,2845068,1399992,1285055,315468,6782435,2173143.75,2070005.625,1692199.25,2409429,7794869,2999116,1614673.625,659450,981009.125,491792,1454222,1953919.875,2101376,904236,0,1417728.25,20828130,1827424.125,32552428,514471,1703060,775729.1875,1272272,622051,783208.625,1625308.625,962238,1465535,2834401,2050600.75,2309007.25,7758182,3837228,522972.1875,1982967,769148,1698779,1928564.75,1257033.5,7654264,1285646.75,3643112,1224045.25,2632472,1678495.25,2076612,1964681.25,7231928,728167,957632,13631685,6673138,5571501.5,1261725,4618360,2756494,136241424,1529309,1353612.75,2786648.75,13586070,1100499.75,52960796,6669954,2274873.5,1972156,817695.6875,1911611,58041572,117438.3984,6127771.5,656980,418098,4634655,419978.6875,1057256,41680640,694189,674961,3371659.75,6983736,5288304,1912760.75,14981114,1764767.25,806615,4116805,9569715,7475577.5,3721129,2014721,5101504.5,719060,2919980.75,971155.625,16373232,791564,4933374.5,10834087,605186.3125,27185096,1045838.5,4656777,834711.875,1008536.813,1688745,4262316.5,928584.875,1167734.75,298063,552401.1875,450518,3796986.5,3371455,1240924.625,1537570.25,227566.7031,164817.9063,5543575,161311.0938,606886,1083729.625,1368120.5,2646523.5,2513155,1167967.125,638201,5274642,1386445.375,730211.1875,8692328,1257144.75,4212149,593296.1875,1767480,14265421,2447861,948157,1739286.75,1121537.75,12029600,6797647,151376,1761137.5,6674179.5,6505995,30135280,696522,7837692.5,767361,1989763.875,1788999,1822830,5232420.5,1554342,1523324,1487660,3556291,2249620.5,6592304,335245,3714139,1594422.5,336461,1956075,1838121,3986982.75,817409.625,2946756.25,1521693.25,1880890,12265516,1965529,4103027.5,1686248,5276777,1988760.875,3515821,2218249,947534,2892779,756908.8125,10721568,357915,4571388,3022275,5051636,2429214,5446624,886771,5326222,584545.625,2158629,726124.625,930611.875,1889990,1423929.125,866126.375,783651.5,8201974,2346692.5,145468.7969,2399762,938096.1875,1145535.75,2468322.5,831577,4829951,431815.6875,2361960,1210262,8787353,2882757,1228837.25,2735811.5,576052,442375.8125,866242,1339220.625,880115.5,1656025.625,65047284,2356376.75,456747,673030,863703,686091,3456131.5,2503690,345866.6875,941564.8125,1012341,15091579,1733511,999856,2154119,6268627,5454482,2407904.5,1815350,799996,593824,585559,5063753,5572503,8228399,6014078,994194.875,27717034],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle de régression linéaire vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predLR"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle régression linéaire\n","pipeLR = make_pipeline(scaler, LinearRegression())\n","\n","pipeLR.fit(BEBM_train, SiteEnergyUse_train)\n","\n","SiteEnergyUse_predLR = pipeLR.predict(BEBM_test)\n","\n","LRr2 = metrics.r2_score(SiteEnergyUse_test, SiteEnergyUse_pred)\n","print(\"r2 :\", LRr2)\n","LRrmse = metrics.mean_squared_error(SiteEnergyUse_test,\n","                                    SiteEnergyUse_pred,\n","                                    squared=False)\n","print(\"rmse :\", LRrmse)\n","\n","fig = px.scatter(\n","        x=SiteEnergyUse_predLR.squeeze(),\n","        y=SiteEnergyUse_test.squeeze(),\n","        labels={\n","            'x': f'{SiteEnergyUse_predLR=}'.partition('=')[0],\n","            'y': f'{SiteEnergyUse_test=}'.partition('=')[0]\n","        },\n","        title=\n","        'Visualisation des données prédites par le modèle de régression linéaire vs les données test')\n","fig.show()"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre     Ridge()\n","0  ridge__alpha  112.993394\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predRidge=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[6280062.179963751,914356.421585279,2377630.4454900217,3054006.5660161553,4876852.575979026,3755538.464949048,375012.4966296316,1629705.8909999218,1745377.5465176057,9727037.188398052,9371047.81672695,13797673.765464984,6622319.797231374,17366533.432987645,1462429.9431687945,3243884.8182055308,822173.7632462266,4080639.953157632,1014948.2583275507,5717343.077578636,1771625.4257429293,916754.3746484001,4447500.705872636,834088.3314528083,565961.8525338543,7433396.61716417,3781956.3593757283,3708896.6729465816,6670259.275022579,23077942.50384108,6240671.58529122,891940.4439690115,1816066.1082729392,1225307.7445993552,4020332.272807891,4260807.413167867,1961014.8565750327,546451.1296188636,3965011.1261490732,1654321.3171692272,1014949.4291859043,1688526.3018602058,2314579.874096942,2755815.1241816445,3089219.747865159,28634937.6699452,1054643.6194532537,2565479.9530558498,1303049.721900007,9888163.246407852,891527.420464131,3107087.760298474,4504564.825105142,2244679.5479037496,1039668.2994713311,1729092.9280899258,1491748.6783401812,7527668.620450655,41775133.95501124,1298192.0468782056,1650515.8082761124,2996997.235770444,1221572.1487824288,20895565.366964336,3187809.637298391,1884221.7031387403,3444777.8071390083,8321719.431964869,1890688.3601831459,1743647.8622544408,2658340.6620080923,973092.6562656923,8052026.905712245,2198170.2464680327,1156512.432847344,1221982.6087341756,4646356.909808405,1288832.2421382212,2099642.517193554,3905750.853368221,3281742.498025669,15339479.185666556,1108880.690515628,1167605.2923899367,6917430.690710094,990225.1761072702,1086391.4456512237,2667151.130978934,-51724.406054119114,2164946.926286894,627180.3383424757,3063493.167111983,3407447.0775918295,1034395.292021038,493249.5642462999,901842.5278196894,2857010.4674673327,277926.8825003365,5791721.843989579,3674878.7443772554,2720703.1374759953,3871920.1824932527,1225062.9053716287,1609202.6681423914,8241259.303846471,5466535.838635644,17380018.72420047,2633673.1892617065,268735.6521544778,2440798.5397251435,963952.9953128512,678612.0701825572,2653711.9564488614,1005567.4658731828,714120.1970250099,6574762.745828826,741551.7211727926,204143.9971805876,1853036.4856290347,2118734.3169509084,3168100.3359704884,20620269.760416556,772192.5307071842,513368.9455939615,4265559.575566515,1752826.709148868,4629637.549060714,3089793.267819818,4712970.296568204,2988483.4800119605,2722513.429665765,11833494.075272013,3000364.8030273146,172154.51162387338,14052556.201001085,517089.8138144454,-58202.60026961146,1592114.7978760311,1334330.9721779462,4960625.562569369,1974885.5880186136,2750094.1693107304,1466672.9236948316,1212643.7737722592,1111743.6856177025,4075346.4899048298,5385178.695867243,9151201.949710421,11154071.40546395,1092202.6779725603,10421334.27243099,4657119.516266469,688222.2147417413,533587.0122450148,6589325.62250326,442520.1257636214,481346.83798167994,4769739.566325313,8569011.243011786,663388.3515913032,3498872.528483392,3425944.0632280903,994682.5792703708,1901233.9557615472,19607303.073129687,1426530.8626846436,5304376.574161014,1488714.799842065,14696083.015617885,630794.0435156426,591692.882571236,1196551.5802326596,1763712.8288665612,658427.0764642141,2617661.6797049567,2928964.9128877125,1705863.3292721354,1565580.5285015446,714982.9682068508,7055294.088166557,2505650.741264271,1050056.599075172,1282129.1137771665,1712564.290939837,2597112.339742752,343126.1269216782,2782361.8370676506,927732.9966836311,8286670.037497859,19891047.615613706,14149299.31850265,3302229.9439221825,3751095.624989953,1693208.838823238,1697640.8415721587,1123932.5768128287,590251.7676160592,1130425.8535672915,12516399.06446871,6894659.217633704,1683712.9795863999,2899162.765037708,1417589.6682872432,477989.5186624187,3041271.1531434576,3179118.308594123,2749520.650014051,253042.3364059073,2506019.657789693,6963282.665959537,1939074.1996896004,622831.6236505741,806663.8501203698,8968627.75890456,2782636.4189487104,3280255.0434660544,6460957.528657706,7805644.0977736395,963242.8882568113,1453047.2047014285,20697309.0800552,2820134.62687946,1136156.6155148344,3311375.424291501,1548545.7351397858,27332445.441482555,1392615.197075414,1782866.4926957558,2755707.127139244,1031590.4021505681,8358250.06700686,4452234.596783537,1531547.9429469844,2900054.4645832516,1126340.800234292,5241504.36903918,21574387.807529952,10008876.410815975,6653417.252443839,677666.1342743058,2668917.117516054,2514644.363274259,2827576.426438938,17834779.391985454,685228.866867899,1477546.1589394414,1278312.0163861895,2610612.612947965,96366437.98926607,820397.7393786246,1164120.9426132161,2268456.2408610717,2928327.415522345,6421186.05526429,10864060.837648265,16571267.813797304,1135687.3897925562,4188572.12661047,1184279.6984570653,797385.8282279896,875244.6459876159,2869915.48733704,1920689.0076564397,2699243.9197222707,421817.6720356734,2920438.0211353013,819988.0201299195,14946181.160199463,807319.7077396293,1195856.3170133412,748727.4103220848,5654449.500728851,4743058.6599839125,6126551.391605588,1723122.8919283894,591221.8957535215,33629.897973083425,1146965.046269295,10974468.34373102,15365.313444334082,690903.9011132643,1030342.7319423398,22706404.689980444,14217844.223103043,638954.0972116196,32135429.090281602,2240556.582062971,9067791.849993752,344194.04143790924,2109557.694000688,1242385.1299766814,4645214.531584827,86594697.24489933,899111.201336513,8214008.20711701,1438311.6311262655,2646128.1755764796,7334287.1226101015,1576172.4636843828,29262650.74587623,1516203.9810328325,10597014.457096616,859706.9663680797,4826624.673233569,3429302.0113883074,1883254.146290516,3289109.247177965,3927365.4199189045,15330228.243761353,5516929.484626553,35637354.7484398,1473060.6020622144,162142.88041918958,1611994.8283680473,4939688.615860911,2591475.3167637438,8857671.237824121,5020136.0661337655,6916669.048110757,865335.0695745468,722906.4858193609,1701376.4697932345,1217534.193978588,573138.5275233295,367898.9941343884,15660830.326209567,3645447.940421018,6679196.064466,1872458.4291586298,4766762.051450996,4866753.820428202,19801051.16098452,1276485.2528658486,994675.1368017031,7174510.586199243,8524506.89868242,1878496.0612481954,2471649.9217424933,660768.8882663769,3448302.802471424,27661956.29761277,1276251.7375145415,-611517.939096055,-50854.39720314508,7535262.667929782,749372.067157161,4579418.072948002,420895.390556365,11286754.095614227,7347960.15097075,3102439.1256171716,4221897.309378082,919384.3228623606,728598.803313073,808427.0260554263,3297931.774203605,-124078.6175368242,533579.0182114942,53430018.21171026,13752660.93546734,1867085.9470531174,791287.3038632823,17845542.729441322,6014749.438858628,852184.8956085546,27477846.47571921,3448359.255797793,13327474.15662301,1357187.6903557735,886941.3090131711,3436970.1465515457,437089.74176389095,1038456.4460338792,734986.3084580041,1246178.4860103054,1843895.421323957,972255.227780425,5461544.740138488,2117628.1109081563,3137766.344710682,1325663.3007112714,1430087.2654203344,5836215.667318482,644744.5885274918,2309320.520516557,980678.4809580203,1635736.5177637017,6317786.098691158,2043664.0872324426,4026446.09358217,1007728.6039839864,996466.0456066406,6387069.830894271,145066.1612939192,10291994.442903712,1004400.5233796681,7518458.254235029,4412372.455611082,5527113.9828723185,988113.4682169391,962444.1099956578,8912357.621717444,3256535.6651748456,656541.6541468939,1160841.8035446105,319579.25453963317,10112042.23157325,3537047.8834843226,732922.1481786044,1071603.6937574993,3067395.0086969477,6423130.189610194,8495077.067443432,1384340.9941229885,60189201.04229235,1367854.0550308602,1867414.5268155097,3215360.7146708695,26687617.43769754,959259.3955780589,4407962.7856396735,2000851.027038173,4385677.065093714,5427801.158512072,1763389.5658670934,1586451.3255252452,5789399.678288982,6269352.343607461,4032278.5503666587,3886027.455137089,9902263.175201891,3928295.8043456646,6030419.226679819,24800802.875224125,320875.8333117203,2195218.852427173,3329133.922183035,1428324.8139099474,2008057.8541509276,2409626.1871313006,25634469.949436452,7181587.1485733185,-389711.4383565248,785694.0557499172,21747252.982104305,955664.4784381853,9577704.185573095,4145581.763166553,628227.2086141801,16055479.396947138,306261.1692243363,3069503.341854997,36676817.33807507,2851153.5204830416,1374792.4331179024,6788871.103949383,6932316.94218551,3462457.3369998727,17970224.051453553,1132769.8908023231,1714357.2873697018,1314729.0141473403,4954409.715227973,1633557.3377264107,2530334.848665703,4524099.028899509,3245408.560376739,14675025.069652312,2282751.212787017,8214888.424112413,1717970.6255130668,16464792.331829064,5675225.886958916,333240.5372534869,2520300.8260525675,5026644.572333497,1082482.5944937817,1255701.3203198805,7377405.854884066,791575.4402750798,1617078.164943956,2501144.8283546967,10583600.046807302,801042.5215520146,1493265.0565776213,3558449.7803258235,1881715.9502709596,1146910.1148856734,1563555.0645587938,2540674.5021521477,1137624.1561891357,2979899.9390937546,388176.83405190497,2033395.028357285,7479699.265708953,1630952.7593388197,1873721.7919057459,616642.1345027667,12457503.795114283,9463420.856530953,8468340.59153489,11215259.319828082,8206259.231935985,1380144.9750648765,3394354.4345157393,2285845.9985140804,920357.0642794562,1143652.9477769178,8251666.253218846,2162789.6276456104,985335.1705156462,11470904.03354284,480875.0321542786,2545664.622422163,1530553.9060428322,13639746.619265357,12775149.385301288,2750573.1551114256,1299390.9269470202,2678747.5920393234,40560084.026485175,10430315.399921006,1105472.7313874671,1237366.1645637404,6350762.10981925,2435574.0978941326,723880.5810999849,2380992.301862985,1389583.9317043354,1036791.8945252337,504119.77921784995,2373390.160736711,1746858.5140131775,4208657.065727391,264168.8062316098,42772344.59386606,2267539.9209619914,5967440.789264025,8967469.304195968,6173375.943200374,688312.2844264398,16522145.387035144,10416831.901802974,3498642.694008067,13520503.975821095,2395929.133261663,8148195.811763296,2672271.3125396203,1211869.4342215653,4398084.963329913,2310850.595327655,1481208.7725922284,10828555.454519324,590238.5154402289,2169145.7905691126,1546991.7058336316,8020303.47330552,4190397.525721898,5505305.791865645,60079110.517764315,868739.7902353909,2490151.7185911867,4474701.079632866,7883470.34293253,5027966.641750605,2818634.650909488,2491082.826391015,532792.2169259898,5027229.554963907,1006978.8288900047,17349724.933334295,1396201.7412376867,1344237.7197136376,12872689.291991465,3192.101311523933,13746006.267129501,661406.8185279346,914142.3537921414,1006776.7987895173,738739.2991029932,3660882.088533573,1009201.7258645433,2475679.049430205,1159630.877363593,506676.362777045,5764540.0252568815,1264415.5396490963,3110560.75426542,19690929.5426954,4037868.1914994167,1571290.7733529583,1337372.3597220865,2401356.134391469,4646352.436178404,713910.0022385372,817922.5607076918,30196698.84483504,739587.3048462991,621362.0964486615,629605.2102568597,1569057.3914893856,4022538.911458113,2052699.10311758,3364663.2268908895,3128988.7670823615,1013579.2856993703,1387306.8197605014,13501838.409182455,614210.3037458803,3647138.408012353,1033477.243549088,2444012.0637169518,2378963.563402799,6584785.8506567385,1549832.6526260069,2039609.836601487,7218600.077346047,8870396.494323285,2820124.8790564286,4645337.608492927,420812.7633403342,1055134.7362533747,3126481.30694533,9089914.316530421,15430145.4213796,6082028.515617721,1119861.513643691,2723199.7271416895,778039.617743924,860658.8884388884,4185437.01443286,1441534.261458817,612083.0374690592,-499801.9628845551,1267143.3754835657,1304231.988999819,744695.907047749,876531.4615348801,1623992.5356753722,2773603.0439475384,905315.5246916809,1237250.6984063426,977479.2694929426,1236119.3269039826,2129993.0275382306,3671865.834847698,1727117.679312382,853482.1588059282,778039.617743924,4085856.2307678275,5509112.018548369,1081581.8976467957,859227.9559952565,4190730.4776856625,3702746.564560709,1253708.344519048,36902423.479857646,21254158.178887382,14652648.034522533,175614.44768331666,6581593.442638189,1198557.048243878,1058462.8338221873,1184855.5383844702,735355.7596155498,810978.4226514653,818007.6386862351,3610556.8447862077,9434247.793258315,3254008.477607175,1608322.302626342,11573193.71870635,1867798.674839255,11962100.363808617,3684070.332085383,2762363.0094511104,-47493.30457368633,982584.7670174988,722173.5619970895,724870.5705337531,478706.8475940712,9585337.39805293,12019128.193587143,358279.03233315237,1573054.8622968865,1838226.255905794,8338963.918947352,2277256.9620204037,2036298.8747800929,1159124.4537036028,104505646.46478528,1818976.658289279,2852341.090175016,925007.6647813485,2817747.138899799,1338773.5791779887,434045.5645223572,785360.632030878,393508.7753773534,17677897.386475768,45766595.6454254,1167861.2131193648,12292043.546217205,4225954.936509283,363959.15063408995,1345909.0164461373,810562.2163887778,22802030.214466415,3539744.4235472432,3014938.1645907825,539861.0176420112,2281814.219043416,2298873.6863480625,534050.0143142308,1614918.236142784,10702692.776265584,420684.6046207659,3640581.7796904165,11477466.586207762,930751.1306932368,1560843.9925295971,37012514.00438568,1160325.7639609901,1978740.9753119377,17787987.9110038,1265702.2605283887,13325315.081567386,1791143.431233517,2008813.4469032413,2656630.4739708095,4445893.605650965,798705.6698591972,3587862.1899806187,5092125.233109362,1541891.7685296286,35809698.93042173,1274830.8136616552,890625.7708008029,15355100.232705817,1440357.8640479632,4312266.048273681,5660600.346390764,2953659.654033049,441881.6270911335,5254751.7114037685,3729779.9573967783,1273015.832346675,10331190.655613782,3415341.210260028,1689069.3766561477,1718258.4282510076,3989852.431768179,14661892.945549902,5531635.549374996,4543177.579686478,7148218.068050774,1028857.8411411189,-33445.68480668962,2665907.0214133067,15034341.454161668,884592.0547423405,1707295.9972478093,1372560.7612963044,1366251.5367899558,3300795.129195908,8492755.954900099,852797.4560437428,1815242.7303599857,4284437.945444338,862843.3811332104,20451481.652887654,1739796.4155279521,741201.116702206,9475246.8735249,1540425.2837480821,1753206.1666177143,2016337.434479169,1782576.3262981179,1049540.3528355628,9748295.943856185,15187.772904547397,3062389.063915405,4546465.588743872,5441726.1408563,1053508.5682675757,789956.8011933533,1304138.1213930885,11233385.40949379,619990.5422511161,2839760.070320718,1467746.4906649983,6574816.069079712,3301141.6652371315,30092029.758221533,11853242.49913758,1909664.65140781,10336180.342987608,1359478.9583315463,750503.6640525288,1038618.4605868822,8802267.097189413,11064445.583779037,50299988.20193767,973912.769716593,847105.1385500308,1421029.1694493627,3316010.3796811844,1107071.618416493,15770920.850737598,4119207.323159546,1458966.8669613553,9396926.072610043,850599.9288955736,4364610.555104836,583678.6449377085,1090769.0054860506,9017416.307242638,2094784.0290147446,3881181.0723696323,1047408.10040291,1187161.6311727432,722030.3702409859,961086.5868778015,1023126.6722774194,1082392.524809083,7151487.355097001,8747580.713296091,1293951.093893742,420542.88088408555,21563518.178023543,12307270.398130823,1321959.9587495956,1609440.4780262439,1493553.1929894187,2739314.4445241033,5777159.400211334,1197890.2728393942,3548551.203307119,3236293.594864481,1931104.7698210627,358124.26279012463,511182.599829576,2602588.6631516637,14615485.44783951,1123649.1996308002,232800.01105023734,11334062.85069492,3672132.1180652254,4660134.283265807,2920818.4823472826,12402134.070745235,1607437.1291638233,5351454.215610458,2923562.7366250576,8997507.586948637,4739728.073588745,75451718.55804162,1871251.235645955,689148.2814150965,-222431.13770150812,10122121.85753697,3939842.661301218,3778505.411987111,3582679.2600666992,2032514.962446001,2246790.8258015267,2616805.878385728,15795994.476180239,16672223.044776311,763858.2638083689,11608849.143696792,714609.3692918811,2218616.158754356,47295215.502455965,1609382.8075117886,5212507.229063867,1334731.4937229021,1270566.023502112,809275.8709766285,2200312.0814614496,4651406.546268007,5911982.80041988,1078105.0346589545,1060799.6648021822,19553062.571936406,2992348.6010891413,4956235.448725579,277903.24595927214,2531001.191173997,2835256.5860857894,961987.2837247876,1343461.2435661864,12117820.77572962,11864888.883102603,4433463.9891625615,25627768.93805335,7663416.381159389,1127275.64003571,1559700.1962735774,19952784.168965522,5486800.106569491,3645591.2763202637,1733804.8967959266,24266307.492311012,4473029.950571606,3199858.7948055784,21857343.50663234,3321234.649372679,77427927.4709642,7630396.944957659,783451.2412945065,349242.1928485506,1391271.3192178307,4038386.328873695,2076265.5761401444,5979385.805681953,1054975.4859453123,5476449.906368478,1631287.6682512444,3597407.750943701,621584.074420895,3663509.9080061093,4334592.412293703,11123294.884965759,4659635.977013165,1098352.7863584326,4313531.262346243,3283209.0677445754,8131168.779318441,724538.2250329694,4224501.887765673,3837313.943547465,7279774.022679992,1363798.632054173,6236194.125524845,4637287.1983446395,2702493.870840087,851644.477500363,1672565.3865657523,2894270.156921059,4722868.458206162,3050641.6125019034,862009.9337093672,7067709.060109196,1199393.0452325344,2640482.6305833953,1554117.490576742,1612326.2619639158,486616.1219951052,28500.489799282514,3171178.21873192,4950912.366691969,3937554.2076675864,21015840.369851235,11571947.689802812,3832185.7234595343,7331329.73704298,76154.54103474459,3878463.998289718,670588.8528365667,1505697.551159451,1528572.3729794635,2089265.791696243,888130.1422719543,2991707.6284072176,-103818.71353731165,1234643.047071283,26911524.3839469,404306.26965219737,1028450.0315589514,964141.5136888856,1307980.7973674245,1462954.870018272,2196410.2398764854,3521120.701641473,922529.319358143,2420226.558486886,936130.2020198312,1841139.5487098945,878042.3375857137,1001600.0176344595,11565362.61182734,972302.0002810527,1522366.4133813637,8368945.059569087,1190762.046163796,925213.4825413986,72588938.79084182,1114720.0991472045,2168344.0209570182,2361633.1384515027,1153918.339348787,585073.0363982669,3005192.7112856344,1811749.183032111,1052422.5125287604,2913844.282101582,944081.6295297754,5283231.470522242,975714.1634105647,1009770.9891156608,4094268.9990563886,1409813.7560461117,1941837.4562029035,2983521.7719886806,2743198.6107798107,4232104.895317905,1925851.8248271844,1269197.0508739315,2330708.015197113,897638.0794559561,13630594.500349125,3658179.997992615,11703647.788566971,2238016.935925057,1286111.2167096564,6950929.968670275,1364681.401543457,8415714.958740741,624741.0143869463,380310.4385147451,1374553.212144897,2076705.7952946906,1591285.9452652042,3775936.9306090586,240914.38572259527,1977505.05134354,205709.24195334548,3339.5442134663463,1010059.1255274583,3976439.4912167583,3236226.388436749,14498123.254883476,2651767.1438932763,2621863.575677159,1711570.834291623,912063.3085754064,1926273.3281077514,1324433.8904715418,1921135.049433511,1619278.0960653601,6043394.470395103,2159772.3335073255,7026759.572638787,744785.9767324473,-182215.57054777816,1307710.5883133288,413128.6547307188,14534123.246581834,2113198.9174284805,1256335.0133197366,3536041.5914046387,7389864.547208022,5368459.025764596,4457904.320500769,4030077.1917568985,3941382.278038608,2574126.642786913,878042.3375857137,2503371.5138469385,2329475.2062052093,2758322.7228270913,1842307.4180937223,4347813.795972738,12253402.5513307,5778358.053179672,3078152.110601764,1175274.0207671956,39193.921869638376,595746.0183826722,4356533.49399904,6170980.3493251065,5138057.166278634,1045894.7753521076,3931195.367347244,774436.8303559811,25098742.10732522,2775049.8111438695,40082099.113317765,970978.2950520047,972694.5100001881,9965744.628168717,1138776.147430392,646886.9433550737,1098.9504301901907,959284.2857364567,2496677.5145998914,3861374.7433595825,5264075.703667613,1745030.2489185436,7691289.084467271,5700889.2402393855,4284189.676474094,1092413.4643733292,1328602.3647309088,883899.9871428171,4184794.908837209,2256767.60494261,1164740.2891336249,1587404.365621728,1921456.3376346556,2184268.759799879,1983552.0244596673,3782530.1763948817,2783135.083788239,2685896.8124422547,1855536.8800916942,4975330.654777868,410084.80844070157,1261649.1247169527,13215224.557039356,12762598.767463434,2673827.63996814,4485729.343217915,3097197.7581165107,5560880.510117875,35292919.05766802,2009713.710854037,832801.9860406588,2259721.785750917,9254207.068944274,845676.1882806197,9697554.08859497,9861055.407839656,5846999.2605681345,1026683.1489989259,1211540.0946262092,3577196.43366337,68397456.38530476,842907.7180846012,14635999.093455268,397352.88311177236,845527.1814095273,9294052.47364426,2440581.7126256595,2209733.041721584,1797917.3731830746,1659026.3125271723,866405.4209018955,3441169.372490042,1505400.3954100194,3649037.664475166,900599.4628480687,5707110.147680959,1074189.173929035,1759799.1809584121,3016546.589671272,26089602.63079111,5388235.823237283,3046642.471904912,3558393.3269994543,3754183.21549523,1117069.353418035,4513470.391808139,3395102.0566921337,17540739.972826526,1441158.0063212165,6135895.401813208,1798716.1482404487,1335949.1721246107,26043621.738122936,1967163.7905338684,5637124.077155169,352838.0889358658,2059055.2660410823,1476883.373792995,2668134.0237858584,1285888.3547899039,4338710.680859659,1190872.2799015727,888717.3328807733,1302828.897981904,4080138.9763439363,4628195.373232707,2809687.7134166174,2207087.1122471592,4733287.609270232,640413.1395244985,5715655.957952463,2534614.463450974,1126526.6703113858,10571388.577347185,3035295.449358557,3821210.4587474065,1290721.935462871,3114987.6569331996,553143.0575202452,1779514.285061784,2266026.6636798177,1081053.1412989423,11713354.542290272,629604.7773606693,6252626.116906969,700471.6918607475,4325713.344536198,5782888.404901607,2328706.6832823865,1206292.5562389214,5688348.196414242,88258.69787774747,6678780.579421353,9319899.00412024,2086382.9758548397,1145199.507290727,3319211.486860277,15594286.756575022,40192189.63784579,793206.6683469699,13208547.72804872,845645.9788473854,4846144.924197549,2224630.085125652,2675626.49943269,14456949.10519766,3768270.5225206455,1629125.827526042,2319045.880381105,3852792.6072756946,5958022.442766665,2613462.038374969,1502235.7374358855,5844683.251488531,4306190.184139008,679418.7530584792,2719536.3787479345,1478374.7134594303,6684290.498282986,1162474.959395152,526835.7713650279,3344386.772845885,3014626.369739963,32172476.634583488,1143392.3966805974,12278472.118110955,2821294.313479584,3860314.0086127557,893581.0958544966,10594759.232067317,8209714.672049873,59236.12732488522,613329.7789781981,1724203.7281447724,3300772.5713234376,854304.3528884058,8090714.691205349,5326421.244410535,7688191.101202264,1594334.7944658212,8079823.320737241,1687642.3814694416,3404373.758798348,445574.4841637749,2956184.697330029,1322186.8706196204,3881080.376556552,3156391.09974485,3325841.1820903067,2480572.6526318276,303719.3794583529,2325944.0276186946,977566.5202714072,1815117.3584550766,2960259.141976132,2045770.4298763936,3179675.6179044796,5032474.423381112,1456490.8018304845,3573029.7905631093,658337.0067795154,2185803.682777781,2888599.25542048,4636573.35506908,326395.7490865679,2492498.1386236334,239151.66832052032,433871.3494181812,554080.5708213635,1656244.413381418,2099163.8255449478,1008579.9154245195,3469575.448014708,53265314.46769298,6165447.97528206,611222.3144188421,862164.7032523947,1343101.549913508,1157083.9880228364,7681430.601466348,3061087.6942038895,1987981.5470153107,645439.1145099171,2819909.82139118,27777169.20161253,3278190.8604985187,1366649.5720895354,2712473.678483927,10248104.816180952,5498361.823230807,7420457.142036486,2439565.7307332396,1059370.5402416508,851291.6412302363,1364734.9432595684,794060.8369237394,4203553.058127189,854013.7779090484,5064500.239756003,1336023.0336027425,45267494.80472862],"xaxis":"x","y":[3195230,730765,1234149,1069095,629036,1328920,1677877,1700057.375,1116040,3466465,5722325.5,5674450,3098638,19459304,1418400,2378077,521341.9063,6135173,1066894,4660015,1713381.75,739711,6779778,760008,593042,2887392,2643948,2668769,1989387,10498344,37951708,1441900.5,804237,860505.625,2014500.375,2349950,1034827.313,965651.125,808968.1875,2074152,1832031.5,687953.6875,932598.6875,1207700,2390284,21346100,1750099.75,1355558,1435730,7166015,1385975.75,3155391,2075744.5,1073156.375,1394408.25,1247361,782483.125,11026945,37430780,1594504.875,2277316,3742915,1399062.75,33556756,1440662,1795462.75,991918,1250669,2545030.75,695828,3220827,1038815.188,6578370,1863468,745575,741850,10343027,659265,1107290.625,14172606,14585813,4926695,3343550,683307.375,2539565,974278,297715,4664399,1533245.75,12204922,515025,3043767,6118300.5,1764409.5,628609,4857581.5,3918038,858932,2313679,1984067.625,4971699,8611054,1344512,682632.3125,39061360,9560965,5696695,1813718,552007.5,3129811,675772.875,609218,1303247,2788860,1331469.75,8058913.5,1297872,1323038.75,1349970.5,993738,1827817.25,11813861,827211,520738,5049446.5,871355.5,3059758,2203026.5,3528086.75,3078672,416885.5938,3644982.25,1900006.25,615894,53166156,511180.6875,1970956,1290284,791052.875,65225380,872114.5,1241917,1019183,161363.4063,395517,4590246.5,2022990.625,14465798,3588718.75,1242666.25,8138616.5,1848656,572611,6293359,12783255,1109851,712057,4135280.25,3944933.5,520405,6807227,2564004,393493,1497886,19854012,2120807,2711776.5,638772,7080118,1164052.75,534110,1169932.75,928409,653175.8125,10608788,415975,230365,1365644,1022614.313,2882489.75,920068.875,1962435,770333,775728.8125,2116209,635698.6875,1403869,781217.875,1976920,27787286,10791309,2476833,238969.7031,312563,1239315.375,796471,551219,924068,15660628,5169726,1280690,0,838394.8125,2094525,1070085.375,12525174,1834290,345399,9191826,4141785.5,2065156,520765.8125,727374.6875,7946961,3647194,2136086,620483,5792926,3275779.25,1003551,14204281,3045139,1094343,284857.3125,523614.0938,10564729,682249.625,798322.375,816508,652425,21795830,1662054.125,664539,10420096,3778355,1049748.625,21169502,4524791,41092076,770082.125,1210798.375,400054,1810353,19344782,1392299.75,563237,1008665,4877959,78272488,1469870,677536.3125,1011667.5,2503804,3471845,6206083.5,6310254,1813864,3625434,1605182,611536,547084.375,2454812,1429196,2826828,5136604.5,3258819.5,979952,16701055,791542,1360340,90558.70313,11935719,1688612.5,2278160,659652.625,3537977,967835,918943.875,3822102.5,636266,642133,608359.6875,9722237,602245,746858.1875,18848780,4560492,7015692,4133811,2563985.5,898115.375,1967434.5,61576184,638507.125,6235947,4291789,6454983,5155090.5,703298,32381836,3176347,3093944,1896626.375,2006647,6781960,2912454.75,2601369,1417778,20311228,6525887,8374301,207572,826416.5,2823184,614768,1868497.125,11208498,1363191.625,5453438.5,855665.125,532670,3468606,905926.875,281191,2147968.25,6219841.5,1539066.375,4934334,982332.8125,605971.1875,4471029,6462938,650205.8125,860364.1875,2613607,3567977,4520838,15324777,2720238.25,1938613.125,25307744,1929118,7798464,747456.875,6859198,934980.875,2150120,662701.1875,6456026,4423958,2037693,1045413,1663324,542998,1193790,4573777,1247972,1208035.375,48479988,23262244,919180,987989,17457560,11924933,576174.125,16849234,4166306.25,4558824,847097.8125,1055624,11521140,1423463,415453,740230.375,603376,1314716,666930,2133372,818836.125,1993273,941366.5,644121.6875,25478086,2225265.5,5348309,907758.5,1007650,8136665.5,1026921,5457975,733249,652449,16917964,890977.8125,5231373,1523508,6960904.5,2966309,7456330,947518,1989287,2634166,2094724.625,947884,1870457.75,1840259,29949248,4103800.75,508057,2621041,2746152,5662234,8381834.5,1021798,25475474,2071670.25,987288.8125,924073,13093350,1960538,2082754.75,832168,1742351.25,408513,1976856,1605220.875,3046719,3366510,8688676,4399828,6344171,1924355.875,4826430,8747819,876237,3347567,2996958,878927.8125,3234871,1433267,11110405,6289567,580688,635697.3125,13567774,665216,3063969.75,14830411,536642.875,6069855,1125724,2923484,14856602,1848878.5,717197.8125,12187231,10328702,2666634,7564604,683274,1136111,847298.8125,5827595.5,719905.1875,1857889,1318096,1190733.5,10317852,842593,11047668,4193371,51168308,2172841,954352.6875,2547582,9290214,823460,1147582,5481995,1605669,2926089.5,1022957.688,6723971.5,2807583,1162226.375,4329966,1547556,5437110.5,2076289.625,821046,711118.375,1108310.875,1210229,1385697,6333706,1160989,2119806.25,691647,9162048,8773058,23612824,7243389,2992693,1811669,1375366,2254220,1431784,1072855,3404992,2046914,436848,11799105,501897.6875,226375,352136,6711975,14431245,906103,1917399,533742.8125,36367960,6482380,764008,1666221,2019254,241478.5938,720170.6875,2842760,132105,676032.625,510517,710275.375,756267,2474457,2023858.75,64547116,1228113.5,6361236.5,14332778,2330242,647906.1875,15036617,12719042,5903142,22394416,9546165,2924362,1375978.875,1022114.5,3637973,1137504.75,831855.875,15640994,2696136,957460,850667,11325513,2962476,1988334.5,26689658,1358519,744615,12704014,2712983.75,2153603.5,2080866.75,2791291,1000180,6182350.5,711594,14169301,3137206,564988.8125,6802622,673834,6335315.5,492481,603420,1208029.25,1067153.75,1618779.125,628023,1856230.625,2361088,793436,12087498,867456.875,1218114,8320719,6797528,829929,684739.6875,1757194,325513,1666762.125,739869.6875,50899248,529263,650008,1072487.875,6879291,2148565,1509292,3377209.75,3292049,666436,400770.9063,7762038,472532,4673827,811907.625,33268410,1834204,6010407,1403820,1603017.5,4750287,465533.3125,1073507,516216,585443,1038667,436252.5,3024478.5,2290681,1381590.125,1494252,1242311.5,685310.875,1000599,3986851,879235.8125,1372213.375,592739.3125,737935,2345314,800626.6875,2817261,2446842,1476969,646994,1241432,1327591,2043592.625,1111858,2443141.5,1216543,779204.875,1224685.125,1396191,2773647.5,730672,1054995,5948007,2569106.5,898845,39403320,15026352,42584028,938908,2270531,789209.8125,866825.375,1991234,768755.125,1900577,2531070.25,1912719,3091348.25,1272821,794724.625,4382240,43943,7405070,719734,5424718,936336,612158.1875,1585441,3076813.25,867184.375,13116256,4998015,721983.5,1097572,1647746,8575114,2179078,3303553,615955.625,57514116,1820814.75,1425599,2051501,14084606,1576953,498649.3125,722891,685764,4545148,44984468,885865.875,13424079,2212058.5,549834,4801472,578150,32675250,3632613.75,1546484.875,607373,918177,2569602,1401315.875,4467292,7319546,941688.875,2003948.375,8669367,1862752,1140620,48981052,794715.6875,9393108,4222682,965786,13315803,804620.5,928084,3473109,4188660.5,445943.8125,3988729,1310236.625,1191803,17463744,742565,670779,6399127.5,216115,1842562.75,1857329,3475804.25,562338.3125,4108523,1651120,714898,4748137,3528230,1700276,1639849.25,1650279.25,49102164,7802205.5,6606832.5,4626443,3158151,964923.8125,2531163,4249212,440419.1875,1000552,405823,538064.875,14329739,3534690.75,659368.625,2114860.25,1811506.375,632517,7370743,1795049,594848.625,12726789,2442220,757167.3125,1076693,1896858,2603931.25,2127077.25,944638.1875,3157579,4486477,3180819.25,557863.6875,930439.375,1826203,19018762,466904,3055176,2347893,9956396,1436773.875,29747868,6657348.5,1056099,1987934.625,1119679,151798,735907.8125,2664502.5,12729858,157606480,1620721,614985,917194.625,19061780,646691,5889882,2299216,7768057.5,9097980,1406687.5,5481549,1027394.313,875789,4337107.5,689082.3125,1634334,425574,1440554.25,836437.6875,729535,592805,634751,2057288,13491170,1509775,961749,37785552,4628171,984054,1038045.313,859137.125,1142949,2411402.25,5833800,7338511,3611538,966812.125,616749,730292,1120197,10053489,1044220.625,492807.6875,9308471,2240549,3241917,1433573.25,13146645,1208407.625,2131817,1424157.25,11353672,2755578,163945984,861643.6875,718362,784652,2757588,2759449,1765813.625,2162760,2227908.75,6458021.5,2087998,22325364,6481807.5,2503240,9085924,785144.8125,2458760.5,61984596,900973,1859571.875,945063,715728,509171.4063,2504814.5,1878442.25,2434208,572139,520011.8125,24017770,1814947,2231695,1725773,53401,1383950,728876,1046995.188,6843855,9155568,2885740.5,7220201,4424263,1694930.375,550324.1875,11356990,1742870,1094622,966677.125,24224888,1602030,3888945,13609040,5981430,16644664,1252323.875,479358,2630101,7004126,1911591,1357553,12184124,896172,2830674.75,1143826.625,4767227,2108012,3548858.25,1986624,19036416,5448687,1602143,2505271,1817513,40613740,1361427.75,2345752.5,4020660,4391348,5976246,3075425,13883385,1984823,677642.125,1456808,1189432,1271021,1696009,609494,41399504,767720,950695.625,1993766.375,702385,2188093,735810,2504797,17186624,1885314,8728584,11951945,2495819,1000796.875,1539176.625,1730542.25,1223296,1392153,847514.5,991513.875,1122837,2098850,637053.1875,819314,25672598,653672,1358652,845961.375,5807879,911366,846274,7879628,1433504.375,2279063,755269,1138520,133880,524479,10110827,624269.625,1890502.5,3902177,563298,919048.6875,85357952,503744.6875,2485521,10642754,1686405,2189124,1427294.375,1353079.5,862945.125,1956439.375,866806,1972491,627698,1553208.375,1929828,1151027,1222509,1404702.25,1725558,8354235,911088.8125,708928,3441643.25,723482.1875,23364636,1566177,7657568,1262255.75,3444270,5328051.5,313528.0938,2567976.75,634056,488181,915286.1875,2559461.75,3494929.5,4116871.5,695803,989257,1152896.25,719471.1875,639407.375,1755264,3619497,13349651,1393073,4258124,1897180,669117,1769137,541943.3125,946138,2214604.25,5229981,966764,6211441,639566.875,602618,523133,749207,4321141.5,2222097.5,648022,5177270.5,4785692,1933811.625,2184912,2845068,1399992,1285055,315468,6782435,2173143.75,2070005.625,1692199.25,2409429,7794869,2999116,1614673.625,659450,981009.125,491792,1454222,1953919.875,2101376,904236,0,1417728.25,20828130,1827424.125,32552428,514471,1703060,775729.1875,1272272,622051,783208.625,1625308.625,962238,1465535,2834401,2050600.75,2309007.25,7758182,3837228,522972.1875,1982967,769148,1698779,1928564.75,1257033.5,7654264,1285646.75,3643112,1224045.25,2632472,1678495.25,2076612,1964681.25,7231928,728167,957632,13631685,6673138,5571501.5,1261725,4618360,2756494,136241424,1529309,1353612.75,2786648.75,13586070,1100499.75,52960796,6669954,2274873.5,1972156,817695.6875,1911611,58041572,117438.3984,6127771.5,656980,418098,4634655,419978.6875,1057256,41680640,694189,674961,3371659.75,6983736,5288304,1912760.75,14981114,1764767.25,806615,4116805,9569715,7475577.5,3721129,2014721,5101504.5,719060,2919980.75,971155.625,16373232,791564,4933374.5,10834087,605186.3125,27185096,1045838.5,4656777,834711.875,1008536.813,1688745,4262316.5,928584.875,1167734.75,298063,552401.1875,450518,3796986.5,3371455,1240924.625,1537570.25,227566.7031,164817.9063,5543575,161311.0938,606886,1083729.625,1368120.5,2646523.5,2513155,1167967.125,638201,5274642,1386445.375,730211.1875,8692328,1257144.75,4212149,593296.1875,1767480,14265421,2447861,948157,1739286.75,1121537.75,12029600,6797647,151376,1761137.5,6674179.5,6505995,30135280,696522,7837692.5,767361,1989763.875,1788999,1822830,5232420.5,1554342,1523324,1487660,3556291,2249620.5,6592304,335245,3714139,1594422.5,336461,1956075,1838121,3986982.75,817409.625,2946756.25,1521693.25,1880890,12265516,1965529,4103027.5,1686248,5276777,1988760.875,3515821,2218249,947534,2892779,756908.8125,10721568,357915,4571388,3022275,5051636,2429214,5446624,886771,5326222,584545.625,2158629,726124.625,930611.875,1889990,1423929.125,866126.375,783651.5,8201974,2346692.5,145468.7969,2399762,938096.1875,1145535.75,2468322.5,831577,4829951,431815.6875,2361960,1210262,8787353,2882757,1228837.25,2735811.5,576052,442375.8125,866242,1339220.625,880115.5,1656025.625,65047284,2356376.75,456747,673030,863703,686091,3456131.5,2503690,345866.6875,941564.8125,1012341,15091579,1733511,999856,2154119,6268627,5454482,2407904.5,1815350,799996,593824,585559,5063753,5572503,8228399,6014078,994194.875,27717034],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Ridge() vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predRidge"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["alphasridge = np.logspace(-3, 5, 1000)\n","param_gridRidge = {'ridge__alpha': alphasridge}\n","\n","GridRidge, \\\n","BestParametresRidge, \\\n","ScoresRidge, \\\n","SiteEnergyUse_predRidge, \\\n","figRidge = reg_modelGrid(model=Ridge(),\n","                            scaler=scaler,\n","                            X_train=BEBM_train,\n","                            X_test=BEBM_test,\n","                            y_train=SiteEnergyUse_train,\n","                            y_test=SiteEnergyUse_test,\n","                            y_test_name='SiteEnergyUse_test',\n","                            y_pred_name='SiteEnergyUse_predRidge',\n","                            score=score,\n","                            param_grid=param_gridRidge)\n","\n","print(BestParametresRidge)\n","ScoresRidge\n","figRidge.show()"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[11333619.964498507,11333619.949322578,11333619.933864227,11333619.918118205,11333619.902079154,11333619.885741618,11333619.869100045,11333619.852148777,11333619.834882054,11333619.817294003,11333619.799378641,11333619.781129878,11333619.762541514,11333619.74360723,11333619.724320585,11333619.704675023,11333619.68466386,11333619.6642803,11333619.64351741,11333619.62236813,11333619.600825276,11333619.578881508,11333619.556529382,11333619.533761295,11333619.510569505,11333619.48694612,11333619.462883122,11333619.438372325,11333619.413405389,11333619.387973836,11333619.362069014,11333619.335682115,11333619.308804171,11333619.281426044,11333619.253538432,11333619.225131843,11333619.196196625,11333619.166722942,11333619.136700775,11333619.106119912,11333619.074969966,11333619.043240333,11333619.010920236,11333618.977998693,11333618.944464495,11333618.910306254,11333618.875512362,11333618.840070976,11333618.803970054,11333618.76719733,11333618.729740294,11333618.691586211,11333618.652722117,11333618.613134807,11333618.572810803,11333618.531736413,11333618.489897665,11333618.447280342,11333618.403869953,11333618.35965174,11333618.314610679,11333618.268731447,11333618.221998457,11333618.174395815,11333618.125907343,11333618.076516563,11333618.026206672,11333617.974960584,11333617.92276087,11333617.86958978,11333617.815429252,11333617.760260867,11333617.704065872,11333617.64682517,11333617.588519296,11333617.529128436,11333617.468632402,11333617.40701063,11333617.344242167,11333617.280305687,11333617.215179449,11333617.148841318,11333617.081268748,11333617.01243877,11333616.94232798,11333616.87091256,11333616.798168223,11333616.724070255,11333616.648593461,11333616.571712188,11333616.49340031,11333616.413631205,11333616.332377758,11333616.249612356,11333616.165306868,11333616.079432636,11333615.991960475,11333615.90286066,11333615.812102895,11333615.719656352,11333615.625489598,11333615.52957063,11333615.431866849,11333615.332345052,11333615.230971413,11333615.12771148,11333615.022530157,11333614.915391702,11333614.806259701,11333614.69509707,11333614.581866022,11333614.466528092,11333614.349044066,11333614.229374029,11333614.107477307,11333613.983312486,11333613.856837366,11333613.728008967,11333613.59678351,11333613.46311641,11333613.326962236,11333613.188274728,11333613.047006754,11333612.903110322,11333612.756536528,11333612.607235564,11333612.455156708,11333612.300248284,11333612.142457658,11333611.981731215,11333611.818014342,11333611.651251419,11333611.481385788,11333611.308359731,11333611.132114464,11333610.952590104,11333610.769725665,11333610.583459023,11333610.39372689,11333610.200464817,11333610.003607143,11333609.80308699,11333609.59883625,11333609.39078553,11333609.178864155,11333608.963000143,11333608.74312016,11333608.519149521,11333608.291012147,11333608.058630552,11333607.8219258,11333607.58081749,11333607.33522374,11333607.085061122,11333606.830244686,11333606.570687879,11333606.306302553,11333606.036998924,11333605.762685526,11333605.483269205,11333605.19865508,11333604.90874649,11333604.613444999,11333604.312650327,11333604.006260334,11333603.694170987,11333603.376276327,11333603.0524684,11333602.72263728,11333602.38667098,11333602.044455435,11333601.69587447,11333601.340809736,11333600.979140703,11333600.610744592,11333600.235496351,11333599.8532686,11333599.463931601,11333599.067353196,11333598.66339879,11333598.251931267,11333597.832810987,11333597.405895703,11333596.971040536,11333596.528097913,11333596.076917518,11333595.617346255,11333595.149228185,11333594.672404464,11333594.186713314,11333593.691989947,11333593.188066516,11333592.674772061,11333592.15193246,11333591.619370345,11333591.076905068,11333590.524352623,11333589.961525595,11333589.388233101,11333588.804280695,11333588.209470345,11333587.603600338,11333586.986465227,11333586.357855748,11333585.717558756,11333585.065357164,11333584.40102985,11333583.724351602,11333583.03509302,11333582.333020478,11333581.617895987,11333580.889477175,11333580.147517156,11333579.391764497,11333578.621963084,11333577.83785206,11333577.03916575,11333576.225633543,11333575.396979827,11333574.552923877,11333573.69317978,11333572.817456316,11333571.925456883,11333571.016879389,11333570.09141614,11333569.148753762,11333568.188573066,11333567.210548967,11333566.214350361,11333565.199640024,11333564.16607448,11333563.113303917,11333562.04097204,11333560.948715974,11333559.836166125,11333558.70294608,11333557.548672454,11333556.372954788,11333555.17539539,11333553.955589239,11333552.713123815,11333551.447578987,11333550.158526864,11333548.84553164,11333547.508149479,11333546.145928338,11333544.758407835,11333543.345119085,11333541.90558456,11333540.43931791,11333538.945823813,11333537.424597824,11333535.875126174,11333534.296885643,11333532.689343339,11333531.05195657,11333529.384172637,11333527.68542864,11333525.955151333,11333524.192756895,11333522.39765076,11333520.569227409,11333518.706870183,11333516.80995107,11333514.877830502,11333512.909857148,11333510.90536769,11333508.863686617,11333506.784126002,11333504.665985264,11333502.508550953,11333500.311096516,11333498.07288205,11333495.793154066,11333493.471145242,11333491.106074166,11333488.697145104,11333486.243547717,11333483.744456794,11333481.199032005,11333478.60641763,11333475.96574224,11333473.27611847,11333470.536642695,11333467.746394735,11333464.904437596,11333462.009817118,11333459.0615617,11333456.058681969,11333453.000170479,11333449.885001374,11333446.712130051,11333443.480492843,11333440.189006675,11333436.836568689,11333433.422055945,11333429.944324993,11333426.402211575,11333422.79453021,11333419.120073834,11333415.377613405,11333411.56589752,11333407.683652036,11333403.729579624,11333399.70235939,11333395.600646451,11333391.423071507,11333387.168240413,11333382.834733745,11333378.421106335,11333373.925886841,11333369.347577274,11333364.684652526,11333359.935559902,11333355.098718643,11333350.172519412,11333345.155323815,11333340.045463875,11333334.841241537,11333329.54092813,11333324.142763829,11333318.644957129,11333313.045684287,11333307.34308875,11333301.535280619,11333295.620336039,11333289.596296629,11333283.461168889,11333277.212923594,11333270.849495161,11333264.368781071,11333257.768641176,11333251.04689711,11333244.2013316,11333237.229687825,11333230.129668731,11333222.898936352,11333215.535111126,11333208.035771158,11333200.398451565,11333192.620643688,11333184.699794397,11333176.63330533,11333168.418532144,11333160.05278374,11333151.53332148,11333142.857358415,11333134.022058452,11333125.024535578,11333115.861853015,11333106.531022388,11333097.029002864,11333087.352700328,11333077.498966476,11333067.464597952,11333057.246335456,11333046.84086282,11333036.244806109,11333025.454732686,11333014.46715027,11333003.278505968,11332991.885185335,11332980.28351138,11332968.469743574,11332956.440076865,11332944.190640628,11332931.717497692,11332919.016643245,11332906.08400383,11332892.915436257,11332879.50672653,11332865.853588775,11332851.95166413,11332837.79651963,11332823.383647108,11332808.708462045,11332793.766302416,11332778.552427564,11332763.062017,11332747.290169258,11332731.23190068,11332714.882144233,11332698.235748291,11332681.287475405,11332664.032001104,11332646.46391263,11332628.57770769,11332610.36779321,11332591.828484058,11332572.954001779,11332553.738473294,11332534.17592963,11332514.260304607,11332493.98543353,11332473.345051898,11332452.332794067,11332430.942191932,11332409.166673597,11332386.99956205,11332364.434073836,11332341.463317683,11332318.080293205,11332294.277889544,11332270.048884002,11332245.38594075,11332220.28160943,11332194.728323871,11332168.718400706,11332142.244038057,11332115.297314225,11332087.87018633,11332059.95448903,11332031.541933198,11332002.624104628,11331973.192462744,11331943.238339331,11331912.752937276,11331881.72732932,11331850.152456816,11331818.019128546,11331785.318019498,11331752.039669724,11331718.174483169,11331683.712726574,11331648.64452837,11331612.959877599,11331576.64862292,11331539.700471576,11331502.104988437,11331463.85159509,11331424.92956894,11331385.328042373,11331345.036001973,11331304.042287761,11331262.335592523,11331219.904461134,11331176.737290023,11331132.822326625,11331088.147668932,11331042.701265102,11330996.470913175,11330949.44426077,11330901.60880499,11330852.951892305,11330803.460718574,11330753.122329133,11330701.923619013,11330649.851333212,11330596.89206711,11330543.032266986,11330488.25823062,11330432.556108046,11330375.91190244,11330318.311471093,11330259.74052656,11330200.184637932,11330139.629232276,11330078.059596173,11330015.460877504,11329951.818087323,11329887.11610194,11329821.339665174,11329754.473390825,11329686.501765255,11329617.409150276,11329547.179786174,11329475.797794972,11329403.247183915,11329329.5118492,11329254.575579926,11329178.422062313,11329101.03488415,11329022.397539536,11328942.493433902,11328861.305889294,11328778.818149956,11328695.013388222,11328609.874710742,11328523.385164976,11328435.527746094,11328346.285404155,11328255.641051687,11328163.577571612,11328070.077825535,11327975.124662466,11327878.70092788,11327780.78947323,11327681.37316587,11327580.434899405,11327477.957604494,11327373.924260104,11327268.317905232,11327161.121651098,11327052.318693865,11326941.892327826,11326829.825959105,11326716.103119936,11326600.707483418,11326483.622878883,11326364.833307775,11326244.322960135,11326122.07623171,11325998.077741574,11325872.31235044,11325744.765179578,11325615.421630353,11325484.267404417,11325351.288524564,11325216.471356275,11325079.802629884,11324941.26946352,11324800.859386656,11324658.560364446,11324514.360822735,11324368.249673814,11324220.216342915,11324070.25079542,11323918.34356486,11323764.485781644,11323608.669202551,11323450.886241008,11323291.129998095,11323129.394294385,11322965.673702491,11322799.963580467,11322632.260105887,11322462.560310805,11322290.862117404,11322117.164374478,11321941.466894621,11321763.770492237,11321584.077022236,11321402.389419548,11321218.711739296,11321033.049197758,11320845.408213977,11320655.79645211,11320464.222864429,11320270.697734974,11320075.23272388,11319877.840912256,11319678.536847726,11319477.336590433,11319274.257759707,11319069.319581114,11318862.54293403,11318653.950399647,11318443.566309366,11318231.41679353,11318017.529830499,11317801.93529595,11317584.665012423,11317365.75279904,11317145.234521303,11316923.148140987,11316699.533766,11316474.433700219,11316247.892493164,11316019.956989516,11315790.676378343,11315560.102242041,11315328.28860482,11315095.291980766,11314861.171421293,11314625.988562012,11314389.807668854,11314152.695683379,11313914.722267231,11313675.959845591,11313436.483649572,11313196.371757448,11312955.705134649,11312714.567672411,11312473.046224961,11312231.2306452,11311989.213818759,11311747.091696274,11311504.963323932,11311262.930872004,11311021.099661428,11310779.57818826,11310538.478145918,11310297.914445156,11310058.005231636,11309818.871901022,11309580.639111534,11309343.434793862,11309107.390158348,11308872.639699398,11308639.321197001,11308407.575715348,11308177.547598425,11307949.384462606,11307723.237186098,11307499.25989528,11307277.60994786,11307058.44791282,11306841.937547123,11306628.245769212,11306417.542629236,11306210.001276046,11306005.797920963,11305805.111798357,11305608.125123044,11305415.023044553,11305225.993598372,11305041.227654126,11304860.918860923,11304685.263589785,11304514.460873434,11304348.712343412,11304188.222164769,11304033.19696838,11303883.845781121,11303740.379953984,11303603.013088414,11303471.960960973,11303347.441446584,11303229.674440576,11303118.881779745,11303015.28716268,11302919.11606963,11302830.595682148,11302749.954802841,11302677.42377546,11302613.234405687,11302557.61988289,11302510.814703207,11302473.054594256,11302444.576441824,11302425.618218886,11302416.418917306,11302417.218482573,11302428.257751938,11302449.778396314,11302482.022866327,11302525.234342862,11302579.656692503,11302645.534428215,11302723.112675656,11302812.637145469,11302914.35411194,11303028.510398354,11303155.353369404,11303295.130931024,11303448.091537923,11303614.484209238,11303794.55855248,11303988.564796213,11304196.753831675,11304419.37726361,11304656.687470596,11304908.937675089,11305176.382023407,11305459.275675844,11305757.874907117,11306072.437217265,11306403.221453173,11306750.487940835,11307114.498628395,11307495.517240101,11307893.809441175,11308309.64301361,11308743.288042903,11309195.017115694,11309665.105528206,11310153.831505481,11310661.476431202,11311188.325088028,11311734.665908266,11312300.791234653,11312886.997591102,11313493.585963067,11314120.862087373,11314769.136751123,11315438.726099446,11316129.951951673,11316843.142125672,11317578.630769845,11318336.758702517,11319117.873758173,11319922.331140202,11320750.493779624,11321602.732699359,11322479.427383546,11323380.966151409,11324307.746535134,11325260.175661275,11326238.670635095,11327243.658927346,11328275.5787629,11329334.87951069,11330422.022074362,11331537.479283128,11332681.736282144,11333855.290921967,11335058.6541464,11336292.350378182,11337556.91790199,11338852.909244128,11340180.89154832,11341541.446947122,11342935.172928283,11344362.682695583,11345824.605523555,11347321.58710557,11348854.289894737,11350423.393437076,11352029.594696518,11353673.608371103,11355356.167200018,11357078.022260861,11358839.94325674,11360642.71879268,11362487.156640932,11364374.083994696,11366304.347709823,11368278.814534105,11370298.37132368,11372363.925246203,11374476.403970309,11376636.755841065,11378845.950040963,11381104.976736084,11383414.84720711,11385776.59396478,11388191.270849425,11390659.95311431,11393183.737492306,11395763.742245715,11398401.107198747,11401096.993752427,11403852.584881566,11406669.08511344,11409547.72048788,11412489.738498425,11415496.40801422,11418569.0191823,11421708.88330997,11424917.3327269,11428195.720626656,11431545.420887237,11434967.827870414,11438464.356199382,11442036.440514492,11445685.535206657,11449413.114128133,11453220.67028025,11457109.715477848,11461081.77999001,11465138.412156725,11469281.177981231,11473511.660697605,11477831.460313335,11482242.193126526,11486745.491217433,11491343.001914037,11496036.387231346,11500827.323284201,11505717.499673277,11510708.618844122,11515802.39541899,11521000.555501278,11526304.835952515,11531716.983641656,11537238.754666772,11542871.91354897,11548618.232398676,11554479.490054255,11560457.471193194,11566553.96541591,11572770.766302537,11579109.670442922,11585572.476440227,11592160.983888606,11598876.992325429,11605722.300158685,11612698.70357025,11619807.995395757,11627051.963981941,11634432.39202238,11641951.055372681,11649609.721846242,11657410.149991762,11665354.087853879,11673443.27171829,11681679.424842885,11690064.256176462,11698599.459066782,11707286.709959641,11716127.667090949,11725123.969173674,11734277.234081775,11743589.057533184,11753061.011774082,11762694.6442667,11772491.476383012,11782453.002106685,11792580.68674574,11802875.965658406,11813340.242994677,11823974.89045618,11834781.246076796,11845760.613026768,11856914.25844277,11868243.41228651,11879749.266234454,11891432.972601173,11903295.643298768,11915338.348834803,11927562.117351113,11939967.933705771,11952556.738600368,11965329.427754778,11978286.85113135,11991429.81221039,12004759.067318749,12018275.325013053,12031979.245519105,12045871.440228682,12059952.471254997,12074222.851047678,12088683.042068142,12103333.456525948,12118174.45617655,12133206.352180691,12148429.405025471,12163843.824506903,12179449.769773614,12195247.349431086,12211236.621705666,12227417.594667384,12243790.226510365,12260354.425889436,12277110.052311417,12294056.916579233,12311194.781286938,12328523.36136353,12346042.32466318,12363751.292599443,12381649.84082083,12399737.49992489,12418013.756207937,12436478.052447367,12455129.788713368,12473968.323206773,12492992.973119684,12512203.015515441,12531597.688224386,12551176.190751886,12570937.685194988,12590881.29716409,12611006.116705954,12631311.199224416,12651795.566395188,12672458.207071092,12693298.07817417,12714314.10557121,12735505.184929157,12756870.182547096,12778407.93616153,12800117.255721757,12821996.924132343,12844045.697959732,12866262.308100222,12888645.460406676,12911193.83627156,12933906.093163917,12956780.865118237,12979816.763173271,13003012.375759084,13026366.269030778,13049876.987147672,13073543.052496724,13097362.965859417,13121335.20652145,13145458.23232481,13169730.479662055,13194150.363412892,13218716.276823327,13243426.591327958,13268279.656316195,13293273.798843417,13318407.323288342,13343678.510958131,13369085.619642926,13394626.883121863,13420300.510622656,13446104.686237281,13472037.568296287,13498097.288704667,13524281.952242294,13550589.63583214,13577018.387779737,13603566.22698744,13630231.142147284,13657011.090916317,13683903.999078479,13710907.759697175,13738020.23226286,13765239.241840024,13792562.578218028,13819987.995070392,13847513.209127024,13875135.899364185,13902853.706216712,13930664.230817229,13958565.034267018,13986553.636943111,14014627.51784625,14042784.11399417,14071020.81986469,14099334.986892933,14127723.92302683,14156184.892345125,14184715.114741657,14213311.76567984,14241971.976020813,14270692.831928715,14299471.374856222,14328304.60161331,14357189.464521995,14386122.871659439,14415101.687191654,14444122.731799718,14473182.783200154,14502278.576760814,14531406.806213278,14560564.12446256,14589747.144494519,14618952.440381099,14648176.548383225,14677415.968150804,14706667.164018985,14735926.566399615,14765190.573266342,14794455.551731652,14823717.839713752,14852973.74769095,14882219.560540864,14911451.539461603,14940665.923971612,14969858.933984827,14999026.771957466,15028165.6251024,15057271.667667175,15086341.063271176,15115369.967297506,15144354.529334893,15173290.895664748,15202175.21178848,15231003.624989916,15259772.286927706,15288477.35625245,15317115.001243189,15345681.402458012,15374172.755393306,15402585.27314632,15430915.189075703,15459158.759454628,15487312.266111335,15515372.019051725,15543334.35905906,15571195.660265591,15598952.332691297,15626600.824744929,15654137.625682702,15681559.268020159,15708862.329892928,15736043.437362134,15763099.26666067,15790026.546376413,15816822.059568932,15843482.64581633,15870005.20318909,15896386.690148061,15922624.127363872,15948714.59945539,15974655.256644959,16000443.31632853,16026076.064558893,16051550.857440546,16076865.122434903,16102016.359574843,16127002.142587807,16151820.11992684,16176468.015709203,16200943.630562479,16225244.842378223,16249369.60697333,16273315.958659822,16297082.010723438,16320665.95581212,16344066.066235255,16367280.694174957,16390308.271810684,16413147.311358744,16435796.405028254,16458254.224895423,16480519.52269798]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[13748873.449116213,13748873.434406582,13748873.419423208,13748873.404161006,13748873.38861478,13748873.372779239,13748873.356649006,13748873.340218592,13748873.32348242,13748873.306434793,13748873.289069915,13748873.271381877,13748873.25336468,13748873.235012192,13748873.216318171,13748873.197276263,13748873.177879991,13748873.158122761,13748873.137997866,13748873.11749845,13748873.096617557,13748873.075348068,13748873.053682774,13748873.031614305,13748873.009135153,13748872.986237668,13748872.962914083,13748872.93915646,13748872.914956717,13748872.890306637,13748872.865197832,13748872.839621767,13748872.813569749,13748872.787032917,13748872.760002267,13748872.732468586,13748872.704422522,13748872.675854545,13748872.646754945,13748872.617113817,13748872.5869211,13748872.55616651,13748872.524839606,13748872.492929745,13748872.460426059,13748872.427317508,13748872.393592848,13748872.359240595,13748872.32424908,13748872.288606415,13748872.252300471,13748872.215318905,13748872.177649153,13748872.139278423,13748872.100193638,13748872.060381535,13748872.019828575,13748871.97852097,13748871.936444681,13748871.893585403,13748871.849928569,13748871.805459332,13748871.760162584,13748871.714022921,13748871.66702466,13748871.619151836,13748871.570388151,13748871.520717058,13748871.470121657,13748871.418584742,13748871.366088813,13748871.312616013,13748871.258148171,13748871.20266677,13748871.14615295,13748871.088587506,13748871.02995087,13748870.97022311,13748870.909383921,13748870.84741263,13748870.784288168,13748870.719989078,13748870.65449351,13748870.587779205,13748870.51982347,13748870.45060323,13748870.38009494,13748870.308274651,13748870.23511794,13748870.160599945,13748870.084695345,13748870.00737834,13748869.928622643,13748869.848401494,13748869.766687633,13748869.683453282,13748869.59867015,13748869.512309434,13748869.424341764,13748869.334737271,13748869.243465483,13748869.150495386,13748869.055795383,13748868.959333293,13748868.861076338,13748868.76099112,13748868.659043629,13748868.555199225,13748868.449422607,13748868.341677846,13748868.231928315,13748868.120136734,13748868.006265093,13748867.890274715,13748867.772126172,13748867.651779337,13748867.5291933,13748867.404326415,13748867.277136244,13748867.147579586,13748867.015612403,13748866.881189868,13748866.744266294,13748866.60479518,13748866.462729126,13748866.318019854,13748866.170618217,13748866.020474128,13748865.867536576,13748865.711753603,13748865.55307228,13748865.3914387,13748865.226797964,13748865.059094125,13748864.888270222,13748864.71426821,13748864.537028996,13748864.35649237,13748864.172596999,13748863.985280424,13748863.79447901,13748863.60012795,13748863.402161237,13748863.200511623,13748862.995110614,13748862.785888456,13748862.572774079,13748862.355695108,13748862.134577814,13748861.909347108,13748861.679926496,13748861.44623806,13748861.208202451,13748860.965738822,13748860.718764853,13748860.467196668,13748860.210948844,13748859.949934376,13748859.684064632,13748859.413249342,13748859.137396555,13748858.856412612,13748858.570202122,13748858.278667908,13748857.981710996,13748857.679230582,13748857.371123983,13748857.057286592,13748856.73761189,13748856.411991365,13748856.080314485,13748855.742468681,13748855.398339277,13748855.047809478,13748854.690760313,13748854.327070616,13748853.956616957,13748853.579273632,13748853.194912577,13748852.803403385,13748852.404613197,13748851.998406718,13748851.58464611,13748851.163191006,13748850.73389842,13748850.296622707,13748849.851215534,13748849.397525819,13748848.935399657,13748848.464680303,13748847.985208105,13748847.49682044,13748846.999351673,13748846.492633108,13748845.976492908,13748845.45075605,13748844.915244272,13748844.369776001,13748843.814166313,13748843.248226827,13748842.671765694,13748842.084587501,13748841.486493217,13748840.87728012,13748840.256741723,13748839.624667726,13748838.980843928,13748838.325052157,13748837.657070197,13748836.976671737,13748836.283626236,13748835.57769892,13748834.858650636,13748834.126237841,13748833.380212447,13748832.620321788,13748831.846308522,13748831.05791054,13748830.254860884,13748829.436887654,13748828.60371393,13748827.75505764,13748826.890631523,13748826.010142999,13748825.113294058,13748824.199781207,13748823.269295314,13748822.321521549,13748821.356139254,13748820.372821853,13748819.371236717,13748818.351045085,13748817.311901936,13748816.253455872,13748815.175348986,13748814.0772168,13748812.958688071,13748811.81938472,13748810.65892167,13748809.476906754,13748808.272940557,13748807.0466163,13748805.797519699,13748804.525228813,13748803.229313938,13748801.909337435,13748800.5648536,13748799.195408508,13748797.800539874,13748796.37977689,13748794.932640065,13748793.4586411,13748791.957282675,13748790.428058337,13748788.870452274,13748787.283939218,13748785.667984225,13748784.022042483,13748782.34555919,13748780.637969326,13748778.898697484,13748777.127157677,13748775.322753148,13748773.484876184,13748771.612907901,13748769.706218055,13748767.764164817,13748765.786094593,13748763.771341784,13748761.719228582,13748759.62906475,13748757.500147391,13748755.331760732,13748753.12317587,13748750.87365056,13748748.58242894,13748746.248741347,13748743.871804,13748741.450818777,13748738.984972965,13748736.473439002,13748733.915374164,13748731.309920363,13748728.656203816,13748725.953334779,13748723.200407285,13748720.396498818,13748717.540670041,13748714.631964482,13748711.669408249,13748708.652009698,13748705.578759111,13748702.448628409,13748699.260570798,13748696.013520429,13748692.706392104,13748689.338080857,13748685.907461697,13748682.413389172,13748678.854697071,13748675.230197988,13748671.53868302,13748667.778921353,13748663.949659867,13748660.04962276,13748656.077511149,13748652.032002652,13748647.911750996,13748643.715385592,13748639.441511089,13748635.088706976,13748630.655527111,13748626.140499305,13748621.542124834,13748616.858878026,13748612.089205734,13748607.231526917,13748602.28423211,13748597.24568297,13748592.11421176,13748586.888120845,13748581.565682175,13748576.145136774,13748570.624694187,13748565.002531983,13748559.276795164,13748553.445595628,13748547.507011618,13748541.459087137,13748535.299831357,13748529.027218068,13748522.639185019,13748516.133633383,13748509.50842708,13748502.761392184,13748495.89031629,13748488.89294785,13748481.766995555,13748474.510127613,13748467.119971167,13748459.594111526,13748451.930091524,13748444.125410812,13748436.17752515,13748428.083845675,13748419.84173818,13748411.448522393,13748402.901471173,13748394.197809828,13748385.334715292,13748376.309315369,13748367.118687907,13748357.759860078,13748348.22980748,13748338.525453374,13748328.643667847,13748318.581266949,13748308.335011862,13748297.90160805,13748287.27770437,13748276.459892195,13748265.444704536,13748254.228615139,13748242.80803756,13748231.179324275,13748219.338765705,13748207.282589342,13748195.006958732,13748182.507972568,13748169.7816637,13748156.823998146,13748143.63087414,13748130.198121114,13748116.521498684,13748102.596695667,13748088.419329042,13748073.98494291,13748059.28900748,13748044.32691801,13748029.093993753,13748013.585476914,13747997.796531554,13747981.722242549,13747965.357614467,13747948.697570527,13747931.736951493,13747914.470514553,13747896.892932259,13747878.998791378,13747860.782591825,13747842.23874551,13747823.361575257,13747804.145313665,13747784.58410198,13747764.671989018,13747744.402929997,13747723.77078545,13747702.76932007,13747681.392201632,13747659.632999878,13747637.485185357,13747614.942128377,13747591.997097889,13747568.643260356,13747544.873678733,13747520.681311322,13747496.059010763,13747470.999522932,13747445.49548591,13747419.539428987,13747393.123771569,13747366.240822254,13747338.882777814,13747311.041722232,13747282.709625762,13747253.878344012,13747224.539617047,13747194.685068524,13747164.306204833,13747133.39441431,13747101.940966431,13747069.937011091,13747037.37357787,13747004.241575386,13746970.531790651,13746936.234888462,13746901.341410922,13746865.84177688,13746829.726281526,13746792.985096,13746755.60826706,13746717.585716797,13746678.90724247,13746639.562516313,13746599.541085534,13746558.832372226,13746517.425673546,13746475.310161823,13746432.47488482,13746388.90876606,13746344.600605305,13746299.539079003,13746253.712741004,13746207.110023253,13746159.719236642,13746111.52857198,13746062.526101103,13746012.699778054,13745962.03744046,13745910.526811022,13745858.155499108,13745804.911002561,13745750.780709649,13745695.751901124,13745639.811752522,13745582.947336592,13745525.14562593,13745466.393495763,13745406.677727,13745345.98500943,13745284.301945131,13745221.615052141,13745157.910768336,13745093.175455496,13745027.395403717,13744960.556835974,13744892.645913016,13744823.64873848,13744753.551364347,13744682.339796627,13744610.000001393,13744536.517911065,13744461.87943108,13744386.070446871,13744309.076831166,13744230.884451646,13744151.479178987,13744070.846895276,13743988.973502763,13743905.844933094,13743821.447156869,13743735.766193697,13743648.788122628,13743560.499093056,13743470.885336095,13743379.933176396,13743287.629044473,13743193.959489526,13743098.911192754,13743002.470981237,13742904.625842318,13742805.362938546,13742704.66962319,13742602.533456359,13742498.942221683,13742393.883943578,13742287.346905235,13742179.319667133,13742069.791086279,13741958.75033607,13741846.186926857,13741732.090727227,13741616.451985925,13741499.261354558,13741380.509911027,13741260.189183684,13741138.291176273,13741014.808393622,13740889.733868187,13740763.061187305,13740634.784521362,13740504.89865268,13740373.39900535,13740240.281675832,13740105.543464463,13739969.181907829,13739831.19531197,13739691.582786549,13739550.344279861,13739407.480614783,13739262.993525615,13739116.885695819,13738969.16079675,13738819.823527232,13738668.879654136,13738516.33605383,13738362.200754609,13738206.482980035,13738049.193193233,13737890.34314207,13737729.945905335,13737568.015939746,13737404.569127977,13737239.622827489,13737073.195920367,13736905.30886392,13736735.983742254,13736565.24431866,13736393.11608883,13736219.626334945,13736044.804180488,13735868.680645945,13735691.288705098,13735512.663342252,13735332.841609986,13735151.86268764,13734969.76794045,13734786.600979285,13734602.40772088,13734417.236448688,13734231.137874115,13734044.165198255,13733856.374173999,13733667.823168453,13733478.573225668,13733288.688129589,13733098.234467188,13732907.281691696,13732715.902185913,13732524.171325468,13732332.167542059,13732139.972386494,13731947.670591578,13731755.350134637,13731563.10229977,13731371.021739628,13731179.206536647,13730987.758263784,13730796.78204449,13730606.386612013,13730416.684367804,13730227.791439075,13730039.827735338,13729852.91700384,13729667.186883887,13729482.768959917,13729299.7988132,13729118.416072225,13728938.76446152,13728760.991848942,13728585.250291351,13728411.696078528,13728240.489775343,13728071.796262057,13727905.784772702,13727742.62893147,13727582.506787099,13727425.600845134,13727272.098098062,13727122.190053262,13726976.072758745,13726833.946826622,13726696.017454337,13726562.49444358,13726433.592216915,13726309.529832123,13726190.53099425,13726076.824065357,13725968.642072093,13725866.222711008,13725769.808351725,13725679.64603803,13725595.987486927,13725519.089085743,13725449.211887376,13725386.621603848,13725331.588598145,13725284.387874672,13725245.299068252,13725214.606432041,13725192.598824333,13725179.569694614,13725175.81706891,13725181.643534806,13725197.356226172,13725223.266808052,13725259.691461751,13725306.950870588,13725365.370206451,13725435.279117519,13725517.011717442,13725610.90657632,13725717.306713741,13725836.559594344,13725969.017126136,13726115.035662001,13726274.976004766,13726449.203416191,13726638.087630272,13726842.002871297,13727061.327876987,13727296.445927208,13727547.744878637,13727815.617205782,13728100.4600488,13728402.675268566,13728722.669509333,13729060.854269505,13729417.645980854,13729793.466096671,13730188.741189204,13730603.903056841,13731039.388841407,13731495.641155941,13731973.10822343,13732472.24402673,13732993.508470194,13733537.36755317,13734104.293555887,13734694.765237894,13735309.268049434,13735948.294356003,13736612.343676347,13737301.92293417,13738017.546723723,13738759.737589527,13739529.026320346,13740325.952257603,13741151.063618371,13742004.917832963,13742888.081897303,13743801.132740047,13744744.657604506,13745719.254445363,13746725.532340143,13747764.111915378,13748835.625787396,13749940.719017575,13751080.049581956,13752254.288855048,13753464.122107558,13754710.249017945,13755993.384197382,13757314.257728007,13758673.615714027,13760072.22084544,13761510.852973934,13762990.309700688,13764511.406975528,13766074.979707174,13767681.882383984,13769332.989704836,13771029.197219584,13772771.421978619,13774560.603190988,13776397.702890562,13778283.70660962,13780219.62405937,13782206.489816707,13784245.364016715,13786337.333050206,13788483.51026572,13790685.036675297,13792943.081663454,13795258.843698598,13797633.551046297,13800068.4624837,13802564.868014384,13805124.089583024,13807747.481789118,13810436.432599058,13813192.364055937,13816016.732986247,13818911.031702831,13821876.788703388,13824915.569363706,13828028.976625014,13831218.651674602,13834486.274619058,13837833.565149315,13841262.28319681,13844774.22957994,13848371.246640071,13852055.218866358,13855828.073508546,13859691.781177012,13863648.356429175,13867699.858341558,13871848.391066585,13876096.104373347,13880445.194171427,13884897.903016957,13889456.520600015,13894123.384212412,13898900.87919499,13903791.439363485,13908797.547411915,13913921.735292576,13919166.584571544,13924534.726758696,13930028.843611099,13935651.66740865,13941405.981200885,13947294.619023642,13953320.466084456,13959486.458915345,13965795.58549171,13972250.88531596,13978855.449464507,13985612.420596648,13992524.992923863,13999596.41213797,14006829.975296583,14014229.030664226,14021796.977507407,14029537.265841927,14037453.396130715,14045548.918930199,14053827.434483575,14062292.592258861,14070948.090429874,14079797.675298152,14088845.140653722,14098094.327072704,14107549.121149631,14117213.454662371,14127091.303667504,14137186.687523982,14147503.667842932,14158046.347361397,14168818.868737843,14179825.41326733,14191070.199514093,14202557.481859563,14214291.5489636,14226276.722137047,14238517.353623513,14251017.824788596,14263782.544214621,14276815.945699263,14290122.486156318,14303706.643417262,14317572.913932085,14331725.810368292,14346169.859106962,14360909.597634958,14375949.571832666,14391294.333156707,14406948.435717428,14422916.433251088,14439202.875986993,14455812.307410073,14472749.2609197,14490018.256385751,14507623.79660336,14525570.363648048,14543862.415133223,14562504.38037244,14581500.656449242,14600855.604197478,14620573.54409579,14640658.752079919,14661115.455277164,14681947.827667534,14703159.985676616,14724755.983705476,14746739.809603408,14769115.380089585,14791886.536130166,14815057.038277632,14838630.561979622,14862610.692864768,14887000.922013333,14911804.641220873,14937025.138263345,14962665.592172217,14988729.06852867,15015218.51478593,15042136.755628956,15069486.488381077,15097270.278466983,15125490.554941827,15154149.606096014,15183249.575145463,15212792.456016928,15242780.089237995,15273214.157941299,15304096.18399222,15335427.524249326,15367209.36696638,15399442.728344783,15432128.44924468,15465267.19206292,15498859.437785499,15532905.483221844,15567405.438427782,15602359.224323638,15637766.570513386,15673627.013310298,15709939.89397394,15746704.357162893,15783919.34960696,15821583.619002061,15859695.713130351,15898253.979207637,15937256.563459404,15976701.410926254,16016586.265498962,16056908.670182629,16097665.967588961,16138855.300655093,16180473.613586625,16222517.653022306,16264983.969416939,16307868.918638792,16351168.663777146,16394879.177155249,16438996.242543504,16483515.457567148,16528432.236302502,16573741.812055385,16619439.240314955,16665519.401876077,16711977.006122831,16758806.594465815,16806002.543925453,16853559.070853498,16901470.234784834,16949729.9424114,16998331.951670107,17047269.87593669,17096537.18831714,17146127.22602869,17196033.194862176,17246248.17371772,17296765.119205765,17347576.870305754,17398676.153074585,17450055.585397415,17501707.681773424,17553624.85812939,17605799.43665409,17658223.6506468,17710889.64937337,17763789.502923623,17816915.207064006,17870258.68807985,17923811.807601504,17977566.36740935,18031514.11421249,18085646.744396545,18139955.908736046,18194433.217067223,18249070.242917262,18303858.528086428,18358789.58717949,18413854.91208352,18469045.97638888,18524354.239750914,18579771.152189806,18635288.158326346,18690896.701551776,18746588.228129618,18802354.191228174,18858186.054882094,18914075.297881886,18970013.417590298,19025991.933684684,19082002.391824618,19138036.36724416,19194085.468268454,19250141.33975416,19306195.66645377,19362240.17630346,19418266.643634815,19474266.89231032,19530232.798782997,19586156.29508041,19642029.37171346,19697844.0805104,19753592.537376583,19809266.924980517,19864859.49536682,19920362.572496694,19975768.55471668,20031069.917156268,20086259.214055233,20141329.08102134,20196272.237219203,20251081.487491027,20305749.72441002,20360269.93026726,20414635.17899265,20468838.638010833,20522873.5700326,20576733.334782667,20630411.39066445,20683901.296362326,20737196.712382205,20790291.402530894,20843179.235334776,20895854.185398463,20948310.3347037,21000541.873849154,21052543.1032314,21104308.434167527,21155832.389959656,21207109.60690173,21258134.83522878,21308902.940008905,21359408.90197835,21409647.818319485,21459614.903382167,21509305.489348486,21558715.02684083,21607839.085473645,21656673.354348667,21705213.642493755,21753455.879245397,21801396.11457481,21849030.519357692,21896355.385587655,21943367.12653322,21990062.276838586,22036437.49256794,22082489.551193617,22128215.351527937,22173611.913598876,22218676.3784697,22263406.00800259,22307798.184566557,22351850.41068964,22395560.3086558,22438925.620046645,22481944.205228362,22524614.04278421,22566933.22889295,22608899.97665365,22650512.615357477,22691769.58970686,22732669.458982848,22773210.896161165,22813392.686977763,22853213.728944667,22892673.030316953,22931769.709011752,22970502.99148023,23008872.21153361,23046876.809124224,23084516.32908285,23121790.419813395,23158698.831946254,23195241.41695156,23231418.125713684,23267229.007068437,23302674.206304274,23337753.96362906,23372468.612603858,23406818.57854535,23440804.37689832,23474426.61158003,23507685.973297834,23540583.237841986,23573119.26435507,23605294.993579883,23637111.446087442,23668569.72048682,23699670.991618525,23730416.50873317,23760807.59365716]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[8918366.4798808,8918366.464238573,8918366.448305245,8918366.432075404,8918366.415543528,8918366.398703996,8918366.381551085,8918366.364078961,8918366.346281689,8918366.328153213,8918366.309687367,8918366.290877879,8918366.271718347,8918366.252202269,8918366.232322998,8918366.212073782,8918366.19144773,8918366.170437837,8918366.149036955,8918366.127237812,8918366.105032995,8918366.082414947,8918366.05937599,8918366.035908284,8918366.012003858,8918365.987654574,8918365.962852161,8918365.93758819,8918365.91185406,8918365.885641035,8918365.858940197,8918365.831742462,8918365.804038594,8918365.775819171,8918365.747074597,8918365.7177951,8918365.687970728,8918365.65759134,8918365.626646606,8918365.595126007,8918365.563018832,8918365.530314157,8918365.497000866,8918365.46306764,8918365.428502932,8918365.393295001,8918365.357431876,8918365.320901357,8918365.283691028,8918365.245788245,8918365.207180116,8918365.167853517,8918365.12779508,8918365.08699119,8918365.045427967,8918365.00309129,8918364.959966755,8918364.916039713,8918364.871295225,8918364.825718079,8918364.779292788,8918364.732003562,8918364.68383433,8918364.634768708,8918364.584790025,8918364.53388129,8918364.482025193,8918364.42920411,8918364.375400083,8918364.320594817,8918364.26476969,8918364.20790572,8918364.149983574,8918364.09098357,8918364.03088564,8918363.969669366,8918363.907313934,8918363.84379815,8918363.779100413,8918363.713198744,8918363.64607073,8918363.577693557,8918363.508043986,8918363.437098335,8918363.36483249,8918363.291221889,8918363.216241507,8918363.139865858,8918363.062068982,8918362.982824432,8918362.902105274,8918362.81988407,8918362.736132873,8918362.650823217,8918362.563926103,8918362.47541199,8918362.385250801,8918362.293411886,8918362.199864026,8918362.104575433,8918362.007513713,8918361.908645874,8918361.807938315,8918361.70535681,8918361.600866487,8918361.49443184,8918361.386016686,8918361.27558418,8918361.163096795,8918361.048516294,8918360.93180373,8918360.812919449,8918360.69182304,8918360.568473343,8918360.442828443,8918360.314845635,8918360.18448143,8918360.05169152,8918359.916430775,8918359.778653232,8918359.63831207,8918359.495359588,8918359.349747214,8918359.201425463,8918359.05034393,8918358.896451274,8918358.739695199,8918358.58002244,8918358.41737874,8918358.251708828,8918358.082956405,8918357.911064137,8918357.735973611,8918357.557625337,8918357.375958707,8918357.190911999,8918357.002422335,8918356.810425676,8918356.614856783,8918356.41564921,8918356.212735275,8918356.00604603,8918355.795511264,8918355.581059437,8918355.362617696,8918355.14011183,8918354.913466241,8918354.682603935,8918354.44744648,8918354.207913997,8918353.963925105,8918353.715396922,8918353.462245028,8918353.204383422,8918352.94172452,8918352.67417909,8918352.401656263,8918352.124063471,8918351.84130642,8918351.553289069,8918351.259913605,8918350.961080369,8918350.656687876,8918350.346632745,8918350.03080967,8918349.709111392,8918349.381428672,8918349.047650209,8918348.707662672,8918348.361350596,8918348.008596385,8918347.649280258,8918347.283280196,8918346.910471927,8918346.530728871,8918346.143922087,8918345.749920242,8918345.348589571,8918344.939793816,8918344.523394194,8918344.099249337,8918343.667215256,8918343.227145296,8918342.778890066,8918342.322297405,8918341.85721233,8918341.383476976,8918340.900930552,8918340.409409272,8918339.908746324,8918339.398771789,8918338.879312592,8918338.35019245,8918337.81123181,8918337.262247782,8918336.703054085,8918336.133460974,8918335.553275188,8918334.962299889,8918334.360334562,8918333.747174995,8918333.122613175,8918332.486437237,8918331.838431377,8918331.17837579,8918330.5060466,8918329.821215771,8918329.123651046,8918328.413115844,8918327.68936922,8918326.952165738,8918326.20125543,8918325.436383676,8918324.657291153,8918323.86371372,8918323.055382334,8918322.232022978,8918321.393356547,8918320.539098771,8918319.6689601,8918318.782645632,8918317.879854992,8918316.960282244,8918316.02361578,8918315.06953822,8918314.097726317,8918313.107850818,8918312.099576386,8918311.072561469,8918310.026458196,8918308.960912244,8918307.875562748,8918306.770042142,8918305.643976076,8918304.496983264,8918303.32867536,8918302.138656838,8918300.926524855,8918299.69186911,8918298.434271723,8918297.153307073,8918295.848541675,8918294.519534029,8918293.165834468,8918291.78698502,8918290.382519241,8918288.95196207,8918287.494829662,8918286.010629248,8918284.49885893,8918282.959007561,8918281.390554547,8918279.792969672,8918278.165712949,8918276.508234404,8918274.819973921,8918273.10036105,8918271.348814797,8918269.564743476,8918267.747544464,8918265.896604035,8918264.01129714,8918262.090987219,8918260.135025958,8918258.142753102,8918256.113496242,8918254.046570562,8918251.941278642,8918249.79691022,8918247.612741945,8918245.388037156,8918243.12204564,8918240.814003369,8918238.463132262,8918236.068639925,8918233.629719391,8918231.145548861,8918228.615291433,8918226.038094811,8918223.413091045,8918220.739396257,8918218.016110316,8918215.242316578,8918212.417081574,8918209.539454691,8918206.608467907,8918203.623135418,8918200.582453359,8918197.485399455,8918194.330932708,8918191.117993051,8918187.84550099,8918184.512357278,8918181.117442552,8918177.659616949,8918174.137719786,8918170.550569128,8918166.896961452,8918163.175671246,8918159.385450598,8918155.525028821,8918151.593112022,8918147.58838272,8918143.50949938,8918139.35509602,8918135.123781754,8918130.814140363,8918126.42472983,8918121.954081899,8918117.40070158,8918112.763066707,8918108.039627437,8918103.228805747,8918098.32899497,8918093.33855926,8918088.25583309,8918083.079120712,8918077.80669564,8918072.436800104,8918066.967644498,8918061.397406813,8918055.724232083,8918049.946231801,8918044.061483312,8918038.068029255,8918031.963876914,8918025.74699763,8918019.41532616,8918012.96676005,8918006.399158966,8917999.710344074,8917992.898097333,8917985.960160837,8917978.894236121,8917971.697983466,8917964.369021172,8917956.904924853,8917949.303226696,8917941.561414702,8917933.676931962,8917925.64717585,8917917.469497269,8917909.141199848,8917900.659539139,8917892.021721806,8917883.22490478,8917874.266194437,8917865.14264573,8917855.851261327,8917846.388990737,8917836.752729408,8917826.93931782,8917816.945540579,8917806.768125473,8917796.40374253,8917785.849003065,8917775.10045869,8917764.154600356,8917753.007857323,8917741.656596169,8917730.097119741,8917718.325666133,8917706.33840762,8917694.131449588,8917681.700829456,8917669.042515552,8917656.15240604,8917643.026327757,8917629.660035092,8917616.049208814,8917602.189454913,8917588.07630341,8917573.705207145,8917559.071540575,8917544.17059855,8917528.997595048,8917513.547661923,8917497.815847648,8917481.797115989,8917465.486344762,8917448.878324447,8917431.967756912,8917414.749254033,8917397.217336344,8917379.366431681,8917361.190873768,8917342.684900826,8917323.842654161,8917304.658176739,8917285.125411732,8917265.238201078,8917244.990284003,8917224.37529555,8917203.38676508,8917182.018114777,8917160.262658136,8917138.113598416,8917115.564027123,8917092.606922466,8917069.235147793,8917045.44145001,8917021.218458032,8916996.5586812,8916971.454507649,8916945.898202766,8916919.88190754,8916893.39763698,8916866.43727848,8916838.992590204,8916811.055199463,8916782.616601093,8916753.668155806,8916724.201088581,8916694.206487024,8916663.675299726,8916632.59833465,8916600.966257505,8916568.769590115,8916535.9987088,8916502.643842783,8916468.695072565,8916434.142328357,8916398.975388467,8916363.183877762,8916326.75726609,8916289.684866736,8916251.95583492,8916213.559166273,8916174.483695349,8916134.718094181,8916094.250870822,8916053.07036795,8916011.164761476,8915968.52205921,8915925.130099513,8915880.976550043,8915836.0489065,8915790.334491426,8915743.820453044,8915696.493764145,8915648.341221046,8915599.349442538,8915549.504868975,8915498.793761358,8915447.202200506,8915394.716086285,8915341.321136924,8915287.00288837,8915231.74669376,8915175.537722949,8915118.36096213,8915060.201213531,8915001.043095233,8914940.871041063,8914879.669300599,8914817.421939272,8914754.112838622,8914689.725696584,8914624.244028008,8914557.651165215,8914489.930258747,8914421.064278208,8914351.036013315,8914279.828075014,8914207.422896836,8914133.802736374,8914058.949676927,8913982.845629351,8913905.472334053,8913826.811363226,8913746.844123233,8913665.551857233,8913582.915647991,8913498.916420933,8913413.534947421,8913326.751848266,8913238.547597457,8913148.90252621,8913057.79682719,8912965.210559094,8912871.123651441,8912775.515909677,8912678.367020596,8912579.656558014,8912479.363988837,8912377.468679363,8912273.949901987,8912168.786842214,8912061.958606055,8911953.444227751,8911843.22267789,8911731.272871919,8911617.573679006,8911502.103931371,8911384.84243397,8911265.767974632,8911144.859334636,8911022.095299704,8910897.454671487,8910770.91627948,8910642.458993413,8910512.061736194,8910379.703497224,8910245.363346323,8910109.02044813,8909970.654077021,8909830.24363256,8909687.768655507,8909543.208844362,8909396.544072464,8909247.75440568,8909096.820120633,8908943.721723542,8908788.439969638,8908630.955883164,8908471.250778,8908309.306278871,8908145.104343172,8907978.627283426,8907809.85779032,8907638.7789564,8907465.374300372,8907289.62779202,8907111.523877751,8906931.047506798,8906748.184157945,8906562.919867001,8906375.241254773,8906185.135555724,8905992.590647172,8905797.595079139,8905600.138104726,8905400.20971112,8905197.800651103,8904992.902475148,8904785.507564034,8904575.609161967,8904363.201410197,8904148.279381119,8903930.839112816,8903710.877644025,8903488.393049506,8903263.38447577,8903035.852177162,8902805.797552241,8902573.22318042,8902338.132858843,8902100.531639447,8901860.425866181,8901617.823212309,8901372.732717786,8901125.16482659,8900875.13142408,8900622.645874154,8900367.723056307,8900110.37940241,8899850.63293325,8899588.503294632,8899324.011793118,8899057.181431219,8898788.036942024,8898516.604823146,8898242.913369954,8897966.99270795,8897688.874824254,8897408.593598079,8897126.18483011,8896841.686270678,8896555.137646692,8896266.580687132,8895976.059147092,8895683.618830223,8895389.307609484,8895093.175446082,8894795.274406513,8894495.6586776,8894194.384579347,8893891.51057564,8893587.09728249,8893281.207473915,8892973.906085167,8892665.260213308,8892355.33911497,8892044.214201214,8891731.959029343,8891418.649291597,8891104.362800624,8890789.179471562,8890473.181300733,8890156.45234074,8889839.07867195,8889521.148370229,8889202.751470875,8888883.979928616,8888564.927573645,8888245.690063598,8887926.36483139,8887607.051028889,8887287.849466331,8886968.862547465,8886650.194200367,8886331.949803896,8886014.236109788,8885697.161160346,8885380.834201729,8885065.365592895,8884750.866710106,8884437.449847175,8884125.228111317,8883814.315314827,8883504.82586249,8883196.874634923,8882890.57686785,8882586.048027437,8882283.403681796,8881982.759368777,8881684.230460195,8881387.932022579,8881093.9786747,8880802.484441971,8880513.562607918,8880227.325562939,8879943.884650555,8879663.350011338,8879385.830424786,8879111.433149373,8878840.263761012,8878572.425990224,8878308.02155824,8878047.150012352,8877789.908560785,8877536.391907403,8877286.692086508,8877040.898298094,8876799.096743828,8876561.370464088,8876327.799176391,8876098.459115501,8875873.422875576,8875652.75925464,8875436.533101734,8875224.805167038,8875017.631955301,8874815.065582866,8874617.153638616,8874423.939049115,8874235.45994828,8874051.749551788,8873872.836036539,8873698.742425457,8873529.486477785,8873365.08058519,8873205.53167383,8873050.841112643,8872901.004627965,8872756.012224708,8872615.848114185,8872480.490648743,8872349.912263298,8872224.079423826,8872102.9525829,8871986.486142304,8871874.628422713,8871767.321640443,8871664.501891244,8871566.099141033,8871472.037223566,8871382.23384483,8871296.6005941,8871215.042961484,8871137.460361749,8871063.74616426,8870993.787728751,8870927.466446739,8870864.657788219,8870805.231353452,8870749.050929412,8870695.974550655,8870645.854564162,8870598.53769786,8870553.865132362,8870511.672575567,8870471.790339664,8870434.043420099,8870398.251576103,8870364.229412256,8870331.786460647,8870300.72726318,8870270.851453483,8870241.953837976,8870213.824475594,8870186.24875566,8870159.007473428,8870131.876902802,8870104.628865689,8870077.030797638,8870048.845809098,8870019.83274198,8869989.746220954,8869958.336699137,8869925.350497583,8869890.529838307,8869853.612870319,8869814.333688334,8869772.422343722,8869727.604847435,8869679.60316446,8869628.13519955,8869572.914773978,8869513.651592892,8869450.051203225,8869381.814941783,8869308.639873408,8869230.218719004,8869146.239773318,8869056.38681238,8868960.338990472,8868857.770726653,8868748.351580776,8868631.74611906,8868507.61376919,8868375.608665174,8868235.37948191,8868086.569259755,8867928.815219231,8867761.748566074,8867584.994286936,8867398.170936044,8867200.890413068,8866992.757732734,8866773.370786395,8866542.320096204,8866299.188562248,8866043.551203238,8865774.974891303,8865493.018081505,8865197.23053673,8864887.15304864,8864562.317155432,8864222.244857151,8863866.448329449,8863494.429636505,8863105.680444244,8862699.681734538,8862275.903521577,8861833.804571386,8861372.832125552,8860892.4216303,8860391.996472122,8859870.967721159,8859328.733883576,8858764.68066431,8858178.180741489,8857568.593553966,8856935.26510342,8856277.527772494,8855594.70016057,8854886.08693871,8854150.97872547,8853388.651985157,8852598.3689504,8851779.377570651,8850930.911488464,8850052.190045467,8849142.418319711,8848200.787196497,8847226.473474428,8846218.640008755,8845176.43589389,8844098.996687125,8842985.444675501,8841834.889187813,8840646.426953759,8839419.142512161,8838152.10867025,8836844.3870159,8835495.028484704,8834103.073983794,8832667.555074086,8831187.494712794,8829661.908057768,8828089.80333529,8826470.182772784,8824802.043597773,8823084.379104398,8821316.179788532,8819496.434552547,8817624.131980484,8815698.261684323,8813717.815721804,8811681.790086107,8809589.186267428,8807439.012886386,8805230.287398834,8802962.037871547,8800633.304827925,8798243.143162616,8795790.624123786,8793274.837361315,8790694.89303918,8788049.924009733,8785339.08804759,8782561.57014026,8779716.584832663,8776803.378622197,8773821.232400805,8770769.463940237,8767647.430416416,8764454.530968579,8761190.20928853,8757853.956235243,8754445.312469715,8750963.871104721,8747409.280364094,8743781.246245772,8740079.535182785,8736303.976696199,8732454.466033885,8728530.966788884,8724533.513490984,8720462.21416521,8716317.252850676,8712098.892073365,8707807.475266397,8703443.429131255,8699007.265933601,8694499.585727304,8689921.07850042,8685272.526236929,8680554.804888234,8675768.88624844,8670915.83972781,8665996.834018668,8661013.138648521,8655966.125415197,8650857.269699063,8645688.151647622,8640460.457228098,8635175.979143733,8629836.617609926,8624444.380986568,8619001.386263156,8613509.859393569,8607972.135477796,8602390.658787943,8596767.9826364,8591106.769084118,8585409.788487379,8579679.918881625,8573920.14520127,8568133.55833459,8562323.354013225,8556492.831535831,8550645.392326001,8544784.538324561,8538913.870216731,8533037.085494842,8527157.976357585,8521280.427446915,8515408.413424086,8509545.996386368,8503697.323126376,8497866.622235999,8492058.201057289,8486276.442482725,8480525.801607586,8474810.802237364,8469136.033253249,8463506.14483906,8457925.84457312,8452399.893388776,8446933.101407465,8441530.323648546,8436196.455620077,8430936.428795213,8425755.205978893,8420657.776569773,8415649.151722606,8410734.359416435,8405918.439434163,8401206.43825937,8396603.403896343,8392114.380619612,8387744.40365942,8383498.493829835,8379381.652106361,8375398.854160205,8371555.044856432,8367855.132723648,8364303.98440277,8360906.419082988,8357667.202932813,8354591.043534662,8351682.584331298,8348946.399092785,8346386.986412683,8344008.764242334,8341816.064472272,8339813.127569732,8338004.097281556,8336393.015411594,8334983.816681895,8333780.323687012,8332786.241950586,8332005.155093554,8331440.520123103,8331095.662851459,8330973.773453626,8331077.902172765,8331410.955182101,8331975.6906117555,8332774.714748863,8333810.478419047,8335085.273556965,8336601.229973569,8338360.312327045,8340364.317304447,8342614.871020285,8345113.426638226,8347861.262221416,8350859.478816599,8354108.998776732,8357610.564326225,8361364.736372474,8365371.893566837,8369632.231617642,8374145.762857178,8378912.316064233,8383931.536542916,8389202.886458144,8394725.645427419,8400498.911367986,8406521.601597907,8412792.454188816,8419310.02956783,8426072.71236516,8433078.713503648,8440326.072525773,8447812.66015312,8455536.181072824,8463494.176944856,8471684.029623738,8480102.964587487,8488748.054566447,8497616.223363973,8506704.249860704,8516008.772193685,8525526.292101257,8535253.17942439,8545185.676754687,8555319.904219266,8565651.864392178,8576177.44732219,8586892.435666315,8597792.509918438,8608873.253722407,8620130.159258675,8631558.632693764,8643153.999681704,8654911.510906667,8666826.347656112,8678893.62741381,8691108.40946232,8703465.700484537,8715960.460154232,8728587.606705647,8741342.022472456,8754218.559386669,8767212.044428378,8780317.28501741,8793529.07433849,8806842.19659166,8820251.432160102,8833751.562687993,8847337.376061227,8861003.67128437,8874745.263247514,8888556.98737717,8902433.704165693,8916370.303574208,8930361.709304392,8944402.882934839,8958488.827918248,8972614.593436003,8986775.278107177,9000966.033549404,9015182.067789346,9029418.648521101,9043671.106211096,9057934.837048339,9072205.305739613,9086478.048149042,9100748.673782254,9115012.86811544,9129266.39477003,9143505.097533928,9157724.902230667,9171921.818437982,9186091.941057678,9200231.4517388]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[8904673.488956796,8904673.470172482,8904673.451038599,8904673.431548635,8904673.411695968,8904673.391473847,8904673.370875401,8904673.34989362,8904673.328521375,8904673.306751398,8904673.284576284,8904673.2619885,8904673.238980364,8904673.215544054,8904673.191671599,8904673.16735489,8904673.142585648,8904673.117355464,8904673.091655748,8904673.065477777,8904673.038812643,8904673.011651276,8904672.983984448,8904672.95580275,8904672.927096603,8904672.897856247,8904672.86807174,8904672.837732956,8904672.806829581,8904672.775351109,8904672.743286837,8904672.71062586,8904672.677357085,8904672.643469188,8904672.608950661,8904672.57378976,8904672.53797453,8904672.501492804,8904672.464332173,8904672.426479997,8904672.387923427,8904672.34864933,8904672.308644371,8904672.26789495,8904672.226387205,8904672.18410703,8904672.141040051,8904672.097171621,8904672.052486831,8904672.006970495,8904671.960607132,8904671.91338098,8904671.86527599,8904671.816275802,8904671.766363759,8904671.715522902,8904671.663735932,8904671.610985255,8904671.557252942,8904671.502520708,8904671.446769966,8904671.389981754,8904671.332136769,8904671.273215344,8904671.213197451,8904671.15206269,8904671.08979028,8904671.026359042,8904670.961747423,8904670.895933457,8904670.828894762,8904670.760608561,8904670.691051632,8904670.620200343,8904670.548030594,8904670.474517863,8904670.399637152,8904670.32336302,8904670.245669523,8904670.16653026,8904670.085918328,8904670.003806328,8904669.920166347,8904669.834969947,8904669.748188183,8904669.659791544,8904669.569749983,8904669.478032902,8904669.384609118,8904669.289446875,8904669.192513835,8904669.093777038,8904668.993202934,8904668.890757332,8904668.786405414,8904668.680111706,8904668.571840085,8904668.461553745,8904668.349215208,8904668.234786281,8904668.118228082,8904667.999500984,8904667.87856464,8904667.75537795,8904667.629899038,8904667.502085261,8904667.37189318,8904667.23927854,8904667.10419628,8904666.966600483,8904666.826444384,8904666.683680357,8904666.538259877,8904666.390133524,8904666.23925096,8904666.085560901,8904665.929011129,8904665.769548425,8904665.607118614,8904665.44166649,8904665.273135822,8904665.101469351,8904664.926608726,8904664.748494536,8904664.567066249,8904664.382262219,8904664.194019644,8904664.002274552,8904663.806961805,8904663.608015021,8904663.4053666,8904663.198947692,8904662.988688156,8904662.77451654,8904662.556360086,8904662.334144665,8904662.107794773,8904661.877233505,8904661.642382529,8904661.40316205,8904661.159490798,8904660.91128598,8904660.658463271,8904660.400936779,8904660.13861902,8904659.871420871,8904659.599251563,8904659.322018633,8904659.039627904,8904658.751983449,8904658.458987553,8904658.160540687,8904657.856541477,8904657.546886655,8904657.231471047,8904656.910187503,8904656.582926912,8904656.249578107,8904655.910027867,8904655.56416087,8904655.211859656,8904654.853004556,8904654.487473704,8904654.115142966,8904653.735885886,8904653.34957368,8904652.956075154,8904652.555256678,8904652.146982154,8904651.731112937,8904651.30750782,8904650.876022972,8904650.436511874,8904649.988825304,8904649.532811267,8904649.068314932,8904648.595178599,8904648.113241645,8904647.62234046,8904647.122308392,8904646.612975694,8904646.094169468,8904645.565713616,8904645.027428756,8904644.479132177,8904643.920637785,8904643.351756025,8904642.77229382,8904642.182054507,8904641.580837786,8904640.968439627,8904640.344652219,8904639.709263884,8904639.062059028,8904638.40281804,8904637.731317263,8904637.047328852,8904636.350620758,8904635.64095663,8904634.918095704,8904634.18179278,8904633.431798097,8904632.66785726,8904631.889711153,8904631.097095858,8904630.289742555,8904629.467377452,8904628.629721677,8904627.776491169,8904626.907396615,8904626.022143336,8904625.120431194,8904624.201954473,8904623.266401807,8904622.313456047,8904621.342794169,8904620.35408716,8904619.346999917,8904618.321191123,8904617.276313134,8904616.212011864,8904615.12792668,8904614.023690257,8904612.898928465,8904611.753260255,8904610.586297518,8904609.397644946,8904608.186899932,8904606.953652395,8904605.697484681,8904604.417971393,8904603.114679264,8904601.787167005,8904600.43498517,8904599.057675995,8904597.654773239,8904596.225802045,8904594.77027877,8904593.287710821,8904591.777596505,8904590.239424847,8904588.672675434,8904587.076818213,8904585.451313356,8904583.795611046,8904582.109151311,8904580.391363839,8904578.641667766,8904576.859471515,8904575.044172572,8904573.195157308,8904571.311800756,8904569.393466413,8904567.439506039,8904565.449259415,8904563.422054151,8904561.357205456,8904559.254015889,8904557.111775167,8904554.929759894,8904552.707233356,8904550.443445232,8904548.137631407,8904545.789013665,8904543.39679945,8904540.96018162,8904538.47833816,8904535.950431928,8904533.37561037,8904530.75300523,8904528.081732292,8904525.360891066,8904522.589564495,8904519.766818669,8904516.8917025,8904513.963247418,8904510.980467066,8904507.942356952,8904504.847894149,8904501.696036926,8904498.485724451,8904495.215876402,8904491.885392657,8904488.49315289,8904485.038016245,8904481.518820949,8904477.93438393,8904474.28350045,8904470.564943686,8904466.777464362,8904462.919790326,8904458.990626153,8904454.988652706,8904450.912526732,8904446.760880422,8904442.53232095,8904438.225430064,8904433.838763589,8904429.370850999,8904424.82019492,8904420.185270656,8904415.464525715,8904410.656379294,8904405.759221785,8904400.771414258,8904395.691287935,8904390.517143674,8904385.247251417,8904379.879849639,8904374.413144791,8904368.845310748,8904363.17448821,8904357.398784118,8904351.516271088,8904345.524986763,8904339.422933215,8904333.208076326,8904326.878345141,8904320.431631211,8904313.865787975,8904307.17863003,8904300.367932502,8904293.431430336,8904286.366817592,8904279.171746733,8904271.843827903,8904264.380628197,8904256.779670885,8904249.038434686,8904241.154352978,8904233.124813007,8904224.94715511,8904216.618671881,8904208.13660737,8904199.498156225,8904190.70046287,8904181.740620626,8904172.615670828,8904163.32260196,8904153.858348737,8904144.219791187,8904134.403753735,8904124.407004243,8904114.226253064,8904103.858152065,8904093.29929363,8904082.54620968,8904071.59537064,8904060.443184406,8904049.085995317,8904037.520083064,8904025.741661647,8904013.746878255,8904001.531812172,8903989.092473645,8903976.42480276,8903963.524668269,8903950.387866426,8903937.010119807,8903923.387076097,8903909.514306862,8903895.387306346,8903881.00149018,8903866.352194136,8903851.43467284,8903836.244098453,8903820.775559379,8903805.02405891,8903788.984513871,8903772.651753278,8903756.020516926,8903739.085453996,8903721.841121625,8903704.281983502,8903686.402408374,8903668.196668621,8903649.65893872,8903630.783293793,8903611.563708048,8903591.99405329,8903572.06809731,8903551.77950238,8903531.12182362,8903510.08850744,8903488.672889892,8903466.868195066,8903444.66753344,8903422.063900199,8903399.050173607,8903375.619113276,8903351.763358483,8903327.475426452,8903302.747710636,8903277.572478956,8903251.941872071,8903225.84790159,8903199.282448307,8903172.237260418,8903144.703951718,8903116.673999796,8903088.138744207,8903059.089384666,8903029.516979214,8902999.412442362,8902968.76654326,8902937.569903845,8902905.81299699,8902873.486144636,8902840.579515938,8902807.083125405,8902772.98683104,8902738.280332465,8902702.953169085,8902666.994718218,8902630.394193245,8902593.140641771,8902555.222943792,8902516.629809849,8902477.349779233,8902437.37121817,8902396.682318034,8902355.271093568,8902313.125381136,8902270.232836979,8902226.580935508,8902182.15696761,8902136.948038975,8902090.941068493,8902044.122786602,8901996.47973377,8901947.998258913,8901898.664517941,8901848.464472285,8901797.383887496,8901745.408331878,8901692.523175193,8901638.713587416,8901583.964537501,8901528.260792302,8901471.586915456,8901413.927266434,8901355.265999572,8901295.587063257,8901234.87419914,8901173.110941483,8901110.280616544,8901046.366342103,8900981.351027066,8900915.21737118,8900847.947864864,8900779.524789147,8900709.93021572,8900639.146007158,8900567.153817218,8900493.935091322,8900419.471067164,8900343.742775472,8900266.731040936,8900188.416483304,8900108.779518645,8900027.800360799,8899945.459022995,8899861.73531972,8899776.60886871,8899690.059093243,8899602.065224577,8899512.606304666,8899421.661189107,8899329.208550302,8899235.226880945,8899139.69449769,8899042.589545166,8898943.890000248,8898843.573676633,8898741.6182297,8898638.001161758,8898532.699827516,8898425.691440003,8898316.95307675,8898206.46168641,8898094.194095701,8897980.127016762,8897864.237054901,8897746.500716768,8897626.894418932,8897505.39449692,8897381.977214688,8897256.618774578,8897129.29532774,8896999.982985048,8896868.657828527,8896735.295923298,8896599.873330057,8896462.366118113,8896322.750378972,8896181.002240514,8896037.097881781,8895891.013548296,8895742.725568125,8895592.210368466,8895439.444492945,8895284.404619593,8895127.067579444,8894967.410375899,8894805.410204744,8894641.044474948,8894474.290830135,8894305.127170881,8894133.53167774,8893959.482835026,8893782.95945546,8893603.940705573,8893422.40613194,8893238.335688258,8893051.709763255,8892862.509209486,8892670.715372942,8892476.310123596,8892279.2758868,8892079.595675588,8891877.253123887,8891672.232520636,8891464.518844817,8891254.097801426,8891040.955858368,8890825.080284247,8890606.459187165,8890385.08155438,8890160.937292933,8889934.017271224,8889704.313361483,8889471.818483178,8889236.526647365,8888998.433001926,8888757.533877693,8888513.82683552,8888267.31071417,8888017.9856791,8887765.853272077,8887510.916461641,8887253.179694338,8886992.64894675,8886729.331778293,8886463.237384712,8886194.37665229,8885922.762212712,8885648.408498537,8885371.331799278,8885091.550318,8884809.084228434,8884523.95573251,8884236.189118322,8883945.810818406,8883652.84946831,8883357.335965404,8883059.303527813,8882758.787753487,8882455.826679254,8882150.460839849,8881842.733326789,8881532.689847076,8881220.378781566,8880905.851242982,8880589.16113345,8880270.365201473,8879949.523098262,8879626.697433267,8879301.953828886,8878975.360974193,8878646.990677575,8878316.917918213,8877985.220896235,8877651.981081462,8877317.283260625,8876981.215582913,8876643.869603768,8876305.340326732,8875965.72624334,8875625.129370771,8875283.655287296,8874941.413165273,8874598.515801612,8874255.079645583,8873911.224823836,8873567.075162461,8873222.758206056,8872878.405233571,8872534.1512709,8872190.135100009,8871846.49926458,8871503.390071955,8871160.957591357,8870819.35564822,8870478.74181454,8870139.277395196,8869801.127410049,8869464.460571826,8869129.449259682,8868796.269488333,8868465.100872727,8868136.12658821,8867809.533326088,8867485.511244575,8867164.253915126,8866845.958264064,8866530.824509552,8866219.056093868,8865910.85961105,8865606.44472985,8865306.024112126,8865009.813326636,8864718.030758368,8864430.89751341,8864148.637319505,8863871.476422371,8863599.64347786,8863333.369440163,8863072.88744612,8862818.432695858,8862570.242329886,8862328.555302817,8862093.612253984,8861865.655375065,8861644.928275013,8861431.675842503,8861226.144106124,8861028.580092624,8860839.23168343,8860658.34746977,8860486.176606653,8860322.96866606,8860168.9734896,8860024.44104101,8859889.621258805,8859764.763909405,8859650.118441105,8859545.933839226,8859452.45848279,8859369.940003112,8859298.625144608,8859238.759628244,8859190.588017916,8859154.353590192,8859130.2982077,8859118.662196564,8859119.684228197,8859133.601205833,8859160.648156071,8859201.05812581,8859255.062084856,8859322.8888345,8859404.7649224,8859500.914563943,8859611.559570497,8859736.919284647,8859877.210522749,8860032.647524957,8860203.441912936,8860389.802655447,8860591.936041914,8860810.04566415,8861044.332406325,8861294.994443275,8861562.227247197,8861846.223602813,8862147.173630968,8862465.264820704,8862800.682069745,8863153.607733356,8863524.221681507,8863912.701364212,8864319.221884957,8864743.956082033,8865187.07461762,8865648.746074446,8866129.13705975,8866628.412316397,8867146.73484081,8867684.266007476,8868241.165699769,8868817.59244668,8869413.703565247,8870029.65530824,8870665.603016797,8871321.701277632,8871998.10408441,8872694.965002932,8873412.43733968,8874150.674313372,8874909.829229046,8875690.05565429,8876491.507597191,8877314.339685574,8878158.707347078,8879024.766989663,8879912.676182155,8880822.59383433,8881754.680376193,8882709.097936004,8883686.010516668,8884685.584170071,8885707.987169016,8886753.390176356,8887821.96641098,8888913.891810305,8890029.345188959,8891168.508393293,8892331.566451503,8893518.707719013,8894730.124018895,8895966.010777064,8897226.567152068,8898511.996159205,8899822.504788859,8901158.304118842,8902519.609420637,8903906.64025942,8905319.620587759,8906758.778832939,8908224.34797785,8909716.565635424,8911235.674116608,8912781.920491911,8914355.556646539,8915956.83932922,8917586.030194756,8919243.395840455,8920929.207836531,8922643.742750632,8924387.28216667,8926160.112698093,8927962.525995875,8929794.818751333,8931657.292694109,8933550.25458547,8935474.01620725,8937428.894346682,8939415.210777368,8941433.292236743,8943483.470400281,8945566.081852783,8947681.468057029,8949829.975320173,8952011.954758123,8954227.76225831,8956477.75844116,8958762.308620535,8961081.782763569,8963436.555450173,8965827.005832521,8968253.517594896,8970716.478914136,8973216.282421065,8975753.325163161,8978328.008568771,8980940.738413157,8983591.924786655,8986281.982065199,8989011.328883477,8991780.388110925,8994589.586830858,8997439.356322842,9000330.132048627,9003262.353641713,9006236.46490082,9009252.913787324,9012312.152426869,9015414.637115227,9018560.82832856,9021751.190738121,9024986.193229513,9028266.308926553,9031592.015219754,9034963.793799529,9038382.130694024,9041847.516311685,9045360.445488453,9048921.417539632,9052530.936316334,9056189.51026645,9059897.652500112,9063655.880859533,9067464.717993114,9071324.691433782,9075236.333681349,9079200.182288853,9083216.779952673,9087286.674606333,9091410.419517778,9095588.573390016,9099821.70046492,9104110.370629998,9108455.159527974,9112856.648668969,9117315.425545067,9121832.083747052,9126407.223083112,9131041.449699266,9135735.376201274,9140489.621777797,9145304.81232453,9150181.580569053,9155120.56619614,9160122.41597319,9165187.783875562,9170317.331211409,9175511.726745764,9180771.64682352,9186097.775490923,9191490.804615255,9196951.434002304,9202480.37151123,9208078.333166385,9213746.043265704,9219484.234485146,9225293.647978766,9231175.033473874,9237129.149360802,9243156.762776695,9249258.649682788,9255435.594934562,9261688.39234416,9268017.844734423,9274424.763983887,9280909.97106201,9287474.296053959,9294118.578174142,9300843.665767774,9307650.416299634,9314539.696329182,9321512.381471226,9328569.356341196,9335711.514484195,9342939.758286845,9350254.99887102,9357658.155968487,9365150.157775486,9372731.940786237,9380404.44960439,9388168.636731358,9396025.462330556,9403975.893966462,9412020.9063175,9420161.480861692,9428398.605534034,9436733.274354607,9445166.487026377,9453699.248501692,9462332.568516517,9471067.461091412,9479904.94399838,9488846.038192613,9497891.767208388,9507043.156518156,9516301.232854197,9525667.02349202,9535141.555494893,9544725.85491891,9554420.945978027,9564227.850168655,9574147.585353397,9584181.164803628,9594329.596200747,9604593.88059593,9614975.011328401,9625473.97290229,9636091.739822282,9646829.275388334,9657687.53044991,9668667.442120252,9679769.93245135,9690995.907070437,9702346.25377888,9713821.84111459,9725423.51687907,9737152.106630528,9749008.412144458,9760993.209843352,9773107.249197317,9785351.25109745,9797725.906204117,9810231.873272287,9822869.777456263,9835640.208596323,9848543.719489895,9861580.824150046,9874751.996054174,9888057.666385943,9901498.222273635,9915074.005028201,9928785.308384402,9942632.376748566,9956615.403456554,9970734.529045688,9984989.839544373,9999381.36478334,10013909.076732436,10028572.887866955,10043372.649567565,10058308.15055792,10073379.115384074,10088585.202939807,10103926.005042,10119401.045060182,10135009.776604317,10150751.582274891,10166625.772479326,10182631.584318597,10198768.18054797,10215034.648615558,10231429.999782398,10247953.168327555,10264603.010841653,10281378.305612044,10298277.752102792,10315299.970532246,10332443.501551038,10349706.80602296,10367088.264911046,10384586.179270932,10402198.77035336,10419924.179817444,10437760.470056005,10455705.624634126,10473757.548841741,10491914.070360776,10510172.940047212,10528531.832827957,10546988.348712318,10565540.013917478,10584184.282107078,10602918.535741812,10621740.087540543,10640646.182050236,10659633.99732269,10678700.646695819,10697843.180676859,10717058.588924754,10736343.802328587,10755695.69517875,10775111.087427303,10794586.747033669,10814119.392391747,10833705.694834119,10853342.281209018,10873025.736525457,10892752.606661722,10912519.40113238,10932322.595908757,10952158.6362877,10972023.939803423,10991914.899177054,11011827.885298477,11031759.250235058,11051705.330261681,11071662.448906647,11091626.920007905,11111595.050774084,11131563.14484489,11151527.505345419,11171484.43792907,11191430.25380371,11211361.272735959,11231273.826028464,11251164.259465234,11271028.936220178,11290864.239724157,11310666.57648605,11330432.378863404,11350158.107778521,11369840.25537601,11389475.34761787,11409059.946812628,11428590.654075045,11448064.11171319,11467477.005539974,11486826.067106288,11506108.075853312,11525319.861181583,11544458.304434836,11563520.340796713,11582502.961098691,11601403.213537905,11620218.205303593,11638945.104111275,11657581.139643924,11676123.604899555,11694569.857444976,11712917.32057556,11731163.484381141,11749305.90671833,11767342.2140897,11785270.102430502,11803087.337803751,11820791.75700467,11838381.268075628,11855853.850732885,11873207.556706604,11890440.509995643,11907550.907038951],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[8353289.889858452,8353289.891097515,8353289.892359639,8353289.893645251,8353289.894954793,8353289.896288708,8353289.897647449,8353289.899031481,8353289.90044127,8353289.9018773,8353289.903340057,8353289.904830038,8353289.906347753,8353289.907893713,8353289.909468451,8353289.911072497,8353289.912706397,8353289.914370709,8353289.916065998,8353289.917792843,8353289.919551824,8353289.921343547,8353289.923168618,8353289.925027659,8353289.926921304,8353289.928850193,8353289.930814981,8353289.932816345,8353289.934854957,8353289.936931517,8353289.939046727,8353289.941201305,8353289.943395986,8353289.945631523,8353289.947908665,8353289.950228195,8353289.9525909005,8353289.954997581,8353289.957449059,8353289.9599461695,8353289.96248976,8353289.965080696,8353289.967719858,8353289.970408146,8353289.973146472,8353289.975935773,8353289.978776992,8353289.981671099,8353289.984619076,8353289.987621929,8353289.990680677,8353289.993796362,8353289.9969700435,8353290.000202804,8353290.003495739,8353290.006849971,8353290.010266645,8353290.013746917,8353290.017291977,8353290.020903027,8353290.024581297,8353290.028328042,8353290.032144531,8353290.036032067,8353290.039991972,8353290.044025594,8353290.048134305,8353290.052319503,8353290.056582615,8353290.060925085,8353290.065348401,8353290.06985406,8353290.074443597,8353290.079118579,8353290.083880591,8353290.088731258,8353290.093672231,8353290.098705187,8353290.103831845,8353290.109053949,8353290.114373274,8353290.119791633,8353290.125310869,8353290.130932861,8353290.1366595235,8353290.142492808,8353290.148434697,8353290.154487215,8353290.160652424,8353290.166932421,8353290.173329344,8353290.179845374,8353290.186482727,8353290.193243663,8353290.200130488,8353290.20714554,8353290.214291217,8353290.221569944,8353290.228984205,8353290.236536525,8353290.244229473,8353290.252065673,8353290.26004779,8353290.268178545,8353290.276460706,8353290.284897099,8353290.2934905905,8353290.302244114,8353290.311160649,8353290.320243236,8353290.329494963,8353290.338918987,8353290.34851852,8353290.35829683,8353290.368257248,8353290.378403171,8353290.388738057,8353290.399265424,8353290.409988865,8353290.42091203,8353290.432038645,8353290.4433725,8353290.454917463,8353290.466677462,8353290.478656513,8353290.490858697,8353290.503288171,8353290.5159491785,8353290.528846035,8353290.541983134,8353290.555364959,8353290.568996074,8353290.582881125,8353290.59702485,8353290.611432068,8353290.626107702,8353290.641056751,8353290.656284316,8353290.671795595,8353290.687595877,8353290.703690554,8353290.720085117,8353290.736785162,8353290.753796388,8353290.771124602,8353290.788775715,8353290.806755754,8353290.825070858,8353290.843727274,8353290.862731379,8353290.882089658,8353290.90180872,8353290.921895301,8353290.942356261,8353290.963198588,8353290.984429401,8353291.006055953,8353291.028085634,8353291.050525966,8353291.07338462,8353291.0966694085,8353291.120388287,8353291.144549365,8353291.169160898,8353291.194231302,8353291.219769148,8353291.24578317,8353291.272282259,8353291.299275482,8353291.326772071,8353291.35478143,8353291.383313145,8353291.412376972,8353291.4419828635,8353291.472140946,8353291.502861548,8353291.534155179,8353291.566032555,8353291.598504592,8353291.631582408,8353291.665277336,8353291.699600912,8353291.734564899,8353291.770181277,8353291.806462249,8353291.843420246,8353291.881067944,8353291.919418247,8353291.9584843,8353291.998279511,8353292.038817518,8353292.080112233,8353292.1221778225,8353292.165028725,8353292.208679645,8353292.253145568,8353292.298441757,8353292.344583773,8353292.3915874595,8353292.439468966,8353292.488244743,8353292.537931553,8353292.588546474,8353292.640106908,8353292.692630586,8353292.746135572,8353292.800640268,8353292.856163434,8353292.9127241755,8353292.970341964,8353293.029036636,8353293.088828408,8353293.149737878,8353293.211786025,8353293.274994243,8353293.339384314,8353293.404978444,8353293.471799259,8353293.539869806,8353293.609213576,8353293.679854508,8353293.75181699,8353293.825125877,8353293.899806495,8353293.975884646,8353294.053386636,8353294.13233926,8353294.212769824,8353294.294706159,8353294.378176622,8353294.4632101115,8353294.549836079,8353294.638084532,8353294.727986055,8353294.819571818,8353294.912873582,8353295.007923714,8353295.104755202,8353295.203401668,8353295.303897373,8353295.406277234,8353295.510576836,8353295.616832448,8353295.725081036,8353295.835360269,8353295.947708543,8353296.062164991,8353296.178769495,8353296.297562712,8353296.41858607,8353296.541881795,8353296.667492937,8353296.79546337,8353296.925837805,8353297.058661827,8353297.193981895,8353297.331845369,8353297.47230052,8353297.615396555,8353297.761183634,8353297.909712887,8353298.061036436,8353298.215207413,8353298.372279978,8353298.532309349,8353298.695351815,8353298.861464756,8353299.030706668,8353299.203137203,8353299.378817149,8353299.5578085,8353299.740174453,8353299.925979443,8353300.11528916,8353300.308170587,8353300.5046920115,8353300.704923064,8353300.908934746,8353301.116799447,8353301.328590977,8353301.544384607,8353301.764257086,8353301.988286678,8353302.216553189,8353302.449138002,8353302.6861241115,8353302.927596156,8353303.173640443,8353303.424345002,8353303.679799601,8353303.940095801,8353304.205326979,8353304.475588375,8353304.750977127,8353305.03159231,8353305.317534986,8353305.608908235,8353305.905817197,8353306.208369136,8353306.516673449,8353306.830841754,8353307.150987902,8353307.477228044,8353307.809680676,8353308.148466688,8353308.493709414,8353308.845534692,8353309.204070907,8353309.569449061,8353309.941802814,8353310.321268551,8353310.707985445,8353311.102095506,8353311.503743653,8353311.913077774,8353312.330248795,8353312.755410744,8353313.18872081,8353313.63033944,8353314.080430381,8353314.539160771,8353315.006701211,8353315.483225844,8353315.968912425,8353316.46394242,8353316.968501068,8353317.482777487,8353318.006964744,8353318.541259965,8353319.085864397,8353319.640983536,8353320.206827199,8353320.783609641,8353321.371549637,8353321.970870607,8353322.581800707,8353323.204572946,8353323.839425305,8353324.486600833,8353325.146347794,8353325.818919755,8353326.504575738,8353327.203580339,8353327.916203856,8353328.642722423,8353329.383418159,8353330.138579297,8353330.908500337,8353331.693482192,8353332.493832348,8353333.309865014,8353334.141901289,8353334.990269321,8353335.855304493,8353336.737349575,8353337.636754933,8353338.553878681,8353339.489086901,8353340.442753823,8353341.415262018,8353342.407002627,8353343.41837555,8353344.449789669,8353345.501663085,8353346.574423327,8353347.668507603,8353348.7843630295,8353349.922446892,8353351.083226902,8353352.267181441,8353353.474799853,8353354.706582713,8353355.96304211,8353357.244701946,8353358.552098232,8353359.8857794115,8353361.246306656,8353362.634254221,8353364.050209758,8353365.494774688,8353366.96856453,8353368.472209292,8353370.006353835,8353371.571658265,8353373.168798338,8353374.798465856,8353376.461369109,8353378.158233297,8353379.88980098,8353381.656832544,8353383.460106662,8353385.300420796,8353387.178591688,8353389.095455883,8353391.051870251,8353393.048712545,8353395.0868819505,8353397.167299675,8353399.290909534,8353401.4586785715,8353403.671597687,8353405.930682283,8353408.23697293,8353410.5915360665,8353412.995464691,8353415.4498791015,8353417.955927637,8353420.514787451,8353423.127665311,8353425.795798401,8353428.520455186,8353431.302936254,8353434.14457522,8353437.046739638,8353440.010831949,8353443.03829045,8353446.130590292,8353449.289244514,8353452.515805098,8353455.811864064,8353459.179054583,8353462.619052145,8353466.133575735,8353469.724389067,8353473.393301838,8353477.142171019,8353480.972902195,8353484.887450934,8353488.887824206,8353492.976081819,8353497.154337935,8353501.424762591,8353505.789583284,8353510.251086617,8353514.811619946,8353519.47359312,8353524.2394802505,8353529.111821539,8353534.093225135,8353539.186369088,8353544.394003312,8353549.718951632,8353555.164113884,8353560.732468061,8353566.427072543,8353572.251068358,8353578.207681549,8353584.300225556,8353590.532103704,8353596.906811744,8353603.427940466,8353610.099178379,8353616.92431447,8353623.907241041,8353631.051956609,8353638.362568902,8353645.843297929,8353653.498479114,8353661.332566548,8353669.350136293,8353677.555889792,8353685.954657369,8353694.55140181,8353703.351222031,8353712.359356865,8353721.581188922,8353731.022248554,8353740.688217915,8353750.584935135,8353760.718398585,8353771.09477124,8353781.720385173,8353792.601746129,8353803.745538221,8353815.158628742,8353826.84807307,8353838.821119711,8353851.085215437,8353863.648010542,8353876.51736423,8353889.701350102,8353903.20826177,8353917.0466186,8353931.225171546,8353945.752909151,8353960.639063616,8353975.893117042,8353991.5248077465,8354007.54413674,8354023.961374303,8354040.787066688,8354058.03204294,8354075.707421847,8354093.8246189905,8354112.395353933,8354131.431657499,8354150.945879188,8354170.950694693,8354191.459113518,8354212.4844867075,8354234.040514686,8354256.141255185,8354278.801131259,8354302.034939411,8354325.857857782,8354350.285454434,8354375.333695698,8354401.018954589,8354427.358019282,8354454.368101654,8354482.066845833,8354510.472336825,8354539.603109141,8354569.478155432,8354600.116935169,8354631.5393832605,8354663.765918691,8354696.817453109,8354730.715399381,8354765.481680063,8354801.138735808,8354837.709533691,8354875.217575391,8354913.686905265,8354953.142118278,8354993.608367733,8355035.11137284,8355077.67742604,8355121.333400125,8355166.106755044,8355212.02554448,8355259.118422043,8355307.414647166,8355356.944090594,8355407.737239473,8355459.825202004,8355513.239711609,8355568.0131305875,8355624.178453237,8355681.769308364,8355740.819961197,8355801.365314614,8355863.440909657,8355927.0829253085,8355992.328177471,8356059.214117079,8356127.778827345,8356198.06102005,8356270.10003084,8356343.935813486,8356419.608933049,8356497.160557875,8356576.632450416,8356658.06695674,8356741.506994776,8356826.99604112,8356914.578116456,8357004.2977694245,8357096.200058976,8357190.3305350635,8357286.73521767,8357385.460574073,8357486.553494312,8357590.061264751,8357696.031539731,8357804.512311206,8357915.551876308,8358029.198802767,8358145.501892187,8358264.510141014,8358386.2726992145,8358510.8388265865,8358638.257846625,8358768.5790979,8358901.851882912,8359038.125414332,8359177.448758607,8359319.870776875,8359465.440063154,8359614.204879749,8359766.21308984,8359921.51208725,8360080.148723326,8360242.169230905,8360407.619145414,8360576.543222978,8360748.985355639,8360924.988483598,8361104.594504551,8361287.8441800615,8361474.7770390455,8361665.4312783545,8361859.843660497,8362058.049408573,8362260.0820983695,8362465.973547822,8362675.753703754,8362889.45052608,8363107.089869477,8363328.695362679,8363554.288285448,8363783.887443352,8364017.509040477,8364255.166550187,8364496.870584091,8364742.628759347,8364992.445564504,8365246.322223999,8365504.256561547,8365766.242862584,8366032.2717359755,8366302.329975215,8366576.400419325,8366854.461813694,8367136.488671105,8367422.451133215,8367712.314832712,8368006.040756478,8368303.585109979,8368604.899183217,8368909.929218541,8369218.616280534,8369530.896128448,8369846.69909132,8370165.94994623,8370488.567799955,8370814.465974346,8371143.551895813,8371475.726989172,8371810.886576267,8372148.919779625,8372489.709431553,8372833.131988953,8373179.057454191,8373527.349302391,8373877.864415394,8374230.453022792,8374584.958650267,8374941.218075566,8375299.0612924285,8375658.31148269,8376018.784996907,8376380.291343673,8376742.633187986,8377105.6063587805,8377468.999865931,8377832.595926898,8378196.170003189,8378559.490846836,8378922.320557017,8379284.414646963,8379645.52212126,8380005.385563649,8380363.741235383,8380720.319184207,8381074.843363982,8381427.031764973,8381776.5965547655,8382123.244229806,8382466.675777489,8382806.586848708,8383142.667940794,8383474.60459068,8383802.077578188,8384124.763139225,8384442.333188742,8384754.455553226,8385060.79421249,8385361.0095505165,8385654.758615103,8385941.69538601,8386221.471051265,8386493.734291412,8386758.131571236,8387014.3074387135,8387261.904830773,8387500.565385495,8387729.929760392,8387949.637956307,8388159.329646599,8388358.64451113,8388547.222574666,8388724.704549262,8388890.732180182,8389044.948594918,8389186.998654887,8389316.529309344,8389433.189951086,8389536.632773506,8389626.513128538,8389702.48988512,8389764.225787682,8389811.387814289,8389843.64753399,8389860.681462992,8389862.171419278,8389847.80487522,8389817.275307894,8389770.28254668,8389706.533117818,8389625.740585566,8389527.625889657,8389411.91767875,8389278.35263954,8389126.675821291,8388956.640955534,8388768.010770611,8388560.5573009215,8388334.062190628,8388088.316991566,8387823.123455266,8387538.293818856,8387233.651084717,8386909.029293766,8386564.273792214,8386199.241491765,8385813.801123073,8385407.833482501,8384981.231671993,8384533.901332156,8384065.760868408,8383576.74167024,8383066.788323581,8382535.858816248,8381983.924736552,8381410.971465036,8380816.998359425,8380202.01893284,8379566.061025323,8378909.166968741,8378231.3937451625,8377532.813138785,8376813.511881492,8376073.591792164,8375313.169909792,8374532.378620584,8373731.365779067,8372910.294823405,8372069.344884953,8371208.710892225,8370328.603669365,8369429.250029253,8368510.892861355,8367573.79121442,8366618.220374184,8365644.471936154,8364652.853873601,8363643.690600887,8362617.323032214,8361574.10863594,8360514.421484511,8359438.652300186,8358347.208496605,8357240.514216311,8356119.010364357,8354983.1546380455,8353833.421552947,8352670.302465251,8351494.305590589,8350305.956019367,8349105.795728769,8347894.383591472,8346672.295381195,8345440.1237752,8344198.478353782,8342947.98559696,8341689.288878352,8340423.048456459,8339149.9414634,8337870.661891281,8336585.920576276,8335296.44518061,8334002.980172551,8332706.286804593,8331407.143089989,8330106.343777812,8328804.700326691,8327503.040877522,8326202.21022521,8324903.069789807,8323606.49758718,8322313.3881994765,8321024.652745669,8319741.218852373,8318464.030625311,8317194.048621589,8315932.249823186,8314679.627611872,8313437.191745926,8312205.968338937,8310986.999840999,8309781.345022662,8308590.078961918,8307414.293034591,8306255.094908418,8305113.608541174,8303990.974183113,8302888.348384095,8301806.904005597,8300747.830238016,8299712.332623368,8298701.633083804,8297716.969955994,8296759.598031694,8295830.78860461,8294931.829523682,8294064.025252915,8293228.696937787,8292427.182478268,8291660.836608378,8290931.0309822615,8290239.154266578,8289586.612239064,8288974.827892975,8288405.24154711,8287879.310961056,8287398.511455144,8286964.336034656,8286578.295517637,8286241.918665678,8285956.752316879,8285724.361520182,8285546.3296701675,8285424.258641286,8285359.768920466,8285354.499736928,8285410.109187945,8285528.274359217,8285710.691438419,8285959.075820466,8286275.162202834,8286660.704669331,8287117.476760521,8287647.271529001,8288251.901577636,8288933.19907873,8289693.015772179,8290533.222940415,8291455.711358043,8292462.391213925,8293555.192003476,8294736.06238883,8296006.970024577,8297369.90134668,8298826.861322178,8300379.873157288,8302030.977961485,8303782.234365141,8305635.718088343,8307593.521458489,8309657.752874334,8311830.536214168,8314114.010185825,8316510.327616386,8319021.654679376,8321650.1700574355,8324398.064038478,8327267.537543487,8330260.801084166,8333380.073648865,8336627.581515219,8340005.556988201,8343516.237062377,8347161.86200732,8350944.6738753645,8354866.914931005,8358930.826001514,8363138.644748504,8367492.6038604,8371994.929166027,8376647.837669755,8381453.535508848,8386414.215833977,8391532.056614092,8396809.218367081,8402247.841817996,8407850.045486806,8413617.923208019,8419553.541584706,8425658.937379856,8431936.114848118,8438387.043011507,8445013.652882675,8451817.834639858,8458801.434757777,8465966.25309904,8473314.039970944,8480846.493152788,8488565.254899042,8496471.908924026,8504567.977373969,8512854.917792503,8521334.120085899,8530006.903494572,8538874.513577469,8547938.119216215,8557198.809646022,8566657.591520373,8576315.386016803,8586173.025990987,8596231.2531865,8606490.715507668,8616951.964362867,8627615.452085646,8638481.529441055,8649550.443224417,8660822.333959732,8672297.233704798,8683975.063969927,8695855.633757051,8707938.637725702,8720223.654492252,8732710.145068418,8745397.451444883,8758284.795325467,8771371.277017048,8784655.874480082,8798137.442544075,8811814.712292183,8825686.290618528,8839750.659961438,8854006.178215507,8868451.078824664,8883083.471058236,8897901.34047133,8912902.549550451,8928084.8385448,8943445.826483095,8958983.012375413,8974693.776598884,8990575.382465726,9006624.977971511,9022839.597721074,9039216.165029055,9055751.494191537,9072442.29292481,9089285.164966833,9106276.612836538,9123413.040745722,9140690.757657856,9158105.980487797,9175654.83743599,9193333.37145046,9211137.543809585,9229063.237818316,9247106.262610352,9265262.357048394,9283527.1937146,9301896.382983007,9320365.477165643,9338929.974723969,9357585.324537028,9376326.930217884,9395150.15446963,9414050.323472438,9433022.731293075,9452062.644308321,9471165.305633891,9490325.939550443,9509539.75591854,9528801.954574427,9548107.729698716,9567452.274150321,9586830.783758083,9606238.461562837,9625670.52200293,9645122.195036346,9664588.73019306,9684065.400551312,9703547.506632015,9723030.380205642,9742509.388006393,9761979.935348619,9781437.469641043,9800877.483794387,9820295.519518575,9839687.170505911,9859048.085497003,9878373.971226564,9897660.595246537,9916903.788624352,9936099.448514463,9955243.540601589,9974332.101414505,9993361.240509443,10012327.142522557,10031226.069091152,10050054.360643694,10068808.438058887,10087484.804194415,10106080.04528614,10124590.83221886,10143013.92166994,10161346.157127328,10179584.469783798,10197725.87930928,10215767.494503478,10233706.51383113,10251540.225842316,10269266.009480568],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[14825992.369331619,14825992.39211549,14825992.415323373,14825992.438963158,14825992.463042878,14825992.487570727,14825992.512555042,14825992.53800431,14825992.5639272,14825992.59033251,14825992.617229229,14825992.644626493,14825992.672533626,14825992.70096011,14825992.729915615,14825992.759409983,14825992.789453242,14825992.820055608,14825992.851227485,14825992.882979471,14825992.915322367,14825992.948267162,14825992.98182506,14825993.016007476,14825993.050826028,14825993.086292552,14825993.12241911,14825993.159217982,14825993.196701687,14825993.234882958,14825993.273774792,14825993.313390398,14825993.353743251,14825993.394847065,14825993.436715836,14825993.47936378,14825993.522805396,14825993.567055468,14825993.61212903,14825993.658041416,14825993.704808226,14825993.752445363,14825993.800969027,14825993.850395726,14825993.900742242,14825993.952025713,14825994.004263565,14825994.057473563,14825994.111673793,14825994.166882697,14825994.223119034,14825994.280401917,14825994.338750834,14825994.398185628,14825994.458726494,14825994.520394025,14825994.583209187,14825994.64719333,14825994.712368216,14825994.778756002,14825994.846379254,14825994.915260969,14825994.985424569,14825995.056893907,14825995.129693279,14825995.203847444,14825995.279381603,14825995.356321447,14825995.434693135,14825995.514523303,14825995.595839104,14825995.678668179,14825995.763038687,14825995.848979322,14825995.936519302,14825996.025688378,14825996.116516877,14825996.209035678,14825996.303276237,14825996.3992706,14825996.497051394,14825996.596651867,14825996.698105883,14825996.801447941,14825996.906713167,14825997.013937356,14825997.123156954,14825997.234409109,14825997.347731635,14825997.463163061,14825997.580742639,14825997.700510336,14825997.822506877,14825997.946773732,14825998.073353162,14825998.20228819,14825998.333622655,14825998.467401214,14825998.603669344,14825998.742473375,14825998.8838605,14825999.027878784,14825999.174577193,14825999.324005602,14825999.476214813,14825999.631256575,14825999.789183598,14825999.95004957,14826000.113909189,14826000.280818153,14826000.450833218,14826000.62401218,14826000.800413918,14826000.9800984,14826001.163126715,14826001.349561093,14826001.539464908,14826001.73290273,14826001.929940313,14826002.130644659,14826002.335083976,14826002.543327797,14826002.755446894,14826002.971513389,14826003.191600746,14826003.415783772,14826003.644138692,14826003.876743132,14826004.113676172,14826004.355018359,14826004.600851739,14826004.851259885,14826005.106327936,14826005.366142586,14826005.630792176,14826005.900366664,14826006.174957704,14826006.454658635,14826006.739564544,14826007.029772291,14826007.325380523,14826007.62648974,14826007.9332023,14826008.245622473,14826008.563856458,14826008.88801244,14826009.218200613,14826009.554533228,14826009.89712461,14826010.246091224,14826010.601551691,14826010.963626854,14826011.332439788,14826011.708115868,14826012.090782803,14826012.480570665,14826012.87761196,14826013.28204165,14826013.693997215,14826014.113618689,14826014.541048704,14826014.976432564,14826015.419918258,14826015.871656535,14826016.331800949,14826016.800507912,14826017.277936745,14826017.76424973,14826018.259612164,14826018.764192432,14826019.27816204,14826019.80169569,14826020.33497133,14826020.878170224,14826021.431476995,14826021.995079724,14826022.569169963,14826023.15394285,14826023.749597134,14826024.356335282,14826024.974363502,14826025.603891859,14826026.245134313,14826026.89830881,14826027.563637352,14826028.241346054,14826028.931665255,14826029.634829579,14826030.351077998,14826031.08065394,14826031.823805356,14826032.580784813,14826033.35184957,14826034.13726168,14826034.937288057,14826035.752200592,14826036.582276218,14826037.427797034,14826038.289050369,14826039.166328914,14826040.059930786,14826040.970159657,14826041.897324838,14826042.841741405,14826043.803730275,14826044.783618346,14826045.781738592,14826046.798430175,14826047.834038567,14826048.888915664,14826049.963419905,14826051.057916395,14826052.172777016,14826053.308380583,14826054.46511294,14826055.643367115,14826056.843543436,14826058.066049682,14826059.311301198,14826060.579721065,14826061.87174023,14826063.187797634,14826064.5283404,14826065.893823951,14826067.284712177,14826068.7014776,14826070.144601509,14826071.614574157,14826073.11189489,14826074.63707236,14826076.19062465,14826077.773079487,14826079.384974405,14826081.026856922,14826082.699284727,14826084.402825892,14826086.138059044,14826087.90557354,14826089.70596971,14826091.539859032,14826093.40786435,14826095.310620084,14826097.248772437,14826099.22297962,14826101.233912071,14826103.28225269,14826105.36869707,14826107.493953709,14826109.658744285,14826111.863803867,14826114.109881196,14826116.397738911,14826118.728153829,14826121.101917174,14826123.51983488,14826125.982727855,14826128.491432253,14826131.046799744,14826133.649697823,14826136.3010101,14826139.001636583,14826141.752493998,14826144.554516092,14826147.408653956,14826150.315876339,14826153.277169956,14826156.29353986,14826159.366009764,14826162.495622374,14826165.683439754,14826168.930543685,14826172.238036025,14826175.607039088,14826179.038696004,14826182.534171132,14826186.094650421,14826189.721341848,14826193.415475786,14826197.178305443,14826201.011107283,14826204.915181438,14826208.891852168,14826212.942468304,14826217.06840368,14826221.271057628,14826225.551855413,14826229.912248744,14826234.35371626,14826238.877763998,14826243.485925924,14826248.179764453,14826252.960870963,14826257.830866322,14826262.791401457,14826267.844157888,14826272.990848292,14826278.233217098,14826283.573041052,14826289.01212982,14826294.552326612,14826300.195508761,14826305.943588395,14826311.798513053,14826317.762266349,14826323.836868625,14826330.024377638,14826336.326889254,14826342.746538132,14826349.285498463,14826355.945984658,14826362.730252156,14826369.640598085,14826376.679362128,14826383.848927237,14826391.151720451,14826398.590213716,14826406.166924689,14826413.88441761,14826421.74530412,14826429.752244163,14826437.90794686,14826446.215171402,14826454.676727995,14826463.29547877,14826472.074338753,14826481.016276836,14826490.124316743,14826499.401538061,14826508.851077262,14826518.476128722,14826528.279945806,14826538.265841931,14826548.437191688,14826558.797431933,14826569.35006295,14826580.098649593,14826591.046822483,14826602.198279206,14826613.556785513,14826625.126176616,14826636.910358392,14826648.913308732,14826661.139078816,14826673.591794468,14826686.27565751,14826699.194947148,14826712.354021396,14826725.757318467,14826739.409358291,14826753.314743968,14826767.478163289,14826781.904390253,14826796.598286698,14826811.564803809,14826826.808983807,14826842.335961571,14826858.150966333,14826874.25932337,14826890.666455766,14826907.377886163,14826924.399238588,14826941.736240253,14826959.394723462,14826977.380627463,14826995.700000426,14827014.359001374,14827033.363902217,14827052.721089737,14827072.43706772,14827092.518459015,14827112.97200769,14827133.804581225,14827155.023172723,14827176.634903155,14827198.647023667,14827221.066917917,14827243.902104441,14827267.160239078,14827290.849117426,14827314.97667732,14827339.551001424,14827364.580319762,14827390.07301238,14827416.037612015,14827442.482806804,14827469.417443069,14827496.850528112,14827524.79123309,14827553.248895913,14827582.233024208,14827611.753298318,14827641.819574375,14827672.441887392,14827703.63045443,14827735.39567783,14827767.748148428,14827800.698648958,14827834.258157331,14827868.43785015,14827903.249106143,14827938.703509737,14827974.81285465,14828011.589147547,14828049.044611786,14828087.19169116,14828126.043053778,14828165.611595951,14828205.910446156,14828246.952969078,14828288.7527697,14828331.323697459,14828374.679850483,14828418.835579885,14828463.80549411,14828509.604463369,14828556.247624159,14828603.750383802,14828652.128425097,14828701.397711052,14828751.574489618,14828802.675298609,14828854.716970582,14828907.716637872,14828961.69173767,14829016.660017174,14829072.639538836,14829129.64868566,14829187.706166612,14829246.831022097,14829307.042629475,14829368.360708753,14829430.805328256,14829494.396910436,14829559.15623776,14829625.10445869,14829692.26309369,14829760.654041408,14829830.299584862,14829901.222397782,14829973.445550956,14830046.992518784,14830121.88718577,14830198.153853223,14830275.817246016,14830354.90251939,14830435.435265893,14830517.441522421,14830600.947777277,14830685.980977435,14830772.568535764,14830860.738338463,14830950.51875249,14831041.938633166,14831135.02733181,14831229.814703492,14831326.331114877,14831424.607452145,14831524.67512904,14831626.56609495,14831730.312843172,14831835.94841913,14831943.506428832,14832053.021047326,14832164.52702725,14832278.059707524,14832393.655022064,14832511.349508638,14832631.18031777,14832753.18522177,14832877.40262379,14833003.871567037,14833132.631743995,14833263.723505797,14833397.187871618,14833533.066538189,14833671.401889352,14833812.23700573,14833955.615674434,14834101.582398867,14834250.182408568,14834401.461669153,14834555.466892313,14834712.24554586,14834871.845863832,14835034.316856686,14835199.708321493,14835368.07085224,14835539.455850134,14835713.915533958,14835891.502950495,14836072.271984976,14836256.277371505,14836443.574703615,14836634.220444763,14836828.271938883,14837025.787420943,14837226.826027514,14837431.447807355,14837639.713731974,14837851.685706198,14838067.426578758,14838287.00015278,14838510.471196352,14838737.905452993,14838969.369652111,14839204.931519415,14839444.65978728,14839688.624205047,14839936.8955493,14840189.545634009,14840446.647320649,14840708.274528211,14840974.502243118,14841245.406529058,14841521.064536678,14841801.554513214,14842086.955811914,14842377.348901404,14842672.81537487,14842973.437959107,14843279.300523382,14843590.488088142,14843907.086833559,14844229.18410786,14844556.868435465,14844890.229524918,14845229.35827662,14845574.346790304,14845925.28837231,14846282.27754257,14846645.410041394,14847014.782835945,14847390.494126465,14847772.643352218,14848161.331197161,14848556.659595279,14848958.731735649,14849367.652067168,14849783.52630298,14850206.461424585,14850636.565685567,14851073.948615046,14851518.721020734,14851970.994991686,14852430.883900678,14852898.502406206,14853373.966454186,14853857.393279267,14854348.901405761,14854848.610648269,14855356.642111894,14855873.118192177,14856398.162574602,14856931.900233852,14857474.457432661,14858025.96172037,14858586.541931195,14859156.328182163,14859735.451870807,14860324.045672555,14860922.243537901,14861530.18068936,14862147.993618205,14862775.820081057,14863413.799096316,14864062.070940517,14864720.777144559,14865390.06048998,14866070.06500519,14866760.935961775,14867462.819870912,14868175.86447995,14868900.21876922,14869636.032949096,14870383.458457444,14871142.647957463,14871913.755336007,14872696.935702547,14873492.345388712,14874300.141948616,14875120.484160041,14875953.532026548,14876799.446780639,14877658.390888074,14878530.528053511,14879416.023227487,14880315.042614987,14881227.753685648,14882154.325185793,14883094.927152421,14884049.730929308,14885018.909185385,14886002.635935575,14887001.08656423,14888014.437851379,14889042.868001983,14890086.556678353,14891145.685035946,14892220.435762808,14893310.99312279,14894417.54300279,14895540.27296433,14896679.372299533,14897835.032091938,14899007.445282258,14900196.806739405,14901403.313337013,14902627.164035765,14903868.559971746,14905127.704551158,14906404.803551594,14907700.065230304,14909013.700439582,14910345.922749689,14911696.948579572,14913066.997335702,14914456.291559273,14915865.057082238,14917293.523192288,14918741.922807232,14920210.492659088,14921699.473488139,14923209.110247321,14924739.65231728,14926291.353732381,14927864.473417973,14929459.275439344,14931076.029262502,14932715.010027258,14934376.498832844,14936060.783036364,14937768.156564424,14939498.920238221,14941253.382112363,14943031.857827727,14944834.670978636,14946662.153494628,14948514.646037057,14950392.498410776,14952296.06999124,14954225.73016709,14956181.858798655,14958164.846692378,14960175.096091537,14962213.021183359,14964279.048622701,14966373.618072579,14968497.182761474,14970650.21005783,14972833.18206162,14975046.596213272,14977290.965919916,14979566.821199127,14981874.70934017,14984215.195582818,14986588.86381377,14988996.31728068,14991438.179323815,14993915.094125278,14996427.727475796,14998976.76755904,15001562.92575335,15004186.937450811,15006849.562893573,15009551.58802731,15012293.825371621,15015077.114907283,15017902.324980123,15020770.353221366,15023682.127484208,15026638.606796399,15029640.782328589,15032689.67837818,15035786.353368368,15038931.900862137,15042127.450590795,15045374.169496775,15048673.262790376,15052025.975019919,15055433.591155145,15058897.437683204,15062418.883717002,15065999.342115276,15069640.270614075,15073343.17296904,15077109.600108005,15080941.151293365,15084839.475293687,15088806.271563873,15092843.291433398,15096952.339301875,15101135.273841321,15105394.009204455,15109730.516238222,15114146.823701926,15118645.019489054,15123227.25185205,15127895.730629196,15132652.728472656,15137500.582076829,15142441.693406012,15147478.53092039,15152613.630799307,15157849.598160695,15163189.10827565,15168634.907776792,15174189.81585936,15179856.725473665,15185638.604507534,15191538.496957486,15197559.524087071,15203704.885570899,15209977.860622806,15216381.809106521,15222920.172627075,15229596.475601273,15236414.326305283,15243377.41789753,15250489.529414758,15257754.526739355,15265176.363535654,15272759.082153024,15280506.814493477,15288423.782841275,15296514.300652161,15304782.773299588,15313233.698775234,15321871.668341136,15330701.367130497,15339727.574694322,15348955.165490761,15358389.109314075,15368034.471660042,15377896.414024433,15387980.19413123,15398291.166087044,15408834.780458242,15419616.584267056,15430642.220903032,15441917.42994594,15453448.046896376,15465240.002809983,15477299.323831419,15489632.130623987,15502244.637690786,15515143.152583294,15528334.074993191,15541823.89572326,15555619.195533104,15569726.64385556,15584152.997379499,15598905.098494967,15613989.87359639,15629414.331239872,15645185.56015043,15661310.727075333,15677797.07447959,15694651.918079851,15711882.644213118,15729496.70703678,15747501.625556605,15765904.980479626,15784714.410888935,15803937.610737646,15823582.32515967,15843656.346594973,15864167.51072743,15885123.692233752,15906532.800342059,15928402.774199279,15950741.578046735,15973557.196203807,15996857.62785984,16020650.88167504,16044944.970191453,16069747.90405558,16095067.68605479,16120912.304970039,16147289.72924808,16174207.900496732,16201674.726807423,16229698.07590979,16258285.768163584,16287445.569393829,16317185.183575615,16347512.245375725,16378434.31255854,16409958.858264547,16442093.263170298,16474844.807538988,16508220.663171796,16542227.885270288,16576873.404220995,16612164.017313657,16648106.380405132,16684706.99954149,16721972.222551242,16759908.230623003,16798521.02988146,16837816.442975562,16877800.100693565,16918477.433619417,16959853.663845573,17001933.796757296,17044722.612903766,17088224.65997124,17132444.24487394,17177385.425977904,17223052.005473234,17269447.521910165,17316575.242913935,17364438.15809354,17413038.97215906,17462380.098261904,17512463.65157211,17563291.44310629,17614864.97381953,17667185.42897386,17720253.672795504,17774070.24343254,17828635.34822395,17883948.8592903,17940010.309455812,17996818.88851057,18054373.439821146,18112672.457296915,18171714.08271867,18231496.10343528,18292015.950433306,18353270.696783595,18415257.056468148,18477971.38358951,18541409.67196436,18605567.555101722,18670440.306565847,18736022.840722542,18802309.71386726,18869295.12573224,18936972.92136932,19005336.59340423,19074379.284657523,19144093.791126452,19214472.565321807,19285507.71995245,19357191.031950463,19429513.946828637,19502467.583361793,19576042.73858312,19650229.893085767,19725019.21662006,19800400.5739761,19876363.531141113,19952897.361720994,20029991.053614747,20107633.315930836,20185812.586133983,20264517.03741087,20343734.586243447,20423452.900177885,20503659.405777838,20584341.296750333,20665485.542232707,20747078.895229213,20829107.901185855,20911558.9066923,20994418.068299633,21077671.361443274,21161304.589460067,21245303.39268918,21329653.25764632,21414339.52626132,21499347.405169133,21584661.9750445,21670268.199971024,21756150.936835304,21842294.944737382,21928684.89440865,22015305.37762882,22102140.916633703,22189175.973505877,22276394.959540322,22363782.24457768,22451322.16629757,22538999.03946498,22626797.165122725,22714700.839723162,22802694.36419274,22890762.052922785,22978888.24268036,23067057.30143314,23155253.637082163,23243461.706097003,23331666.02204719,23419851.16402475,23508001.784952145,23596102.619770385,23684138.493502136,23772094.32918454,23859955.15566695,23947706.11526844,24035332.471290447,24122819.61537982,24210153.07473754,24297318.519168682,24384301.767969307,24471088.796645783,24557665.743462622,24644018.915814485,24730134.796418704,24816000.049324293,24901601.525733855,24986926.269634992,25071961.52323762,25156694.73221414,25241113.55073937,25325205.846327316,25408959.70446224,25492363.43302139,25575405.56648712,25658074.869946375,25740360.342875607,25822251.22270953,25903736.98819222,25984807.362509437,26065452.31620118,26145662.069853757,26225427.096571114,26304738.12422487,26383586.137483504,26461962.379620776,26539858.35410406,26617265.825963542,26694176.82294319,26770583.63643506,26846478.822198577,26921855.200866647,26996705.858240798,27071024.14537779,27144803.67847035,27218038.338524997,27290722.27084014,27362849.884287715,27434415.850402158,27505415.102280505,27575842.833297495,27645694.495640278,27714965.79866683,27783652.707092904,27851751.439012196,27919258.46375475,27986170.499588665,28052484.511270355,28118197.70744869,28183307.537928514,28247811.690799072,28311708.089433085,28374994.889362115,28437670.475034066,28499733.45645861,28561182.665746506,28622017.153548717,28682236.18540111,28741839.23798088,28800825.99528045,28859196.344704818,28916950.3730983,28974088.362706322,29030610.78707818,29086518.3069165,29141811.765878934,29196492.186337825,29250560.765103176,29304018.869114462,29356868.03110644,29409109.945254266,29460746.462802947,29511779.58768602,29562211.472138386,29612044.412307885,29661280.843870275,29709923.337651953,29757974.595264744,29805437.444756806,29852314.83628373,29898609.83780358,29944325.630799647,29989465.506034255,30034032.859337203,30078031.187431872,30121464.083802126,30164335.234602958,30206648.414617438,30248407.483262792,30289616.38064782,30330279.123684064,30370399.802252863,30409982.575430177,30449031.66777111,30487551.36565574,30525546.013698],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[12711102.390889684,12711102.304839898,12711102.217188727,12711102.127906388,12711102.036962533,12711101.944326203,12711101.849965919,12711101.7538496,12711101.655944575,12711101.55621755,12711101.45463461,12711101.351161223,12711101.245762223,12711101.138401767,12711101.02904334,12711100.917649772,12711100.804183194,12711100.68860501,12711100.570875954,12711100.45095596,12711100.328804305,12711100.204379395,12711100.07763899,12711099.948539972,12711099.817038452,12711099.683089698,12711099.546648208,12711099.407667583,12711099.266100539,12711099.12189899,12711098.97501387,12711098.825395267,12711098.672992304,12711098.517753158,12711098.359625079,12711098.198554263,12711098.03448598,12711097.867364427,12711097.697132831,12711097.52373325,12711097.347106785,12711097.167193351,12711096.983931798,12711096.79725983,12711096.60711396,12711096.413429528,12711096.216140758,12711096.015180498,12711095.810480459,12711095.601971047,12711095.389581375,12711095.173239222,12711094.95287105,12711094.728401959,12711094.499755574,12711094.266854217,12711094.029618692,12711093.787968341,12711093.541821012,12711093.291093014,12711093.035699135,12711092.775552513,12711092.510564731,12711092.240645684,12711091.965703622,12711091.68564508,12711091.400374815,12711091.109795893,12711090.813809494,12711090.512314992,12711090.20520992,12711089.892389841,12711089.57374843,12711089.24917735,12711088.91856626,12711088.581802782,12711088.238772424,12711087.889358565,12711087.533442425,12711087.170903001,12711086.801617056,12711086.425459048,12711086.042301105,12711085.65201298,12711085.254461963,12711084.849512951,12711084.437028259,12711084.016867682,12711083.588888366,12711083.152944833,12711082.708888907,12711082.256569618,12711081.795833193,12711081.326523032,12711080.848479599,12711080.36154039,12711079.865539864,12711079.360309444,12711078.845677348,12711078.321468685,12711077.787505236,12711077.243605494,12711076.68958457,12711076.125254162,12711075.550422432,12711074.964893982,12711074.36846978,12711073.760947103,12711073.142119419,12711072.511776438,12711071.869703855,12711071.215683457,12711070.54949289,12711069.870905751,12711069.179691376,12711068.475614844,12711067.758436825,12711067.027913548,12711066.283796724,12711065.525833437,12711064.75376606,12711063.967332166,12711063.166264478,12711062.350290747,12711061.519133609,12711060.672510555,12711059.81013389,12711058.931710497,12711058.03694183,12711057.12552378,12711056.197146593,12711055.251494767,12711054.288246898,12711053.307075623,12711052.307647495,12711051.289622858,12711050.252655735,12711049.196393734,12711048.120477896,12711047.02454257,12711045.908215337,12711044.771116821,12711043.612860631,12711042.43305315,12711041.231293451,12711040.007173177,12711038.760276334,12711037.490179231,12711036.19645028,12711034.878649874,12711033.53633024,12711032.169035235,12711030.776300304,12711029.357652185,12711027.912608877,12711026.44067938,12711024.94136357,12711023.414152043,12711021.858525889,12711020.273956591,12711018.659905786,12711017.015825072,12711015.341155926,12711013.635329379,12711011.897765892,12711010.127875188,12711008.325056,12711006.488695836,12711004.618170902,12711002.712845769,12711000.772073178,12710998.795193877,12710996.781536326,12710994.730416514,12710992.641137736,12710990.512990316,12710988.345251406,12710986.137184713,12710983.888040252,12710981.597054128,12710979.263448223,12710976.886429997,12710974.465192119,12710971.998912334,12710969.48675305,12710966.927861108,12710964.321367571,12710961.666387282,12710958.962018665,12710956.207343396,12710953.401426097,12710950.543314,12710947.632036638,12710944.666605541,12710941.646013856,12710938.569236023,12710935.435227456,12710932.242924143,12710928.991242358,12710925.679078171,12710922.305307208,12710918.868784212,12710915.36834266,12710911.80279433,12710908.170928974,12710904.471513875,12710900.703293396,12710896.86498864,12710892.955296913,12710888.972891388,12710884.916420562,12710880.784507882,12710876.575751206,12710872.288722439,12710867.921966901,12710863.474002963,12710858.94332148,12710854.3283853,12710849.627628783,12710844.8394572,12710839.962246269,12710834.994341534,12710829.934057873,12710824.779678915,12710819.529456418,12710814.181609744,12710808.734325228,12710803.185755543,12710797.534019135,12710791.777199555,12710785.913344797,12710779.940466685,12710773.856540198,12710767.659502748,12710761.347253496,12710754.917652743,12710748.36852102,12710741.697638592,12710734.902744478,12710727.981535876,12710720.931667285,12710713.75074976,12710706.436350115,12710698.98599004,12710691.397145415,12710683.667245291,12710675.793671124,12710667.77375593,12710659.604783308,12710651.283986581,12710642.808547823,12710634.175597021,12710625.382210966,12710616.42541244,12710607.302169025,12710598.009392299,12710588.543936685,12710578.90259835,12710569.082114302,12710559.079161154,12710548.890354065,12710538.512245627,12710527.941324687,12710517.174015202,12710506.206675034,12710495.03559475,12710483.656996362,12710472.067032082,12710460.261783067,12710448.237258064,12710435.989392139,12710423.514045289,12710410.807001095,12710397.863965293,12710384.680564377,12710371.252344102,12710357.574768106,12710343.643216299,12710329.452983364,12710314.999277221,12710300.277217517,12710285.281833785,12710270.008064074,12710254.450753076,12710238.604650512,12710222.464409431,12710206.024584351,12710189.279629534,12710172.223897126,12710154.851635374,12710137.156986628,12710119.133985426,12710100.776556652,12710082.078513432,12710063.033555085,12710043.635265216,12710023.877109364,12710003.752433125,12709983.254459787,12709962.376288228,12709941.110890538,12709919.451109877,12709897.389658066,12709874.919113163,12709852.031917134,12709828.72037334,12709804.976644048,12709780.792747902,12709756.160557318,12709731.071795847,12709705.51803548,12709679.490693977,12709652.98103202,12709625.980150416,12709598.478987293,12709570.468315037,12709541.938737465,12709512.880686678,12709483.284420123,12709453.140017323,12709422.4373768,12709391.166212793,12709359.316051958,12709326.876230044,12709293.835888533,12709260.183971051,12709225.90921997,12709191.00017279,12709155.445158461,12709119.23229373,12709082.349479387,12709044.784396335,12709006.524501871,12708967.55702554,12708927.868965266,12708887.447083192,12708846.27790151,12708804.347698273,12708761.642503079,12708718.148092726,12708673.849986726,12708628.733442841,12708582.78345249,12708535.984736077,12708488.321738249,12708439.778623097,12708390.3392693,12708339.987265054,12708288.705903145,12708236.47817575,12708183.286769249,12708129.114058884,12708073.942103483,12708017.75263988,12707960.527077455,12707902.246492466,12707842.891622297,12707782.442859689,12707720.880246816,12707658.183469323,12707594.331850167,12707529.304343512,12707463.079528432,12707395.635602534,12707326.950375495,12707257.001262495,12707185.765277587,12707113.219026906,12707039.338701822,12706964.100072017,12706887.47847833,12706809.448825706,12706729.985575845,12706649.06273988,12706566.653870882,12706482.732056279,12706397.269910133,12706310.23956542,12706221.612666054,12706131.360358922,12706039.45328571,12705945.861574702,12705850.554832464,12705753.50213528,12705654.672020707,12705554.03247884,12705451.5509435,12705347.194283364,12705240.92879289,12705132.720183263,12705022.533573067,12704910.333478976,12704796.08380625,12704679.74783911,12704561.288231123,12704440.666995328,12704317.845494265,12704192.784429984,12704065.443833824,12703935.783056276,12703803.76075639,12703669.334891442,12703532.462706273,12703393.100722494,12703251.204727827,12703106.72976499,12702959.630120762,12702809.859314831,12702657.370088411,12702502.114393173,12702344.043379413,12702183.107384808,12702019.25592262,12701852.437669968,12701682.600456027,12701509.691250045,12701333.656149374,12701154.44036739,12700971.988221228,12700786.243119624,12700597.147550534,12700404.643068762,12700208.670283457,12700009.16884564,12699806.077435605,12699599.333750203,12699388.874490323,12699174.635348035,12698956.55099385,12698734.555064,12698508.580147538,12698278.55777352,12698044.418398257,12697806.091392323,12697563.505027864,12697316.58646555,12697065.261742037,12696809.455756977,12696549.092260357,12696284.09383976,12696014.381907817,12695739.876689453,12695460.497209605,12695176.161280658,12694886.785490198,12694592.285188818,12694292.574478028,12693987.56619835,12693677.171917513,12693361.301918907,12693039.865190046,12692712.769411413,12692379.920945423,12692041.224825624,12691696.584746167,12691345.903051488,12690989.080726402,12690626.017386207,12690256.611267569,12689880.75921926,12689498.356693646,12689109.297738286,12688713.47498815,12688310.77965804,12687901.101535743,12687484.328975402,12687060.348891556,12686629.046753598,12686190.306580972,12685744.010938697,12685290.040933777,12684828.276211992,12684358.594955586,12683880.873881578,12683394.988240771,12682900.811817555,12682398.216930592,12681887.07443425,12681367.25372092,12680838.622724347,12680301.047923757,12679754.39434914,12679198.525587432,12678633.303789841,12678058.589680245,12677474.2425648,12676880.12034268,12676276.079518069,12675661.975213528,12675037.661184566,12674402.98983568,12673757.812237814,12673101.978147201,12672435.336025909,12671757.733063813,12671069.015202153,12670369.027159037,12669657.612456322,12668934.613448566,12668199.87135365,12667453.226285273,12666694.51728767,12665923.582371894,12665140.258554505,12664344.38189821,12663535.787554773,12662714.309810016,12661879.782131178,12661032.037216697,12660170.907048212,12659296.222945191,12658407.815621898,12657505.515247101,12656589.151506275,12655658.55366655,12654713.550644366,12653753.971075907,12652779.643390423,12651790.395886367,12650786.056810569,12649766.45444038,12648731.417168772,12647680.773592666,12646614.352604302,12645531.983485881,12644433.496007284,12643318.720527247,12642187.488097662,12641039.630571341,12639874.980713053,12638693.372314049,12637494.640309898,12636278.62090192,12635045.151681865,12633794.07176035,12632525.221898519,12631238.444643328,12629933.584466336,12628610.487905921,12627269.003713084,12625908.983000541,12624530.279395534,12623132.749195721,12621716.251528876,12620280.648515532,12618825.805435224,12617351.59089585,12615857.877006344,12614344.539552348,12612811.458175056,12611258.516552998,12609685.602586707,12608092.608586235,12606479.431461278,12604845.972913926,12603192.139633847,12601517.843495814,12599823.001759341,12598107.537270403,12596371.378664978,12594614.460574312,12592836.723831633,12591038.115680266,12589218.589982701,12587378.107430806,12585516.635756485,12583634.149942875,12581730.632435841,12579806.073355203,12577860.470705913,12575893.83058839,12573906.167408232,12571897.50408461,12569867.872257287,12567817.312491788,12565745.87448266,12563653.6172541,12561540.609358044,12559406.929069044,12557252.664575776,12555077.914168766,12552882.786423918,12550667.400381612,12548431.88572089,12546176.382928355,12543901.043461492,12541606.029905979,12539291.516126586,12536957.687411334,12534604.740608504,12532232.884256124,12529842.33870353,12527433.336224737,12525006.121123075,12522560.949826904,12520098.09097597,12517617.82549802,12515120.446675435,12512606.260201514,12510075.584226051,12507528.74938998,12504966.098848727,12502387.988284146,12499794.785904571,12497186.872432863,12494564.641082402,12491928.49752046,12489278.85981918,12486616.158393674,12483940.835927442,12481253.347284734,12478554.159409953,12475843.75121401,12473122.613447644,12470391.248561667,12467650.170554273,12464899.904805403,12462140.98789834,12459373.967428656,12456599.401800696,12453817.860011809,12451029.92142458,12448236.175527323,12445437.221683174,12442633.668868117,12439826.135398347,12437015.248647315,12434201.64475302,12431385.968315901,12428568.872087933,12425751.016653406,12422933.070101999,12420115.707694719,12417299.611523308,12414485.470163863,12411673.978325175,12408865.836492645,12406061.75056838,12403262.431508273,12400468.594956702,12397680.96087977,12394900.253197666,12392127.199417109,12389362.530264474,12386606.979320584,12383861.28265773,12381126.178479923,12378402.406766996,12375690.708923398,12372991.82743244,12370306.505516712,12367635.486805413,12364979.515009314,12362339.33360406,12359715.685522398,12357109.312856084,12354520.95656797,12351951.356214914,12349401.249682045,12346871.37292882,12344362.45974746,12341875.241534062,12339410.447072843,12336968.802333841,12334551.030284312,12332157.850714156,12329789.980075447,12327448.1313363,12325133.013849126,12322845.333233317,12320585.791272342,12318355.085825179,12316153.910751997,12313982.955853874,12311842.90682635,12309734.445226531,12307658.248453412,12305614.989741066,12303605.338164244,12301629.958655924,12299689.512036296,12297784.65505263,12295916.040429419,12294084.316928107,12292290.12941583,12290534.11894233,12288816.9228244,12287139.174737033,12285501.50481045,12283904.539732268,12282348.902853847,12280835.21430007,12279364.091081573,12277936.147208631,12276551.993805729,12275212.23922596,12273917.489164375,12272668.346769305,12271465.41275088,12270309.285485767,12269200.561117316,12268139.83365028,12267127.695039207,12266164.7352698,12265251.542432401,12264388.702786861,12263576.800818114,12262816.419281747,12262108.139238942,12261452.540080188,12260850.19953721,12260301.693682637,12259807.596916882,12259368.481941914,12258984.919721508,12258657.479427695,12258386.728373185,12258173.23192952,12258017.553430917,12257920.254063671,12257881.89274112,12257903.025964238,12257984.207667993,12258125.989053605,12258328.918406956,12258593.540903497,12258920.398399953,12259310.029213272,12259762.96788733,12260279.744947886,12260860.886646414,12261506.914693462,12262218.345982244,12262995.692303237,12263839.460050557,12264750.149921047,12265728.256606903,12266774.268482855,12267888.667288847,12269071.927809283,12270324.517549884,12271646.896413257,12273039.516374322,12274502.82115672,12276037.245911438,12277643.216898754,12279321.15117485,12281071.456284188,12282894.529958973,12284790.759826938,12286760.52312865,12288804.186445655,12290922.105440645,12293114.624610903,12295382.077056285,12297724.784262884,12300143.05590361,12302637.18965686,12305207.47104441,12307854.173289647,12310577.557197236,12313377.871055286,12316255.350561017,12319210.218770938,12322242.686076425,12325352.950205669,12328541.196252756,12331807.596734777,12335152.311677616,12338575.48873122,12342077.263314903,12345657.758793334,12349317.086683711,12353055.346894616,12356872.627996936,12360769.007527212,12364744.552323733,12368799.318895556,12372933.35382465,12377146.694201253,12381439.368092446,12385811.395043986,12390262.786615206,12394793.546946907,12399403.673361955,12404093.1569983,12408861.983474094,12413710.133584414,12418637.584029166,12423644.308171583,12428730.276826678,12433895.45907899,12439139.823128883,12444463.337166542,12449865.970272837,12455347.69334607,12460908.48005364,12466548.30780754,12472267.158762582,12478065.020836122,12483941.888748104,12489897.765080065,12495932.661351783,12502046.5991141,12508239.611056514,12514511.742127934,12520863.050669065,12527293.609554784,12533803.50734477,12540392.849440694,12547061.759248154,12553810.37934149,12560638.872629618,12567547.423520917,12574536.239085194,12581605.550210655,12588755.612753868,12595986.708680525,12603299.147194866,12610693.265855579,12618169.431675872,12625728.042205518,12633369.526592456,12641094.34662167,12648902.997728948,12656796.009987043,12664773.949061884,12672837.4171363,12680987.053798802,12689223.536894858,12697547.583338186,12705959.949879494,12714461.433830079,12723052.873737734,12731735.150012407,12740509.185498983,12749375.94599462,12758336.440708114,12767391.722658647,12776542.889011396,12785791.081347482,12795137.485865723,12804583.333513666,12814129.900045525,12823778.506004503,12833530.516627189,12843387.34166767,12853350.435139082,12863421.294970388,12873601.462576233,12883892.52233781,12894296.100992724,12904813.866931979,12915447.529402252,12926198.83761177,12937069.579738194,12948061.581837023,12959176.706649218,12970416.852306772,12981783.950935248,12993279.967152303,13004906.89646151,13016666.763540877,13028561.62042571,13040593.544585574,13052764.636895375,13065077.019500813,13077532.833578547,13090134.236991806,13102883.40184229,13115782.511919482,13128833.760048771,13142039.345340012,13155401.470338421,13168922.338079965,13182604.149053702,13196449.0980738,13210459.371064242,13224637.141759526,13238984.568324953,13253503.78990039,13268196.923071744,13283066.058274565,13298113.256134607,13313340.543750454,13328749.910923503,13344343.30634108,13360122.633718515,13376089.747906504,13392246.45097018,13408594.488246677,13425135.544388205,13441871.23939781,13458803.124665376,13475932.679011455,13493261.304746851,13510790.323755985,13528520.973612234,13546454.403733585,13564591.671587097,13582933.73895064,13601481.468240658,13620235.618914487,13639196.843956029,13658365.68645342,13677742.576277329,13697327.826868514,13717121.63214315,13737124.063524246,13757335.067107484,13777754.460969463,13798381.932626259,13819217.036649916,13840259.192450197,13861507.682228716,13882961.64911213,13904620.095470866,13926481.881429374,13948545.723573487,13970810.193860175,13993273.71873438,14015934.578457244,14038790.906649522,14061840.690053454,14085081.768515855,14108511.835194679,14132128.436990684,14155928.97520532,14179910.706425369,14204070.74363432,14228406.05754979,14252913.478185842,14277589.69663839,14302431.267091274,14327434.609040089,14352596.009730276,14377911.626805298,14403377.491160411,14428989.509996738,14454743.470070055,14480635.04112812,14506659.779529827,14532813.13203917,14559090.439786416,14585486.942388544,14611997.782220729,14638618.008830052,14665342.58348256,14692166.383834353,14719084.208717123,14746090.783028487,14773180.762717001,14800348.739851845,14827589.247766918,14854896.766268888,14882265.726898981,14909690.518237874,14937165.491243387,14964684.964610564,14992243.230143746,15019834.55813051,15047453.202707294,15075093.407206804,15102749.409477448,15130415.447165238,15158085.762948863,15185754.609718878,15213416.25569226,15241064.989453852,15268695.124916535,15296301.006192414,15323877.012367452,15351417.562172577,15378917.118544502,15406370.193070002,15433771.350307718,15461115.211982016,15488396.461043792,15515609.8455936,15542750.182662843,15569812.361849226,15596791.348803012,15623682.188561209,15650480.008727023,15677180.022492452,15703777.531502327,15730267.928558344,15756646.700162202,15782909.42889723,15809051.795648267,15835069.581659982,15860958.670434073,15886715.049466167,15912334.811823588,15937814.157565404,15963149.395006498,15988336.941827672,16013373.32603405,16038255.186764296,16062979.274953416],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[11873041.683455983,11873041.688387504,11873041.6934108,11873041.698527588,11873041.703739598,11873041.709048606,11873041.714456422,11873041.71996488,11873041.725575857,11873041.731291255,11873041.737113023,11873041.743043136,11873041.749083612,11873041.755236506,11873041.761503916,11873041.767887965,11873041.774390826,11873041.781014705,11873041.787761865,11873041.794634597,11873041.801635234,11873041.808766158,11873041.816029796,11873041.823428618,11873041.830965137,11873041.838641917,11873041.846461572,11873041.85442676,11873041.862540182,11873041.870804604,11873041.879222838,11873041.88779774,11873041.89653223,11873041.905429281,11873041.914491914,11873041.923723213,11873041.933126317,11873041.94270442,11873041.952460788,11873041.962398728,11873041.97252163,11873041.982832925,11873041.993336128,11873042.004034806,11873042.014932606,11873042.026033225,11873042.037340438,11873042.048858097,11873042.060590109,11873042.072540475,11873042.084713249,11873042.097112577,11873042.109742675,11873042.122607838,11873042.135712441,11873042.149060939,11873042.16265787,11873042.176507862,11873042.190615626,11873042.204985954,11873042.219623743,11873042.23453396,11873042.249721684,11873042.265192073,11873042.280950394,11873042.297002006,11873042.313352365,11873042.330007037,11873042.346971683,11873042.36425207,11873042.381854076,11873042.399783693,11873042.418047016,11873042.436650254,11873042.455599736,11873042.474901905,11873042.494563328,11873042.514590696,11873042.534990814,11873042.555770626,11873042.576937193,11873042.598497722,11873042.62045954,11873042.64283012,11873042.66561707,11873042.68882814,11873042.712471224,11873042.736554371,11873042.761085764,11873042.786073754,11873042.81152683,11873042.837453663,11873042.863863064,11873042.890764017,11873042.918165674,11873042.946077354,11873042.974508557,11873043.003468947,11873043.032968381,11873043.06301689,11873043.093624696,11873043.124802213,11873043.156560045,11873043.188909,11873043.221860074,11873043.255424485,11873043.289613644,11873043.32443919,11873043.359912964,11873043.396047039,11873043.432853702,11873043.47034548,11873043.508535126,11873043.547435632,11873043.58706024,11873043.62742242,11873043.668535912,11873043.710414702,11873043.753073037,11873043.796525434,11873043.840786673,11873043.88587182,11873043.931796214,11873043.978575474,11873044.026225526,11873044.074762575,11873044.12420315,11873044.174564062,11873044.225862449,11873044.278115775,11873044.331341814,11873044.385558682,11873044.440784829,11873044.497039048,11873044.554340485,11873044.612708636,11873044.672163371,11873044.732724922,11873044.794413896,11873044.857251294,11873044.921258498,11873044.9864573,11873045.052869886,11873045.120518861,11873045.18942725,11873045.259618508,11873045.331116535,11873045.403945656,11873045.478130672,11873045.553696832,11873045.630669862,11873045.70907596,11873045.788941829,11873045.870294645,11873045.953162113,11873046.037572442,11873046.123554375,11873046.211137183,11873046.300350685,11873046.391225262,11873046.483791852,11873046.578081977,11873046.674127745,11873046.771961857,11873046.871617636,11873046.973129014,11873047.076530566,11873047.181857502,11873047.289145697,11873047.39843169,11873047.509752708,11873047.62314666,11873047.738652177,11873047.856308604,11873047.976156013,11873048.098235233,11873048.222587856,11873048.349256242,11873048.478283549,11873048.609713733,11873048.743591577,11873048.879962698,11873049.018873563,11873049.160371503,11873049.304504737,11873049.451322388,11873049.600874485,11873049.753212,11873049.908386856,11873050.06645194,11873050.227461128,11873050.391469313,11873050.558532393,11873050.728707323,11873050.902052123,11873051.078625891,11873051.258488832,11873051.441702273,11873051.628328692,11873051.81843172,11873052.012076205,11873052.209328176,11873052.41025491,11873052.614924945,11873052.823408093,11873053.035775468,11873053.252099534,11873053.472454077,11873053.69691429,11873053.925556755,11873054.158459496,11873054.395701995,11873054.637365213,11873054.883531634,11873055.134285282,11873055.389711749,11873055.649898235,11873055.914933564,11873056.184908232,11873056.459914418,11873056.740046035,11873057.025398746,11873057.316070015,11873057.61215912,11873057.913767202,11873058.220997307,11873058.533954388,11873058.852745386,11873059.17747923,11873059.508266902,11873059.845221454,11873060.18845806,11873060.538094057,11873060.894248972,11873061.257044584,11873061.626604946,11873062.003056444,11873062.386527827,11873062.77715027,11873063.175057402,11873063.580385357,11873063.993272826,11873064.413861107,11873064.842294147,11873065.278718598,11873065.72328386,11873066.176142147,11873066.637448529,11873067.10736098,11873067.586040461,11873068.073650945,11873068.57035949,11873069.076336296,11873069.59175476,11873070.11679155,11873070.65162665,11873071.196443425,11873071.751428701,11873072.31677282,11873072.892669698,11873073.47931691,11873074.076915758,11873074.685671318,11873075.305792551,11873075.937492343,11873076.580987602,11873077.23649931,11873077.90425264,11873078.584476996,11873079.277406119,11873079.983278152,11873080.702335749,11873081.434826134,11873082.181001205,11873082.941117616,11873083.715436878,11873084.50422544,11873085.307754798,11873086.126301568,11873086.960147617,11873087.809580134,11873088.674891753,11873089.556380646,11873090.454350634,11873091.369111296,11873092.300978074,11873093.250272404,11873094.21732181,11873095.202460026,11873096.206027135,11873097.228369666,11873098.269840736,11873099.330800168,11873100.411614625,11873101.512657745,11873102.634310275,11873103.776960203,11873104.941002913,11873106.126841305,11873107.334885966,11873108.565555308,11873109.81927572,11873111.096481724,11873112.39761614,11873113.723130243,11873115.073483925,11873116.449145865,11873117.850593712,11873119.278314246,11873120.73280357,11873122.214567274,11873123.72412065,11873125.261988852,11873126.828707118,11873128.424820941,11873130.0508863,11873131.707469849,11873133.39514913,11873135.114512788,11873136.866160795,11873138.650704682,11873140.468767742,11873142.320985299,11873144.208004918,11873146.130486658,11873148.089103336,11873150.08454075,11873152.117497971,11873154.18868759,11873156.298835987,11873158.448683621,11873160.6389853,11873162.87051047,11873165.144043522,11873167.460384073,11873169.820347294,11873172.224764202,11873174.674482008,11873177.17036441,11873179.71329196,11873182.304162383,11873184.943890933,11873187.633410761,11873190.373673262,11873193.165648455,11873196.010325361,11873198.90871239,11873201.861837758,11873204.870749854,11873207.936517695,11873211.06023132,11873214.243002245,11873217.485963885,11873220.790272025,11873224.157105278,11873227.587665547,11873231.083178518,11873234.64489416,11873238.274087222,11873241.972057743,11873245.740131592,11873249.57966101,11873253.492025152,11873257.478630656,11873261.540912231,11873265.680333221,11873269.898386246,11873274.19659378,11873278.576508831,11873283.039715532,11873287.587829849,11873292.222500237,11873296.945408335,11873301.758269668,11873306.662834387,11873311.660888001,11873316.754252128,11873321.944785295,11873327.234383706,11873332.624982068,11873338.118554434,11873343.71711503,11873349.42271915,11873355.237464031,11873361.163489785,11873367.202980313,11873373.358164283,11873379.631316097,11873386.024756897,11873392.540855601,11873399.182029938,11873405.950747551,11873412.84952707,11873419.880939273,11873427.04760822,11873434.352212444,11873441.79748618,11873449.386220584,11873457.121265028,11873465.005528387,11873473.041980395,11873481.233652994,11873489.583641754,11873498.095107311,11873506.77127681,11873515.615445446,11873524.630978016,11873533.821310462,11873543.189951539,11873552.740484446,11873562.476568555,11873572.401941143,11873582.520419188,11873592.835901206,11873603.352369128,11873614.073890233,11873625.004619122,11873636.148799745,11873647.510767478,11873659.094951255,11873670.905875735,11873682.94816356,11873695.226537645,11873707.74582351,11873720.51095172,11873733.526960336,11873746.798997454,11873760.33232381,11873774.132315438,11873788.20446639,11873802.554391563,11873817.187829552,11873832.110645598,11873847.328834608,11873862.848524246,11873878.675978117,11873894.817599008,11873911.279932244,11873928.069669085,11873945.193650255,11873962.658869527,11873980.472477425,11873998.641784988,11874017.174267657,11874036.077569254,11874055.35950605,11874075.028070964,11874095.091437807,11874115.557965722,11874136.436203653,11874157.734894978,11874179.46298224,11874201.629611991,11874224.244139772,11874247.316135207,11874270.855387235,11874294.87190944,11874319.375945577,11874344.37797516,11874369.888719244,11874395.919146324,11874422.48047839,11874449.58419712,11874477.242050238,11874505.466058008,11874534.268519906,11874563.66202145,11874593.65944118,11874624.273957822,11874655.519057618,11874687.408541828,11874719.956534415,11874753.17748991,11874787.086201461,11874821.697809074,11874857.02780804,11874893.092057563,11874929.906789584,11874967.488617806,11875005.85454692,11875045.021982063,11875085.00873845,11875125.833051257,11875167.51358569,11875210.069447314,11875253.520192562,11875297.885839498,11875343.186878815,11875389.44428506,11875436.679528063,11875484.914584655,11875534.171950601,11875584.474652769,11875635.84626155,11875688.31090353,11875741.893274413,11875796.618652156,11875852.512910426,11875909.602532228,11875967.91462385,11876027.476929016,11876088.317843324,11876150.466428913,11876213.952429395,11876278.806285042,11876345.05914819,11876412.742898954,11876481.890161129,11876552.53431835,11876624.709530529,11876698.450750472,11876773.79374078,11876850.775090935,11876929.432234654,11877009.803467419,11877091.927964244,11877175.845797641,11877261.597955773,11877349.2263608,11877438.77388743,11877530.284381572,11877623.802679233,11877719.374625487,11877817.04709361,11877916.868004344,11878018.886345252,11878123.152190149,11878229.716718638,11878338.632235682,11878449.952191215,11878563.731199773,11878680.025060112,11878798.890774822,11878920.386569878,11879044.571914103,11879171.507538559,11879301.255455798,11879433.878978949,11879569.442740656,11879708.012711728,11879849.656219646,11879994.44196668,11880142.440047756,11880293.721967952,11880448.3606596,11880606.430498932,11880768.00732231,11880933.168441884,11881101.992660705,11881274.560287254,11881450.953149296,11881631.254607039,11881815.549565542,11882003.924486343,11882196.467398172,11882393.267906852,11882594.417204143,11882800.008075636,11883010.134907538,11883224.89369233,11883444.38203324,11883668.699147468,11883897.945868066,11884132.224644503,11884371.63954173,11884616.296237798,11884866.302019875,11885121.765778678,11885382.798001178,11885649.510761587,11885922.01771051,11886200.434062252,11886484.876580138,11886775.463559896,11887072.314810945,11887375.551635567,11887685.29680593,11888001.674538855,11888324.810468342,11888654.831615701,11888991.866357371,11889336.044390257,11889687.496694628,11890046.355494475,11890412.754215369,11890786.827439656,11891168.710859125,11891558.54122501,11891956.456295302,11892362.594779441,11892777.096280305,11893200.101233505,11893631.750844004,11894072.187020073,11894521.55230455,11894979.98980348,11895447.643112145,11895924.65623847,11896411.173523925,11896907.33956193,11897413.299113797,11897929.197022315,11898455.178123057,11898991.387153398,11899537.96865949,11900095.066901146,11900662.825754782,11901241.388614617,11901830.898292106,11902431.496913854,11903043.325818116,11903666.525449999,11904301.235255579,11904947.593575068,11905605.737535203,11906275.802941024,11906957.92416731,11907652.234049741,11908358.863776105,11909077.942777695,11909809.59862115,11910553.956900911,11911311.141132586,11912081.272647418,11912864.470488073,11913660.851306034,11914470.529260816,11915293.615921274,11916130.22016918,11916980.44810542,11917844.402958972,11918722.18499895,11919613.891449966,11920519.616411021,11921439.450778188,11922373.482171327,11923321.794865023,11924284.469724024,11925261.584143348,11926253.21199329,11927259.423569545,11928280.285548557,11929315.860948421,11930366.209095318,11931431.385595826,11932511.442315059,11933606.427360961,11934716.385074664,11935841.356027193,11936981.377022447,11938136.481106604,11939306.697583994,11940492.05203941,11941692.566366928,11942908.25880516,11944139.14397901,11945385.232947748,11946646.533259438,11947923.049011597,11949214.780917948,11950521.726381142,11951843.879571341,11953181.231510436,11954533.77016172,11955901.480524803,11957284.344735542,11958682.342170753,11960095.44955736,11961523.641085777,11962966.888527181,11964425.161354303,11965898.426865514,11967386.650311718,11968889.795025762,11970407.822553968,11971940.692789376,11973488.36410629,11975050.793495718,11976627.936701283,11978219.748355148,11979826.182113554,11981447.190791465,11983082.72649592,11984732.74075763,11986397.184660314,11988076.008967416,11989769.164245635,11991476.600984888,11993198.26971427,11994934.121113477,11996684.10611937,11998448.176027175,12000226.282585923,12002018.378087712,12003824.415450405,12005644.348293398,12007478.131006027,12009325.718808321,12011187.067803737,12013062.135023557,12014950.878462637,12016853.257106265,12018769.230947796,12020698.760996895,12022641.809278125,12024598.338819714,12026568.3136323,12028551.698677547,12030548.45982646,12032558.563807376,12034581.978143472,12036618.67107985,12038668.611500083,12040731.768832337,12042808.112945022,12044897.614032108,12047000.242488151,12049115.968773216,12051244.763267802,12053386.596117979,12055541.437070938,12057709.255301215,12059890.019227846,12062083.69632275,12064290.252910666,12066509.65396102,12068741.862872066,12070986.841247765,12073244.54866775,12075514.94245096,12077797.977413302,12080093.605619943,12082401.776132718,12084722.434753258,12087055.52376235,12089400.981656201,12091758.742880203,12094128.737560814,12096510.891236272,12098905.124586776,12101311.353164865,12103729.487126667,12106159.430964807,12108601.08324366,12111054.33633774,12113519.076173935,12115995.181978488,12118482.526029343,12120980.973414779,12123490.381799085,12126010.601196066,12128541.47375121,12131082.833533322,12133634.506336426,12136196.30949278,12138768.05169774,12141349.53284738,12143940.543889578,12146540.866689416,12149150.273909664,12151768.528907146,12154395.385645716,12157030.588626623,12159673.872837076,12162324.96371758,12164983.577148931,12167649.419459444,12170322.187453156,12173001.568459641,12175687.24040607,12178378.87191212,12181076.12240839,12183778.642278755,12186486.073027361,12189198.047470633,12191914.189954855,12194634.116599752,12197357.435568495,12200083.747364514,12202812.645155469,12205543.715124743,12208276.536850631,12211010.68371364,12213745.723331919,12216481.21802516,12219216.725306982,12221951.79840593,12224685.986815156,12227418.836870745,12230149.892358681,12232878.695150344,12235604.78586643,12238327.704569105,12241046.991482165,12243762.187738955,12246472.836157713,12249178.482043957,12251878.674019525,12254572.964877792,12257260.912464526,12259942.08058383,12262616.039928555,12265282.36903448,12267940.655257557,12270590.495773409,12273231.498598292,12275863.283630587,12278485.483711876,12281097.745706648,12283699.73159952,12286291.119608916,12288871.60531601,12291440.902807701,12293998.74583239,12296544.888967184,12299079.10879518,12301601.205091318,12304111.00201543,12306608.349310817,12309093.123506822,12311565.22912372,12314024.599878227,12316471.199887853,12318905.024872325,12321326.103350174,12323734.497828608,12326130.305984711,12328513.661835942,12330884.736897912,12333243.741327327,12335590.925047936,12337926.578857396,12340251.03551267,12342564.67079188,12344867.90453018,12347161.201627424,12349445.073025158,12351720.076650651,12353986.818325493,12356245.952636302,12358498.18376512,12360744.26627698,12362985.005862135,12365221.260030491,12367453.93875566,12369684.00506614,12371912.475581089,12374140.420988163,12376368.966460915,12378599.292013232,12380832.632788325,12383070.279279836,12385313.577482577,12387563.928970536,12389822.790899763,12392091.67593383,12394372.152089585,12396665.842500996,12398974.425098937,12401299.632204872,12403643.250036387,12406007.11812273,12408393.12862851,12410803.22558385,12413239.404019468,12415703.709005073,12418198.234589916,12420725.122644125,12423286.561599886,12425884.785091475,12428522.0704935,12431200.737356706,12433923.145740995,12436691.694445496,12439508.819135666,12442376.990367645,12445298.711510336,12448276.516565846,12451312.967889203,12454410.653808499,12457572.186146839,12460800.197647758,12464097.339306017,12467466.277605945,12470909.6916698,12474430.27031888,12478030.70905037,12481713.706933254,12485481.963426858,12489338.175125884,12493285.032436116,12497325.216185197,12501461.394173253,12505696.217668302,12510032.317851761,12514472.302219622,12519018.75094501,12523674.21320829,12528441.203500964,12533322.197909858,12538319.630388446,12543435.889022192,12548673.312295062,12554034.185364576,12559520.736352835,12565135.13266114,12570879.477315959,12576755.805354062,12582766.080254706,12588912.190426867,12595195.945759427,12601619.074242404,12608183.218667056,12614889.933412887,12621740.68132928,12628736.830719542,12635879.652434962,12643170.317086266,12650609.89237974,12658199.340585092,12665939.516141757,12673831.163410256,12681874.914574794,12690071.287703017,12698420.684968486,12706923.391041107,12715579.57165026,12724389.272325149,12733352.417316195,12742468.808701135,12751738.125678835,12761159.924053423,12770733.63591082,12780458.569489306,12790333.909245184,12800358.716114167,12810531.92796848,12820852.360269278,12831318.70691335,12841929.54127266,12852683.31742466,12863578.371570848,12874612.923640603,12885785.079076707,12897092.830798626,12908534.061339065,12920106.545148883,12931807.951065088,12943635.844936157,12955587.692398531,12967660.86179786,12979852.627248138,12992160.17182163,13004580.590862123,13017110.895413863,13029748.015758278,13042488.805050315,13055330.043046176,13068268.439914005,13081300.640118932,13094423.226373943,13107632.723647792,13120925.603221333,13134298.286783513,13147747.150558358,13161268.52945438,13174858.721227806,13188513.990651255,13202230.573679568,13216004.681604663,13229832.505191466,13243710.21878718,13257633.984396378,13271599.955714615,13285604.282113599,13299643.112571087,13313712.599539133,13327808.902744463,13341928.192915162,13356066.655428147,13370220.49387222,13384385.93352182,13398559.224716997,13412736.646145368,13426914.508022297,13441089.155165747,13455256.969962696,13469414.375224328,13483557.836927557,13497683.86684071,13511789.0250317,13525869.922257122,13539923.222231204,13553945.643773794,13567933.962836843,13581885.014409157,13595795.6942995,13609662.96079839,13623483.836219123,13637255.408318963],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Ridge en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Ridge pour tout les paramètres de GridSearchCV\n","fig1 = go.Figure([\n","    go.Scatter(name='RMSE moyenne',\n","               x=alphasridge,\n","               y=GridRidge.ScoresMean,\n","               mode='lines',\n","               marker=dict(color='red', size=2),\n","               showlegend=True),\n","    go.Scatter(name='SDup RMSE',\n","               x=alphasridge,\n","               y=GridRidge.ScoresMean + GridRidge.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               showlegend=False),\n","    go.Scatter(name='SDdown RMSE',\n","               x=alphasridge,\n","               y=GridRidge.ScoresMean - GridRidge.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               fillcolor='rgba(68, 68, 68, .3)',\n","               fill='tonexty',\n","               showlegend=False)\n","])\n","\n","fig2 = px.line(GridRidge,\n","               x=alphasridge,\n","               y=[\n","                   'ScoresSplit0', 'ScoresSplit1', 'ScoresSplit2',\n","                   'ScoresSplit3', 'ScoresSplit4'\n","               ])\n","\n","fig3 = go.Figure(data=fig1.data + fig2.data)\n","fig3.update_xaxes(type='log', title='alpha')\n","fig3.update_yaxes(title='RMSE')\n","fig3.update_layout(\n","    title=\"RMSE du modèle Ridge en fonction de l'hyperparamètre alpha\")\n","fig3.show()\n","if write_data is True:\n","    fig3.write_image('./Figures/graphRMSERidge.pdf')"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"name":"stdout","output_type":"stream","text":["      paramètre  Lasso()\n","0  lasso__alpha      1.0\n"]},{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning:\n","\n","Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.394e+15, tolerance: 2.023e+14\n","\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predLasso=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[6441302.1877972055,909385.1101607846,2460951.9814617727,3106835.81437172,4984032.768402731,3915345.1983997324,188996.00063211028,1516529.1045765893,1774989.864581465,9766458.547780138,9726046.015629116,14300973.214158118,6616343.672901198,17648689.5687645,1303971.715931456,3337591.7430550987,870525.7386832549,4256590.19509561,1039047.2032856119,5827368.029424623,1841724.581842946,926987.6446560193,4389963.712849002,851699.8707940164,557547.6173752644,7729235.177691213,3356522.173712029,3808939.9882452004,6737675.952447779,23924603.98669659,6431554.656439173,801120.5499982475,1890352.5134280897,1277776.5648386325,4162714.523288238,4411678.090891194,2050216.8104082216,508874.43718828517,4146782.9474750906,1484993.7027229934,1069307.6792546709,1631233.7449448854,2235364.1802195474,2842358.5789816063,3137933.0020538904,27732641.311587896,1111334.2372693147,2634852.6452828394,1202991.543791173,10230542.452493545,942367.0962622035,3102796.4665604783,4674659.872285848,2238049.7970131095,923281.596803443,1572630.0866870496,1428239.2091893172,7401920.385734005,43875287.32940639,1363618.6929751285,1718099.384901879,3019924.8282665303,1211457.6558996392,21109609.987042684,3269060.896713404,1939638.6574102195,3843020.8870191216,7737800.287534225,1987711.2001674082,1814572.065079324,2722281.9550488116,791446.1719033013,8029879.64499898,2024871.3687718255,1216857.0624924195,1232820.8660213747,4380215.234583231,1312411.8630438584,2162834.6562526356,3656834.969265405,3439005.624625866,15939594.145190984,1073604.2786396309,1219709.6164601552,7173937.340468879,908210.401276903,1123396.5239918958,2337214.1020935024,-113203.21920882771,2237421.053181285,627026.6516478127,3161072.9653318506,3809159.712675425,822144.9072314098,310487.243537541,801258.1317710311,2892743.797928846,113809.8697154047,5997506.08409819,3835900.0992341484,2598958.1773062097,4018925.8970935442,1246355.1342569813,1685785.1965077377,7456631.2351453,5671531.899865264,19390939.857582334,2757346.730906039,292905.4948078387,2218289.9587658774,998802.4903880744,675722.6389934777,2757279.9572581486,1026845.6729832261,421676.03057553666,6788084.888910435,640026.5054294206,102920.30377928447,1877294.7113969512,2172082.3419171455,3134540.2076747483,18950403.040311147,777240.3993128866,481743.3314642366,4434430.461906504,1729629.1843404253,4836013.949176566,2912373.739061208,4462897.969745642,3041998.7691752072,2857845.5222771624,11521642.159068096,3147534.9738769894,106383.64639178943,14580341.897270408,544155.0133538644,-424401.35302609205,1636916.5790854844,1409605.7354964241,3484446.1382963806,2064585.0819240112,2744385.78499265,1545867.8788374404,1285345.3042527018,1032278.5886052209,4261076.01635069,5040438.711760958,8804986.352838406,10022247.79744553,1150240.5309192128,9478577.970959142,4873531.160077086,721425.8956915294,399385.0239304572,6740971.877613626,228425.82214592,479372.516035591,4729441.59640271,8445653.912242483,664533.4383318869,3550008.3959705783,3588379.668111446,1018054.5987982568,1957123.7728659401,18949572.678702373,1444706.2206470787,4949482.144445554,1529807.6459677792,15210921.241538048,630907.3425369996,590266.268678844,925703.9633855605,1682513.2381490087,700905.494425426,2251676.2200362873,3042406.1764286244,1712085.3799452018,1578400.0852161227,759619.606569051,7949071.116971211,2604664.253194561,899756.9942055631,1402536.1002189224,1941114.752457794,2677963.160729519,315001.4648062093,2688751.1385661024,903531.9979354888,8592291.000098974,21179278.61588723,14301008.299564514,3444516.095550744,3925194.3445530096,1762323.8050219067,1621109.6891321526,1151940.765195388,588773.461248632,1010798.0301501674,14626194.22576803,6907760.385799289,1566964.1413266552,3011672.395029773,1257962.4415170306,400080.01596042607,3165659.1697553527,3078218.8384793936,2638377.2831994826,103371.9637999013,1519008.0879260157,5621359.153649948,1715405.4095486805,653689.7585456634,844116.0063620713,8714332.158556448,2753786.8743364783,3364923.0782215437,6701090.586949253,8340101.694115341,602955.4611839219,1492860.6620700343,18789335.68025955,2835317.259446242,1135694.0880923993,3469701.4774095984,1653127.1041855281,28984570.725443024,1372031.3351015863,1855378.0130592661,2831903.2260708124,984790.7620591936,8697606.398476116,4850975.906376208,1591005.7707440197,2650511.538053042,846561.6390011054,5656707.874496426,21660095.756796535,10705632.387681056,5897456.373575078,700147.6889116988,2564089.6595294164,2613224.040242699,2919493.393844285,18108598.72880252,483036.43174675945,1518238.388383637,1311857.7611568398,2516086.5390122617,94305583.91214092,837518.2002070032,1221729.5134306815,2565614.7375670983,3031539.6930451584,5943206.165809151,11282956.565775067,15784620.71455684,893667.9161266373,4285412.770263523,1162735.114567871,828041.2148554462,897987.5825364026,2950208.2149151065,1977276.673173801,2825269.468980681,71047.4254679801,2508917.877806344,681734.1510629873,15501158.16033269,813627.5804243018,1192062.8076695632,639865.7552967705,5741668.872702474,4616247.1380317975,6404157.314628845,1793448.2315124613,456610.97283081524,-131522.11477404647,1206967.2132672656,11386980.882668652,-116439.23376162536,628308.5901794324,1096505.1643308946,22372346.58053157,14683505.94150332,670390.5416711594,31761049.9890909,1942708.2264055074,9398177.755918872,200132.8537502631,2214431.328630843,1274779.5481795873,4331979.39275555,88021416.576974,960566.3877172219,8658586.862007063,1498283.4908031737,2739080.6658031303,7627276.100791582,1620402.3968887648,28336069.61682661,1568626.3400525998,11047000.17103041,860001.1539094846,5167425.100584796,3560690.2561053243,1903727.5604712386,3503242.4612573227,3440991.2450356823,15795408.744635224,5879622.780935291,36893030.469359785,1523935.4176106306,5324.5967127396725,1688677.5109037734,5106150.801023459,2596480.35559939,7486563.542664612,5135714.086423814,7075470.943089541,925578.7135716302,715842.9928461108,1757158.4075534742,1146204.2927924031,571046.3730148657,317208.3776664778,16210411.187914196,3832473.5601057843,6254373.711340174,1948140.4014681713,4901161.130108782,4956749.168502103,15109479.126591269,1341133.2810575615,1059558.1804331497,7217273.727365159,9065315.068920007,1702424.6165673612,2347632.0458477642,703331.3064995205,3547164.9623597153,27667889.319307808,1147950.657851152,-940403.1535016308,-185850.13931493182,7634660.645872745,933955.284551865,4767516.680576054,444510.11738721887,11782754.091418397,7519075.000789389,3222102.8708403837,4371372.290275472,940055.4105696843,732082.9745489759,551022.9904934249,3306941.888571681,-287874.77174498513,433036.7688228425,56151904.34797008,13007429.188314974,1936517.467021293,797020.0977631945,15815375.74978944,4852823.894096357,911956.8457709465,28148218.69828318,3327307.0440681176,13793220.68515867,1424730.4971494288,896105.1909435103,3631805.8004117496,403269.97095243004,1063398.6244909437,780210.889155434,1212339.2614901334,1918415.4819434402,994822.7831655839,5614111.672329649,2202104.265532973,3254021.237348446,1392075.3346135432,1458084.0971087727,5625518.5757325115,623658.9324939717,1940467.4248840436,1024372.5035601598,1488680.7504494486,6468636.0185840065,2059027.496284915,4168910.282552981,928234.8334834639,1009558.5556396157,6026743.51767856,138099.99053760152,10300897.730828265,921960.665294233,7838034.920966528,4226382.253418505,5408740.81133317,1022284.3935407556,1026814.0023528584,8845027.072302997,2760762.9178349935,591746.0754822334,1129539.98104192,167098.22708196682,9580260.826578625,3154086.9494735547,736561.3968396117,1076276.7095872357,3175595.6100606085,6629616.796756757,8654349.127552144,1314620.9708644212,61323905.485866204,1106023.9901814323,1942915.5754624296,3162201.0983745726,26733846.772654004,971146.8843550782,4385163.356638834,2060314.0864793388,4324063.723382801,5624965.922090299,1490559.1179021439,1631187.3221426825,5595903.823116521,6149605.459518711,3517835.8045682553,4023454.8585717073,10283082.72067295,4125563.43766147,5933239.2276120465,25667751.687880043,233243.7753159022,1910036.1822431635,3358952.6325899456,1488075.8115697363,1930345.5953161737,2375676.1457623737,26520978.73381123,7437225.550073194,-572577.2432808648,633303.7446340933,22546182.925824665,967293.4452717411,10081165.645995906,3224638.1222887086,666598.992589622,16650246.679738313,299190.3836343158,2653315.17166534,35260405.37587434,3018770.9515293073,1356282.3410475529,7213747.919243376,6483513.243668923,3495522.7458902,18612848.474961538,1212949.8447867348,1773887.3617067384,1360061.8735297539,4946021.192488494,1731700.4267853757,2588103.411725018,4694757.400744439,3380681.4325845917,15147734.023549963,2341982.487568139,7989766.414218311,558498.4948913157,17043211.133043665,5856144.478751378,207775.0786121341,2608739.9170527523,5236153.468772925,1098660.4991303897,1090507.1409316268,6279363.546839744,807662.051602765,1603381.1169872158,2599377.6428981195,11087967.813269168,246251.26152989967,1555345.4463936607,3410178.6823620657,1924596.2201368944,732564.1673447182,1724709.4993466507,2640187.8744509015,1179250.2836862346,3128956.2950077867,172506.03624744946,2104506.2788835587,7428219.243771291,1687491.1316832236,2039558.410946969,584360.1483167286,11161089.701630004,9504723.231831599,8780478.036770064,8401654.178952646,8547295.68579239,1396656.4814871326,3464926.151740603,2360094.0812317827,969732.2051294299,1284371.3488987817,8429157.48699345,2078259.0001252024,980859.2208303509,11839025.35584895,454192.9202162628,2614326.5431174254,1593971.8386503942,14013259.471657429,10933251.033698067,2826585.099600682,1209417.8060968982,2714178.2582401345,42636659.23640362,10468651.192494271,1090801.8725038623,1290130.154753,6514426.294733121,2531454.9048234774,791533.6447750404,2423197.86163656,1447807.9395691333,1082498.6928740365,530719.7464819569,2446353.4795027766,1685670.876282202,4357657.1220104005,86651.96916703042,43561881.700218245,2378080.343167824,6169321.909986907,8893173.347126756,6403330.876471246,721519.1961559176,16754441.03424269,9938281.52278116,2188106.3242861032,14045029.214423973,1436136.4496320672,8137643.862629608,2605879.5002721543,1253512.7431983785,4502299.709164895,2119802.724123141,1334215.2856551777,11235834.130359694,563322.4988426007,2153742.334317365,1513532.663235076,7400678.136661963,3771478.1911961497,5240643.1172143705,61241033.84757226,918762.0787719775,2508403.735802156,4840420.191642685,7584229.565322791,4896867.420206243,2419207.9282645267,2506069.4999305084,384108.65928584477,5682797.701400091,1051616.2391615272,16570360.684089012,1162569.8623716976,1334205.646253002,13498657.349081751,-140378.00607200246,13580266.823559392,662480.8281153454,924281.9311887603,1055673.3730208883,668949.704989413,3800714.4223611588,1043438.0260111701,2464148.787184155,682192.4503712289,390326.1661244682,5824540.510996422,1218645.4647653257,2934826.1889926274,20219046.475911375,3527640.588689565,1605002.1193653939,1404204.394984015,2245848.5843769526,4831876.85567935,758378.5804885847,855778.5644106017,31102045.023148086,743465.631204342,589969.2645877474,619332.3840535469,1561314.6984344525,3106337.636305806,1995387.3618312543,3478564.7030273005,3177830.8671173304,1027285.6438733826,1476616.9571479145,12094575.417620111,613591.3847759052,3236958.587767503,1089408.6281380772,2415445.475910963,2451989.4359811903,6767299.711269043,1750130.0911711962,2110944.0109263477,7545148.098817511,9384996.877786191,2908975.1105548134,4747697.345218305,402913.23528793803,1006113.0638057732,3280334.7999246335,9394559.891268868,15972910.836852942,6307735.806141011,1137380.1918515114,2797498.73184873,824808.5111330152,879223.5077885487,4343947.654644638,1340138.282323422,750705.5038119983,-655448.8815748128,1300288.5035726973,1336589.582989174,779925.2868629587,813896.0493877796,1620641.4372215667,2709934.2462135945,915138.4856787124,1194371.231610605,1000234.2101001022,1054045.5378088318,2183744.899965676,3273650.535418081,1798371.8910467064,882270.1953975526,824808.5111330152,4209764.48415641,5715239.951607941,1097727.494486507,665263.7925757843,4339461.833389558,3833737.0858552866,1290944.9123699423,36386950.846177086,21952653.06465655,13615415.189646322,-117432.87872798974,6909077.859645363,1250066.326915324,1084259.9998987918,1235735.9840146534,718326.5386417846,681341.9224491944,369609.4165269607,3243065.17328876,8952141.74807557,3368765.499959394,1669567.699902934,11539161.32569924,1451419.1179035977,12420382.527132161,3814267.975144266,2412467.3506687344,-342357.8911598199,1029492.1463879724,833577.1421059466,625293.5653883019,402266.859833098,9490288.526487347,12368333.579809487,369304.2421909827,1624415.490362173,1920303.0410060347,8237169.214506176,2336291.1592404563,1896588.00178923,1219562.775959679,108241933.88862956,1746106.7384666833,3124131.4818816613,922053.4425613047,2740714.618199551,1330087.6983092083,458131.9851879026,801224.319559976,327725.5512533898,16973772.87132871,47308475.33787354,1228612.9210053384,12772504.180632707,4397567.482614455,338385.0443407395,1419071.6397391008,816986.3971422787,23628313.939515334,3533259.7015517214,3152287.596661479,496989.86914961017,2428934.4037358994,2358683.270693635,509800.0007340575,1576270.7571730656,10476936.342052333,419493.69388214126,3656885.420409508,10976675.676316302,868327.3046605762,1554803.2671150786,36469822.48447103,1143762.3630584986,1884591.4472553493,17056644.50962266,1298795.6961424854,13480679.703407504,1874252.1345719919,2037531.6704706752,2729272.715243743,4593197.165076288,834445.5297808601,3037396.03772987,5303982.906383179,1605579.0248496556,35217901.86961891,1277109.428562012,742309.8159093419,16040129.11224759,1531570.9306725904,4475463.380816945,5855656.907310703,2181807.9648406245,466249.1255896799,5358471.033760864,2403903.370745718,1316715.0862043388,10720628.966007806,3546228.684125146,1558474.0383176806,1755561.7426574226,4162171.8231998403,13225772.395062983,5691442.027962239,4189061.2389585446,7423483.854991624,855028.2082295418,-169386.2539770794,2055555.1729177763,15582137.262522373,935182.9605043088,1745885.8205916435,1430174.1517997552,1373867.9888823417,3910626.3290237575,8626244.756136896,902247.8965752583,1850835.496600605,4822029.5384925995,885388.241191701,21193451.72037531,1599400.7428705376,786648.621198223,9407416.888193399,1635227.7466079304,1824610.5866181352,2096456.828659154,1813522.9563738103,599320.8120772806,10085795.503612256,104902.52795590833,2561586.4259135914,4676689.678905146,5688618.477107346,1130845.4361250796,435398.2592279848,1415712.644762895,10981630.672664914,602381.8956672659,2929314.6117914505,1518430.690211724,6598206.847251783,3484900.071612978,19839956.16072296,11702014.549103256,1945169.9116256235,10746484.596441973,1157476.9875809716,765117.0398417255,1030709.2363869394,8762155.43400905,10660188.256521206,53724878.157220885,965509.334649743,886007.914872393,1480518.4739542885,3663761.016307825,1124131.5259083805,16293282.826208144,4085433.497556243,1478443.0601405045,10708842.555041019,879284.5805371287,4757548.553348737,431958.9048400596,1107244.141854108,9349386.339994464,2113228.762558321,4169027.1129675964,1023190.4048344987,1170746.6287750667,754365.607549926,983253.5255814414,1037175.4930985365,1098567.1986660014,8251653.152881477,7403691.904370664,1207769.1078855817,215643.81522017904,22666385.14356017,11974426.999124523,1336384.3814923265,1489061.1917064835,1565987.4002332315,2814922.5415521516,5992902.133627358,1280406.0805394356,3468261.790297351,3360896.033234973,1705214.3119424796,337976.1120737195,472445.73467580415,2673292.436610796,14937649.6046318,980661.7650908357,107403.0096240202,12397513.55895561,3809063.283591479,4548288.487033465,2919539.950394534,12855375.818926655,1704643.2921127845,5531240.0340357,3108059.300862145,8613902.749257147,4918885.587470514,87277029.23230642,1977920.3523059534,691217.371146925,-361095.3709337064,10026493.087568782,3721653.734809309,3496461.524849288,3449063.8961948142,2117495.722378834,2183237.228839919,2553492.4554940755,16239393.141405523,15651813.871593826,310850.3194653769,11598946.649809614,684390.5348703826,1798890.650933096,48944024.79095605,1685971.7974365142,5919110.578535798,1390987.9567566928,1303833.9212194504,846821.7198293302,2287754.091841382,4406464.095616804,6132424.256502902,1114812.8812681772,865035.6659796415,19608485.447675005,3139231.2325464357,5132050.976432832,-67182.84846519213,2609480.6198685477,2924649.5885720383,984186.5302253242,1389824.7216696038,12376228.913784534,9913715.461666055,4580321.70099071,26493350.394553687,7601560.041188386,1207258.5164590518,1655194.0459870147,20635492.13973636,5660959.907251169,4026450.5311991274,1835543.843649492,24577456.135773156,4590139.075668663,3253593.9602561905,22629054.564118613,3475675.199501398,80548358.63832773,7943645.345754711,788902.9573614171,215619.86866535386,839439.3931647076,4208435.075955419,2066730.2143737306,6196357.975540765,1049479.7525991632,5665937.440271548,1719005.8627292635,3156438.862987027,585814.5490864166,4132326.6180321975,4498453.223671572,10898759.034370966,4824812.547202341,1125443.4333091266,4478364.2557280045,3419981.805866465,7373759.596851352,769388.0352863977,4415581.585377624,3211237.1478774673,7539071.728570038,1452265.5359425824,4944272.989588916,3437417.70322088,2787124.7040637657,911397.042984617,1678878.9844629518,2996123.4451927445,5340154.067859481,3109419.665069899,880623.0147543724,6466905.856349952,1219764.5019063314,2743713.461306734,1683216.262279596,1678539.9880012628,306460.0372402342,-137364.71307901852,3109288.149901304,4276035.241930879,4038940.0117891175,21767710.377785645,10969324.080809694,3812275.4733280763,7613163.699092926,-46223.09479511948,4058335.845057315,574393.377327662,1570001.3307850717,1591919.2284338528,1506092.699093579,907680.1494269634,3086712.4952144194,-292281.13995374274,1226510.7702373897,27907569.94595352,238813.91234799242,949374.8341908534,1027929.3230055345,1363277.7188333839,1503123.7131527413,2273231.447112624,3912233.628407074,984824.5084581655,2468628.134498422,898421.5412381534,1884530.3106725751,897230.4974154802,1014876.6821097459,11703666.06632423,1015695.5603720532,1223310.0626653342,8553179.482106147,1135268.419858616,977261.4699434072,76037697.8687209,1173565.647016274,2165909.0575338965,2514355.0277835242,1224513.4014715664,353288.7501329966,3131849.0940611428,1866084.680975861,1057355.902834495,2905918.419406407,999807.4218373988,5035490.742681263,967375.3439375078,1054508.553557563,4187448.570469688,1448076.439163677,1988840.2298586327,3130087.7870363877,2619595.4016421265,4126400.36394591,1993105.5244039944,1292072.361807221,2135418.3204719294,845363.1951895156,14127900.852717921,3797915.408429512,12130849.560424257,2332907.7761776648,1079698.224077749,6748458.873378555,1442836.488140059,8705415.132988175,666011.1207442225,362003.10253705154,1432375.4343299542,2078447.8696705261,1674286.769259526,3940583.2202777592,154542.55542681972,2025787.2137563778,108852.82978419727,-64970.46873866441,1065150.5073971339,4106766.472371099,3186746.772121243,13880420.972319596,2672517.528399137,2693258.7359898807,1647628.2342671193,963639.6021427233,1983061.3019658723,1190517.1003091591,1938831.5711199578,1600697.888618018,6268549.634045351,2285008.1426321836,7158342.581383489,780018.5873273471,-282844.66061897157,1362997.8174402192,446330.70048419386,13938434.379556812,1769873.3656433458,1289092.4478461077,4183412.0380686363,7621943.366863987,5590228.731126314,4626245.8908074815,4171369.289660896,4036159.6293320023,2643809.489864111,897230.4974154802,2580249.579975375,2460075.1246596007,2839222.871099803,1915858.4333050363,4543374.2525135325,13916329.582318392,5451819.679935439,3055041.181169166,1182848.716559777,-108696.45439146133,594464.7895763151,4540956.383023352,6369818.9070579335,4979739.058500191,1094820.355870407,4153009.614705983,821076.4925574854,26485872.068590336,2788317.645055402,40066029.64705443,963704.6746138241,824270.1772392837,10349401.012198474,1156973.2893730425,669530.8838239063,-31718.256447625346,982917.3798660415,2496626.6973511204,4006144.981529031,5440589.911265446,1654029.5710778076,7682173.02071448,4950435.80759749,4415212.106739325,1108950.7734890035,1211993.1423867322,754664.2237543897,4253489.920820575,2239914.5292630694,1194237.7902680638,1341566.2476475986,1998895.8540953763,2270997.936866617,1948417.1683837385,3949646.2107584253,2881139.344451308,2683665.4489441733,1905494.0003176979,5100113.713115498,409787.05319789564,1294597.1752450143,13397808.065113556,13415785.710787803,2633445.800273867,4655011.402915047,2844146.052565783,5999919.355596166,37635339.503416836,2090181.6368821987,881535.1934810679,2380325.255212453,10302867.341025734,870964.2143070977,9531569.852161698,9608062.911113724,6085934.098421058,852660.3343074201,1210670.0921123917,3713889.618630822,63061408.24267323,902346.8979389572,15190193.413425982,219336.23517114203,863549.0297713233,9137808.66085836,2567672.5877040485,2245706.2945465837,1680225.8408765076,1780137.0697574657,874832.6850629905,2767425.8361000847,1125662.39532365,4269898.120569034,747939.6962658851,5722719.126626722,1079863.4762739227,1841783.572964883,2990779.7525078226,24602903.24380952,5635498.597672638,3250550.115347258,3630036.6006536633,3390314.2327382388,1134487.8774554755,3984954.9318903764,3450693.3174508647,17796898.348962136,1480545.000770786,6433169.84458439,1952952.4540451998,1413073.6400002087,25251934.678693634,2076970.932206421,5757185.091275601,156880.85771876434,2070870.6960234167,1418270.9736204618,2772356.7038739254,1371560.6342467507,4482169.6124542765,1231625.0626822598,929112.729419762,1347597.539916848,4172949.069874391,4498150.089619359,2898300.928999434,2386949.743868831,4932273.856260412,682245.4015477775,5777491.885993038,2665078.2725253757,1144284.4262162412,10959099.252084848,3137360.213330596,3764770.0415801667,1178853.0094799283,3245582.360150413,550333.6699206752,1879272.7376448421,2386856.28771963,1042648.6314026387,11785440.39515705,671049.3458211878,6383570.329859,734114.7588483305,4719609.306761415,5904011.695233367,1881762.2892270442,1206224.5327714146,5429110.486575078,-23642.08512892481,7130876.280949427,8814348.14744227,2163576.631438392,1225825.3088723128,3477818.6178113758,16151824.19717769,40148901.28534837,770953.7815840328,12803551.180592671,904038.5742703055,5049179.338138883,2197683.6520006713,2268240.5028360025,15031956.715979114,3880787.04672346,1595163.1036514128,2240625.663530716,3885593.0636771633,5723019.115890792,2663121.2182693235,1595668.3497073145,6000649.795390837,4489971.71690816,644122.60814303,2761510.8476637267,1529440.1450095372,6057005.001245942,1233376.9455884497,523865.10391931515,3329105.9468754265,2902696.152983586,33355700.529352844,1136227.35099852,10588128.295518262,2740586.298035671,3690314.4976576846,934150.9544967273,10994069.202257626,8491888.298618603,-102978.50102098333,494682.94369071326,1794567.8370851204,3427550.493423299,753533.0658642566,8358276.332715325,5335861.439970368,7593274.640229574,1634922.6535917881,7896309.925539797,1837442.4868614639,3166057.6479663774,470074.4446295977,2618262.6003882303,1357443.7596275723,4042637.197480615,3258592.4256254784,3147480.9632574376,2609097.993892429,169142.17992307013,1573043.3161804797,838818.793937868,1705257.5661153398,3030578.3373016072,2138012.54739756,3322934.146027579,4908269.79732085,1425751.3400823975,3566676.206667665,700812.1939610378,2241269.530944378,2826887.829635391,4091235.4488287694,50595.20931146899,2590420.798316848,132748.23037140537,414002.523694525,513330.2289126136,1703346.5097299144,2182977.670333383,952668.4174851086,3278267.476915296,48909047.70532909,6490993.098205031,553439.3450469642,911951.1448716358,1432985.605037388,1131155.1717014043,7975822.604170033,3026416.5116073554,2098837.7541531185,677108.1751071129,2681810.7883509216,27034529.257226765,3217411.8459686963,1324936.8630341825,2787119.003164455,9980681.024516763,4881502.005845388,7068544.629453287,2542946.762953549,1095406.3846754227,869520.2594921712,1327564.4795288164,713156.0484815808,3682876.892932009,790602.4412225471,5028892.830782443,1361164.0943186744,46571773.83195448],"xaxis":"x","y":[3195230,730765,1234149,1069095,629036,1328920,1677877,1700057.375,1116040,3466465,5722325.5,5674450,3098638,19459304,1418400,2378077,521341.9063,6135173,1066894,4660015,1713381.75,739711,6779778,760008,593042,2887392,2643948,2668769,1989387,10498344,37951708,1441900.5,804237,860505.625,2014500.375,2349950,1034827.313,965651.125,808968.1875,2074152,1832031.5,687953.6875,932598.6875,1207700,2390284,21346100,1750099.75,1355558,1435730,7166015,1385975.75,3155391,2075744.5,1073156.375,1394408.25,1247361,782483.125,11026945,37430780,1594504.875,2277316,3742915,1399062.75,33556756,1440662,1795462.75,991918,1250669,2545030.75,695828,3220827,1038815.188,6578370,1863468,745575,741850,10343027,659265,1107290.625,14172606,14585813,4926695,3343550,683307.375,2539565,974278,297715,4664399,1533245.75,12204922,515025,3043767,6118300.5,1764409.5,628609,4857581.5,3918038,858932,2313679,1984067.625,4971699,8611054,1344512,682632.3125,39061360,9560965,5696695,1813718,552007.5,3129811,675772.875,609218,1303247,2788860,1331469.75,8058913.5,1297872,1323038.75,1349970.5,993738,1827817.25,11813861,827211,520738,5049446.5,871355.5,3059758,2203026.5,3528086.75,3078672,416885.5938,3644982.25,1900006.25,615894,53166156,511180.6875,1970956,1290284,791052.875,65225380,872114.5,1241917,1019183,161363.4063,395517,4590246.5,2022990.625,14465798,3588718.75,1242666.25,8138616.5,1848656,572611,6293359,12783255,1109851,712057,4135280.25,3944933.5,520405,6807227,2564004,393493,1497886,19854012,2120807,2711776.5,638772,7080118,1164052.75,534110,1169932.75,928409,653175.8125,10608788,415975,230365,1365644,1022614.313,2882489.75,920068.875,1962435,770333,775728.8125,2116209,635698.6875,1403869,781217.875,1976920,27787286,10791309,2476833,238969.7031,312563,1239315.375,796471,551219,924068,15660628,5169726,1280690,0,838394.8125,2094525,1070085.375,12525174,1834290,345399,9191826,4141785.5,2065156,520765.8125,727374.6875,7946961,3647194,2136086,620483,5792926,3275779.25,1003551,14204281,3045139,1094343,284857.3125,523614.0938,10564729,682249.625,798322.375,816508,652425,21795830,1662054.125,664539,10420096,3778355,1049748.625,21169502,4524791,41092076,770082.125,1210798.375,400054,1810353,19344782,1392299.75,563237,1008665,4877959,78272488,1469870,677536.3125,1011667.5,2503804,3471845,6206083.5,6310254,1813864,3625434,1605182,611536,547084.375,2454812,1429196,2826828,5136604.5,3258819.5,979952,16701055,791542,1360340,90558.70313,11935719,1688612.5,2278160,659652.625,3537977,967835,918943.875,3822102.5,636266,642133,608359.6875,9722237,602245,746858.1875,18848780,4560492,7015692,4133811,2563985.5,898115.375,1967434.5,61576184,638507.125,6235947,4291789,6454983,5155090.5,703298,32381836,3176347,3093944,1896626.375,2006647,6781960,2912454.75,2601369,1417778,20311228,6525887,8374301,207572,826416.5,2823184,614768,1868497.125,11208498,1363191.625,5453438.5,855665.125,532670,3468606,905926.875,281191,2147968.25,6219841.5,1539066.375,4934334,982332.8125,605971.1875,4471029,6462938,650205.8125,860364.1875,2613607,3567977,4520838,15324777,2720238.25,1938613.125,25307744,1929118,7798464,747456.875,6859198,934980.875,2150120,662701.1875,6456026,4423958,2037693,1045413,1663324,542998,1193790,4573777,1247972,1208035.375,48479988,23262244,919180,987989,17457560,11924933,576174.125,16849234,4166306.25,4558824,847097.8125,1055624,11521140,1423463,415453,740230.375,603376,1314716,666930,2133372,818836.125,1993273,941366.5,644121.6875,25478086,2225265.5,5348309,907758.5,1007650,8136665.5,1026921,5457975,733249,652449,16917964,890977.8125,5231373,1523508,6960904.5,2966309,7456330,947518,1989287,2634166,2094724.625,947884,1870457.75,1840259,29949248,4103800.75,508057,2621041,2746152,5662234,8381834.5,1021798,25475474,2071670.25,987288.8125,924073,13093350,1960538,2082754.75,832168,1742351.25,408513,1976856,1605220.875,3046719,3366510,8688676,4399828,6344171,1924355.875,4826430,8747819,876237,3347567,2996958,878927.8125,3234871,1433267,11110405,6289567,580688,635697.3125,13567774,665216,3063969.75,14830411,536642.875,6069855,1125724,2923484,14856602,1848878.5,717197.8125,12187231,10328702,2666634,7564604,683274,1136111,847298.8125,5827595.5,719905.1875,1857889,1318096,1190733.5,10317852,842593,11047668,4193371,51168308,2172841,954352.6875,2547582,9290214,823460,1147582,5481995,1605669,2926089.5,1022957.688,6723971.5,2807583,1162226.375,4329966,1547556,5437110.5,2076289.625,821046,711118.375,1108310.875,1210229,1385697,6333706,1160989,2119806.25,691647,9162048,8773058,23612824,7243389,2992693,1811669,1375366,2254220,1431784,1072855,3404992,2046914,436848,11799105,501897.6875,226375,352136,6711975,14431245,906103,1917399,533742.8125,36367960,6482380,764008,1666221,2019254,241478.5938,720170.6875,2842760,132105,676032.625,510517,710275.375,756267,2474457,2023858.75,64547116,1228113.5,6361236.5,14332778,2330242,647906.1875,15036617,12719042,5903142,22394416,9546165,2924362,1375978.875,1022114.5,3637973,1137504.75,831855.875,15640994,2696136,957460,850667,11325513,2962476,1988334.5,26689658,1358519,744615,12704014,2712983.75,2153603.5,2080866.75,2791291,1000180,6182350.5,711594,14169301,3137206,564988.8125,6802622,673834,6335315.5,492481,603420,1208029.25,1067153.75,1618779.125,628023,1856230.625,2361088,793436,12087498,867456.875,1218114,8320719,6797528,829929,684739.6875,1757194,325513,1666762.125,739869.6875,50899248,529263,650008,1072487.875,6879291,2148565,1509292,3377209.75,3292049,666436,400770.9063,7762038,472532,4673827,811907.625,33268410,1834204,6010407,1403820,1603017.5,4750287,465533.3125,1073507,516216,585443,1038667,436252.5,3024478.5,2290681,1381590.125,1494252,1242311.5,685310.875,1000599,3986851,879235.8125,1372213.375,592739.3125,737935,2345314,800626.6875,2817261,2446842,1476969,646994,1241432,1327591,2043592.625,1111858,2443141.5,1216543,779204.875,1224685.125,1396191,2773647.5,730672,1054995,5948007,2569106.5,898845,39403320,15026352,42584028,938908,2270531,789209.8125,866825.375,1991234,768755.125,1900577,2531070.25,1912719,3091348.25,1272821,794724.625,4382240,43943,7405070,719734,5424718,936336,612158.1875,1585441,3076813.25,867184.375,13116256,4998015,721983.5,1097572,1647746,8575114,2179078,3303553,615955.625,57514116,1820814.75,1425599,2051501,14084606,1576953,498649.3125,722891,685764,4545148,44984468,885865.875,13424079,2212058.5,549834,4801472,578150,32675250,3632613.75,1546484.875,607373,918177,2569602,1401315.875,4467292,7319546,941688.875,2003948.375,8669367,1862752,1140620,48981052,794715.6875,9393108,4222682,965786,13315803,804620.5,928084,3473109,4188660.5,445943.8125,3988729,1310236.625,1191803,17463744,742565,670779,6399127.5,216115,1842562.75,1857329,3475804.25,562338.3125,4108523,1651120,714898,4748137,3528230,1700276,1639849.25,1650279.25,49102164,7802205.5,6606832.5,4626443,3158151,964923.8125,2531163,4249212,440419.1875,1000552,405823,538064.875,14329739,3534690.75,659368.625,2114860.25,1811506.375,632517,7370743,1795049,594848.625,12726789,2442220,757167.3125,1076693,1896858,2603931.25,2127077.25,944638.1875,3157579,4486477,3180819.25,557863.6875,930439.375,1826203,19018762,466904,3055176,2347893,9956396,1436773.875,29747868,6657348.5,1056099,1987934.625,1119679,151798,735907.8125,2664502.5,12729858,157606480,1620721,614985,917194.625,19061780,646691,5889882,2299216,7768057.5,9097980,1406687.5,5481549,1027394.313,875789,4337107.5,689082.3125,1634334,425574,1440554.25,836437.6875,729535,592805,634751,2057288,13491170,1509775,961749,37785552,4628171,984054,1038045.313,859137.125,1142949,2411402.25,5833800,7338511,3611538,966812.125,616749,730292,1120197,10053489,1044220.625,492807.6875,9308471,2240549,3241917,1433573.25,13146645,1208407.625,2131817,1424157.25,11353672,2755578,163945984,861643.6875,718362,784652,2757588,2759449,1765813.625,2162760,2227908.75,6458021.5,2087998,22325364,6481807.5,2503240,9085924,785144.8125,2458760.5,61984596,900973,1859571.875,945063,715728,509171.4063,2504814.5,1878442.25,2434208,572139,520011.8125,24017770,1814947,2231695,1725773,53401,1383950,728876,1046995.188,6843855,9155568,2885740.5,7220201,4424263,1694930.375,550324.1875,11356990,1742870,1094622,966677.125,24224888,1602030,3888945,13609040,5981430,16644664,1252323.875,479358,2630101,7004126,1911591,1357553,12184124,896172,2830674.75,1143826.625,4767227,2108012,3548858.25,1986624,19036416,5448687,1602143,2505271,1817513,40613740,1361427.75,2345752.5,4020660,4391348,5976246,3075425,13883385,1984823,677642.125,1456808,1189432,1271021,1696009,609494,41399504,767720,950695.625,1993766.375,702385,2188093,735810,2504797,17186624,1885314,8728584,11951945,2495819,1000796.875,1539176.625,1730542.25,1223296,1392153,847514.5,991513.875,1122837,2098850,637053.1875,819314,25672598,653672,1358652,845961.375,5807879,911366,846274,7879628,1433504.375,2279063,755269,1138520,133880,524479,10110827,624269.625,1890502.5,3902177,563298,919048.6875,85357952,503744.6875,2485521,10642754,1686405,2189124,1427294.375,1353079.5,862945.125,1956439.375,866806,1972491,627698,1553208.375,1929828,1151027,1222509,1404702.25,1725558,8354235,911088.8125,708928,3441643.25,723482.1875,23364636,1566177,7657568,1262255.75,3444270,5328051.5,313528.0938,2567976.75,634056,488181,915286.1875,2559461.75,3494929.5,4116871.5,695803,989257,1152896.25,719471.1875,639407.375,1755264,3619497,13349651,1393073,4258124,1897180,669117,1769137,541943.3125,946138,2214604.25,5229981,966764,6211441,639566.875,602618,523133,749207,4321141.5,2222097.5,648022,5177270.5,4785692,1933811.625,2184912,2845068,1399992,1285055,315468,6782435,2173143.75,2070005.625,1692199.25,2409429,7794869,2999116,1614673.625,659450,981009.125,491792,1454222,1953919.875,2101376,904236,0,1417728.25,20828130,1827424.125,32552428,514471,1703060,775729.1875,1272272,622051,783208.625,1625308.625,962238,1465535,2834401,2050600.75,2309007.25,7758182,3837228,522972.1875,1982967,769148,1698779,1928564.75,1257033.5,7654264,1285646.75,3643112,1224045.25,2632472,1678495.25,2076612,1964681.25,7231928,728167,957632,13631685,6673138,5571501.5,1261725,4618360,2756494,136241424,1529309,1353612.75,2786648.75,13586070,1100499.75,52960796,6669954,2274873.5,1972156,817695.6875,1911611,58041572,117438.3984,6127771.5,656980,418098,4634655,419978.6875,1057256,41680640,694189,674961,3371659.75,6983736,5288304,1912760.75,14981114,1764767.25,806615,4116805,9569715,7475577.5,3721129,2014721,5101504.5,719060,2919980.75,971155.625,16373232,791564,4933374.5,10834087,605186.3125,27185096,1045838.5,4656777,834711.875,1008536.813,1688745,4262316.5,928584.875,1167734.75,298063,552401.1875,450518,3796986.5,3371455,1240924.625,1537570.25,227566.7031,164817.9063,5543575,161311.0938,606886,1083729.625,1368120.5,2646523.5,2513155,1167967.125,638201,5274642,1386445.375,730211.1875,8692328,1257144.75,4212149,593296.1875,1767480,14265421,2447861,948157,1739286.75,1121537.75,12029600,6797647,151376,1761137.5,6674179.5,6505995,30135280,696522,7837692.5,767361,1989763.875,1788999,1822830,5232420.5,1554342,1523324,1487660,3556291,2249620.5,6592304,335245,3714139,1594422.5,336461,1956075,1838121,3986982.75,817409.625,2946756.25,1521693.25,1880890,12265516,1965529,4103027.5,1686248,5276777,1988760.875,3515821,2218249,947534,2892779,756908.8125,10721568,357915,4571388,3022275,5051636,2429214,5446624,886771,5326222,584545.625,2158629,726124.625,930611.875,1889990,1423929.125,866126.375,783651.5,8201974,2346692.5,145468.7969,2399762,938096.1875,1145535.75,2468322.5,831577,4829951,431815.6875,2361960,1210262,8787353,2882757,1228837.25,2735811.5,576052,442375.8125,866242,1339220.625,880115.5,1656025.625,65047284,2356376.75,456747,673030,863703,686091,3456131.5,2503690,345866.6875,941564.8125,1012341,15091579,1733511,999856,2154119,6268627,5454482,2407904.5,1815350,799996,593824,585559,5063753,5572503,8228399,6014078,994194.875,27717034],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Lasso() vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predLasso"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["alphaslasso = np.linspace(0.1, 1, 5)\n","param_gridLasso = {'lasso__alpha': alphaslasso}\n","\n","GridLasso, \\\n","BestParametresLasso, \\\n","ScoresLasso, \\\n","SiteEnergyUse_predLasso, \\\n","figLasso = reg_modelGrid(model=Lasso(),\n","                            scaler=RobustScaler(quantile_range=(10, 90)),\n","                            X_train=BEBM_train,\n","                            X_test=BEBM_test,\n","                            y_train=SiteEnergyUse_train,\n","                            y_test=SiteEnergyUse_test,\n","                            y_test_name='SiteEnergyUse_test',\n","                            y_pred_name='SiteEnergyUse_predLasso',\n","                            score=score,\n","                            param_grid=param_gridLasso)\n","\n","print(BestParametresLasso)\n","ScoresLasso\n","figLasso.show()\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[11333620.70015842,11333620.521882024,11333620.343556961,11333620.161851767,11333619.988796085]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[13748874.147779834,13748873.943481164,13748873.739454482,13748873.52664448,13748873.327171464]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[8918367.252537005,8918367.100282883,8918366.94765944,8918366.797059055,8918366.650420707]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[8904674.430885125,8904674.278824463,8904674.125658559,8904673.983518874,8904673.842158446],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[8353289.780921943,8353289.685612489,8353289.590303104,8353289.4949937705,8353289.3996845],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[14825991.122948844,14825991.073201865,14825991.023455175,14825990.973708797,14825990.923962709],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[12711106.760652166,12711106.195828365,12711105.63186608,12711105.03997651,12711104.490554871],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[11873041.40538401,11873041.375942938,11873041.34650189,11873041.317060886,11873041.287619896],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Lasso en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Lasso pour tout les paramètres de GridSearchCV\n","fig1 = go.Figure([\n","    go.Scatter(name='RMSE moyenne',\n","               x=alphaslasso,\n","               y=GridLasso.ScoresMean,\n","               mode='lines',\n","               marker=dict(color='red', size=2),\n","               showlegend=True),\n","    go.Scatter(name='SDup RMSE',\n","               x=alphaslasso,\n","               y=GridLasso.ScoresMean + GridLasso.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               showlegend=False),\n","    go.Scatter(name='SDdown RMSE',\n","               x=alphaslasso,\n","               y=GridLasso.ScoresMean - GridLasso.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               fillcolor='rgba(68, 68, 68, .3)',\n","               fill='tonexty',\n","               showlegend=False)\n","])\n","\n","fig2 = px.line(GridLasso,\n","               x=alphaslasso,\n","               y=[\n","                   'ScoresSplit0', 'ScoresSplit1', 'ScoresSplit2',\n","                   'ScoresSplit3', 'ScoresSplit4'\n","               ])\n","\n","fig3 = go.Figure(data=fig1.data + fig2.data)\n","fig3.update_xaxes(type='log', title='alpha')\n","fig3.update_yaxes(title='RMSE')\n","fig3.update_layout(\n","    title=\"RMSE du modèle Lasso en fonction de l'hyperparamètre alpha\")\n","fig3.show()\n","if write_data is True:\n","    fig3.write_image('./Figures/graphRMSELasso.pdf')\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.479e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.021e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.916e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.571e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.952e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.625e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.985e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.969e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.025e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.020e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.077e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.529e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.682e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.137e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.581e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.055e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.085e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.743e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.635e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.200e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.147e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.091e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.806e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.693e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.265e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.213e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.128e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.873e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.335e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.165e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.281e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.754e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.944e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.817e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.407e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.203e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.353e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.018e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.884e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.483e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.095e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.429e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.242e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.562e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.954e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.281e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.508e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.028e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.176e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.644e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.590e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.320e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.730e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.104e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.261e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.675e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.360e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.349e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.820e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.184e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.764e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.400e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.441e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.267e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.912e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.439e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.857e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.536e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.354e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.009e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.635e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.479e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.952e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.108e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.051e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.519e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.443e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.211e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.737e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.536e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.842e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.559e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.153e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.632e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.598e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.950e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.316e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.258e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.730e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.425e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.365e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.061e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.636e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.536e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.674e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.475e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.831e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.649e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.174e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.934e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.290e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.587e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.712e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.765e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.039e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.701e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.749e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.882e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.408e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.146e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.785e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.527e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.817e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.255e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.001e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.934e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.820e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.648e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.121e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.052e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.365e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.854e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.769e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.242e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.476e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.170e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.887e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.363e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.891e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.587e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.013e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.919e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.289e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.484e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.699e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.950e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.408e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.135e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.810e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.605e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.980e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.256e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.921e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.526e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.376e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.726e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.643e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.008e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.036e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.845e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.031e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.759e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.140e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.495e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.963e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.612e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.062e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.874e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.247e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.079e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.087e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.727e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.986e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.353e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.840e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.193e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.096e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.111e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.456e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.950e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.134e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.204e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.304e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.413e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.557e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.057e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.155e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.309e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.656e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.411e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.161e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.519e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.176e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.622e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.510e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.262e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.195e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.721e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.751e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.844e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.606e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.213e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.817e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.359e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.933e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.230e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.452e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.020e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.542e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.910e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.247e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.787e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.102e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.999e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.871e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.629e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.262e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.084e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.276e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.711e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.953e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.182e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.258e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.290e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.165e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.330e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.030e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.790e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.104e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.302e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.864e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.243e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.399e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.317e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.175e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.935e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.314e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.465e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.387e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.003e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.325e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.241e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.527e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.453e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.335e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.067e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.304e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.586e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.516e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.364e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.345e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.127e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.576e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.642e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.184e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.354e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.694e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.362e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.632e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.474e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.237e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.685e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.744e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.370e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.288e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.524e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.790e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.734e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.335e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.571e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.377e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.615e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.380e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.781e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.383e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.875e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.834e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.825e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.390e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.421e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.656e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.914e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.695e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.460e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.395e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.866e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.904e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.950e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.731e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.401e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.133e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.984e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.617e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.940e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.497e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.531e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.406e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.765e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.973e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.015e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.133e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.617e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.796e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.562e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.617e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.133e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.410e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.005e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.044e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.415e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.826e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.133e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.617e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.592e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.072e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.034e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.619e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.853e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.419e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.617e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.133e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.097e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.061e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.422e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.617e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.645e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.879e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.133e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.121e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.669e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.133e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.426e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.086e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.617e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.902e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.143e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.110e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.924e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.429e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.132e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.691e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.133e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.164e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.617e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.432e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.945e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.183e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.152e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.712e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.618e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.133e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.964e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.434e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.731e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.133e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.201e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.171e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.133e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.982e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.618e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.437e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.618e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.749e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.188e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.217e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.765e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.618e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.439e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.998e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.133e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.013e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.134e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.619e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.205e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.441e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.780e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.247e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.028e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.134e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.234e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.619e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.443e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.795e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.041e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.445e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.260e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.272e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.134e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.808e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.247e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.619e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.134e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.053e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.820e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.447e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.258e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.619e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.283e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.064e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.831e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.448e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.134e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.270e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.620e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.294e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.134e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.842e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.450e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.074e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.303e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.280e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.620e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.451e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.084e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.135e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.621e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.851e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.312e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.289e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.093e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.860e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.452e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.135e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.320e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.298e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.621e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.868e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.101e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.453e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.135e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.621e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.211e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.328e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.306e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.109e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.454e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.622e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.135e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.314e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.335e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.211e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.883e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.116e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.135e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.455e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.623e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.321e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.342e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.211e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.122e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.136e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.456e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.890e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.623e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.327e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.128e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.212e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.896e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.348e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.457e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.333e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.136e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.353e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.624e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.212e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.624e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.458e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.134e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.901e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.136e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.212e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.339e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.359e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.459e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.139e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.906e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.625e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.137e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.363e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.344e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.213e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.459e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.144e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.911e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.137e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.626e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.348e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.368e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.148e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.213e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.353e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.460e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.915e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.138e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.627e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.372e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.138e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.920e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.460e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.214e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.152e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.628e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.376e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.357e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.923e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.156e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.214e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.461e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.139e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.629e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.379e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.360e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.215e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.927e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.461e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.159e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.630e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.139e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.364e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.382e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.462e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.140e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.215e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.163e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.930e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.385e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.367e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.631e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.462e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.933e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.166e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.216e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.140e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.388e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.217e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.370e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.632e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.634e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.936e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.141e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.168e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.463e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.372e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.463e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.390e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.217e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.142e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.393e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.171e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.635e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.938e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.218e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.173e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.143e+15, tolerance: 1.794e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.375e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.463e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.940e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.637e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.377e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.943e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.395e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.219e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.175e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.463e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.638e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.379e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.397e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.220e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.945e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.177e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.464e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.640e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.381e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.399e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.179e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.946e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.221e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.383e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.464e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.222e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.400e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.464e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.642e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.948e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.181e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.402e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.644e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.385e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.464e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.223e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.950e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.182e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.403e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.386e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.646e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.224e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.465e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.648e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.951e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.184e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.405e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.226e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.388e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.185e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.465e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.952e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.651e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.389e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.406e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.465e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.227e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.186e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.407e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.653e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.390e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.465e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.228e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.955e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.408e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.187e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.656e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.956e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.391e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.230e+15, tolerance: 1.881e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.465e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.188e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.659e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.409e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.189e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.465e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.957e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.392e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.410e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.662e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.393e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.190e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.665e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.958e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.465e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.394e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.411e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.191e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.466e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.668e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.395e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.958e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.412e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.466e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.959e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.192e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.672e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.412e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.395e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.192e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.960e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.466e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.675e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.396e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.413e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.678e+15, tolerance: 8.934e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.466e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.193e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.960e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.413e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.397e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.194e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.961e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.466e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.397e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.466e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.194e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.414e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.415e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.962e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.398e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.195e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.466e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.398e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.962e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.195e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.415e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.466e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.963e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.399e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.415e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.963e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.196e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.466e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.196e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.466e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.416e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.399e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.963e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.400e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.416e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.196e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.466e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.964e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.400e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.416e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.197e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.466e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.964e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.417e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.400e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.466e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.964e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.197e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.417e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.466e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.197e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.401e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.965e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.401e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.417e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.197e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.466e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.965e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.401e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.418e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.466e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.198e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.965e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.418e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.198e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.401e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.965e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.402e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.966e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.418e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.198e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.418e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.198e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.402e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.966e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.966e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.402e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.199e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.402e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.419e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.199e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.418e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.966e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.419e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.199e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.402e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.966e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.403e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.419e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.199e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.199e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.403e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.419e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.966e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.966e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.403e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.199e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.419e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.199e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.967e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.419e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.403e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.403e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.967e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.199e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.419e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.419e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.403e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.967e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.967e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.403e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.967e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.403e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.419e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.403e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.967e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.403e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.967e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.967e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.403e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.967e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.967e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.967e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.967e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.967e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+17, tolerance: 1.840e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 8.934e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.404e+17, tolerance: 1.881e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+17, tolerance: 1.684e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.794e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"name":"stdout","output_type":"stream","text":["              paramètre  ElasticNet()\n","0     elasticnet__alpha  65931.882713\n","1  elasticnet__l1_ratio      1.000000\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predEN=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[6444844.27503826,969560.4006572596,2374908.9935609647,3070900.31727812,4844034.203712286,3834919.7326332107,152123.60535750724,1522438.0370191173,1766089.2603077453,9671724.289353441,9657000.634869946,14048854.857409023,6602684.371238205,17622803.43890877,1207507.506725413,3286851.9008283936,908528.2011961995,4230713.414504178,993142.6700217943,5907181.807507655,1867905.154700191,905387.8147206686,4452911.23479566,808060.041043805,545739.377590098,7661377.670056782,3278957.255031512,3775452.463355638,6687633.601686206,23683922.726752117,6401989.593459994,831028.4414894353,1834299.7036930001,1333806.1710402376,4206820.343642984,4302054.216007081,2073958.8589639775,527724.6125402022,4099280.9604821997,1502603.9753453652,1058863.5363982087,1682151.0168399205,2255664.886267039,2774655.1465589353,3074928.101693225,28022463.608307913,1146425.743722191,2592607.696912947,1328191.2418632011,10073528.722694416,979501.1216708273,3261445.773892397,4689627.065378204,2212779.31920547,982518.7919102898,1564943.2530761252,1458597.102284947,7479794.463907238,43292627.713313274,1395660.5189993242,1618086.0952140244,3261445.773892397,1198817.5661384745,21568248.022735007,3242098.901787325,2033567.6978757286,3800753.077687932,7651176.081703084,1989258.7347741579,1713392.58842281,2726593.1221148996,851859.3066947956,8150690.791659793,1965456.4339958045,1250672.9814582998,1230474.142284291,4487884.873356855,1286152.924435845,2185009.503269972,4071930.666726101,3400059.43487113,15726587.476589791,1076927.204768857,1253433.6605580314,7007972.455830014,870921.6100454668,1053522.2559969923,2362948.2847773503,-92734.0495686899,2131039.961308981,609052.8285831013,3089516.8301191004,3767333.174532402,904987.8254172509,249120.85250636912,862108.2159454739,2893126.05657214,78014.982188805,5868712.306016486,3815106.2434519427,2532821.5268223537,3891097.8934904775,1220894.7066487852,1713932.5896472312,7349072.713800219,5523732.02400886,19294950.184999205,2749590.7100926014,429936.89213172114,2133133.335418485,989093.1694646226,636689.4064106632,2644704.78124831,958053.3091178713,416828.15767163015,6846354.742411635,575157.1238001585,126336.46501108771,1948758.97396934,2135431.4819075554,3213127.826292903,18833186.013589893,757450.7532118675,488362.5748047782,4429331.61674011,1779568.2306676444,4826223.0785065135,2900634.538715988,4535085.110999528,3063682.426361981,2825912.1126801134,11517949.005824964,3112112.1575169275,80109.07111103833,14567742.930395551,609052.8285831013,-201745.7075732248,1583784.8445690833,1441148.4766587794,3590085.1216259943,2088153.4430589029,2693158.394842689,1471076.3487509603,1295383.2213546839,1010845.922328644,4212192.424873652,5093391.470681205,8775209.893230207,10008374.901370674,1184861.7279272815,9472707.891604584,4863439.437823694,784180.8141698439,314238.2157285679,6786791.675532837,213892.97126759123,544869.0179483558,4811341.982074047,8481174.46233797,646106.2234282959,3611451.296232977,3547627.8058839594,972403.8296233644,1900121.2901131369,19115557.60495976,1439798.1713724453,5003486.200462522,1477970.6722250935,14993703.39144196,763606.767240095,572736.81472985,960829.4251498503,1650971.0882002655,740958.3707768847,2254256.7181794783,2926384.3201709874,1611723.9111527377,1594826.007668941,798963.5922762346,8216553.437857584,2644658.207396658,854369.1619142338,1388438.9604452346,1967160.7009546934,2612246.9830832295,382344.1241292597,2644720.098038037,963647.0919415653,8409180.685771942,21051589.744653832,14414770.870628603,3346754.9397273604,3880370.978498771,1661775.9189867172,1672019.3493598131,1104671.545053352,571262.0527459618,997843.8425137354,15137131.622857325,6912704.816011979,1582639.295404105,3046742.2153265695,1353036.67254171,545500.2698335298,3198912.610752157,3041943.1520951325,2643432.4698714027,44786.34627487697,1644379.0296639914,5781068.496173223,1613420.827940167,717263.4891509092,905387.8147206686,8692203.772047741,2732418.694639238,3336802.9457728434,6540841.597433375,8381450.651810871,595406.1119218715,1441470.3131238564,18777146.935972348,2813247.133505979,1088429.1886288363,3430384.228164835,1635832.1167208196,28796845.48770529,1426396.3284357595,1904425.7089756224,2787276.2787862113,939067.0766559704,8595092.868365869,4819305.707242533,1492503.1865823579,2559841.040664298,905620.005529068,5615219.053463755,21695298.021033294,10834033.535369322,5883865.128945094,786109.9220139291,2672237.317144104,2502390.249803083,2827857.3147826516,17942340.035142124,593147.4407908008,1466541.2668499583,1262655.4225773928,2532821.5268223537,93164728.94803065,794049.8021968659,1255466.541232843,2561416.100117727,3089319.4503515777,5902451.100912612,11172140.745327018,15817187.187921226,801686.2525975732,4223011.102648356,1230085.900009014,786838.7629141959,935475.2841505113,2904151.1660093637,1920030.5768956298,2816692.380359522,70120.18425043393,2646693.3859029133,593079.057014328,15362301.714790057,793398.0765691462,1098042.5087235135,725667.1091875853,6019138.959360336,4629890.946827343,6375526.823836833,1843244.2638496736,370755.94990168605,-109117.74551401334,1240902.6833150391,11297857.687193736,-126154.73103519715,590415.8295869585,1108825.8303928066,22234748.508963358,14508076.012763325,733762.3888456605,32013857.851042982,1942092.8538057911,9205304.054066628,300462.697572988,2213238.211077203,1376745.5793014495,4347844.219209137,88551328.56901878,974530.3172349725,8557278.48576397,1400927.3930864632,2649675.585684165,7560655.788173218,1567470.290122318,28545632.547264945,1493370.0176916965,10898472.545600124,874783.777806446,5121902.305688309,3438403.2464522277,1965978.645405705,3451289.5273588146,3315928.0605148016,15731776.14355971,5873950.328099324,36587383.10829204,1449219.3307990388,75598.7990812948,1716789.940991015,5011092.578585583,2612909.370022106,7778739.805785309,5152643.867285605,7014728.979604927,939965.5832375889,719745.6327389528,1679601.2511614964,1179984.8892317852,553749.2541872878,545500.2698335298,16131831.400728501,3834919.7326332107,6432482.444906209,1996066.4744301052,4844034.203712286,5174857.275363553,14926735.857919235,1373446.9166170056,1072325.4712915693,7095341.237480807,9038518.388823736,1729364.737041997,2337001.2618808,743354.8590007033,3529502.4225896117,27987072.99179416,1185837.8355450293,-652486.2012777408,-136088.0362553671,7635673.408288615,1038257.644496348,4653762.74794842,510612.466158553,11643289.314007051,7569820.9467851315,3103908.7939815484,4262235.642442096,895347.5159651977,712839.2031992446,578253.187851513,3385343.027210458,-157584.30948823038,475407.2330829222,56424610.59973033,13788097.407217566,1906320.8345312923,776991.3494983881,15840434.359046534,5139157.501632226,926508.3801346077,28019451.49832115,3313428.3686956298,13593140.369867984,1456033.587714754,874878.6761789783,3590741.6307766954,549124.2341457023,1017199.7248839734,819305.1011709538,1196574.852790772,1815980.718927044,949452.8462491019,5581752.858487133,2246960.356939113,3286961.131606618,1423773.1693171961,1511722.703216618,5686646.401944855,664226.5249319333,2054485.0968574965,1083465.3242751884,1482093.4678711111,6530413.629054046,2029606.3157811952,4062221.0483772364,906002.0974772437,986960.5869544938,6163456.376506505,253806.50119445496,10140721.664490934,968841.453722114,7722962.0174036585,4282022.888445203,5391569.345782958,953636.9867978385,1109037.8466763678,8794347.527888972,2794739.1846374553,527724.6125402022,1186820.2990009254,107707.73870895151,9690190.551613038,3137953.644673575,717263.4891509092,1180498.813770157,3231633.981796805,6689792.97552139,8661048.444061264,1310711.5015816242,61302159.406195804,1162007.604888423,1990904.807486496,3194275.2214098196,26512548.31827974,949014.1068771072,4379352.665100062,2002064.21224942,4341757.871128696,5500704.968298588,1391007.7748178227,1728845.002954796,5546900.339263164,6351566.208098512,3391632.2117513926,3918523.927572114,10079648.267774554,4154040.1633301456,6253116.447495655,25347079.03682245,219521.8924188828,1934975.7471353575,3307509.78285937,1541563.2655205107,1872753.700288705,2352800.4724201364,26212942.149353135,7291028.066852818,-522099.97374005197,695867.585967991,22322164.379879203,945206.3882856544,10100839.911636919,3685643.7366377776,707046.4875428313,16497499.751888126,274716.29272463033,2725646.6028325004,35163661.85377997,2985076.080438508,1387836.9175732613,7094488.728857976,6503600.545271684,3373362.337104148,18331557.141361054,1200912.799021604,1696149.7904763464,1438046.891516578,5033473.818300247,1713392.58842281,2569373.678406937,4558761.491087417,3388340.4771044794,15023080.389456663,2303277.8301988496,8390014.116569053,855299.1875862954,16954564.242490217,5774959.712896045,252750.71921880217,2520910.4299659124,5198432.984339226,1074985.4428678302,1038865.4844201137,6271620.58518737,764554.5625190986,1700979.39683773,2639431.389901403,11003162.606047876,252927.9593718683,1608019.7274194798,3313428.3686956298,1867905.154700191,746063.9569041794,1752959.1419945487,2529028.1381370667,1167363.2975371445,3093775.471616187,112613.99424252892,2173492.258356905,7419421.873888277,1610797.9406588073,2040940.4469123092,635747.6006894354,11338174.788374808,9390209.400333868,8595092.868365869,8429614.377619749,8492409.304875981,1392329.2700160388,3397889.1757563027,2252213.0593120437,983500.0124084183,1276024.456586666,8406686.645063628,1972759.2712312187,935475.2841505113,11731418.158677608,519829.75290984847,2572329.719634482,1646179.1937525915,14040020.251012467,11482128.027086865,2782022.4392186087,1160628.1024140762,2683328.2186830984,42092124.217290804,10836042.73479139,1067011.3743895497,1195290.2689580298,6401989.593459994,2572329.719634482,837280.2892905697,2580131.806375047,1351062.0035062376,1140888.8690228413,595779.9707281056,2534157.2187818345,1654227.5326263472,4248686.2667151205,201818.86941287713,43204253.3946105,2374908.9935609647,6212121.433975415,8923515.169957623,6397401.442720927,784272.986793837,16654365.229413968,10412662.125059664,2187908.8606063835,13877874.639901966,1644379.0296639914,8085662.207338847,2644720.098038037,1332785.7549165462,4657051.025550056,2163777.060601797,1388176.4268702362,11148538.036325037,530990.9169411995,2116844.3724482846,1495856.751986056,7479905.416096909,3642805.4800629225,5199603.878169684,61302159.406195804,956181.4478005923,2421327.999092976,4755371.09179862,7560101.580420044,4837966.169531738,2504095.127888697,2580131.806375047,345088.13018978434,5780754.477283139,1110379.7304811508,16396534.906262137,1283033.6798074883,1366090.7016002066,13257273.051630184,-103841.75385602657,13626946.13023867,644078.4257004496,902714.8086248711,1045359.7054051221,662586.1787953225,3826246.096435562,974530.3172349725,2522355.298464501,607697.9252989551,436262.7634598287,6019138.959360336,1274727.8412439658,2869865.0671487306,20091348.217455525,3769646.2738586133,1575206.273277964,1435755.610436289,2247111.1252740296,4821993.470691953,797736.7071565865,916909.3927197964,31059932.685637455,724084.263326393,641293.6952013539,775121.8279794543,1623847.3497072065,3340238.665681617,1972759.2712312187,3461851.27568577,3128751.5550516155,1004473.3855131681,1461392.634425886,12136228.075468708,595779.9707281056,3137953.644673575,1124765.1770838308,2456893.800379798,2389004.887772129,6743950.957155387,1893318.3002692917,2179852.169412424,7651315.349132973,9370013.666280653,2840466.400089953,4665593.881541746,410598.6518663552,960111.2545271765,3243321.01267601,9329254.056934394,15700454.823887508,6325974.31949049,1113237.0818249348,2765889.1410556445,863363.6154396185,835250.9651217468,4212192.424873652,1394340.2310441858,804204.7029702938,-522099.97374005197,1251226.0172022576,1241036.5674161701,841973.0494134694,763919.0171427235,1597127.7814408909,2688866.9813604048,893681.8914735548,1100053.4746249355,954798.858440697,1182755.3530073704,2146953.059906683,3278957.255031512,1743432.0193066732,966030.8840483092,863363.6154396185,4148481.5899151685,5717628.340772385,1074063.7166278998,645484.7656245555,4230713.414504178,3881819.671443586,1311082.2206201113,35914220.31840717,21919410.047575537,13580920.651220243,-128245.85627799248,6861960.382693221,1306430.90171431,1188529.0811076974,1141553.6291700974,757755.1209463524,592750.9709484745,570977.0253741653,3236354.0333087705,8978950.002192639,3271748.6250130744,1651771.8029705607,11432482.665975772,1397399.923555425,12295817.20442596,3711865.8455112847,2466381.6041041636,-333848.61350232456,1065525.1543809325,804204.7029702938,642281.9833438674,449864.263215696,9493526.274449207,12138977.53359373,459265.7973346708,1610385.2932356037,1905252.6495698493,8436539.812803004,2297655.300135275,1890528.3904187542,1253345.9875540973,106900132.84637271,1795500.810301181,3077248.506820946,885498.9491986749,2627100.680991449,1280369.9942915067,524069.66926153423,758194.6514635799,301648.60901452904,16900173.831330523,46945467.36734793,1262286.7320814205,12620732.221261133,4438979.54192304,329072.22639629245,1427464.3417266558,796716.291032895,23263444.250615485,3606558.415344295,3139757.4274550662,457306.1382315343,2389762.8083702596,2319776.7298936006,551749.3063219469,1477274.6107305633,10335994.132673968,508752.5091180522,3749818.4358087424,11121867.479465898,823931.6844593638,1479314.9728350947,35914220.31840717,1200968.8025827887,1886094.3815616868,16900173.831330523,1249751.2552183697,13972880.408174172,1900121.2901131369,2048407.4903386051,2685886.392393886,4655049.296861284,849787.1165153035,2962904.340036433,5265442.481982153,1506925.910678439,35012263.45281849,1182082.5860066738,698876.4211888239,15872505.239086486,1515682.3099577762,4492838.513153101,5751556.633471616,2405079.9274230865,532088.6875489273,5343997.760101713,2380150.7732232525,1244503.9329105355,10639560.806635568,3424116.4897333095,1663397.4495683305,1782578.6964217382,4137433.9095555427,13134691.57302424,5670786.143444853,4275462.860446801,7382272.447346142,787010.9666995408,-119833.5969610149,2097435.6671881126,15465252.018530495,972403.8296233644,1714386.935507428,1333641.3775715567,1405385.6063990742,3834538.3194320886,8546212.840079334,939866.8933538275,1899685.0985916867,4768652.944276232,866337.3936740865,20858015.282269575,1522438.0370191173,825665.0122264724,9493526.274449207,1618086.0952140244,1874029.920263343,2071989.927315685,1804156.5540168637,607697.9252989551,10081251.671330525,235566.1491663684,2677690.5724466816,4783432.7275640275,5622559.111359479,1119800.8899077445,499010.2069492573,1342748.33109671,11041457.76255899,587562.9358021119,2860560.0321204322,1443781.1459834503,6634491.254082607,3445570.509907649,20081928.47861119,11738344.660094246,1934212.1264710184,10619203.691252645,1062518.8744303016,722523.8459782805,1020312.3340917872,8794347.527888972,10754792.449087504,53869250.36563373,989344.0406587755,946773.322893536,1534097.2829770758,3564711.3977774214,1100148.5692179254,16131831.400728501,4199357.833635084,1623847.3497072065,10634268.495348252,963081.3600805325,4755371.09179862,450971.6138418261,1083465.3242751884,9307844.21792461,2135707.1135388375,4206145.522869454,1006929.5676704689,1204602.6454290391,793689.6946092944,938023.4408739668,1014243.6836564282,1074893.2702438373,8133301.042770774,7778739.805785309,1240688.6034614823,178312.98712988384,22876488.227521777,11988026.90667022,1332785.7549165462,1587334.4213560089,1595582.9404401903,2770500.8612194806,5991934.0697756205,1267553.6061685593,3345657.268837766,3391744.346932453,1754059.30737036,346446.5055672112,440827.3854998504,2630582.8179980726,14804136.994547043,1085046.2243963962,153616.36582163163,12463428.889675364,3683752.8319777665,4608910.633867705,2931943.498828811,12620732.221261133,1686662.527464833,5581752.858487133,3073379.9777223747,9025157.553301383,4826223.0785065135,86446082.12666172,1956636.1431403966,672467.5938903007,-359195.9889599802,9891998.887029715,3693573.6268360377,3476802.4580794377,3493555.5992167974,2209435.8616414885,2319382.3762486577,2465299.567484063,16844595.793412637,15667075.953263681,328827.6565957931,11791413.838077657,747165.4770604772,1891284.5385068615,48295778.66570683,1714116.9348952172,5899411.564177878,1294928.8754944871,1254728.5769139926,908060.8208164664,2331574.8257647073,4467918.43514302,5979049.454195903,1045042.3745896344,878406.8913717696,19605398.36478748,3103908.7939815484,5013719.045857208,11059.742810660508,2544592.277072351,2855951.4009207813,938945.1671138974,1467449.9585703523,12250832.252066895,9849503.708772622,4642329.474750247,26231547.984880187,7542015.426084956,1195290.2689580298,1637811.0367485313,20398600.855249275,5582134.583502642,3957687.2189406026,1815980.718927044,24741452.765192937,4570158.31016371,3210247.666682667,22322164.379879203,3459276.3937742272,79658780.26303743,7850245.893878255,768972.3312109953,155761.06645639613,857621.9641607446,4154040.1633301456,2030945.2007083902,6215909.278628927,1072299.402252496,5691837.85632304,1723801.577674253,3157110.0831251685,614141.250759297,4086978.3702643826,4364830.290206098,11041457.76255899,4710194.595048185,1078494.5198393334,4344995.641446811,3427166.7337448085,7349072.713800219,808613.0767877633,4364830.290206098,3244035.483937307,7542363.260702485,1437335.579563707,5134548.864562131,3376544.5083047934,2720088.953155066,925955.3443906493,1717040.3898239206,2903611.1647849423,5199276.466844501,2992195.5505062668,836633.5544816423,6529025.879789138,1194625.5088107735,2782022.4392186087,1719221.629582169,1579004.9026409828,268180.0453716747,-140896.46260497393,3083310.4586088397,4442777.986565339,4025645.432898407,21448282.249061123,11384776.698172707,3905621.696895034,7569659.391923957,1904.726349175442,4080967.493544707,510369.7088862036,1471791.0110701588,1644151.3960247447,1698732.3692629132,863363.6154396185,3016055.2487966614,-123148.95854607504,1185003.879212543,27606214.1229331,283188.8113630079,903865.5745558413,1041078.9517579344,1267553.6061685593,1451609.301763089,2189457.6740870033,3871400.8354161363,998495.1994731582,2409779.180081035,885247.0881359384,1851355.4547610595,853040.2815524002,992214.4265220962,11711239.802737884,1074893.2702438373,1208535.2755460078,8454893.406034011,1110648.8448526652,1013973.6830442178,75965838.81982836,1207904.8839255373,2164575.5188374687,2486775.3642006805,1235286.6705112334,383132.6586008072,3142516.088915088,1937894.1270768326,1122916.8481642632,2987500.5869035944,1013248.2118973909,4938452.829611126,991187.493138636,1113237.0818249348,4195285.731124319,1397227.4536072058,1954404.448385259,3094875.8768302323,2634854.3601747314,4242267.53913865,2063438.1453092361,1266059.292405366,2133133.335418485,906002.0974772437,13877874.639901966,3823480.917715771,11927976.498762202,2353204.364510443,1283033.6798074883,6860706.687143425,1450970.610654906,8717557.699353755,706485.8094034942,327545.97771914233,1486536.2089966764,2055202.355752164,1679601.2511614964,3918523.927572114,133070.1650474863,1990904.807486496,109207.09993043868,-62277.08542951429,1100800.2948456449,4023778.5469123777,3237360.4626668836,13763884.388377592,2744567.7675786,2650307.759532579,1634376.3146757814,1000516.4799412363,1925745.279583197,1200301.9062773022,1904810.5296799103,1629239.1745255534,6113529.312601723,2247460.050156485,7014728.979604927,842065.222037462,-281806.47716560494,1267277.0882965806,436435.01200088277,14127316.311128415,1874614.492064924,1240165.302323095,4372655.380142699,7542363.260702485,5548228.092392746,4491078.548802648,4215360.92352966,3999835.020933859,2601456.2688162774,853040.2815524002,2582311.0899572102,2456031.116116486,2807144.3956729514,2010205.4075040054,4491078.548802648,13947067.776107363,5448118.241982317,3088643.0250563296,1160138.1725604003,-59974.13781110663,576884.5828095363,4429969.866703391,6433144.869050914,4837966.169531738,1107043.159889859,4128725.2804681296,859676.7104798979,26112657.215824775,2825457.6494510965,40512896.55989661,895494.119840333,779825.7455870218,10226831.390153509,1132593.3328634694,654216.2407377162,131941.25775614288,996345.4933594931,2386551.344368686,3878459.1375964815,5341478.379516629,1658396.2165158626,7681853.286859975,4874005.309001019,4351445.707947805,1097757.6700337857,1209026.0156808668,745893.5050386274,4168269.2243729653,2331759.0848724185,1182082.5860066738,1399374.1221266584,2046208.3818823094,2164301.0649966765,1995577.150051714,3845829.415455713,2940737.180474825,2617307.1812784676,1884712.2488954556,5096918.339024067,476152.84467042657,1245603.4871386834,13972880.408174172,13257273.051630184,2994389.095885768,4519495.95326639,2797102.1959305895,5965229.810415469,37541697.94121468,1985670.5196981984,919404.5708273763,2354176.670422303,10192147.63267525,870469.5787487845,9530392.57784208,9536039.607032344,6037941.243667675,784670.9510235037,1244140.9750198203,3589750.695048771,63439038.331054226,917014.5998633262,15032145.892906822,182144.9062546175,819765.9642909188,9302683.895973058,2539259.2994002923,2185009.503269972,1837813.3956490103,1735889.8713428783,853863.3179085688,3019634.4230045616,1203611.2729473594,4376230.149474552,809088.6137637568,5572984.859929999,1230085.900009014,1868045.216963565,2897455.0157604953,24371233.195082046,5570096.567908206,3235226.345169996,3529502.4225896117,3521823.1097621107,1110379.7304811508,4049653.390857189,3433922.1379524497,17716023.091013588,1429303.5267567774,6565448.8372686235,2036919.1280904822,1421567.5436011318,24947991.144978166,2054487.365010927,5720412.887168071,155961.060405849,2116844.3724482846,1398045.2693245197,2810319.434784467,1357606.2598097427,4545363.874309587,1160442.4998288988,989357.0751783124,1275013.071452226,4331681.662854688,4408545.770382229,2980641.4094040357,2453202.6256578285,4898226.747993952,722523.8459782805,5897262.242550959,2635487.518849008,1120057.8560004183,10898098.499676228,3101752.385985527,3858450.4314704235,1159594.4426803654,3254874.517562583,533286.9316608366,1882175.7751812637,2360628.7541018142,1077932.3190163232,11873213.185809206,711463.1310991175,6307752.912273537,796716.291032895,4630881.097226631,5798806.537866434,1891284.5385068615,1227149.410560774,5500879.528013561,47298.31515187025,7094488.728857976,8819674.749792276,2081157.090705231,1213632.6211326416,3438403.2464522277,16051002.526746389,40512896.55989661,755060.0813005352,12921437.008715633,889999.8311317854,5013719.045857208,2160164.6011618455,2204214.590263199,14991097.059503864,3823480.917715771,1495966.32127191,2233562.5489607602,4011829.6230793633,5848833.443137448,2582311.0899572102,1579004.9026409828,5986568.50580472,4461272.037719242,579658.5469215875,2765667.9909150223,1454657.515614627,6095482.397544436,1244043.0697905705,554082.5800706516,3382402.219150855,2932473.2784430636,32978063.611724325,1095107.375880048,10995988.92335673,2765984.0245953747,3682793.777310921,994334.3968739354,10909698.399821393,8355891.874584439,-136088.0362553671,523562.5962006268,1844350.33533759,3306872.9120141845,664250.904753336,8246845.143140925,5190853.350208844,7511797.426940988,1617411.63931569,8122338.557687606,1820911.6694920915,3258433.6924894,535867.765132641,2632062.971162799,1458410.5241592675,3996279.844899512,3208884.8362919223,3155856.040962139,2580183.9444531943,283587.7833218789,2060982.0108768586,921962.0508009044,1685844.130116089,2964513.2866908973,2160693.298141412,3308341.1567383045,4918331.906693054,1328665.0091430794,3531751.5922138914,740866.1981528916,2203780.405031846,2781373.5778044118,4124551.70436641,39946.817232534755,2630582.8179980726,155761.06645639613,379495.39909467706,555122.4578956801,1649411.7528521153,2228064.9690205427,1017168.2753462743,3307960.1136139296,48255528.86323384,6415584.512303409,489938.336323122,949452.8462491019,1418303.0576141626,1083770.9537589487,7927934.381384836,3083310.4586088397,2076091.8638353536,740398.8177731582,2568502.653137712,26698537.488832947,3213127.826292903,1425774.5950921788,2743033.4192695604,10029719.74107669,4916595.4559025485,7510787.180861944,2456031.116116486,1025870.4687990858,825665.0122264724,1354130.5692127852,624508.326056296,3820409.355452166,747662.5464467276,5033473.818300247,1415977.7382948291,46009042.238230236],"xaxis":"x","y":[3195230,730765,1234149,1069095,629036,1328920,1677877,1700057.375,1116040,3466465,5722325.5,5674450,3098638,19459304,1418400,2378077,521341.9063,6135173,1066894,4660015,1713381.75,739711,6779778,760008,593042,2887392,2643948,2668769,1989387,10498344,37951708,1441900.5,804237,860505.625,2014500.375,2349950,1034827.313,965651.125,808968.1875,2074152,1832031.5,687953.6875,932598.6875,1207700,2390284,21346100,1750099.75,1355558,1435730,7166015,1385975.75,3155391,2075744.5,1073156.375,1394408.25,1247361,782483.125,11026945,37430780,1594504.875,2277316,3742915,1399062.75,33556756,1440662,1795462.75,991918,1250669,2545030.75,695828,3220827,1038815.188,6578370,1863468,745575,741850,10343027,659265,1107290.625,14172606,14585813,4926695,3343550,683307.375,2539565,974278,297715,4664399,1533245.75,12204922,515025,3043767,6118300.5,1764409.5,628609,4857581.5,3918038,858932,2313679,1984067.625,4971699,8611054,1344512,682632.3125,39061360,9560965,5696695,1813718,552007.5,3129811,675772.875,609218,1303247,2788860,1331469.75,8058913.5,1297872,1323038.75,1349970.5,993738,1827817.25,11813861,827211,520738,5049446.5,871355.5,3059758,2203026.5,3528086.75,3078672,416885.5938,3644982.25,1900006.25,615894,53166156,511180.6875,1970956,1290284,791052.875,65225380,872114.5,1241917,1019183,161363.4063,395517,4590246.5,2022990.625,14465798,3588718.75,1242666.25,8138616.5,1848656,572611,6293359,12783255,1109851,712057,4135280.25,3944933.5,520405,6807227,2564004,393493,1497886,19854012,2120807,2711776.5,638772,7080118,1164052.75,534110,1169932.75,928409,653175.8125,10608788,415975,230365,1365644,1022614.313,2882489.75,920068.875,1962435,770333,775728.8125,2116209,635698.6875,1403869,781217.875,1976920,27787286,10791309,2476833,238969.7031,312563,1239315.375,796471,551219,924068,15660628,5169726,1280690,0,838394.8125,2094525,1070085.375,12525174,1834290,345399,9191826,4141785.5,2065156,520765.8125,727374.6875,7946961,3647194,2136086,620483,5792926,3275779.25,1003551,14204281,3045139,1094343,284857.3125,523614.0938,10564729,682249.625,798322.375,816508,652425,21795830,1662054.125,664539,10420096,3778355,1049748.625,21169502,4524791,41092076,770082.125,1210798.375,400054,1810353,19344782,1392299.75,563237,1008665,4877959,78272488,1469870,677536.3125,1011667.5,2503804,3471845,6206083.5,6310254,1813864,3625434,1605182,611536,547084.375,2454812,1429196,2826828,5136604.5,3258819.5,979952,16701055,791542,1360340,90558.70313,11935719,1688612.5,2278160,659652.625,3537977,967835,918943.875,3822102.5,636266,642133,608359.6875,9722237,602245,746858.1875,18848780,4560492,7015692,4133811,2563985.5,898115.375,1967434.5,61576184,638507.125,6235947,4291789,6454983,5155090.5,703298,32381836,3176347,3093944,1896626.375,2006647,6781960,2912454.75,2601369,1417778,20311228,6525887,8374301,207572,826416.5,2823184,614768,1868497.125,11208498,1363191.625,5453438.5,855665.125,532670,3468606,905926.875,281191,2147968.25,6219841.5,1539066.375,4934334,982332.8125,605971.1875,4471029,6462938,650205.8125,860364.1875,2613607,3567977,4520838,15324777,2720238.25,1938613.125,25307744,1929118,7798464,747456.875,6859198,934980.875,2150120,662701.1875,6456026,4423958,2037693,1045413,1663324,542998,1193790,4573777,1247972,1208035.375,48479988,23262244,919180,987989,17457560,11924933,576174.125,16849234,4166306.25,4558824,847097.8125,1055624,11521140,1423463,415453,740230.375,603376,1314716,666930,2133372,818836.125,1993273,941366.5,644121.6875,25478086,2225265.5,5348309,907758.5,1007650,8136665.5,1026921,5457975,733249,652449,16917964,890977.8125,5231373,1523508,6960904.5,2966309,7456330,947518,1989287,2634166,2094724.625,947884,1870457.75,1840259,29949248,4103800.75,508057,2621041,2746152,5662234,8381834.5,1021798,25475474,2071670.25,987288.8125,924073,13093350,1960538,2082754.75,832168,1742351.25,408513,1976856,1605220.875,3046719,3366510,8688676,4399828,6344171,1924355.875,4826430,8747819,876237,3347567,2996958,878927.8125,3234871,1433267,11110405,6289567,580688,635697.3125,13567774,665216,3063969.75,14830411,536642.875,6069855,1125724,2923484,14856602,1848878.5,717197.8125,12187231,10328702,2666634,7564604,683274,1136111,847298.8125,5827595.5,719905.1875,1857889,1318096,1190733.5,10317852,842593,11047668,4193371,51168308,2172841,954352.6875,2547582,9290214,823460,1147582,5481995,1605669,2926089.5,1022957.688,6723971.5,2807583,1162226.375,4329966,1547556,5437110.5,2076289.625,821046,711118.375,1108310.875,1210229,1385697,6333706,1160989,2119806.25,691647,9162048,8773058,23612824,7243389,2992693,1811669,1375366,2254220,1431784,1072855,3404992,2046914,436848,11799105,501897.6875,226375,352136,6711975,14431245,906103,1917399,533742.8125,36367960,6482380,764008,1666221,2019254,241478.5938,720170.6875,2842760,132105,676032.625,510517,710275.375,756267,2474457,2023858.75,64547116,1228113.5,6361236.5,14332778,2330242,647906.1875,15036617,12719042,5903142,22394416,9546165,2924362,1375978.875,1022114.5,3637973,1137504.75,831855.875,15640994,2696136,957460,850667,11325513,2962476,1988334.5,26689658,1358519,744615,12704014,2712983.75,2153603.5,2080866.75,2791291,1000180,6182350.5,711594,14169301,3137206,564988.8125,6802622,673834,6335315.5,492481,603420,1208029.25,1067153.75,1618779.125,628023,1856230.625,2361088,793436,12087498,867456.875,1218114,8320719,6797528,829929,684739.6875,1757194,325513,1666762.125,739869.6875,50899248,529263,650008,1072487.875,6879291,2148565,1509292,3377209.75,3292049,666436,400770.9063,7762038,472532,4673827,811907.625,33268410,1834204,6010407,1403820,1603017.5,4750287,465533.3125,1073507,516216,585443,1038667,436252.5,3024478.5,2290681,1381590.125,1494252,1242311.5,685310.875,1000599,3986851,879235.8125,1372213.375,592739.3125,737935,2345314,800626.6875,2817261,2446842,1476969,646994,1241432,1327591,2043592.625,1111858,2443141.5,1216543,779204.875,1224685.125,1396191,2773647.5,730672,1054995,5948007,2569106.5,898845,39403320,15026352,42584028,938908,2270531,789209.8125,866825.375,1991234,768755.125,1900577,2531070.25,1912719,3091348.25,1272821,794724.625,4382240,43943,7405070,719734,5424718,936336,612158.1875,1585441,3076813.25,867184.375,13116256,4998015,721983.5,1097572,1647746,8575114,2179078,3303553,615955.625,57514116,1820814.75,1425599,2051501,14084606,1576953,498649.3125,722891,685764,4545148,44984468,885865.875,13424079,2212058.5,549834,4801472,578150,32675250,3632613.75,1546484.875,607373,918177,2569602,1401315.875,4467292,7319546,941688.875,2003948.375,8669367,1862752,1140620,48981052,794715.6875,9393108,4222682,965786,13315803,804620.5,928084,3473109,4188660.5,445943.8125,3988729,1310236.625,1191803,17463744,742565,670779,6399127.5,216115,1842562.75,1857329,3475804.25,562338.3125,4108523,1651120,714898,4748137,3528230,1700276,1639849.25,1650279.25,49102164,7802205.5,6606832.5,4626443,3158151,964923.8125,2531163,4249212,440419.1875,1000552,405823,538064.875,14329739,3534690.75,659368.625,2114860.25,1811506.375,632517,7370743,1795049,594848.625,12726789,2442220,757167.3125,1076693,1896858,2603931.25,2127077.25,944638.1875,3157579,4486477,3180819.25,557863.6875,930439.375,1826203,19018762,466904,3055176,2347893,9956396,1436773.875,29747868,6657348.5,1056099,1987934.625,1119679,151798,735907.8125,2664502.5,12729858,157606480,1620721,614985,917194.625,19061780,646691,5889882,2299216,7768057.5,9097980,1406687.5,5481549,1027394.313,875789,4337107.5,689082.3125,1634334,425574,1440554.25,836437.6875,729535,592805,634751,2057288,13491170,1509775,961749,37785552,4628171,984054,1038045.313,859137.125,1142949,2411402.25,5833800,7338511,3611538,966812.125,616749,730292,1120197,10053489,1044220.625,492807.6875,9308471,2240549,3241917,1433573.25,13146645,1208407.625,2131817,1424157.25,11353672,2755578,163945984,861643.6875,718362,784652,2757588,2759449,1765813.625,2162760,2227908.75,6458021.5,2087998,22325364,6481807.5,2503240,9085924,785144.8125,2458760.5,61984596,900973,1859571.875,945063,715728,509171.4063,2504814.5,1878442.25,2434208,572139,520011.8125,24017770,1814947,2231695,1725773,53401,1383950,728876,1046995.188,6843855,9155568,2885740.5,7220201,4424263,1694930.375,550324.1875,11356990,1742870,1094622,966677.125,24224888,1602030,3888945,13609040,5981430,16644664,1252323.875,479358,2630101,7004126,1911591,1357553,12184124,896172,2830674.75,1143826.625,4767227,2108012,3548858.25,1986624,19036416,5448687,1602143,2505271,1817513,40613740,1361427.75,2345752.5,4020660,4391348,5976246,3075425,13883385,1984823,677642.125,1456808,1189432,1271021,1696009,609494,41399504,767720,950695.625,1993766.375,702385,2188093,735810,2504797,17186624,1885314,8728584,11951945,2495819,1000796.875,1539176.625,1730542.25,1223296,1392153,847514.5,991513.875,1122837,2098850,637053.1875,819314,25672598,653672,1358652,845961.375,5807879,911366,846274,7879628,1433504.375,2279063,755269,1138520,133880,524479,10110827,624269.625,1890502.5,3902177,563298,919048.6875,85357952,503744.6875,2485521,10642754,1686405,2189124,1427294.375,1353079.5,862945.125,1956439.375,866806,1972491,627698,1553208.375,1929828,1151027,1222509,1404702.25,1725558,8354235,911088.8125,708928,3441643.25,723482.1875,23364636,1566177,7657568,1262255.75,3444270,5328051.5,313528.0938,2567976.75,634056,488181,915286.1875,2559461.75,3494929.5,4116871.5,695803,989257,1152896.25,719471.1875,639407.375,1755264,3619497,13349651,1393073,4258124,1897180,669117,1769137,541943.3125,946138,2214604.25,5229981,966764,6211441,639566.875,602618,523133,749207,4321141.5,2222097.5,648022,5177270.5,4785692,1933811.625,2184912,2845068,1399992,1285055,315468,6782435,2173143.75,2070005.625,1692199.25,2409429,7794869,2999116,1614673.625,659450,981009.125,491792,1454222,1953919.875,2101376,904236,0,1417728.25,20828130,1827424.125,32552428,514471,1703060,775729.1875,1272272,622051,783208.625,1625308.625,962238,1465535,2834401,2050600.75,2309007.25,7758182,3837228,522972.1875,1982967,769148,1698779,1928564.75,1257033.5,7654264,1285646.75,3643112,1224045.25,2632472,1678495.25,2076612,1964681.25,7231928,728167,957632,13631685,6673138,5571501.5,1261725,4618360,2756494,136241424,1529309,1353612.75,2786648.75,13586070,1100499.75,52960796,6669954,2274873.5,1972156,817695.6875,1911611,58041572,117438.3984,6127771.5,656980,418098,4634655,419978.6875,1057256,41680640,694189,674961,3371659.75,6983736,5288304,1912760.75,14981114,1764767.25,806615,4116805,9569715,7475577.5,3721129,2014721,5101504.5,719060,2919980.75,971155.625,16373232,791564,4933374.5,10834087,605186.3125,27185096,1045838.5,4656777,834711.875,1008536.813,1688745,4262316.5,928584.875,1167734.75,298063,552401.1875,450518,3796986.5,3371455,1240924.625,1537570.25,227566.7031,164817.9063,5543575,161311.0938,606886,1083729.625,1368120.5,2646523.5,2513155,1167967.125,638201,5274642,1386445.375,730211.1875,8692328,1257144.75,4212149,593296.1875,1767480,14265421,2447861,948157,1739286.75,1121537.75,12029600,6797647,151376,1761137.5,6674179.5,6505995,30135280,696522,7837692.5,767361,1989763.875,1788999,1822830,5232420.5,1554342,1523324,1487660,3556291,2249620.5,6592304,335245,3714139,1594422.5,336461,1956075,1838121,3986982.75,817409.625,2946756.25,1521693.25,1880890,12265516,1965529,4103027.5,1686248,5276777,1988760.875,3515821,2218249,947534,2892779,756908.8125,10721568,357915,4571388,3022275,5051636,2429214,5446624,886771,5326222,584545.625,2158629,726124.625,930611.875,1889990,1423929.125,866126.375,783651.5,8201974,2346692.5,145468.7969,2399762,938096.1875,1145535.75,2468322.5,831577,4829951,431815.6875,2361960,1210262,8787353,2882757,1228837.25,2735811.5,576052,442375.8125,866242,1339220.625,880115.5,1656025.625,65047284,2356376.75,456747,673030,863703,686091,3456131.5,2503690,345866.6875,941564.8125,1012341,15091579,1733511,999856,2154119,6268627,5454482,2407904.5,1815350,799996,593824,585559,5063753,5572503,8228399,6014078,994194.875,27717034],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle ElasticNet() vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predEN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["alphasEN = np.logspace(0, 7, 200)\n","l1ratioEN = np.linspace(0, 1, 6)\n","param_gridEN = {\n","    'elasticnet__alpha': alphasEN,\n","    'elasticnet__l1_ratio': l1ratioEN\n","}\n","\n","GridEN, \\\n","BestParametresEN, \\\n","ScoresEN, \\\n","SiteEnergyUse_predEN, \\\n","figEN = reg_modelGrid(model=ElasticNet(),\n","                         scaler=scaler,\n","                         X_train=BEBM_train,\n","                         X_test=BEBM_test,\n","                         y_train=SiteEnergyUse_train,\n","                         y_test=SiteEnergyUse_test,\n","                         y_test_name='SiteEnergyUse_test',\n","                         y_pred_name='SiteEnergyUse_predEN',\n","                         score=score,\n","                         param_grid=param_gridEN)\n","\n","print(BestParametresEN)\n","ScoresEN\n","figEN.show()\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"y":[11333619.988796085,11333619.922188938,11333619.849974187,11333619.771679733,11333619.68679372,11333619.594761204,11333619.49498051,11333619.386799317,11333619.269510377,11333619.1423469,11333619.00447755,11333618.855000969,11333618.692939952,11333618.517234996,11333618.326737404,11333618.120201768,11333617.896277834,11333617.653501661,11333617.390286066,11333617.104910243,11333616.795508493,11333616.460058073,11333616.096365923,11333615.70205437,11333615.274545558,11333614.811044613,11333614.308521364,11333613.763690552,11333613.172990326,11333612.532559004,11333611.838209791,11333611.085403446,11333610.269218607,11333609.384319615,11333608.424921658,11333607.384752976,11333606.257013856,11333605.034332229,11333603.70871548,11333602.27149824,11333600.713285778,11333598.633060003,11333596.768697996,11333594.747397104,11333592.55594784,11333590.180029145,11333587.604114894,11333584.811372552,11333581.783553336,11333578.5008731,11333574.94188328,11333571.083330978,11333566.900007281,11333562.364582907,11333557.447430018,11333552.116429118,11333546.336759757,11333540.070673706,11333533.277249131,11333525.912124196,11333517.927208338,11333509.270369472,11333499.88509498,11333489.710124427,11333478.679051602,11333466.719893375,11333453.754622618,11333439.698662242,11333424.460337136,11333407.940280568,11333390.030791324,11333370.615137517,11333349.566802807,11333326.748670276,11333302.012138931,11333275.19616744,11333246.126239216,11333214.613242539,11333180.452259023,11333143.421253089,11333103.279654682,11333064.109399587,11333017.311923057,11332966.591275658,11332911.608290227,11332852.012408417,11332787.41809286,11332717.407844165,11332641.529597497,11332758.294139478,11332819.870413905,11332751.590224562,11332676.782526884,11332593.886288164,11332510.978759548,11332415.422417197,11332302.031576458,11332199.430882538,11332070.817026582,11331922.774263652,11331738.47887677,11331527.708045255,11331319.573320245,11331100.756571647,11330848.617032673,11330561.390313137,11330260.6785408,11329925.93053545,11329587.929531537,11329202.153554402,11328748.13587994,11328308.364271233,11327824.952768944,11327345.603539273,11326797.55562848,11326317.849665906,11325417.143965114,11324572.663565611,11323771.977015961,11322995.822761787,11322155.476317547,11321255.469926814,11320280.61882494,11319256.606306614,11318143.988994062,11316971.577895483,11315683.368591215,11314285.53387342,11312788.13707493,11311179.774141714,11309696.165676115,11308319.137070749,11306652.506387036,11304937.666272197,11303143.224572483,11301361.348425845,11299896.97324417,11299836.56304555,11300198.100204762,11300652.31138236,11301190.604160836,11301843.506763263,11302511.724283427,11303151.663658299,11303934.70601593,11304900.520966873,11306082.930720787,11307511.45703989,11309256.433158334,11311353.162159096,11313456.684185842,11315943.939502122,11318989.66309328,11322725.106360115,11327294.021385953,11332842.91072644,11339541.070957188,11346054.630506767,11352265.149213023,11359657.814717416,11368421.060978496,11378902.363383695,11391432.588677537,11406136.351389373,11421223.069959054,11438083.203695927,11458275.377989009,11482296.75931808,11511026.353867535,11545460.237233866,11585714.99902274,11630626.944598874,11670687.895897657,11716997.701222282,11767947.612279525,11808129.991975246,11850410.329024337,11890988.36574417,11932286.15740421,11978128.999349555,12030033.720419534,12088660.188411435,12154692.64116849,12229441.822723875,12314059.996371934,12409815.182279864,12518115.699456865,12640216.247295862,12777858.293695504,12933209.347555606,13108505.349060368,13305885.063530581,13527798.567295734,13776985.008499602,14056434.943291392,14369483.717879754,14712305.604258075,15074776.784041584,15480201.02061393,15613800.982054118]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"y":[13748873.327171464,13748873.250518069,13748873.167424474,13748873.077349603,13748872.97970691,13748872.873860564,13748872.75912132,13748872.634742014,13748872.499912726,13748872.353755463,13748872.195318505,13748872.023570128,13748871.837391967,13748871.63557167,13748871.416795043,13748871.179637475,13748870.922554674,13748870.643872594,13748870.341776565,13748870.01429946,13748869.659308858,13748869.274493242,13748868.857346863,13748868.405153481,13748867.914968673,13748867.383600648,13748866.807589492,13748866.18318464,13748865.506320447,13748864.772589767,13748863.97721523,13748863.11501816,13748862.180384902,13748861.16723026,13748860.06895791,13748858.87841751,13748857.587858137,13748856.188877918,13748854.672369335,13748853.028460013,13748851.246448524,13748848.701006891,13748846.555597018,13748844.229973115,13748841.709000047,13748838.976272088,13748836.014006322,13748832.802927153,13748829.32214114,13748825.549001347,13748821.45896036,13748817.025411012,13748812.219513768,13748807.010009754,13748801.363018077,13748795.241816334,13748788.60660272,13748781.414238364,13748773.617968183,13748765.167118493,13748756.006769441,13748746.07740027,13748735.3145051,13748723.648176806,13748711.002656471,13748697.295845535,13748682.438777577,13748666.33504658,13748648.880188018,13748629.961009052,13748609.454863733,13748587.228868768,13748563.1390552,13748537.029450875,13748508.731088208,13748478.060931493,13748444.820717454,13748408.795702249,13748369.753307978,13748327.4416608,13748281.588012734,13748231.872429062,13748178.022297831,13748119.668757651,13748056.437184071,13747987.922028366,13747913.684461256,13747833.249676753,13747746.104026178,13747650.5899393,13747758.305440769,13747663.195679545,13747558.803813785,13747442.580542326,13747328.031233808,13747194.544529496,13747034.7137731,13746892.731794264,13746712.935217256,13746503.290278004,13746242.470781632,13745944.579248365,13745639.668106774,13745330.710041475,13744978.524155283,13744576.533568505,13744159.309038613,13743708.595198743,13743270.771998638,13742756.41398553,13742166.778536528,13741563.5637925,13740884.012884527,13740194.202268273,13739407.409019373,13738717.134701619,13737398.289988551,13736168.655013278,13735011.982070114,13733900.943653062,13732692.941249415,13731405.220067117,13730013.79875042,13728526.354975168,13726918.916340806,13725199.16490849,13723362.745570663,13721413.12360869,13719319.603856254,13717089.421135748,13714816.09191691,13712508.117876885,13709746.540945709,13706887.543751843,13703908.6202377,13700893.982797185,13698333.698766354,13697924.778681342,13698156.301080186,13698524.945532192,13699025.081441415,13699708.299850179,13700590.00289425,13701776.332584482,13703251.000140937,13705101.155497096,13707397.69688325,13710198.773594182,13713661.165250942,13717852.014842305,13722314.709944801,13727641.417437961,13734184.30336861,13742206.44146718,13752023.05759049,13763936.950102594,13778339.788823642,13791800.957216084,13803903.241276335,13818416.194627771,13835824.328114336,13856632.973101495,13881496.785400549,13911115.682851594,13946506.654850237,13988773.016529754,14038877.461856574,14098177.293404192,14168554.14720787,14251998.419641558,14349553.948902166,14457991.644641476,14551725.749174995,14659697.775501702,14777166.305332363,14862030.00235272,14953880.277981134,15049363.799382148,15157249.71063018,15279001.10392709,15416606.503971562,15572023.483833347,15747395.736436684,15945659.436520033,16169759.136887744,16422921.154046126,16708719.417195395,17030941.801620953,17393843.44420763,17802111.288685452,18260714.066153865,18774983.912672147,19350714.085407466,19994058.977985546,20711590.353158094,21510377.960319348,22375985.21763613,23274825.827257037,24271126.8101449,24428239.188423317]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"y":[8918366.650420707,8918366.593859807,8918366.5325239,8918366.466009863,8918366.393880531,8918366.315661844,8918366.230839701,8918366.13885662,8918366.039108029,8918365.930938337,8918365.813636594,8918365.68643181,8918365.548487937,8918365.398898322,8918365.236679764,8918365.060766062,8918364.870000994,8918364.663130729,8918364.438795567,8918364.195521027,8918363.931708127,8918363.645622903,8918363.335384984,8918362.99895526,8918362.634122442,8918362.238488577,8918361.809453236,8918361.344196463,8918360.839660205,8918360.29252824,8918359.699204352,8918359.055788733,8918358.358052311,8918357.60140897,8918356.780885406,8918355.891088441,8918354.926169574,8918353.87978654,8918352.745061625,8918351.514536468,8918350.180123031,8918348.565113114,8918346.981798973,8918345.264821094,8918343.402895633,8918341.383786201,8918339.194223465,8918336.819817951,8918334.244965531,8918331.452744853,8918328.424806202,8918325.141250944,8918321.580500795,8918317.71915606,8918313.53184196,8918308.991041902,8918304.066916794,8918298.727109049,8918292.93653008,8918286.657129899,8918279.847647235,8918272.463338673,8918264.45568486,8918255.772072049,8918246.355446734,8918236.143941216,8918225.070467658,8918213.062277904,8918200.040486254,8918185.919552084,8918170.606718915,8918154.001406267,8918135.994550414,8918116.467889678,8918095.293189654,8918072.331403386,8918047.431760978,8918020.430782828,8917991.151210068,8917959.400845379,8917924.971296629,8917896.346370112,8917856.601548282,8917813.513793666,8917766.779396383,8917716.102788467,8917661.151724463,8917601.566011578,8917536.955168815,8917865.998339657,8917881.435387041,8917839.984769579,8917794.761239983,8917745.192034002,8917693.926285287,8917636.300304899,8917569.349379817,8917506.129970811,8917428.698835908,8917342.2582493,8917234.486971907,8917110.836842146,8916999.478533717,8916870.803101819,8916718.709910063,8916546.24705777,8916362.048042987,8916143.265872158,8915905.087064436,8915647.893123275,8915329.49322335,8915053.164749965,8914765.892653361,8914497.004810274,8914187.702237587,8913918.564630194,8913435.997941677,8912976.672117945,8912531.971961807,8912090.701870512,8911618.01138568,8911105.71978651,8910547.438899461,8909986.857638061,8909369.061647318,8908743.990882477,8908003.991611768,8907157.94413815,8906256.670293607,8905270.12714768,8904576.23943532,8904130.156264612,8903558.471828364,8902987.78879255,8902377.828907266,8901828.714054504,8901460.247721985,8901748.347409759,8902239.899329338,8902779.67723253,8903356.126880256,8903978.713676346,8904433.445672603,8904526.994732115,8904618.411890922,8904699.886436649,8904768.164558325,8904824.1404856,8904851.701065727,8904854.309475888,8904598.658426883,8904246.461566282,8903795.02281795,8903243.77125305,8902564.985181415,8901748.871350285,8900742.353090733,8900308.30379745,8900627.057149712,8900899.43480706,8901017.793842657,8901171.753665896,8901368.391954524,8901157.019927152,8895939.48506787,8887393.3908621,8877673.294121444,8866416.225231968,8853498.5605272,8838922.054826174,8821876.049143316,8803262.244556272,8789650.042620318,8774297.626942862,8758728.919226687,8754229.981597772,8746940.38006754,8732612.932106191,8707322.60417824,8677256.89477202,8643460.936867505,8605296.892989524,8561989.545900296,8513224.208927717,8458360.855856124,8396709.210513603,8327511.981718334,8249490.692970772,8161873.143183375,8064307.406425759,7956296.631966869,7836786.214389013,7704883.049184,7559911.039013656,7401279.5334246885,7228589.475440158,7048625.99088002,6874727.740826128,6689275.231082957,6799362.775684919]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"xaxis":"x","y":[8904673.842158446,8904673.786491765,8904673.72610255,8904673.66059016,8904673.58951997,8904673.512420481,8904673.428780196,8904673.338044222,8904673.239610597,8904673.132826287,8904673.016982863,8904672.891311787,8904672.754979333,8904672.607081046,8904672.446635732,8904672.272578968,8904672.083756026,8904671.87891422,8904671.656694597,8904671.415622914,8904671.154099865,8904670.870390467,8904670.56261255,8904670.228724286,8904669.866510615,8904669.473568585,8904669.047291372,8904668.584851038,8904668.083179718,8904667.538949309,8904666.948549375,8904666.308063202,8904665.613241816,8904664.859475793,8904664.041764705,8904663.154683918,8904662.192348637,8904661.148374874,8904660.015837073,8904658.787222216,8904657.454379959,8904656.008468594,8904654.439896403,8904652.738258041,8904650.89226553,8904648.889673406,8904646.717197496,8904644.36042684,8904641.803728126,8904639.030142019,8904636.021270722,8904632.75715598,8904629.216146773,8904625.374755755,8904621.207503568,8904616.686749931,8904611.782510443,8904606.462257827,8904600.690706387,8904594.429578153,8904587.637349248,8904580.268974796,8904572.275590476,8904563.604188884,8904554.197268438,8904543.992452612,8904532.92207695,8904520.912741125,8904507.884823142,8904493.75195247,8904478.42043863,8904461.788651543,8904443.746349506,8904424.173950477,8904402.941741867,8904379.909023685,8904354.923179459,8904327.818668926,8904298.415935911,8904266.52022436,8904231.920294866,8904216.09989689,8904177.229511393,8904135.089246113,8904089.34756916,8904039.728966286,8903985.905197902,8903927.520300537,8903864.188257892,8904790.49159893,8904800.14707152,8904810.510668881,8904822.265306327,8904835.013025654,8904848.10346361,8904863.019553274,8904879.042277215,8904896.99881129,8904915.312018111,8904936.295883885,8904949.402169615,8904956.384295441,8904958.727247886,8904952.89552252,8904928.005573792,8904903.537307857,8904863.496808054,8904783.075462535,8904706.030733963,8904599.530860461,8904415.924935348,8904343.266014602,8904342.875428397,8904393.125626018,8904448.725916538,8904556.049045062,8904628.097731035,8904710.223703505,8904799.201613862,8904899.430333445,8905009.113249976,8905129.799145337,8905263.136384523,8905409.678728173,8905570.650402501,8905748.264847519,8905943.921515586,8906159.650471428,8906397.748281462,8906660.79390035,8906951.788901325,8907274.05433295,8907631.41405695,8908028.224491445,8908469.455312971,8909132.902731122,8909922.47384691,8910785.838464119,8911688.03087011,8912723.256896058,8913858.182342846,8915103.42908395,8916470.889240174,8917973.91014,8919627.490840543,8921448.513840588,8923456.01478911,8925671.493387211,8928119.270440249,8930826.890329096,8933825.616868032,8937150.981466942,8940843.406707887,8944948.962468106,8949520.222722217,8954617.268176794,8960308.825517634,8966673.615206389,8973801.927836986,8981538.971197462,8989685.016088828,8998873.867673634,9009244.989938073,9019847.829618154,9024477.954810115,9029680.908902163,9035536.781250821,9041685.652969465,9048146.420195617,9055525.131441412,9063963.93380348,9073627.843411384,9084708.744079448,9097429.3272142,9112049.421937194,9128846.040790617,9148224.908183722,9170532.1984485,9185369.27456579,9197491.232787339,9211031.35577842,9226173.79197021,9243132.374837158,9262151.148196165,9283481.455969758,9307502.14899339,9334552.05743001,9363603.138735354,9395252.115269607,9431175.80458016,9471995.503372053,9518424.341627667,9571277.700066442,9631493.121316044,9700132.90049719,9778417.635173347,9867727.51956448,9969636.20789042,10085902.027834237,10218534.200967407],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"xaxis":"x","y":[8353289.3996845,8353289.363947366,8353289.325195242,8353289.283173774,8353289.237607134,8353289.188196237,8353289.13461676,8353289.0765170185,8353289.013515662,8353288.945199169,8353288.871119121,8353288.790789277,8353288.703682383,8353288.609226685,8353288.506802213,8353288.395736674,8353288.275301081,8353288.144704936,8353288.003091048,8353287.84952992,8353287.6830136385,8353287.502449257,8353287.306651642,8353287.0943356585,8353286.864107773,8353286.614456895,8353286.343744454,8353286.050193652,8353285.731877808,8353285.386707715,8353285.012417915,8353284.606551859,8353284.166445778,8353283.689211191,8353283.171715985,8353282.61056383,8353282.002071927,8353281.342246836,8353280.626758275,8353279.850910727,8353279.009612631,8353278.097342996,8353277.108115184,8353276.035437659,8353274.872271421,8353273.610983834,8353272.243298609,8353270.760241536,8353269.152081668,8353267.408267526,8353265.51735797,8353263.466947223,8353261.243583602,8353258.832681395,8353256.2184253605,8353253.383667128,8353250.309812969,8353246.976702091,8353243.3624746995,8353239.443429015,8353235.193866235,8353230.585922543,8353225.589386975,8353220.171504052,8353214.296759842,8353207.926650124,8353201.0194291575,8353193.529837408,8353185.408806551,8353176.603139789,8353167.055165561,8353156.702362335,8353145.476952164,8353133.305460455,8353120.108239126,8353105.798950232,8353090.284006794,8353073.461967369,8353055.222880624,8353035.447575908,8353014.006895426,8352990.760863417,8352965.557787315,8352938.233285492,8352908.609235819,8352876.492638943,8352841.67438951,8352803.927948484,8352763.007908846,8352718.648446696,8352670.561649333,8352618.435711046,8352561.932987225,8352500.68789655,8352434.304660666,8352362.354870138,8352284.374865029,8352199.862918024,8352108.276207485,8352009.027567674,8351901.482002989,8351784.952953096,8351702.283356294,8351586.594923931,8351463.0425644275,8351326.817502033,8351180.082480939,8351016.233035789,8350846.962735495,8350652.868645986,8350448.551614797,8350240.577814299,8349998.888513865,8349740.267023547,8349467.5415335195,8349166.324732676,8348836.989668554,8348480.596133792,8348099.149369546,8347695.917784702,8347275.733551163,8346816.888835366,8346308.464131168,8345862.497742878,8345350.244622487,8344899.631776219,8344266.556259567,8343484.246749531,8342666.346875943,8341762.206974135,8341512.614689097,8341861.324847387,8342243.167702857,8342661.104652141,8343118.507159919,8343619.180510751,8344190.0560693545,8344790.508584412,8345448.642248961,8346170.452144799,8346962.658115146,8347832.834123151,8348332.984695828,8348034.9858445,8347738.904259597,8347449.604400385,8347173.194931848,8346917.27326562,8346691.226509166,8346506.5875784345,8346377.452727881,8346343.877341766,8346380.664856338,8346536.000464118,8346840.150438065,8347329.49173453,8348047.6280476395,8349046.761200772,8350389.237631425,8352149.405669123,8354415.784176076,8357293.609063803,8360907.8337302795,8365406.606725376,8366195.490246292,8361684.880477025,8357143.534106695,8352633.887051623,8348232.533540878,8344035.914419858,8340164.105141541,8336764.998670093,8334020.158333545,8332150.908007677,8331452.216284273,8332196.104876658,8334790.4615607215,8336236.578214483,8326041.484121975,8315586.649510515,8304958.689907851,8294273.175909004,8283693.566912405,8273371.292257669,8263558.169094581,8254545.379907043,8246693.088706455,8240443.237452201,8236335.204830541,8235030.892019074,8238105.225739869,8245841.284516041,8258694.523788054,8277859.291797132,8304748.571899032,8341001.344226395,8388561.035919163,8449660.644001728,8526874.476636218,8623162.23783677],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"xaxis":"x","y":[14825990.923962709,14825990.905310012,14825990.885083696,14825990.863151029,14825990.83936805,14825990.813578678,14825990.785613662,14825990.755289443,14825990.722407036,14825990.686750598,14825990.648086164,14825990.606159963,14825990.56069687,14825990.511398513,14825990.457941394,14825990.3999747,14825990.337118056,14825990.268958986,14825990.19505025,14825990.114906894,14825990.028003033,14825989.933768494,14825989.831584984,14825989.720782092,14825989.600632906,14825989.470349196,14825989.32907631,14825989.175887551,14825989.009778084,14825988.829658434,14825988.634347286,14825988.42256375,14825988.192919075,14825987.943907445,14825987.673896242,14825987.381115312,14825987.063645411,14825986.719405735,14825986.34614025,14825985.941403022,14825985.502542293,14825985.026683148,14825984.51070886,14825983.951240545,14825983.344615227,14825982.686862044,14825981.973676447,14825981.200392323,14825980.3619517,14825979.452872071,14825978.467210952,14825977.398527494,14825976.239840982,14825974.983585926,14825973.62156339,14825972.14488852,14825970.543933552,14825968.808266416,14825966.926584233,14825964.886641495,14825962.67517245,14825960.277807297,14825957.67898173,14825954.861839246,14825951.808125833,14825948.498076413,14825944.910292396,14825941.02160989,14825936.806957807,14825932.239205174,14825927.288997166,14825921.924578834,14825916.111606121,14825909.812943218,14825902.988445634,14825895.594728207,14825887.584917447,14825878.908387404,14825869.510478705,14825859.332199987,14825848.309911622,14825836.374991307,14825823.45348165,14825809.46571997,14825794.325950868,14825777.941922754,14825760.214469573,14825741.037080262,14825720.295458626,14825697.867077745,14825673.620733868,14825647.416106364,14825619.103331894,14825588.522603063,14825555.503804147,14825519.866199652,14825481.418194726,14825439.957190795,14825395.269564781,14825347.130806198,14825295.305853456,14825239.54967928,14825179.60818507,14825115.2194759,14825046.115601845,14824972.024868352,14824892.674837664,14824807.796167001,14824814.73972318,14824719.317026857,14824624.975894744,14824529.08290119,14824417.846127408,14824306.81896104,14824193.287924608,14824060.71315828,14823930.027680552,14823799.290269246,14823649.415266616,14823509.166986015,14823359.388677444,14823216.462587265,14823057.321214074,14822924.846818164,14822770.818632673,14822644.922091356,14822521.433995936,14822432.074464941,14822347.30413496,14822304.573660703,14822268.577305453,14822333.393611675,14822446.15769735,14822621.681444997,14822942.699024938,14823376.473066676,14823950.336751195,14824749.533297962,14825705.611343248,14826971.486707296,14828544.914883783,14830483.911805816,14832856.504969055,14835801.187363395,14839359.788318109,14843708.037128024,14848988.044425832,14855315.232849255,14863002.867639046,14872181.208158419,14883244.996079804,14896417.129148558,14912148.709581977,14930903.285481777,14953281.273680137,14979808.562996145,15011290.226336129,15040786.70963779,15067397.424078276,15098629.87503268,15135282.343787849,15178279.518052826,15228697.680854723,15287786.113711873,15356992.462388229,15437991.154442435,15532713.761401705,15643381.696960818,15772540.123849705,15923088.954204125,16098324.711037287,16291576.519720336,16454322.00488745,16640912.915245991,16841926.9437351,16978744.671771515,17132281.565142877,17304605.36170165,17498175.478713863,17715704.548041046,17960228.990303125,18235134.823970016,18544188.884118974,18891564.042962104,19281865.053283352,19720155.202375673,20212001.179554403,20763386.157476723,21380916.935329143,22071650.64590778,22843214.310233973,23703795.98964245,24662156.491654824,25727669.206328783,26910318.01855549,28220709.863212924,29632881.301220965,31088668.702110376,32695340.773443475,32869031.336734362],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"xaxis":"x","y":[12711104.490554871,12711104.27861482,12711104.048879232,12711103.799853902,12711103.52991915,12711103.237319306,12711102.920151282,12711102.576352194,12711102.20368595,12711101.799728703,12711101.361853082,12711100.887211107,12711100.372715672,12711099.815020466,12711099.21049818,12711098.55521696,12711097.844914816,12711097.07497188,12711096.240380395,12711095.335712139,12711094.35508307,12711093.292115157,12711092.139894806,12711090.890927952,12711089.537091292,12711088.069579469,12711086.478847828,12711084.7545503,12711082.885472167,12711080.859457098,12711078.663328115,12711076.282801913,12711073.702395959,12711070.905327866,12711067.873406187,12711064.586912233,12711061.024471799,12711057.162916223,12711052.977131745,12711048.439896269,12711043.52170237,12711036.236402385,12711030.29392579,12711023.85253997,12711016.870361378,12711009.30199091,12711001.098218927,12710992.205705542,12710982.566634132,12710972.118335754,12710960.792882094,12710948.516644388,12710935.20981525,12710920.785890572,12710905.15110795,12710888.203838127,12710869.833925493,12710849.921973344,12710828.338569429,12710804.943446636,12710779.584573513,12710752.09716882,12710722.302633684,12710690.007394608,12710655.001649862,12710617.058011316,12710575.930032942,12710531.350616742,12710483.030285846,12710430.655313907,12710373.885698868,12710312.35296835,12710245.65780282,12710173.367461571,12710095.0129954,12710010.08622852,12709918.036491055,12709818.267081602,12709710.13143831,12709592.928994628,12709465.900694642,12709328.224140508,12709179.00834269,12709017.288041554,12708842.017566204,12708652.064194473,12708446.200974887,12708223.098968936,12707981.318869049,12707719.30194454,12708104.683515303,12707847.314222462,12707565.113439655,12707246.425628308,12706940.253257213,12706576.203802772,12706135.023488477,12705750.952513283,12705262.89046694,12704675.944417514,12703939.801225582,12703089.451270515,12702231.423586566,12701350.642431062,12700348.887985986,12699211.816185335,12698010.446156109,12696729.236447366,12695353.778433256,12693867.259571461,12692174.714697225,12690414.540305365,12688486.838170266,12686440.971304843,12684169.484019836,12682218.037752207,12678327.748748176,12674647.069625793,12671230.091686614,12667947.783375945,12664375.850413937,12660537.498630876,12656392.130937442,12651913.645662421,12647081.56614181,12641861.162063265,12636241.91336542,12630190.8784406,12623668.955201246,12616632.951990027,12609554.113677224,12602301.508883357,12593509.415189479,12584083.103057345,12573967.958568238,12563539.807862626,12554364.64297121,12551881.813395271,12551257.208330415,12550587.120732585,12549857.543630496,12549143.105969995,12548313.173633661,12547423.30836934,12546470.139485156,12545450.41210432,12544360.966353681,12543198.816047017,12541961.318553925,12540646.817361781,12537079.7977692,12532833.752524378,12528300.800282734,12523561.205108594,12518628.770858046,12513587.817804521,12508336.66965336,12502980.602157947,12497626.974183643,12492225.657732202,12486970.765448099,12481863.735708883,12477132.769291798,12472853.069300279,12469303.627624014,12466665.535329815,12465285.376228094,12465501.977465302,12469024.541685658,12477346.28627349,12487311.860239,12499251.472666383,12513530.571560677,12530570.972526327,12550869.788284848,12575085.564076966,12601299.15466988,12605611.610553332,12610855.486684991,12617207.653837055,12624877.723183783,12634113.058720691,12645204.715138838,12658443.113567835,12674329.838301904,12693292.642487839,12715887.468975728,12742754.710313085,12774583.645537626,12812422.382818084,12857213.328155123,12910074.572205393,12972589.682746073,13046286.123695195,13132890.0977529,13234804.582555724,13354331.239772093,13494162.243251028,13657707.155064432,13848292.525056878],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"xaxis":"x","y":[11873041.287619896,11873041.27658072,11873041.26461021,11873041.251629809,11873041.237554306,11873041.222291315,11873041.20574066,11873041.187793702,11873041.168332644,11873041.14722975,11873041.12434651,11873041.099532714,11873041.072625505,11873041.04344827,11873041.0118095,11873040.977501532,11873040.940299192,11873040.899958285,11873040.85621404,11873040.808779346,11873040.757342858,11873040.70156698,11873040.641085634,11873040.57550186,11873040.504385205,11873040.427268917,11873040.343646862,11873040.252970217,11873040.154643854,11873040.048022456,11873039.932406269,11873039.807036512,11873039.671090411,11873039.523675775,11873039.363825172,11873039.19048958,11873039.002531508,11873038.798717482,11873038.577710059,11873038.338058963,11873038.07819163,11873037.796402883,11873037.490843734,11873037.159509301,11873036.800225649,11873036.410635535,11873035.98818298,11873035.530096525,11873035.033371046,11873034.494748127,11873033.910694666,11873033.277379805,11873032.590649802,11873031.846000899,11873031.038549824,11873030.163001886,11873029.213616332,11873028.184168853,11873027.067910917,11873025.85752568,11873024.545080245,11873023.121973904,11873021.578882035,11873019.90569535,11873018.091454038,11873016.124276413,11873013.991281647,11873011.678506043,11873009.170812339,11873006.451791506,11873003.503656393,11873000.307126522,11872996.841303425,11872993.083535654,11872989.009272626,11872984.591906553,11872979.802601326,11872974.610107392,11872968.98056157,11872962.877270557,11872956.260476861,11872949.08710581,11872941.310492236,11872932.880085172,11872923.741129076,11872913.834319623,11872903.09543243,11872891.454922613,11872878.837493071,11872865.161629476,11872850.339099508,11872834.274414062,11872815.49756932,11872798.782287247,11872776.728612099,11872755.667660145,11872730.299056843,11872709.382979294,11872672.336875591,11872645.47264299,11872606.403132211,11872568.202027947,11872525.824225409,11872498.43050483,11872457.033437314,11872392.755702114,11872356.692421224,11872293.311564565,11872218.13603179,11872171.791667247,11872076.51225758,11872014.354320709,11871878.315604782,11871846.83478092,11871708.738747893,11871588.123641312,11871362.855997251,11871226.138095723,11871082.02714317,11870926.81532883,11870757.295695217,11870576.700435217,11870382.041457497,11870172.362581432,11869946.665170835,11869703.908699054,11869443.017819572,11869160.819240598,11868860.330881046,11868538.344183357,11868193.733807476,11867825.403678374,11867432.377288548,11867294.217715055,11867217.502796356,11867138.377958043,11867057.356582176,11866975.121485995,11866891.00823108,11866809.240431065,11866729.721831914,11866654.252833404,11866585.068878414,11866524.926574262,11866477.207176246,11866446.037361043,11866436.433103468,11866454.469650352,11866507.48264928,11866604.307367755,11866755.557484284,11866973.957028963,11867274.73403747,11867676.078277985,11868199.689231306,11868871.412920207,11869722.005231172,11870785.464330932,11872110.182334788,11873745.163955608,11875751.395391628,11878201.086419329,11881179.669572815,11884788.137591183,11889145.814726615,11894393.539328193,11900697.436957736,11908280.582143197,11917188.150065806,11927304.899830442,11938810.384892406,11951913.888526179,11966858.00062716,11983924.38311721,12003439.69115622,12025777.578360468,12035455.555564487,12037956.079802891,12040989.062934427,12044654.912571821,12049071.842924492,12053606.09148726,12057243.664835073,12061679.5166356,12067065.465210073,12073580.537635373,12081444.702617725,12090893.99250194,12102203.567510603,12115767.01245294,12131998.377800832,12151289.12966135,12174274.43822326,12201617.299360855,12234085.127752349,12272485.164230384,12318026.924813675,12371756.122954376,12435180.670091288,12509984.60967517],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle EN pour le paramètre l1=1.0 en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE ElasticNet pour tout le meilleur paramètre l1 ratio\n","for i in BestParametresEN['ElasticNet()'][BestParametresEN['paramètre'] ==\n","                                          'elasticnet__l1_ratio']:\n","    fig1 = go.Figure([\n","        go.Scatter(name='RMSE moyenne',\n","                   x=alphasEN,\n","                   y=GridEN.ScoresMean.where(\n","                       GridEN.elasticnet__l1_ratio == i).dropna(),\n","                   mode='lines',\n","                   marker=dict(color='red', size=2),\n","                   showlegend=True),\n","        go.Scatter(\n","            name='SDup RMSE',\n","            x=alphasEN,\n","            y=GridEN.ScoresMean.where(\n","                GridEN.elasticnet__l1_ratio == i).dropna() +\n","            GridEN.ScoresSD.where(GridEN.elasticnet__l1_ratio == i).dropna(),\n","            mode='lines',\n","            marker=dict(color=\"#444\"),\n","            line=dict(width=1),\n","            showlegend=False),\n","        go.Scatter(\n","            name='SDdown RMSE',\n","            x=alphasEN,\n","            y=GridEN.ScoresMean.where(\n","                GridEN.elasticnet__l1_ratio == i).dropna() -\n","            GridEN.ScoresSD.where(GridEN.elasticnet__l1_ratio == i).dropna(),\n","            mode='lines',\n","            marker=dict(color=\"#444\"),\n","            line=dict(width=1),\n","            fillcolor='rgba(68, 68, 68, .3)',\n","            fill='tonexty',\n","            showlegend=False)\n","    ])\n","\n","    fig2 = px.line(GridEN.where(GridEN.elasticnet__l1_ratio == i).dropna(),\n","                   x=alphasEN,\n","                   y=[\n","                       'ScoresSplit0', 'ScoresSplit1', 'ScoresSplit2',\n","                       'ScoresSplit3', 'ScoresSplit4'\n","                   ])\n","\n","    fig3 = go.Figure(data=fig1.data + fig2.data)\n","    fig3.update_xaxes(type='log', title='alpha')\n","    fig3.update_yaxes(title='RMSE')\n","    fig3.update_layout(\n","        title=\n","        \"RMSE du modèle EN pour le paramètre l1={:.2} en fonction de l'hyperparamètre alpha\"\n","        .format(i))\n","    fig3.show()\n","    if write_data is True:\n","        fig3.write_image('./Figures/graphRMSEEN{:.2}.pdf'.format(i))\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                          paramètre  KNeighborsRegressor(n_jobs=-1)\n","0  kneighborsregressor__n_neighbors                               6\n"]},{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning:\n","\n","\n","5 fits failed out of a total of 250.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n","    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n","  File \"/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/neighbors/_regression.py\", line 213, in fit\n","    return self._fit(X, y)\n","  File \"/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/neighbors/_base.py\", line 566, in _fit\n","    raise ValueError(\"Expected n_neighbors > 0. Got %d\" % self.n_neighbors)\n","ValueError: Expected n_neighbors > 0. Got 0\n","\n","\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning:\n","\n","One or more of the test scores are non-finite: [               nan -14158414.94566695 -13403239.5942038\n"," -13343787.4869865  -13444654.13123597 -13530752.65135069\n"," -13364395.38057512 -13507637.3187071  -13593855.69904039\n"," -13769613.43463732 -13866642.10794283 -13935421.56212009\n"," -13944669.0409824  -13994556.14837058 -13992916.08681111\n"," -14089947.61240314 -14169786.76283106 -14186153.37094466\n"," -14198620.10924004 -14264881.36780619 -14327129.21639\n"," -14371262.21458533 -14457184.1121061  -14508393.11015556\n"," -14548230.29612243 -14607351.09167005 -14652539.09175539\n"," -14699250.83441698 -14732983.9616672  -14771194.6523989\n"," -14799935.11551203 -14824775.69685718 -14863552.36317475\n"," -14892640.69904841 -14930235.65332496 -14960288.21116948\n"," -14985455.36659834 -15018678.17627562 -15052182.87122589\n"," -15074149.65476861 -15102437.66958665 -15131452.35691429\n"," -15160464.03442907 -15184295.38565697 -15210810.31307076\n"," -15229133.61123816 -15248363.967235   -15254486.00425518\n"," -15275415.59700342 -15306818.12739537]\n","\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predkNN=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[5372572.333333333,740365.0833333334,2930499.3333333335,1439231.8333333333,3824835.1666666665,2044589.1666666667,1270792.1666666667,2695090.5416666665,1017644.1666666666,6432036.333333333,5600746.666666667,6559923.333333333,3876096.8333333335,12159460.916666666,1213578.5,3596771.5,623064.4401,2149824.4166666665,1191947.6666666667,5282191.666666667,1629839.8541666667,748486.3333333334,2829615.9375,1068866.8333333333,636359.6666666666,8181365.125,2507565.6666666665,2240943.5,10481214.833333334,10356503.333333334,3905360.7083333335,4257906.520833333,1089113.1666666667,750059.28125,2376873.3333333335,2086737.5,1750868.9479166667,853253.1718833334,3220735.25,1883973.5,849431.5416666666,1290063.4791666667,1738763.5208333333,2158624.8333333335,3443255.1666666665,29213416.666666668,858780.375,1226515.8333333333,2175203.4583333335,6171061.5,1046858.6770833334,4776105.166666667,3314172.5208333335,3229293.2604166665,661177.7553,1625506.3333333333,1018339.8854166666,7254109,33370537.333333332,1031284.8645833334,2212827.5,4904778.375,1301364.3020833333,20579414,1859688.8333333333,1362547.5,2701679.8333333335,18020979.333333332,2261851.6666666665,1628464.3333333333,1706496.8333333333,655030.3125166666,7471822.666666667,2361755.6666666665,804149.59375,922949.5,3043106.5208333335,716065.1666666666,1690659.375,11129177.916666666,4493394.583333333,6250067.583333333,4245092.020833333,1237063.6354166667,2190765.6666666665,1329514.3333333333,784078.6666666666,3837828.6666666665,4086916.25,1330503.5,696215.1666666666,2585718.6666666665,2701679.8333333335,1737784.5104166667,1307487.3333333333,628362.1406333334,2016576.8333333333,1233034.1666666667,5423936.333333333,2792180.25,2611779.3333333335,1929141,924776,751108.4309899999,8165800.333333333,2922272.5,9949464.666666666,3014432.6289066668,974002,3868015.6666666665,853004.90625,742511.5,1796128.1666666667,1085159,1737916.7083333333,3822800.875,2063871.8333333333,1185864.5468833332,1267559.698,1252954,2347089.5208333335,17428611.833333332,830780.3333333334,873326.8333333334,1899862.7708333333,1503922.40625,1733080.125,2279570.7708333335,7921279.083333333,2268044.3333333335,3013160.6041833335,7291263.333333333,1420175.2708333333,677994.3333333334,17055119.416666668,644406.1614666666,4102387.1666666665,991734.5,1031284.8645833334,18120796.666666668,1644460.0729166667,2895225.6666666665,882676.6666666666,968087.96875,1019343.5,3353183.5416666665,3472564.2083333335,13842200.666666666,6173852.833333333,1073280.9166666667,5499304.75,2927545.6666666665,743169.8854166666,1879749,5611222.166666667,1599647,639515.89585,3552561.5208333335,4044748.5833333335,565165.5,7402567.333333333,6925162.0625,1204822.6666666667,1263206.5,26573693.5,831859.5,3472564.2083333335,855530.6666666666,8674905.833333334,997701.3958333334,648638.5,2412683.375,1474513.3333333333,946611.1406333334,3054011.8333333335,1605831.5,1199939.3333333333,2173904.5,937918.78125,9427271.208333334,1677102.8541666667,2716085.6666666665,562847.1614833333,1383074.7395833333,1818238,760121.875,1574283.6666666667,721394.0416666666,11789652,13100504,18874350.666666668,3312730,3189160.2083333335,3605797.8333333335,1631673.4791666667,870283.1666666666,648638.5,1038060.1666666666,20615801.666666668,3642072.1666666665,1555889.3333333333,2507560.2708333335,904938.9505166666,6575589.333333333,1505190.5833333333,6078500.979166667,2967713.1666666665,1723642.6666666667,4731936.333333333,2137207.8125,985112,608000.4114666666,765848.323,9041215.239583334,1880257.8333333333,1999941.1666666667,3082548.1666666665,4034427.5833333335,1352611.3386333333,900961.1666666666,19645926.833333332,2050055.3333333333,1268689.1666666667,4148385.7708333335,2388406.09375,21561416.333333332,1253824.9166666667,1083316.375,2187460,1693434.3333333333,8715333.0625,1449050.8333333333,2580413.5,1513565.8333333333,1298040.9375,1100470.875,16662884.166666666,6752044.166666667,5838902.5,942112.84375,3144575.7916666665,1509006,2487721.6666666665,9009905.333333334,1575493.5104166667,783331.1666666666,1068915.8333333333,2392416.9166666665,126176818.66666667,1202078,1184566.1979166667,2595636.46875,2136091.9166666665,5801091.333333333,2887152.5833333335,20816834.166666668,1395764.5,2638763.1666666665,1398312.5,742511.5,715003.21875,3790020.6666666665,1294979.1666666667,1775293.6914066665,1582532.2552166667,3052385.7708333335,2195783.3333333335,6556882.333333333,583168.8333333334,533265,812133.28125,4830124.25,1867130.1875,3631098.125,1104952.7291666667,2031626.8333333333,1465808.3333333333,804149.59375,4010485.9583333335,1022816.5,737765.3333333334,805329.8021,20244301.166666668,6534594.083333333,707590.3593833334,23958433.333333332,4091614.09375,8489671.166666666,1338507.9218833332,1852265.3020833333,2455020.5833333335,4476325.25,54948193.333333336,779412.1927166666,9491038.645833334,1272016,2377832.5,8181365.125,978379.1666666666,65085382.666666664,1123208,10537061.666666666,982865.02605,2603443.1666666665,5416326.333333333,1402508.562583333,3662382,6999384.166666667,16486749.666666666,3466457.8333333335,18177237.166666668,1400952.1666666667,723476.7448,751108.4309899999,1938023,2498997.2916666665,13274135.5,4623098.854166667,8255336.083333333,577777.4414166666,901090.1666666666,1205274.8333333333,1585268.8854166667,503019.5,2912239.7291666665,6862367.541666667,1479302.6458333333,9300943.166666666,1123247.9375,4428902.833333333,5259489.291666667,15518724.083333334,895799.96875,886594.1354333333,3053750.6666666665,5178228.333333333,2741217.8333333335,11511931.333333334,946611.1406333334,2706810.1666666665,23359441,1337629.6666666667,2763667.5,1266340.8229166667,3450393.6041666665,1329424.875,2056935,515035.3593833333,5736969.416666667,5818485.458333333,1776522.1666666667,2086737.5,666671.3333333334,697834.1666666666,1560730.5,2229975.1458333335,2330517.8333333335,1532191.7291666667,31396860,14127254.833333334,1107916,615007.6666666666,19231714.583333332,18581647.333333332,442047.32163333334,25329083.333333332,2279570.7708333335,5694625,801902.3020833334,697477.3333333334,4282957.770833333,1762420.71875,1368867.1666666667,797690.2708333334,1139203.1666666667,948141.5,1669960.1666666667,3974791.6666666665,1594205.3958333333,2925608.4270833335,898887.8854166666,1039107.8125,12380565.416666666,947963.2291666666,2951775.7083333335,1069211.9531333332,1824646.1666666667,3680787.3125,1574165.5,7429660.5,810149.8333333334,930874.3333333334,18263870.833333332,769949,5886488.333333333,1754488,3008043.9166666665,3390563.6666666665,5381943.666666667,757976.3333333334,869104.0520833334,8902050,2283310.0416666665,1356570,1139622.03385,1262485.8333333333,8191113.5,4010472.25,697834.1666666666,1353115.3020833333,2114918.5,3680787.3125,11130575.5,1220075.8333333333,42736261,1200696.0520833333,1123247.9375,2225982.9583333335,21043258,1110008.1666666667,2490303.375,1925839.8333333333,1955225.7083333333,5316790.333333333,943915.8333333334,1651207.8229166667,3842923.8333333335,5324844.333333333,5006192.166666667,6354513.5,3463272.8333333335,2469978.7188333333,8730345.916666666,21816893.333333332,950367.6666666666,2030379.3333333333,2289045,881733.625,2863640.1666666665,1272044.3333333333,26279379.833333332,5904260.166666667,1028644,747474.72405,10495037,1110008.1666666667,5786278.625,14677278.833333334,883012.9479166666,7570132,662426.6666666666,3732523.6666666665,33230970.666666668,1058554.4791833332,1689539.6458333333,3272038.8333333335,20198596.333333332,3455461.6666666665,9145840.333333334,972253.34375,1205274.8333333333,1292981.8020833333,4423569.583333333,1516261.5729166667,1703902,5499090.166666667,1779521.6666666667,13028546.5,1616061.8333333333,8463205.166666666,3544986.2916666665,7062134.166666667,3637893.1666666665,1331002.323,2299508.1666666665,4939120.375,1173439.5,1370914.6666666667,8208673.666666667,1008394.1666666666,1225665.8958333333,1677102.8541666667,4531093.875,1691419.3333333333,802908.8645833334,1902504.5,1450085.3333333333,3887666.9791666665,1100398.1770833333,1563691,1408943.84375,1420175.2708333333,1148969.3333333333,1208680.8958333333,6473694.666666667,1281694,3394294.9583333335,1672147,13517797.25,1972259.125,11811583.166666666,15955564.416666666,4864320,834597.1666666666,2717808.3333333335,2985468,917741.2187666666,1306295.6666666667,5356631.166666667,2835522.1666666665,1086671.1666666667,19063982,593894.4844,1032738.6666666666,892234.6145833334,8354614.833333333,17075188.833333332,2187460,1619825.3333333333,1151116.4166666667,33370537.333333332,7547487.333333333,877661,1408538.5,3190285.8333333335,1479751.03125,1219660.9270833333,3347955.125,1076146.6666666667,1012784.78125,582674.5677166666,1497475.2083333333,1611336.3333333333,2086737.5,1362384.8125,57925829.666666664,3073100.71355,3530830.5416666665,3878829.3333333335,3013096.8229166665,743169.8854166666,18966769.833333332,13591711.5,4334609.833333333,6136295.208333333,4685900.166666667,6167998.25,1148247.9791666667,2057987.6458333333,4212288.125,1284142.75,1045053.9895833334,3923695.2083333335,729025.6666666666,1877186.5,945532.1666666666,29526066.666666668,6999384.166666667,3857248.7291666665,42747834,1603785.3958333333,2391678.6666666665,3116452.8333333335,3422412.7916666665,6355543.75,1889897.7291666667,2909322.1666666665,1460809.6666666667,2843727.2708333335,761809.2604166666,12445032.833333334,6789624.5,1683628.8125,6054959.333333333,853571.3333333334,6896245.541666667,565165.5,748486.3333333334,849431.5416666666,1916955.4323000002,1779702.9791666667,966291.6666666666,1209295.8359333333,2568199.6666666665,1168944.3333333333,4077661.1666666665,891147.7604166666,3117231.6666666665,18740056.5,5814949.666666667,850578.1666666666,810832.9791666666,1862945,1182958.2291666667,937918.78125,719745.2396666667,27616291.333333332,746673.6666666666,1672147,1560217.125,1559507,3637727.9791666665,3322690.6770833335,2794425.3020833335,2459867,894950.8333333334,1632029.6901,15859012.083333334,649797.6666666666,3573408,987197.0208333334,1376035.5104166667,1345803.1666666667,5368352.5,1552263.75,1208680.8958333333,5048463.166666667,10782749.604166666,2188088.6666666665,2450444.1666666665,791175.1666666666,1693434.3333333333,3401493.7916666665,5600746.666666667,6631322.833333333,4590025.9375,957068.3333333334,1295182.52085,484928.11981666664,641768.3333333334,1617671.5,970188.7395833334,1836369.1302166667,1152211.0468833332,765703.8333333334,839588,993195.4166666666,3905271.8333333335,1550440.1666666667,1880257.8333333333,697969.6666666666,1462917.6666666667,1614097.5,2217948.75,1159986.6666666667,2831860.0416666665,894530.6666666666,899495.40625,484928.11981666664,1919105.8333333333,3010389.6145833335,1173439.5,1354333.1666666667,6658146,2162425.2708333335,1109351.8333333333,36639754.666666664,16884582.416666668,16333320.75,1599647,4256284.166666667,702465.1979166666,987403.59375,1124497,814292.6979166666,1900869.5,4393844.020833333,3320034.3333333335,5193551.541666667,3312730,1235671.3854166667,17144241.666666668,2085389.6666666667,6746689.416666667,2771326.6666666665,4395387.666666667,1369536.3333333333,737293.71875,870402.1666666666,2769132.3541666665,1266020.7291666667,9055193.5,6362222.333333333,806178.4739666666,1123364.6666666667,1407126.8333333333,10120807.166666666,1297645.5,2079713,848490.3020833334,126176818.66666667,1646845.53125,1452438.1666666667,787575.1666666666,1425464.8333333333,869955,515035.3593833333,946803.8333333334,1532054.3333333333,15053065.916666666,70867682,1184566.1979166667,8779631.791666666,2328414.5416666665,586502.3333333334,1050703.09115,583168.8333333334,9496851.333333334,3418043.5416666665,2499944.4583333335,812254.1666666666,1059785.1666666667,1590789.5,789730.5729166666,1118939.6666666667,5235960,829609.2604166666,2930160.0625,15069706.75,1719588.5,1886982.8333333333,46254893.833333336,1019141.4192666666,3276332.9791666665,12445032.833333334,765703.8333333334,9748335.166666666,1521975.3854166667,1584922,1829358.6666666667,2152341.3855000003,2019424.592466667,2921807.5,3997588.59375,2580413.5,23705891.333333332,775320.8333333334,2174069.8333333335,6674895.666666667,2744758.1276,3272373.1458333335,6779958.166666667,3637727.9791666665,507885.1927166667,3910942.1666666665,5030286.666666667,942844.8333333334,3923695.2083333335,5881019.5,2574188.8333333335,1192770.8333333333,2680155.7708333335,15956292.333333334,3569163.7916666665,3231053.9583333335,3391696.7916666665,2493588.3333333335,1266340.8229166667,2276393.0833333335,6016060.166666667,1068833.0625,1044941.5,826162.6666666666,1518200.2656333335,2898036.5,6031122.354166667,1390369.7708333333,990169.6875,2077841.875,563511.8333333334,8776096.666666666,2994433.1666666665,745146.28125,18149341.916666668,2503127.3020833335,1097891.625,1342964,1391733,2855140.9791666665,7976888.125,1375225.4166666667,4135248.1666666665,3535902.5625,5500255.757816667,1025277.4479166666,1231750.0156333332,607490,8460334.833333334,657547.1666666666,2188088.6666666665,1400952.1666666667,3124242.2916666665,4680561.833333333,33053696.333333332,9374415.083333334,1329616.1666666667,1861516.3333333333,1207058.6666666667,751733.3333333334,911755.5364666666,4269344.916666667,10806193.75,28494522.5,1093384.1666666667,1342166.6666666667,881733.625,2300932.1666666665,868910.5,14530866.5,2714972.3333333335,1714834,4609115.864583333,845749.8645833334,4496026.416666667,2288872.0625,1126784.8333333333,7743609.583333333,2954709.1145833335,3422213.6666666665,1026520.5,775045.96355,711080.125,904666,1005504.1666666666,1173439.5,4364120.5,16143351.666666666,1337157.9270833333,1811679.8333333333,20798243,6445856.458333333,877979.6666666666,950536.0729166666,1041815.2916666666,2168098,3799994.6979166665,542846.5521,5656827.5,2965278.75,2088905.875,791175.1666666666,687124.6666666666,1226515.8333333333,6212707.5,1086393.1458333333,1009982.85425,21971728,3617208.3333333335,3557199.9791666665,2592815.5208333335,6784076.5,3475007.84375,3904768.5833333335,1025363.1250166666,7505264.333333333,1698317.1666666667,142536792.66666666,3152913.3333333335,468272,777024.1666666666,6737442.5,2931826.3333333335,4193763.0416666665,3567332.5,1242124.7291666667,1928874.0625,3023126,22423066.333333332,21288359.166666668,2632796.8333333335,11457957.833333334,778047.8958333334,1957595.6875,99984410,751108.4309899999,1671964.7708333333,440751.6666666667,765703.8333333334,765848.323,1810396.4583333333,3451736.25,2863034.8333333335,785134.8333333334,3500179.1875,21288359.166666668,1420175.2708333333,3331708.8333333335,1880635.6666666667,1818238,2188088.6666666665,1405132.1666666667,1292981.8020833333,5127871.041666667,6382325.333333333,2152341.3855000003,26279379.833333332,10408624.5,972253.34375,868511.40625,14263646.333333334,2300610.5,4970070.166666667,2333214.5520833335,18062448.166666668,2255441.3333333335,1908339,16628439,2483562.3229166665,92435542.33333333,6067527.6875,672530.5,1280280.6666666667,3422519.8333333335,1885749.3333333333,1501606.6666666667,4364170.3125,1093384.1666666667,3839968.0208333335,1382468.33335,3666468.1666666665,818185.8333333334,2693019.6875,1584662.8333333333,9421122.083333334,1773187.3333333333,825395.6666666666,1402293.3333333333,1774471,3872624.0833333335,764277.3958333334,1420815.1198000002,11323613.666666666,3480475.7291666665,1065078.9817666665,5582410.666666667,6760010,2344450.3333333335,442047.32163333334,1943448,2407111.5,3190944.6666666665,1298598,641768.3333333334,5358644.125,1131994.6666666667,2164623.4791666665,3023933.7291666665,3612883.5,1592364.1666666667,779793.1666666666,1745297.3333333333,39719754.5,2399655,7396667.333333333,10533514.916666666,2892486.4583333335,3391696.7916666665,1266340.8229166667,2146805.0208333335,2021128.5,1579201.6666666667,867052.1041666666,1625600.2708333333,656270.5,2590325.6666666665,1607796.3958333333,1329514.3333333333,26279379.833333332,1113830.8333333333,1775266.5,678651.45835,458202.3333333333,792787.8333333334,2814418.1666666665,5010969.5,653808.8229333333,3673250.6666666665,822565.6666666666,1010979.5,668498.6666666666,992368.8333333334,6876685.333333333,793515.2448,2256932.1198833333,9709568.833333334,865048.8333333334,1290231.3020833333,52308409.333333336,1256825.0833333333,2769047.5416666665,2897944.02605,624255.6223966667,1505698.5,2324079.0208333335,1458564.5521666668,1055968.7083333333,3014401.0625,678651.45835,3015613,1093384.1666666667,761809.2604166666,2703781.6666666665,1019134.3333333334,1131458.8333333333,1420175.2708333333,2756410.5625,3625787.8333333335,1089080.6355,933329.1666666666,3595805.03125,800714.1770833334,6559923.333333333,1779702.9791666667,5629275.666666667,1506085.0416666667,3232610.6458333335,6678338.291666667,1050703.09115,2830054.625,883012.9479166666,768058.5,845778.90625,3143601.9479166665,2121002.9896,7150178.583333333,2230021.3333333335,1135060.8333333333,863022.9323,2349994.1979166665,856383.53125,4196371,2330683.6666666665,14400412.5,7321857.5,1762991.5,1366787.8333333333,1258261.3333333333,1294979.1666666667,4087508.3125,1292104.6666666667,868501.8020833334,2863034.8333333335,1123663.3333333333,5882130.333333333,993195.4166666666,778700.3333333334,458202.3333333333,586502.3333333334,7189721.625,7921279.083333333,854730.1666666666,8000430.25,2675436.5,5064943.177083333,3625055.8333333335,2376873.3333333335,2272464.1666666665,1226515.8333333333,668498.6666666666,4438884.625,3435449.9843833335,945532.14585,1336662.2083333333,1851262.9531333335,17562299.333333332,8665489.5,2465875.4166666665,833104.3333333334,1266340.8229166667,648638.5,1816912.5,3680787.3125,5445252,430019.82031333336,6658290.041666667,484928.11981666664,16964243.666666668,1576339.3541666667,47460904,808984.1666666666,2361087.6666666665,1861516.3333333333,815668.6666666666,648834.8333333334,1192486.9166666667,736618.7343833334,1455094.1666666667,1929141,5663990.666666667,2577563.75,3782600.5416666665,6541463.666666667,2981625,730798.9739666666,1597737.6666666667,1016681.8333333334,2777407.6666666665,1527374.8906333335,1300256.3776,7435569.666666667,968102.65625,1413934.8333333333,1887621.4166666667,2442738.6666666665,2732398.4375,2225892.1666666665,5097502.395833333,3037926,581746.8281333334,765703.8333333334,12397236.5,7841449.541666667,4677635.083333333,5499090.166666667,3081712.6666666665,6981072.5,20359162,2911077.3333333335,1228364.8906333332,4161972.375,10168495.333333334,1473610.3906333335,9349805.708333334,9975921,5064943.177083333,2493588.3333333335,931774.4218833334,5312127.333333333,46799953.333333336,442047.32163333334,6674895.666666667,1013584.5,899051.5,5740103.583333333,1179171.6614666667,1312569.1666666667,3985048.1875,1308206.6666666667,720343.8333333334,3637727.9791666665,3072953.7083333335,7757484.166666667,701173.2136333333,4909904.833333333,1693622.4375,1120315.2500833333,2493553.5,20388824.5,5964352.75,3584675.6666666665,2748850.6666666665,10652016.75,957068.3333333334,2895527.4480000003,2648554.2291666665,14918515,900961.1666666666,5817703.75,2413999.84375,1050703.09115,14453425.666666666,2859030.6666666665,10260553,1704136.1250166667,1921887.0625,1281728.8333333333,1763742.3645833333,725709.0026166666,1926290.8438333336,695346,1033592.8541666666,672315.6666666666,4214939.375,3242886.8333333335,2988391.6458333335,1092496.4688333333,3556831.0833333335,833889.9843833334,4264210,1595757.5520833333,728317,7693778.833333333,3597838.4375,2892486.4583333335,2316296.5,1898028.875,519082.5,2006982.6145833333,4161972.375,962946.8958333334,10699492.666666666,883012.9479166666,3867399.5,764967.6875,2381243,3264133.1666666665,2700926.1666666665,1489166.6666666667,5056686.791666667,802482.8229166666,1967725.375,6770981.5,1465005.8333333333,858763.0468833334,4148385.7708333335,5599568.416666667,40421379.666666664,778200.8333333334,6470226.541666667,643761.5,2566197.125,1355258.6666666667,2555194.5,7405952,1914023,1118939.6666666667,2695078.5,4766837.833333333,4793197.375,5348330.833333333,3835383.6145833335,4875794.166666667,1218195.375,1341470.6666666667,1882807.5,1508799.3333333333,3724300.2083333335,792396.3645833334,1794327.8125,1876822.3229166667,3161037.5,14570323.75,1319280.8333333333,4915194.916666667,2200806.6666666665,6597585.833333333,1052088.3854166667,3923695.2083333335,3471213.1666666665,1317258.3333333333,813404.5,1104952.7291666667,4374544.333333333,1647367.3333333333,2730119.5,5688528.666666667,4580386.666666667,1471352.1041666667,10120807.166666666,1012616.3333333334,3175752.5,507885.1927166667,2174301.375,2511226.125,3459784.6458333335,3644183.1666666665,2025379.6458333333,1421520.71875,1156803.09375,9111860.333333334,1310213.5104166667,2818531.0573,2114889.5,2054259.4895833333,2629486.4270833335,3276241.375,1458523.5,3034714.3333333335,946611.1406333334,1255777.3333333333,2460008.3333333335,9786015,1791049.5,1479751.03125,1211119.6718833332,737427,1075737.9166666667,972094.3333333334,1785954.2708333333,993647.4270833334,2356257.5625,37675564.166666664,3702903.0416666665,1332205.8333333333,1471706.7708333333,722510.2916666666,991292.5,3671045,1646164.6354166667,1341851.5833333333,794140.1302166666,2889811.8333333335,15602948.083333334,3184874.1666666665,1417621.7395833333,1829358.6666666667,7231560.166666667,7827754.333333333,7680262.833333333,2930499.3333333335,856529.5,717858.6666666666,1765644.5,1605241.1666666667,4667743.791666667,1789319.6666666667,2929154,1260525.9791666667,70247555.83333333],"xaxis":"x","y":[3195230,730765,1234149,1069095,629036,1328920,1677877,1700057.375,1116040,3466465,5722325.5,5674450,3098638,19459304,1418400,2378077,521341.9063,6135173,1066894,4660015,1713381.75,739711,6779778,760008,593042,2887392,2643948,2668769,1989387,10498344,37951708,1441900.5,804237,860505.625,2014500.375,2349950,1034827.313,965651.125,808968.1875,2074152,1832031.5,687953.6875,932598.6875,1207700,2390284,21346100,1750099.75,1355558,1435730,7166015,1385975.75,3155391,2075744.5,1073156.375,1394408.25,1247361,782483.125,11026945,37430780,1594504.875,2277316,3742915,1399062.75,33556756,1440662,1795462.75,991918,1250669,2545030.75,695828,3220827,1038815.188,6578370,1863468,745575,741850,10343027,659265,1107290.625,14172606,14585813,4926695,3343550,683307.375,2539565,974278,297715,4664399,1533245.75,12204922,515025,3043767,6118300.5,1764409.5,628609,4857581.5,3918038,858932,2313679,1984067.625,4971699,8611054,1344512,682632.3125,39061360,9560965,5696695,1813718,552007.5,3129811,675772.875,609218,1303247,2788860,1331469.75,8058913.5,1297872,1323038.75,1349970.5,993738,1827817.25,11813861,827211,520738,5049446.5,871355.5,3059758,2203026.5,3528086.75,3078672,416885.5938,3644982.25,1900006.25,615894,53166156,511180.6875,1970956,1290284,791052.875,65225380,872114.5,1241917,1019183,161363.4063,395517,4590246.5,2022990.625,14465798,3588718.75,1242666.25,8138616.5,1848656,572611,6293359,12783255,1109851,712057,4135280.25,3944933.5,520405,6807227,2564004,393493,1497886,19854012,2120807,2711776.5,638772,7080118,1164052.75,534110,1169932.75,928409,653175.8125,10608788,415975,230365,1365644,1022614.313,2882489.75,920068.875,1962435,770333,775728.8125,2116209,635698.6875,1403869,781217.875,1976920,27787286,10791309,2476833,238969.7031,312563,1239315.375,796471,551219,924068,15660628,5169726,1280690,0,838394.8125,2094525,1070085.375,12525174,1834290,345399,9191826,4141785.5,2065156,520765.8125,727374.6875,7946961,3647194,2136086,620483,5792926,3275779.25,1003551,14204281,3045139,1094343,284857.3125,523614.0938,10564729,682249.625,798322.375,816508,652425,21795830,1662054.125,664539,10420096,3778355,1049748.625,21169502,4524791,41092076,770082.125,1210798.375,400054,1810353,19344782,1392299.75,563237,1008665,4877959,78272488,1469870,677536.3125,1011667.5,2503804,3471845,6206083.5,6310254,1813864,3625434,1605182,611536,547084.375,2454812,1429196,2826828,5136604.5,3258819.5,979952,16701055,791542,1360340,90558.70313,11935719,1688612.5,2278160,659652.625,3537977,967835,918943.875,3822102.5,636266,642133,608359.6875,9722237,602245,746858.1875,18848780,4560492,7015692,4133811,2563985.5,898115.375,1967434.5,61576184,638507.125,6235947,4291789,6454983,5155090.5,703298,32381836,3176347,3093944,1896626.375,2006647,6781960,2912454.75,2601369,1417778,20311228,6525887,8374301,207572,826416.5,2823184,614768,1868497.125,11208498,1363191.625,5453438.5,855665.125,532670,3468606,905926.875,281191,2147968.25,6219841.5,1539066.375,4934334,982332.8125,605971.1875,4471029,6462938,650205.8125,860364.1875,2613607,3567977,4520838,15324777,2720238.25,1938613.125,25307744,1929118,7798464,747456.875,6859198,934980.875,2150120,662701.1875,6456026,4423958,2037693,1045413,1663324,542998,1193790,4573777,1247972,1208035.375,48479988,23262244,919180,987989,17457560,11924933,576174.125,16849234,4166306.25,4558824,847097.8125,1055624,11521140,1423463,415453,740230.375,603376,1314716,666930,2133372,818836.125,1993273,941366.5,644121.6875,25478086,2225265.5,5348309,907758.5,1007650,8136665.5,1026921,5457975,733249,652449,16917964,890977.8125,5231373,1523508,6960904.5,2966309,7456330,947518,1989287,2634166,2094724.625,947884,1870457.75,1840259,29949248,4103800.75,508057,2621041,2746152,5662234,8381834.5,1021798,25475474,2071670.25,987288.8125,924073,13093350,1960538,2082754.75,832168,1742351.25,408513,1976856,1605220.875,3046719,3366510,8688676,4399828,6344171,1924355.875,4826430,8747819,876237,3347567,2996958,878927.8125,3234871,1433267,11110405,6289567,580688,635697.3125,13567774,665216,3063969.75,14830411,536642.875,6069855,1125724,2923484,14856602,1848878.5,717197.8125,12187231,10328702,2666634,7564604,683274,1136111,847298.8125,5827595.5,719905.1875,1857889,1318096,1190733.5,10317852,842593,11047668,4193371,51168308,2172841,954352.6875,2547582,9290214,823460,1147582,5481995,1605669,2926089.5,1022957.688,6723971.5,2807583,1162226.375,4329966,1547556,5437110.5,2076289.625,821046,711118.375,1108310.875,1210229,1385697,6333706,1160989,2119806.25,691647,9162048,8773058,23612824,7243389,2992693,1811669,1375366,2254220,1431784,1072855,3404992,2046914,436848,11799105,501897.6875,226375,352136,6711975,14431245,906103,1917399,533742.8125,36367960,6482380,764008,1666221,2019254,241478.5938,720170.6875,2842760,132105,676032.625,510517,710275.375,756267,2474457,2023858.75,64547116,1228113.5,6361236.5,14332778,2330242,647906.1875,15036617,12719042,5903142,22394416,9546165,2924362,1375978.875,1022114.5,3637973,1137504.75,831855.875,15640994,2696136,957460,850667,11325513,2962476,1988334.5,26689658,1358519,744615,12704014,2712983.75,2153603.5,2080866.75,2791291,1000180,6182350.5,711594,14169301,3137206,564988.8125,6802622,673834,6335315.5,492481,603420,1208029.25,1067153.75,1618779.125,628023,1856230.625,2361088,793436,12087498,867456.875,1218114,8320719,6797528,829929,684739.6875,1757194,325513,1666762.125,739869.6875,50899248,529263,650008,1072487.875,6879291,2148565,1509292,3377209.75,3292049,666436,400770.9063,7762038,472532,4673827,811907.625,33268410,1834204,6010407,1403820,1603017.5,4750287,465533.3125,1073507,516216,585443,1038667,436252.5,3024478.5,2290681,1381590.125,1494252,1242311.5,685310.875,1000599,3986851,879235.8125,1372213.375,592739.3125,737935,2345314,800626.6875,2817261,2446842,1476969,646994,1241432,1327591,2043592.625,1111858,2443141.5,1216543,779204.875,1224685.125,1396191,2773647.5,730672,1054995,5948007,2569106.5,898845,39403320,15026352,42584028,938908,2270531,789209.8125,866825.375,1991234,768755.125,1900577,2531070.25,1912719,3091348.25,1272821,794724.625,4382240,43943,7405070,719734,5424718,936336,612158.1875,1585441,3076813.25,867184.375,13116256,4998015,721983.5,1097572,1647746,8575114,2179078,3303553,615955.625,57514116,1820814.75,1425599,2051501,14084606,1576953,498649.3125,722891,685764,4545148,44984468,885865.875,13424079,2212058.5,549834,4801472,578150,32675250,3632613.75,1546484.875,607373,918177,2569602,1401315.875,4467292,7319546,941688.875,2003948.375,8669367,1862752,1140620,48981052,794715.6875,9393108,4222682,965786,13315803,804620.5,928084,3473109,4188660.5,445943.8125,3988729,1310236.625,1191803,17463744,742565,670779,6399127.5,216115,1842562.75,1857329,3475804.25,562338.3125,4108523,1651120,714898,4748137,3528230,1700276,1639849.25,1650279.25,49102164,7802205.5,6606832.5,4626443,3158151,964923.8125,2531163,4249212,440419.1875,1000552,405823,538064.875,14329739,3534690.75,659368.625,2114860.25,1811506.375,632517,7370743,1795049,594848.625,12726789,2442220,757167.3125,1076693,1896858,2603931.25,2127077.25,944638.1875,3157579,4486477,3180819.25,557863.6875,930439.375,1826203,19018762,466904,3055176,2347893,9956396,1436773.875,29747868,6657348.5,1056099,1987934.625,1119679,151798,735907.8125,2664502.5,12729858,157606480,1620721,614985,917194.625,19061780,646691,5889882,2299216,7768057.5,9097980,1406687.5,5481549,1027394.313,875789,4337107.5,689082.3125,1634334,425574,1440554.25,836437.6875,729535,592805,634751,2057288,13491170,1509775,961749,37785552,4628171,984054,1038045.313,859137.125,1142949,2411402.25,5833800,7338511,3611538,966812.125,616749,730292,1120197,10053489,1044220.625,492807.6875,9308471,2240549,3241917,1433573.25,13146645,1208407.625,2131817,1424157.25,11353672,2755578,163945984,861643.6875,718362,784652,2757588,2759449,1765813.625,2162760,2227908.75,6458021.5,2087998,22325364,6481807.5,2503240,9085924,785144.8125,2458760.5,61984596,900973,1859571.875,945063,715728,509171.4063,2504814.5,1878442.25,2434208,572139,520011.8125,24017770,1814947,2231695,1725773,53401,1383950,728876,1046995.188,6843855,9155568,2885740.5,7220201,4424263,1694930.375,550324.1875,11356990,1742870,1094622,966677.125,24224888,1602030,3888945,13609040,5981430,16644664,1252323.875,479358,2630101,7004126,1911591,1357553,12184124,896172,2830674.75,1143826.625,4767227,2108012,3548858.25,1986624,19036416,5448687,1602143,2505271,1817513,40613740,1361427.75,2345752.5,4020660,4391348,5976246,3075425,13883385,1984823,677642.125,1456808,1189432,1271021,1696009,609494,41399504,767720,950695.625,1993766.375,702385,2188093,735810,2504797,17186624,1885314,8728584,11951945,2495819,1000796.875,1539176.625,1730542.25,1223296,1392153,847514.5,991513.875,1122837,2098850,637053.1875,819314,25672598,653672,1358652,845961.375,5807879,911366,846274,7879628,1433504.375,2279063,755269,1138520,133880,524479,10110827,624269.625,1890502.5,3902177,563298,919048.6875,85357952,503744.6875,2485521,10642754,1686405,2189124,1427294.375,1353079.5,862945.125,1956439.375,866806,1972491,627698,1553208.375,1929828,1151027,1222509,1404702.25,1725558,8354235,911088.8125,708928,3441643.25,723482.1875,23364636,1566177,7657568,1262255.75,3444270,5328051.5,313528.0938,2567976.75,634056,488181,915286.1875,2559461.75,3494929.5,4116871.5,695803,989257,1152896.25,719471.1875,639407.375,1755264,3619497,13349651,1393073,4258124,1897180,669117,1769137,541943.3125,946138,2214604.25,5229981,966764,6211441,639566.875,602618,523133,749207,4321141.5,2222097.5,648022,5177270.5,4785692,1933811.625,2184912,2845068,1399992,1285055,315468,6782435,2173143.75,2070005.625,1692199.25,2409429,7794869,2999116,1614673.625,659450,981009.125,491792,1454222,1953919.875,2101376,904236,0,1417728.25,20828130,1827424.125,32552428,514471,1703060,775729.1875,1272272,622051,783208.625,1625308.625,962238,1465535,2834401,2050600.75,2309007.25,7758182,3837228,522972.1875,1982967,769148,1698779,1928564.75,1257033.5,7654264,1285646.75,3643112,1224045.25,2632472,1678495.25,2076612,1964681.25,7231928,728167,957632,13631685,6673138,5571501.5,1261725,4618360,2756494,136241424,1529309,1353612.75,2786648.75,13586070,1100499.75,52960796,6669954,2274873.5,1972156,817695.6875,1911611,58041572,117438.3984,6127771.5,656980,418098,4634655,419978.6875,1057256,41680640,694189,674961,3371659.75,6983736,5288304,1912760.75,14981114,1764767.25,806615,4116805,9569715,7475577.5,3721129,2014721,5101504.5,719060,2919980.75,971155.625,16373232,791564,4933374.5,10834087,605186.3125,27185096,1045838.5,4656777,834711.875,1008536.813,1688745,4262316.5,928584.875,1167734.75,298063,552401.1875,450518,3796986.5,3371455,1240924.625,1537570.25,227566.7031,164817.9063,5543575,161311.0938,606886,1083729.625,1368120.5,2646523.5,2513155,1167967.125,638201,5274642,1386445.375,730211.1875,8692328,1257144.75,4212149,593296.1875,1767480,14265421,2447861,948157,1739286.75,1121537.75,12029600,6797647,151376,1761137.5,6674179.5,6505995,30135280,696522,7837692.5,767361,1989763.875,1788999,1822830,5232420.5,1554342,1523324,1487660,3556291,2249620.5,6592304,335245,3714139,1594422.5,336461,1956075,1838121,3986982.75,817409.625,2946756.25,1521693.25,1880890,12265516,1965529,4103027.5,1686248,5276777,1988760.875,3515821,2218249,947534,2892779,756908.8125,10721568,357915,4571388,3022275,5051636,2429214,5446624,886771,5326222,584545.625,2158629,726124.625,930611.875,1889990,1423929.125,866126.375,783651.5,8201974,2346692.5,145468.7969,2399762,938096.1875,1145535.75,2468322.5,831577,4829951,431815.6875,2361960,1210262,8787353,2882757,1228837.25,2735811.5,576052,442375.8125,866242,1339220.625,880115.5,1656025.625,65047284,2356376.75,456747,673030,863703,686091,3456131.5,2503690,345866.6875,941564.8125,1012341,15091579,1733511,999856,2154119,6268627,5454482,2407904.5,1815350,799996,593824,585559,5063753,5572503,8228399,6014078,994194.875,27717034],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle KNeighborsRegressor(n_jobs=-1) vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predkNN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["n_neighbors = np.linspace(0, 100, dtype=int)\n","param_gridkNN = {'kneighborsregressor__n_neighbors': n_neighbors}\n","\n","\n","GridkNN, \\\n","BestParametreskNN, \\\n","ScoreskNN, \\\n","SiteEnergyUse_predkNN, \\\n","figkNN = reg_modelGrid(model=KNeighborsRegressor(n_jobs=-1),\n","                         scaler=scaler,\n","                         X_train=BEBM_train,\n","                         X_test=BEBM_test,\n","                         y_train=SiteEnergyUse_train,\n","                         y_test=SiteEnergyUse_test,\n","                         y_test_name='SiteEnergyUse_test',\n","                         y_pred_name='SiteEnergyUse_predkNN',\n","                         score=score,\n","                         param_grid=param_gridkNN)\n","\n","print(BestParametreskNN)\n","ScoreskNN\n","figkNN.show()\n"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[null,14158414.94566695,13403239.5942038,13343787.486986501,13444654.131235966,13530752.651350686,13364395.380575122,13507637.318707105,13593855.699040389,13769613.43463732,13866642.107942834,13935421.562120091,13944669.0409824,13994556.148370575,13992916.08681111,14089947.612403136,14169786.762831062,14186153.37094466,14198620.10924004,14264881.367806192,14327129.216390004,14371262.214585334,14457184.112106103,14508393.110155558,14548230.296122432,14607351.091670055,14652539.09175539,14699250.83441698,14732983.961667199,14771194.652398895,14799935.115512032,14824775.69685718,14863552.363174748,14892640.699048406,14930235.653324958,14960288.211169478,14985455.366598343,15018678.176275617,15052182.87122589,15074149.654768607,15102437.669586647,15131452.356914291,15160464.034429073,15184295.385656971,15210810.313070755,15229133.611238156,15248363.967235003,15254486.004255181,15275415.597003425,15306818.127395373]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[null,21670134.17093419,20413972.72328853,19664233.37389612,20134236.017666794,20431679.758420717,20700833.641183272,20971747.409982853,20955322.8479319,21183760.382892936,21269962.190333363,21380162.815050945,21501063.501382276,21506227.78876864,21487148.22917446,21601148.115062173,21721235.807482064,21709032.646961335,21675853.89140894,21737830.15554136,21813687.15890424,21849988.396938905,21948406.344541647,22011260.97297934,22065207.80476094,22153987.090829115,22204633.13590651,22248196.09467885,22271116.86330194,22316864.784841128,22347480.16390217,22389953.070740744,22435677.571830604,22463297.392122656,22507020.354621433,22544627.228114404,22567375.49397192,22607180.92398748,22650884.279579308,22680275.659195226,22713128.101184905,22748773.239401076,22783400.445547573,22813014.03084469,22842218.81607984,22866671.21911292,22890237.24904356,22896910.772022717,22922285.35922002,22961621.717927735]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[null,6646695.720399712,6392506.465119069,7023341.600076885,6755072.24480514,6629825.544280655,6027957.119966973,6043527.227431359,6232388.550148879,6355466.486381704,6463322.025552306,6490680.309189237,6388274.58058252,6482884.50797251,6498683.944447759,6578747.109744097,6618337.718180061,6663274.094927986,6721386.327071142,6791932.5800710255,6840571.273875769,6892536.032231762,6965961.879670561,7005525.247331775,7031252.787483924,7060715.0925109945,7100445.047604269,7150305.574155109,7194851.060032457,7225524.519956661,7252390.067121891,7259598.3229736155,7291427.154518892,7321984.005974155,7353450.952028482,7375949.194224552,7403535.239224767,7430175.428563752,7453481.462872471,7468023.650341987,7491747.237988391,7514131.474427506,7537527.623310575,7555576.740469253,7579401.810061671,7591596.003363389,7606490.685426442,7612061.236487646,7628545.83478683,7652014.536863009]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[null,11366650.783153532,8430113.571121756,8945299.633028418,8982905.450402632,9037521.07185096,7966523.69793712,8185640.683529056,8282324.029570459,8504849.015092976,8628703.583767442,8561029.199541183,8601223.677625822,8798170.355943648,8869323.379936166,8971088.596847568,9076322.609407403,9176332.52867184,9242778.288040914,9362144.253341146,9452362.849722864,9533640.21707534,9592326.1767365,9670805.990839258,9688172.954530867,9742343.23390388,9782531.61574835,9840071.081938766,9882094.7248394,9917277.644996414,9929996.466649506,9964645.134536361,10006120.801188976,10031659.536090503,10072041.441385264,10102019.214706851,10139711.676826494,10169631.522199836,10194690.984700553,10189378.765344176,10215542.698121812,10242412.3674915,10273133.402135687,10283878.699214054,10307070.46018663,10323805.113467129,10343057.971433844,10360871.387007415,10382868.692386232,10405674.771642135],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[null,7873652.013295755,7916888.389177888,8768543.927868094,8379692.838797002,8199770.104839675,7852640.440407012,7800014.185671345,7991517.419471307,8065355.2173812045,8361807.9497686215,8457811.5696283,8144836.262814575,8328914.867702249,8303041.382166323,8356559.728863496,8355544.206596374,8308898.963888823,8301168.658331139,8371564.712774081,8389470.352700736,8376492.044567143,8450052.143249944,8471432.511415303,8509672.819284298,8505396.794949587,8523117.827473138,8552610.547720168,8598159.594524097,8608751.368767353,8632408.933046442,8618761.556903224,8656792.92764886,8678782.677379215,8714128.055700518,8724679.86374735,8736014.32050728,8763696.884963028,8786404.728724893,8815872.212883009,8843536.308323393,8869484.910540184,8896202.527500255,8921232.403289,8951164.503676735,8958470.53267196,8969474.796920422,8975235.359831033,8994130.046195263,9016996.218792029],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[null,28924172.132537168,27015966.12858142,25737279.834054653,26551239.500028282,27043970.360560745,27624476.88117131,28024825.168825265,27892509.02258762,28175972.493626926,28287776.749308053,28423837.30711922,28623803.38479036,28641165.87681058,28615458.641690444,28742105.88084418,28901585.67431191,28855944.869932216,28769656.118687984,28839469.337947723,28928242.30739119,28946790.99447096,29050318.28880024,29120906.920834083,29185707.112967726,29292965.948749606,29342479.815528207,29379136.32146699,29392845.72329616,29440719.574531056,29469206.57326839,29523390.785207123,29576340.460249264,29598978.85959018,29648958.85549681,29689824.494711373,29706699.41106458,29751914.18386947,29803913.75301715,29838569.169624176,29876952.630296163,29920093.725046802,29960298.491698332,29994029.65337022,30025430.423684835,30054101.785485957,30081058.44190663,30093271.94037311,30123519.50041633,30168931.96361982],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[null,10754254.763130536,12393205.073524605,11719338.397211244,11678809.361156292,11903095.472681656,12253735.115798296,12409522.938352913,12668785.304784032,12806175.808536598,12914096.460587354,13040961.354969475,13064686.03238495,12920625.458208261,12836183.016496554,12935245.43513095,12977247.448730096,12991532.296297397,13039365.590822699,13046259.72464332,13086248.319862679,13157453.556116963,13288187.354568928,13383128.190955238,13451828.557993593,13546446.349518461,13607141.39653793,13665847.894856736,13698659.095729213,13745144.61209479,13787139.779185096,13838407.00969896,13889145.812724857,13932101.69501117,13978246.49123888,14024829.248668572,14067311.046388222,14108233.013190502,14146160.631065277,14183652.239168739,14206599.926163506,14229923.510189334,14266097.879443804,14302147.319507228,14340028.932124019,14363614.542513842,14383296.560848588,14356344.82700236,14377747.15651342,14413562.952295782],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[null,11873345.036217753,11260024.808613334,11548475.642770099,11630623.505795624,11469406.24682039,11124600.767561873,11118183.617156953,11134142.71878853,11295714.638548888,11140825.796282712,11193468.379342288,11288795.847296288,11283904.183188133,11340574.01376606,11444738.420329476,11538233.875109524,11598058.195933027,11640131.890317474,11704968.8103247,11779322.252272552,11841934.260696271,11905036.597174894,11895691.936733916,11905770.035835681,11949603.131228732,12007424.803489335,12058588.326102238,12093160.669947116,12144080.061604878,12180923.825410718,12178673.997940248,12189361.814061793,12221680.72717096,12237803.42280333,12260088.234013235,12277540.37820515,12299915.277155243,12329744.258621575,12343275.886822935,12369556.785028346,12395347.271303637,12406587.871367294,12420188.852904342,12430357.24568156,12445676.082051894,12464932.06506552,12486706.507061996,12498812.58950588,12528924.730627108],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle kNN en fonction de l'hyperparamètre n le nombre de voisins"},"xaxis":{"title":{"text":"n neighbors"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE kNN pour tout les paramètres de GridSearchCV\n","fig1 = go.Figure([\n","    go.Scatter(name='RMSE moyenne',\n","               x=n_neighbors,\n","               y=GridkNN.ScoresMean,\n","               mode='lines',\n","               marker=dict(color='red', size=2),\n","               showlegend=True),\n","    go.Scatter(name='SDup RMSE',\n","               x=n_neighbors,\n","               y=GridkNN.ScoresMean + GridkNN.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               showlegend=False),\n","    go.Scatter(name='SDdown RMSE',\n","               x=n_neighbors,\n","               y=GridkNN.ScoresMean - GridkNN.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               fillcolor='rgba(68, 68, 68, .3)',\n","               fill='tonexty',\n","               showlegend=False)\n","])\n","\n","fig2 = px.line(GridkNN,\n","               x=n_neighbors,\n","               y=[\n","                   'ScoresSplit0', 'ScoresSplit1', 'ScoresSplit2',\n","                   'ScoresSplit3', 'ScoresSplit4'\n","               ])\n","\n","fig3 = go.Figure(data=fig1.data + fig2.data)\n","fig3.update_xaxes(type='log', title='n neighbors')\n","fig3.update_yaxes(title='RMSE')\n","fig3.update_layout(\n","    title=\n","    \"RMSE du modèle kNN en fonction de l'hyperparamètre n le nombre de voisins\"\n",")\n","fig3.show()\n","if write_data is True:\n","    fig3.write_image('./Figures/graphRMSEkNN.pdf')\n"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                             paramètre RandomForestRegressor()\n","0  randomforestregressor__n_estimators                     100\n","1  randomforestregressor__max_features                    log2\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predRF=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4492542.49,792779.5576958001,2006549.624376,2139078.8632199992,3380206.87,1700656.60125,1214892.05876,2469346.7507052002,7747184.561875,3866175.7425,4658005.8825,1911134.79,4520559.8525,14005292.47,1374214.223438,2628655.83,371412.27373799996,2055059.6490640002,1200294.66,5095933.30125,2082850.84127,588341.77375,3414066.63751,1173925.32,693941.95125,8028095.9125,2711912.593125,1998746.0275,12997310.03,14856932.93,3432519.04875,3418456.187505,1104901.62625,812419.396875,2329028.295625,2012511.05875,1333778.5793909999,1441423.048834,2806325.5825,2060306.9875,2140273.715938,1265771.525938,1950293.540625,1531104.878125,3037408.99,23790592.75,1060693.895,1339122.1695,1826520.113125,7524393.45,1455587.049375,3250042.97625,2140398.3525,1520310.344691,1145473.9443879998,2274308.584375,930141.896875,8461798.1625,50605688.45,1143932.648125,2147347.325,3341506.30126,1322928.784848,27000766.05,1607767.847188,1293000.775625,8632403.22375,5764596.1225,2051635.385625,991482.9375,2098466.35,1130881.36344,6864047.165,2995151.99125,853324.401563,939502.26875,3278889.16313,714499.9575,2434309.13625,8639351.0725,5984307.15875,12318122.175,2945447.218438,940018.129376,1991843.99375,1393673.20625,681661.395008,4345126.18,2213239.3681260003,4081352.505625,543884.5265,2558495.254375,6243221.951875,1200880.260005,1571980.2377083332,2006160.4103150002,2438059.81969,1203075.49625,4586513.0575,2992369.944375,2018715.69375,3953226.45,1293419.34375,685143.5295318003,6752109.3025,7136905.77,13191001.29,1244726.0477748,1209859.4865625,2937067.47875,892261.8496909998,812510.30625,797452.01,1400351.2175,1714554.665941,4620073.7875,1166638.315,1326370.153126,1493861.1656499999,1679781.8425599996,2371466.109375,16673930.24,975857.4254166665,709399.83,2403739.555,1744262.513131,2309112.0625,2683114.2029688,2384687.3725,2617102.1075,1808769.4588819998,7336866.99,1128918.8425,730336.434375,26353667.08,563481.6432835,2993708.491875,1223434.62625,1120737.88625,40015553.395,1187848.345941,2957931.9425,1308113.6491666667,848137.32125,994321.4943799999,14689102.045,3697229.15,12746092.065,8421243.9125,1156318.415,5706442.9325,4041894.3475,621835.298125,1184918.38,6145579.35,1621378.735625,600230.9133780334,4213777.93,4110344.9775,576448.860625,4393586.26625,2258129.53625,1215549.341875,928594.385,19675612.89,844457.84125,3772277.3206299995,858622.53375,8327003.745,1039489.18,585821.6925,1579837.126172,1282018.94125,659709.1806500005,4733292.1375,651671.473815,1492825.85625,1159669.31625,1129838.4678149999,5282545.6475,1370546.808125,1757335.99375,892070.3525109995,1403866.12375,1506530.68875,824379.6503150001,2844476.11375,725521.144375,3616139.86625,23539538.86,13698310.38,4740950.56375,2926101.70125,1760650.56875,1349854.904375,1757429.0920833335,597032.1509430001,1340174.306875,19240997.9,4032688.3,1241348.654375,1305002.11125,1730228.643125,2110558.44875,1649709.159375,4403126.952188,2166403.515,1427011.5315729997,6602456.27125,5034465.055,1963104.48,1154889.4991691664,767981.87875,7048698.86,2781308.84875,1933861.75875,2577459.315,5450565.97,2052962.215642,921844.40625,24751004.605,2412281.268125,1151750.79,2497727.1075,2220606.736875,15057952.725,1227326.80938,1244745.1433333333,3192187.66875,1063505.23375,6334274.1725,2697139.971875,957419.7997393336,2251019.54,1980963.6300010001,2117011.4973440003,21922599.41,8688322.4875,7011812.7175,762141.005625,2907571.610625,1508680.2318800003,2689916.85,12388745.265,1317878.350001,723631.885625,985707.675,2023361.955625,74615513.04,946766.985938,1493149.51375,2335175.594066,2454991.0325,4328437.0675,4451717.315,19503818.795,1655112.48375,2863665.44125,1831470.346563,683172.376875,974527.076875,4294226.3475,1242400.5,1937155.2092188,3590340.5062550004,3960478.39375,1355604.409375,9762102.525,600068.75,973565.315,825938.0434399998,5436053.029379999,2426178.74,3500394.00875,1091325.670625,2188917.502969,1089398.229375,775129.7022809999,3602848.495,885205.59,833961.0375,678476.0796930001,14670380.41,5262847.705,774028.4262580001,27388649.42,2243621.780625,4381252.65,2699493.2431260003,4408723.657189,1957172.2075049998,2586576.985,59405515.88,922229.461875,6443942.08125,571578.2092188,3387816.74,5505961.84,1042292.680625,28451151.92,1832867.69,8870791.22,1450112.4062675994,2675435.1,2020345.1025,2267746.323125,2792383.86,5142951.36,16873193.545,4349255.1825,31449031.72,1512681.0645269996,1201361.8096939998,849266.9389856007,1794284.97,1792138.596875,20740257.96,4894891.636875,7140636.125,803753.316563,782054.0156350001,1398145.6631260002,1925891.012191,640028.7503140001,2325574.07,7523034.205,1691143.4475,7511850.965,1089347.01875,4107967.88,4974554.1925,24313801.93,1122368.755625,711898.0078380002,5664655.0975,6500581.3625,3445927.8575,8982085.95,1585101.5656299999,2195578.9359399998,32048076.45,1625279.325625,3611807.648125,1004942.0556300001,4418650.001925,1932724.561875,1341026.97875,630410.119143,4118989.8175,4397863.7975,858009.7675,1729953.1559380002,1079563.34125,592255.8891666667,1996294.41875,3324383.294375,1565938.6275,1325061.336875,48945291.16,37355236.19,1493205.918125,748347.781875,16554469.7,14551706.24,377172.8368919998,19468287.91,3268461.636875,5163106.945,805048.5575,798459.20375,5906261.5725,1638250.545,1048116.26,610164.44125,1333115.201875,968859.600625,2634855.9418333336,3813836.0375,1156976.37627,2407776.456875,848128.3082617,1138188.3212549998,15682373.075,1212238.867501,3976991.9293799996,1476760.8021880002,1457323.111875,4906412.05625,1630967.02,7084383.235,870780.889375,1096216.909375,15671468.415,644272.4668790001,7074884.5975,1956498.27,4099948.7875,3890684.61,5155720.03,2124066.760313,1673968.792656,4176851.555,2214566.41,1031777.840625,1141759.288125,1349430.03625,19145915.715,3251117.6075,1117068.029688,1562993.86625,1904497.5209380002,3989529.88875,6983884.205,905846.3975,62011996.42,1599439.0231299999,1845279.103125,2762836.215625,17022992.345,1337847.2525,2387466.43625,2384676.19,2353672.46875,6390812.5275,1495092.69813,1424697.423125,5003804.545,6387842.3825,6089268.8725,2177867.125,2555458.83,2617969.0943949996,7551813.005,14780054.23,835860.7625,2779641.15375,2658619.25125,814428.7825,1820181.0175,1631664.30875,13604321.825,4331007.83,767744.133125,1350138.7991159996,13366814.155,846772.86,7001165.3425,15017133.6,614464.9000809999,7627389.705,844525.5959380001,5108575.2875,36672245.15,1690053.5931774003,1177520.785,3954804.92625,14908560.41,3266705.4775,8617521.405,1150072.9281269999,1768910.29125,1443704.0525099998,4209825.55,1080263.253125,1622134.105,3547094.775,1711805.54125,8440763.48,1433750.92,9106575.92,2785412.515,6802823.9325,3068292.67,1281551.3181659994,2346813.920313,4390198.485,781295.067188,2541115.425,8949332.625,1235736.88,1618783.9606299999,1094212.430625,5866709.7375,2886675.445,806400.1039059999,3325951.355,2053587.86875,3541927.9950049995,1738590.6553220002,861673.96,904464.2409387998,1343840.033125,1326598.07,1321573.4050049998,7145479.565,778645.27125,2254941.9042200004,794145.853125,10568922.835,5223042.430625,6594695.04625,14983711.72,3880008.1025,1046902.5225,3487737.76813,2703924.215,1206608.7240709995,1412427.1375,5896471.1075,2510633.8175,937400.9072393334,8438039.0425,693699.4890865003,1294749.6726250001,986039.1975499999,11141351.46,17245472.35,3538370.82625,1509332.83875,1618004.78,69761235.66,11295921.78,761640.741875,1141119.54,3337097.50375,1169527.614375,952160.43,2553686.1625,1037405.2664090001,1026934.51125,718130.838129,3082536.193125,1187646.36125,1881771.2409380001,1215893.143598,46698603.02,1865995.704695,4604819.61125,9206831.2125,3065643.55125,624235.763125,17476301.995,21788960.73,6111630.115,12697868.4075,7038741.55313,4879818.61126,2374428.32125,874151.475,3709543.08,1258270.085625,1131708.07625,6782695.275,814791.10125,1524562.477291667,1292778.659375,16910674.6,5955053.285,4492129.0475,55132154.73,1663207.78625,2769245.895,3730411.2725,4068929.3075,5896811.56125,2352185.37188,2606490.72875,1290260.1275,4345829.79875,675335.19875,17112048.91,5988201.71625,902623.9777259998,11581572.65,895321.79875,5993354.51,464848.6,883720.51625,1201422.2343769998,1009533.5862629993,2325781.48375,940348,2662184.124066,5778580.223438,1387964.885,6349670.8775,886889.9481449999,2914344.505625,15549204.245,6516069.51,813304.715,1027626.248125,2268195.811874,1430904.42125,1140823.5871899999,727472.1706849998,29530107.94,575234.1600070001,804533.406875,1252907.8625,1815858.7575,1925968.8390609997,2333577.1040649996,2403146.770625,2724679.40375,990516.465,681286.3007700006,11956399.86,816469.23375,2796583.8575,763409.6018899999,11742461.295,1766524.99375,5307610.80375,2037672.526875,1210350.9237549999,5444319.17,3349605.5675,1722755.905,2264315.0875,599927.572501,1033442.4975,1008124.61125,3906154.875,8184539.63,2547429.78375,719592.2175,1793167.3893760003,618195.4797090003,895587.674375,13761994.6375,1086735.3553229999,1006199.401565,806580.053442,743383.5675,986027.8,696655.403906,2599163.115,2698389.811875,2021013.961875,721906.7095833335,1940066.96875,1641155.5025,2167358.04375,1872036.89375,2535411.65001,1268159.636875,1085596.45875,618195.4797090003,1715390.92375,3152117.58375,1369131.7209380001,1626504.71625,2111472.213438,2728378.985,1012915.803125,27333478.61,14488949.14,22605338.42,1465963.355625,3333061.36875,879952.73,1237879.080625,931366.4772916667,903577.200005,1300835.898438,4642985.3825,2635155.3975,6882761.9825,3441793.77,1819821.174375,6220187.97,1789979.82125,7101488.345,2097332.24,4741754.98,1733627.97875,1087954.32375,878008.580939,2633195.9328489997,983806.6699003332,10549751.965,5585571.8175,598872.025629,1422346.86,1779469.16625,7001479.31,1583698.9975,2402654.8975,827347.3871880001,89576468.8,1664060.030625,1601179.405,1299625.155,1751089.61875,970005.9503129999,594268.289688,816457.2075,1023997.0087519999,12011271,52764253.275,1061253.5625,6459998.825,2908677.3475,534298.3987610001,2563455.2074999996,652456.90375,14238114.8875,3478493.3275,2218172.189375,783177.455,1723170.27125,1762014.855,999118.24375,2778683.12,9210584.705,650830.890005,3933302.67125,23086859.29,1441903.9225,2545403.385,29112392.565,1109109.439998,5056034.49,13010371.025,761504.925,60195132.77,979740.1993799999,1551223.685,2173995.76,3282514.327545,1994415.911329,3546140.625,3027692.409375,1635258.835156,23670924.565,622738.042188,1422593.77,7505137,1985302.6209359998,2179996.247505,3506207.8475,2122264.883438,643834.131563,4070304.864375,3955431.1475,922107.7625,4285763.6975,2710227.4,3556531.85,1675302.7075,2505307.8175,33982604.85,4069490.4575,5123981.405,4052545.87875,2553254.9875,965416.973128,2341833.83,7414584.26,1063200.550938,1291695.06375,606697.5309420001,1474007.981563,4937762.28,7378851.0281299995,772938.5921880001,1470283.013165,2352113.60375,836935.989689,8335621.025,2405472.611876,807987.3609379999,10646833.34,2466161.544375,839764.0255363333,2465635.843125,1152134.43875,5890183.906891,4785281.67,1187186.7334389999,4889790.525,3835560.15,9177765.95125,844260.3578663333,1735295.5075059999,1287999.995,10424084.95,714866.1745863333,2973970.71375,1386809.829062,4932726.61375,4871644.81125,32093193.07,8597167.32,1606161.59125,5623418.0075,1805841.45375,637523.2872659999,2342463.584224,3774847.7225,10794735.235,115207567.26,1254511.32125,1125155.29125,993556.429375,7874903.279062999,621778.6225,6959537.75,2359664.133755,1779385.57625,5487517.4425,1156240.9975,3257402.538125,2054541.843125,1805524.87875,3704759.2875,3227624.2225,3267607.145,1536266.785,761256.490003,696875.6037630001,870520.08625,1086114.258125,781295.067188,6049427.044375,21083317.97,1518380.5768799998,1258250.625,28044332.935,12864297.16,841019.59625,1263782.5775,907566.609375,1827978.32875,4140175.255,562964.7200360005,5814991.9325,2305548.334375,2262941.835,599022.89375,1145636.18,1474083.3469999998,7679148.74,1152443.855,981307.4203179999,21539837.085,3175073.28625,3411299.38626,1747082.517189,5892087.63,1727729.205625,4511113.570625,1626750.8431774003,11823600.205,2227653.3475,101002832.9,5513019.299063001,616578.930938,796424.395625,7500025.66,3399970.748125,2956094.1543799997,3546589.61,1521933.981875,3601936.423126,1928043.92375,21448213.27,13033507.52,2234536.89375,12210587.345,739446.312501,2262835.15189,81148441.43,645495.1960164004,2053672.58375,505899.001875,736004.989375,749252.0900099999,1657431.0673048003,2998047.12625,2579767.1925,739328.389375,1808780.380941,21007947.46,944979.595,3274388.65,1879799.673125,2065293.1212241338,2410805.8825,892351.95125,1207908.1025049998,8303358.1175,9880050.505,3442878.5094149993,13509080.39,7140791.645,1192244.501876,1217739.94125,10206906.135,2385523.76,3684406.1675,1177758.709375,20558494,1863337.27625,2711938.86375,12492452.98,2030943.739375,176060913.08,5047221.09688,679502.9225,1202884.325314,7135795.485,2527509.44063,1859510.7361666665,5163291.02375,959734.24125,5069633.629375,1345267.1140829986,3496445.00125,1297692.145,4758445.809844,2518897.631876,9912790.85,3169314.2775,933591.776875,2495595.9725,2138151.6825,5676651.34,936729.2103139999,2534428.765944,9934001.6,3060839.1675,1617282.012812,4833780.125,8866432.4425,1201021.6475,373440.02704999986,1544937.87125,2426837.90375,2067440.3871880001,1532914.80625,807471.199375,7273102.43,843177.468125,2514689.39375,2301581.7571880003,1088562.545,1767744.460625,836187.614689,2506693.08,21397645.51,2534275.2525,9532412.4525,14082480.83,2554532.72375,2759508.10625,1465548.574376,2003715.06,1007437.68875,597392.467031,944689.65944,1810665.6684380001,817640.4040630001,2687710.835,1154833.285,1698978.78,17108526.19,1645088.405625,3986439.208333334,803251.5968780001,741213.0921939999,925783.131875,3124283.6525,4141114.71625,933331.0912580001,3042253.520625,1018829.601875,1219037.7575,689617.0103190002,726261.205,11063368.43,725248.02501,1663035.816397999,4416490.76,898814.078125,989002.673125,60854048.8,1087673.6573958332,2108984.3214844,6724316.368751001,1098851.8525035998,1596555.514375,2728686.558125,1380752.256275,1323299.8789893999,2001098.64,675979.4690820001,2302100.385,1068001.92125,716750.814375,2399422.2075,1198076.51875,1178730.3722708335,1386686.253125,1726653.53125,6015188.005,1144623.0179043994,711572.235,3668353.83,752211.8328130001,14188139.07,2408646.67625,4895548.455,1589989.310625,3851722.629375,6756835.6975,1614058.0421769994,3212142.285,978531.6187519999,575065.477813,1448733.813125,2820237.5425,1678509.5015749997,2182751.7825,1065594.7725129994,1638466.4675,1251489.888125,3262482.98625,846558.1758333333,2783617.945,3029509.50625,13697591.74,3260599.6675,1828656.3192500002,2042876.65,931596.543125,1530515.3675,1831555.887192,1199484.54625,1744264.21125,3869946.57,1555594.555,7243778.895,678350.818906,605290.574688,741213.0921939999,623018.601564,5675773.1875,1182873.7067190001,787614.85375,6517385.795,3253437.18875,3482811.151875,2707136.24375,2308873.18125,3081008.50125,1356164.263875,689617.0103190002,3328375.194375,1834124.8034459995,1855532.7890630001,1336301.6444972,2158612.179696,25026458.965,9102877.1675,1650358.48375,769131.49375,1469686.5409380002,510594.3750080001,2045763.515,4058401.46125,6523767.26375,774475.6789164001,1861079.530625,673026.9409610003,22061383.1875,2596473.29375,49337246.68,1090391.0075,1897788.31,2172736.7125,743112.1225,613635.176875,1083134.988125,896518.7551707993,1320353.33875,2579129.18,2475938.41625,1959121.4324224002,4637041.02626,4504107.1875,2674970.5325,837500.5631289999,1397143.595,983124.660625,2458187.215,2267890.716255,796842.7261104999,6351754.45,1389044.230625,1258348.615625,2076423.05625,2909025.03,1764150.095625,2014441.625,3591570.4256260004,4313073.6425,680499.0406924001,1067661.0375,60440741.85,11966070.37,4418703.895,3035830.43,4417106.71375,3950577.8825,44484649.41,3132086.368125,1062392.3953219994,2355916.187343,7614667.4025,1620670.8051619998,29119239.3475,9616140.26,4548369.7225,2221533.4325,1035761.136462333,2186786.4825,48798856.2,253971.82150800026,6499671.38,1294059.6012899997,572893.92,6442847.34,915419.769691,1708973.768125,17952839.6775,1694365.86375,944265.84125,2785742.5064060003,2173913.39,6077780.76,1161751.569377,3962109.73875,1967006.965938,1077017.1251249998,1813627.09875,14529169.53,9428099.8825,2799718.7925,2069396.2525,7082616.605,689354.49,4011020.445625,8850000.991875,12339708.975,909076.574375,5126542.98125,5895152.43625,1143712.8409364999,20140163.65,1626031.955625,4391793.315,1130686.049067,1390307.074375,1562720.0875,2466608.8675,1387533.254534,1782121.3300199998,731572.6825,1080571.039375,839453.1381250002,3820435.15125,2542291.615,2130684.516563,1670811.651255,3699466.53125,604303.700236,5963354.8725,1310277.880938,693942.618125,4287302.855,1982669.01,2920129.3175,2265529.47625,1267201.225,588996.8575,2570673.571875,2465552.282656,834519.4240679997,12300757.695,685735.5095339998,4117274.621875,692436.609376,2353017.585,4955565.37,2695698.19875,1031736.35251,3698883.640625,1013522.4185947999,3560559.70625,6575784.985,1227769.145,810445.7843769998,2044611.8025,7497934.105,45004259.38,795341.61375,5773991.6875,753417.931875,3731472.57125,1499052.691666667,2592467.23,9865253.72,2209006.2125,2289131.775,2408455.294375,3776507.195,4843319.48875,3433217.383125,1719621.664375,4702881.7075,1641376.8275,651465.28875,2472569.854378,1352203.2375,4877817.5525,856411.7446885001,2489003.2542713336,2248929.556875,3418569.2325,15768404.44,1461721.6625,7207569.7225,2023035.1125,5525993.7325,863704.315625,3970279.48,5165857.96,1089172.350627,1457211.191875,1106421.6725,4921377.315,814484.7493799998,5749288.02,4840921.365,4312131.4725,1253830.5275019999,9930974.2725,1678274.495,4878591.3125,582814.6403140001,2209442.16875,1874428.5228429998,2305332.23625,1831257.555,2406673.76938,1010832.61,837172.959375,10976746.545,1386831.1693900002,1076159.4712559998,2161715.31625,1398761.4127083332,3334297.696875,2880196.0137549997,1495067.285,2739689.1325,659709.1806500005,1499902.23125,2502224.76875,12739525.6075,2163280.61625,1962412.31375,1260672.8540769992,647245.610313,885969.033125,960143.99,1292729.6618849998,1289566.406885,2812179.9468799997,51954293.11,3480420.509219,3227862.11,2801061.54625,1218541.80125,632782.70375,2736421.73875,2263435.3175,2049915.705625,926770.3152103332,3057621.095,21586724.82,2670116.6975,1399272.771885,1803750.13875,8827574.455,6067873.145,6339187.13,1697649.919376,602439.46875,791352.95875,1471172.101875,1964462.70875,5773249.61,5133821.3225,3753932.48875,1055546.858202,55378464.57],"xaxis":"x","y":[3195230,730765,1234149,1069095,629036,1328920,1677877,1700057.375,1116040,3466465,5722325.5,5674450,3098638,19459304,1418400,2378077,521341.9063,6135173,1066894,4660015,1713381.75,739711,6779778,760008,593042,2887392,2643948,2668769,1989387,10498344,37951708,1441900.5,804237,860505.625,2014500.375,2349950,1034827.313,965651.125,808968.1875,2074152,1832031.5,687953.6875,932598.6875,1207700,2390284,21346100,1750099.75,1355558,1435730,7166015,1385975.75,3155391,2075744.5,1073156.375,1394408.25,1247361,782483.125,11026945,37430780,1594504.875,2277316,3742915,1399062.75,33556756,1440662,1795462.75,991918,1250669,2545030.75,695828,3220827,1038815.188,6578370,1863468,745575,741850,10343027,659265,1107290.625,14172606,14585813,4926695,3343550,683307.375,2539565,974278,297715,4664399,1533245.75,12204922,515025,3043767,6118300.5,1764409.5,628609,4857581.5,3918038,858932,2313679,1984067.625,4971699,8611054,1344512,682632.3125,39061360,9560965,5696695,1813718,552007.5,3129811,675772.875,609218,1303247,2788860,1331469.75,8058913.5,1297872,1323038.75,1349970.5,993738,1827817.25,11813861,827211,520738,5049446.5,871355.5,3059758,2203026.5,3528086.75,3078672,416885.5938,3644982.25,1900006.25,615894,53166156,511180.6875,1970956,1290284,791052.875,65225380,872114.5,1241917,1019183,161363.4063,395517,4590246.5,2022990.625,14465798,3588718.75,1242666.25,8138616.5,1848656,572611,6293359,12783255,1109851,712057,4135280.25,3944933.5,520405,6807227,2564004,393493,1497886,19854012,2120807,2711776.5,638772,7080118,1164052.75,534110,1169932.75,928409,653175.8125,10608788,415975,230365,1365644,1022614.313,2882489.75,920068.875,1962435,770333,775728.8125,2116209,635698.6875,1403869,781217.875,1976920,27787286,10791309,2476833,238969.7031,312563,1239315.375,796471,551219,924068,15660628,5169726,1280690,0,838394.8125,2094525,1070085.375,12525174,1834290,345399,9191826,4141785.5,2065156,520765.8125,727374.6875,7946961,3647194,2136086,620483,5792926,3275779.25,1003551,14204281,3045139,1094343,284857.3125,523614.0938,10564729,682249.625,798322.375,816508,652425,21795830,1662054.125,664539,10420096,3778355,1049748.625,21169502,4524791,41092076,770082.125,1210798.375,400054,1810353,19344782,1392299.75,563237,1008665,4877959,78272488,1469870,677536.3125,1011667.5,2503804,3471845,6206083.5,6310254,1813864,3625434,1605182,611536,547084.375,2454812,1429196,2826828,5136604.5,3258819.5,979952,16701055,791542,1360340,90558.70313,11935719,1688612.5,2278160,659652.625,3537977,967835,918943.875,3822102.5,636266,642133,608359.6875,9722237,602245,746858.1875,18848780,4560492,7015692,4133811,2563985.5,898115.375,1967434.5,61576184,638507.125,6235947,4291789,6454983,5155090.5,703298,32381836,3176347,3093944,1896626.375,2006647,6781960,2912454.75,2601369,1417778,20311228,6525887,8374301,207572,826416.5,2823184,614768,1868497.125,11208498,1363191.625,5453438.5,855665.125,532670,3468606,905926.875,281191,2147968.25,6219841.5,1539066.375,4934334,982332.8125,605971.1875,4471029,6462938,650205.8125,860364.1875,2613607,3567977,4520838,15324777,2720238.25,1938613.125,25307744,1929118,7798464,747456.875,6859198,934980.875,2150120,662701.1875,6456026,4423958,2037693,1045413,1663324,542998,1193790,4573777,1247972,1208035.375,48479988,23262244,919180,987989,17457560,11924933,576174.125,16849234,4166306.25,4558824,847097.8125,1055624,11521140,1423463,415453,740230.375,603376,1314716,666930,2133372,818836.125,1993273,941366.5,644121.6875,25478086,2225265.5,5348309,907758.5,1007650,8136665.5,1026921,5457975,733249,652449,16917964,890977.8125,5231373,1523508,6960904.5,2966309,7456330,947518,1989287,2634166,2094724.625,947884,1870457.75,1840259,29949248,4103800.75,508057,2621041,2746152,5662234,8381834.5,1021798,25475474,2071670.25,987288.8125,924073,13093350,1960538,2082754.75,832168,1742351.25,408513,1976856,1605220.875,3046719,3366510,8688676,4399828,6344171,1924355.875,4826430,8747819,876237,3347567,2996958,878927.8125,3234871,1433267,11110405,6289567,580688,635697.3125,13567774,665216,3063969.75,14830411,536642.875,6069855,1125724,2923484,14856602,1848878.5,717197.8125,12187231,10328702,2666634,7564604,683274,1136111,847298.8125,5827595.5,719905.1875,1857889,1318096,1190733.5,10317852,842593,11047668,4193371,51168308,2172841,954352.6875,2547582,9290214,823460,1147582,5481995,1605669,2926089.5,1022957.688,6723971.5,2807583,1162226.375,4329966,1547556,5437110.5,2076289.625,821046,711118.375,1108310.875,1210229,1385697,6333706,1160989,2119806.25,691647,9162048,8773058,23612824,7243389,2992693,1811669,1375366,2254220,1431784,1072855,3404992,2046914,436848,11799105,501897.6875,226375,352136,6711975,14431245,906103,1917399,533742.8125,36367960,6482380,764008,1666221,2019254,241478.5938,720170.6875,2842760,132105,676032.625,510517,710275.375,756267,2474457,2023858.75,64547116,1228113.5,6361236.5,14332778,2330242,647906.1875,15036617,12719042,5903142,22394416,9546165,2924362,1375978.875,1022114.5,3637973,1137504.75,831855.875,15640994,2696136,957460,850667,11325513,2962476,1988334.5,26689658,1358519,744615,12704014,2712983.75,2153603.5,2080866.75,2791291,1000180,6182350.5,711594,14169301,3137206,564988.8125,6802622,673834,6335315.5,492481,603420,1208029.25,1067153.75,1618779.125,628023,1856230.625,2361088,793436,12087498,867456.875,1218114,8320719,6797528,829929,684739.6875,1757194,325513,1666762.125,739869.6875,50899248,529263,650008,1072487.875,6879291,2148565,1509292,3377209.75,3292049,666436,400770.9063,7762038,472532,4673827,811907.625,33268410,1834204,6010407,1403820,1603017.5,4750287,465533.3125,1073507,516216,585443,1038667,436252.5,3024478.5,2290681,1381590.125,1494252,1242311.5,685310.875,1000599,3986851,879235.8125,1372213.375,592739.3125,737935,2345314,800626.6875,2817261,2446842,1476969,646994,1241432,1327591,2043592.625,1111858,2443141.5,1216543,779204.875,1224685.125,1396191,2773647.5,730672,1054995,5948007,2569106.5,898845,39403320,15026352,42584028,938908,2270531,789209.8125,866825.375,1991234,768755.125,1900577,2531070.25,1912719,3091348.25,1272821,794724.625,4382240,43943,7405070,719734,5424718,936336,612158.1875,1585441,3076813.25,867184.375,13116256,4998015,721983.5,1097572,1647746,8575114,2179078,3303553,615955.625,57514116,1820814.75,1425599,2051501,14084606,1576953,498649.3125,722891,685764,4545148,44984468,885865.875,13424079,2212058.5,549834,4801472,578150,32675250,3632613.75,1546484.875,607373,918177,2569602,1401315.875,4467292,7319546,941688.875,2003948.375,8669367,1862752,1140620,48981052,794715.6875,9393108,4222682,965786,13315803,804620.5,928084,3473109,4188660.5,445943.8125,3988729,1310236.625,1191803,17463744,742565,670779,6399127.5,216115,1842562.75,1857329,3475804.25,562338.3125,4108523,1651120,714898,4748137,3528230,1700276,1639849.25,1650279.25,49102164,7802205.5,6606832.5,4626443,3158151,964923.8125,2531163,4249212,440419.1875,1000552,405823,538064.875,14329739,3534690.75,659368.625,2114860.25,1811506.375,632517,7370743,1795049,594848.625,12726789,2442220,757167.3125,1076693,1896858,2603931.25,2127077.25,944638.1875,3157579,4486477,3180819.25,557863.6875,930439.375,1826203,19018762,466904,3055176,2347893,9956396,1436773.875,29747868,6657348.5,1056099,1987934.625,1119679,151798,735907.8125,2664502.5,12729858,157606480,1620721,614985,917194.625,19061780,646691,5889882,2299216,7768057.5,9097980,1406687.5,5481549,1027394.313,875789,4337107.5,689082.3125,1634334,425574,1440554.25,836437.6875,729535,592805,634751,2057288,13491170,1509775,961749,37785552,4628171,984054,1038045.313,859137.125,1142949,2411402.25,5833800,7338511,3611538,966812.125,616749,730292,1120197,10053489,1044220.625,492807.6875,9308471,2240549,3241917,1433573.25,13146645,1208407.625,2131817,1424157.25,11353672,2755578,163945984,861643.6875,718362,784652,2757588,2759449,1765813.625,2162760,2227908.75,6458021.5,2087998,22325364,6481807.5,2503240,9085924,785144.8125,2458760.5,61984596,900973,1859571.875,945063,715728,509171.4063,2504814.5,1878442.25,2434208,572139,520011.8125,24017770,1814947,2231695,1725773,53401,1383950,728876,1046995.188,6843855,9155568,2885740.5,7220201,4424263,1694930.375,550324.1875,11356990,1742870,1094622,966677.125,24224888,1602030,3888945,13609040,5981430,16644664,1252323.875,479358,2630101,7004126,1911591,1357553,12184124,896172,2830674.75,1143826.625,4767227,2108012,3548858.25,1986624,19036416,5448687,1602143,2505271,1817513,40613740,1361427.75,2345752.5,4020660,4391348,5976246,3075425,13883385,1984823,677642.125,1456808,1189432,1271021,1696009,609494,41399504,767720,950695.625,1993766.375,702385,2188093,735810,2504797,17186624,1885314,8728584,11951945,2495819,1000796.875,1539176.625,1730542.25,1223296,1392153,847514.5,991513.875,1122837,2098850,637053.1875,819314,25672598,653672,1358652,845961.375,5807879,911366,846274,7879628,1433504.375,2279063,755269,1138520,133880,524479,10110827,624269.625,1890502.5,3902177,563298,919048.6875,85357952,503744.6875,2485521,10642754,1686405,2189124,1427294.375,1353079.5,862945.125,1956439.375,866806,1972491,627698,1553208.375,1929828,1151027,1222509,1404702.25,1725558,8354235,911088.8125,708928,3441643.25,723482.1875,23364636,1566177,7657568,1262255.75,3444270,5328051.5,313528.0938,2567976.75,634056,488181,915286.1875,2559461.75,3494929.5,4116871.5,695803,989257,1152896.25,719471.1875,639407.375,1755264,3619497,13349651,1393073,4258124,1897180,669117,1769137,541943.3125,946138,2214604.25,5229981,966764,6211441,639566.875,602618,523133,749207,4321141.5,2222097.5,648022,5177270.5,4785692,1933811.625,2184912,2845068,1399992,1285055,315468,6782435,2173143.75,2070005.625,1692199.25,2409429,7794869,2999116,1614673.625,659450,981009.125,491792,1454222,1953919.875,2101376,904236,0,1417728.25,20828130,1827424.125,32552428,514471,1703060,775729.1875,1272272,622051,783208.625,1625308.625,962238,1465535,2834401,2050600.75,2309007.25,7758182,3837228,522972.1875,1982967,769148,1698779,1928564.75,1257033.5,7654264,1285646.75,3643112,1224045.25,2632472,1678495.25,2076612,1964681.25,7231928,728167,957632,13631685,6673138,5571501.5,1261725,4618360,2756494,136241424,1529309,1353612.75,2786648.75,13586070,1100499.75,52960796,6669954,2274873.5,1972156,817695.6875,1911611,58041572,117438.3984,6127771.5,656980,418098,4634655,419978.6875,1057256,41680640,694189,674961,3371659.75,6983736,5288304,1912760.75,14981114,1764767.25,806615,4116805,9569715,7475577.5,3721129,2014721,5101504.5,719060,2919980.75,971155.625,16373232,791564,4933374.5,10834087,605186.3125,27185096,1045838.5,4656777,834711.875,1008536.813,1688745,4262316.5,928584.875,1167734.75,298063,552401.1875,450518,3796986.5,3371455,1240924.625,1537570.25,227566.7031,164817.9063,5543575,161311.0938,606886,1083729.625,1368120.5,2646523.5,2513155,1167967.125,638201,5274642,1386445.375,730211.1875,8692328,1257144.75,4212149,593296.1875,1767480,14265421,2447861,948157,1739286.75,1121537.75,12029600,6797647,151376,1761137.5,6674179.5,6505995,30135280,696522,7837692.5,767361,1989763.875,1788999,1822830,5232420.5,1554342,1523324,1487660,3556291,2249620.5,6592304,335245,3714139,1594422.5,336461,1956075,1838121,3986982.75,817409.625,2946756.25,1521693.25,1880890,12265516,1965529,4103027.5,1686248,5276777,1988760.875,3515821,2218249,947534,2892779,756908.8125,10721568,357915,4571388,3022275,5051636,2429214,5446624,886771,5326222,584545.625,2158629,726124.625,930611.875,1889990,1423929.125,866126.375,783651.5,8201974,2346692.5,145468.7969,2399762,938096.1875,1145535.75,2468322.5,831577,4829951,431815.6875,2361960,1210262,8787353,2882757,1228837.25,2735811.5,576052,442375.8125,866242,1339220.625,880115.5,1656025.625,65047284,2356376.75,456747,673030,863703,686091,3456131.5,2503690,345866.6875,941564.8125,1012341,15091579,1733511,999856,2154119,6268627,5454482,2407904.5,1815350,799996,593824,585559,5063753,5572503,8228399,6014078,994194.875,27717034],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle RandomForestRegressor() vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predRF"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["n_estimatorsRF = np.logspace(0, 3, 10, dtype=int)\n","param_gridRF = {\n","    'randomforestregressor__n_estimators': n_estimatorsRF,\n","    'randomforestregressor__max_features': ['auto', 'sqrt', 'log2'],\n","}\n","\n","GridRF, \\\n","BestParametresRF, \\\n","ScoresRF, \\\n","SiteEnergyUse_predRF, \\\n","figRF = reg_modelGrid(model=RandomForestRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBM_train,\n","                         X_test=BEBM_test,\n","                         y_train=SiteEnergyUse_train.ravel(),\n","                         y_test=SiteEnergyUse_test,\n","                         y_test_name='SiteEnergyUse_test',\n","                         y_pred_name='SiteEnergyUse_predRF',\n","                         score=score,\n","                         param_grid=param_gridRF)\n","\n","print(BestParametresRF)\n","ScoresRF\n","figRF.show()\n"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[17032132.276611768,14959158.888371017,12478137.434446137,11971565.213305943,11210402.848789908,11513365.93022883,11184751.835069502,11420165.478678785,11305280.71161221,11378056.793743279]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[24063433.920493256,20732510.357820485,19871270.00693537,18741060.981595837,18521027.95447279,19010131.2481045,18612902.598363012,18996579.54480042,18871287.650450982,18973317.112721667]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[4925966.778470863,5946188.328787225,4083785.3981604846,4564124.00338984,5153263.44628152,4753043.444291532,4586905.926278556,4184088.1169130113,3930898.2429672247,3738359.3678153316]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[20101224.53384877,8536956.529304339,8865865.563292665,7948633.24646804,6794624.6048721,7351819.124361929,6528384.56554991,6599771.462290557,6574282.873661156,6693212.59728866],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[7014339.842755853,11655456.150249911,5537545.397529615,6166383.557667703,5717389.701861397,6206618.365460396,5766858.0808474915,6007873.825883353,5781420.581126326,5876062.271534977],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[27532134.174300384,25245901.726052843,26686956.63295372,25080562.992374796,25586541.623561356,26226324.498065352,25730222.496724658,26325320.38674944,26151251.532511048,26284812.69063032],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[18486389.28600904,16743256.103326721,11972690.076074034,11335694.168731486,9648273.590055179,10519377.857524594,10230789.584897423,9921046.105400044,10101723.708895994,10216014.571442697],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[12026573.5461448,12614223.932921274,9327629.502380647,9326552.101287685,8305184.723599499,7262689.805731868,7667504.447328025,8246815.61307054,7917724.861866522,7820181.837819735],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle RF pour le paramètre max_features=log2 en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"n_estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE ElasticNet pour tout le meilleur paramètre max features\n","for i in BestParametresRF['RandomForestRegressor()'][\n","        BestParametresRF['paramètre'] ==\n","        'randomforestregressor__max_features']:\n","    fig1 = go.Figure([\n","        go.Scatter(\n","            name='RMSE moyenne',\n","            x=n_estimatorsRF,\n","            y=GridRF.ScoresMean.where(\n","                GridRF.randomforestregressor__max_features == i).dropna(),\n","            mode='lines',\n","            marker=dict(color='red', size=2),\n","            showlegend=True),\n","        go.Scatter(\n","            name='SDup RMSE',\n","            x=n_estimatorsRF,\n","            y=GridRF.ScoresMean.where(\n","                GridRF.randomforestregressor__max_features == i).dropna() +\n","            GridRF.ScoresSD.where(\n","                GridRF.randomforestregressor__max_features == i).dropna(),\n","            mode='lines',\n","            marker=dict(color=\"#444\"),\n","            line=dict(width=1),\n","            showlegend=False),\n","        go.Scatter(\n","            name='SDdown RMSE',\n","            x=n_estimatorsRF,\n","            y=GridEN.ScoresMean.where(\n","                GridRF.randomforestregressor__max_features == i).dropna() -\n","            GridRF.ScoresSD.where(\n","                GridRF.randomforestregressor__max_features == i).dropna(),\n","            mode='lines',\n","            marker=dict(color=\"#444\"),\n","            line=dict(width=1),\n","            fillcolor='rgba(68, 68, 68, .3)',\n","            fill='tonexty',\n","            showlegend=False)\n","    ])\n","\n","    fig2 = px.line(\n","        GridRF.where(GridRF.randomforestregressor__max_features == i).dropna(),\n","        x=n_estimatorsRF,\n","        y=[\n","            'ScoresSplit0', 'ScoresSplit1', 'ScoresSplit2', 'ScoresSplit3',\n","            'ScoresSplit4'\n","        ])\n","\n","    fig3 = go.Figure(data=fig1.data + fig2.data)\n","    fig3.update_xaxes(type='log', title='n_estimators')\n","    fig3.update_yaxes(title='RMSE')\n","    fig3.update_layout(\n","        title=\n","        \"RMSE du modèle RF pour le paramètre max_features={} en fonction de l'hyperparamètre alpha\"\n","        .format(i))\n","    fig3.show()\n","    if write_data is True:\n","        fig3.write_image('./Figures/graphRMSERF{}.pdf'.format(i))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
