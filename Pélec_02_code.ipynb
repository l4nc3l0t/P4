{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import pandas as pd\n","\n","pd.options.plotting.backend = 'plotly'\n","import numpy as np\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from sklearn import metrics\n","from sklearn.preprocessing import RobustScaler, StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n","from sklearn.pipeline import make_pipeline\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","\n","from Pélec_04_fonctions import *\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 17] File exists: './Figures/'\n","[Errno 17] File exists: './Tableaux/'\n"]}],"source":["write_data = True\n","\n","if write_data is True:\n","    try:\n","        os.mkdir(\"./Figures/\")\n","    except OSError as error:\n","        print(error)\n","    try:\n","        os.mkdir(\"./Tableaux/\")\n","    except OSError as error:\n","        print(error)\n","else:\n","    print(\"\"\"Visualisation uniquement dans le notebook\n","    pas de création de figures ni de tableaux\"\"\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["BEB = pd.read_csv('BEB.csv')\n","\n","BEBM = BEB.drop(columns=['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'])\n","SiteEnergyUse = np.array(BEB['SiteEnergyUse(kBtu)']).reshape(-1, 1)\n","TotalGHGEmissions = np.array(BEB.TotalGHGEmissions).reshape(-1, 1)\n","\n","BEBM_train, BEBM_test, SiteEnergyUse_train, SiteEnergyUse_test = train_test_split(\n","    BEBM, SiteEnergyUse, test_size=.2)\n","\n","score = 'neg_root_mean_squared_error'\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Scaler moins sensible aux outlier d'après la doc\n","scaler = RobustScaler(quantile_range=(10, 90))\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# ACP sur toutes les colonnes\n","numPCA = BEBM.select_dtypes('number').drop(columns='DataYear').dropna().values\n","RobPCA = make_pipeline(StandardScaler(), PCA())\n","components = RobPCA.fit_transform(numPCA)\n","pca = RobPCA.named_steps['pca']\n","loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"Composantes=%{x}<br>Variance expliquée cumulée=%{y}<extra></extra>","legendgroup":"","line":{"color":"#636efa"},"marker":{"symbol":"circle"},"mode":"lines","name":"","orientation":"v","showlegend":false,"stackgroup":"1","type":"scatter","x":[1,2,3,4,5,6],"xaxis":"x","y":[0.639683279146419,0.8551499910170413,0.9483422002934013,0.9944143074603773,0.9999999999999998,0.9999999999999998],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"margin":{"t":60},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Scree plot"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"Composantes"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"Variance expliquée cumulée"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# visualisation de la variance expliquée de chaque composante (cumulée)\n","exp_var_cum = np.cumsum(pca.explained_variance_ratio_)\n","fig = px.area(x=range(1, exp_var_cum.shape[0] + 1),\n","              y=exp_var_cum,\n","              labels={\n","                  'x': 'Composantes',\n","                  'y': 'Variance expliquée cumulée'\n","              })\n","fig.update_layout(title='Scree plot')\n","fig.show()\n","if write_data is True:\n","    fig.write_image('./Figures/ScreePlot.pdf', height=300)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# création des graphiques\n","for a1, a2 in [[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]:\n","    fig = visuPCA(\n","        BEBM.select_dtypes('number').drop(columns='DataYear').dropna(),\n","        pca,\n","        components,\n","        loadings, [(a1, a2)],\n","        color=None)\n","    fig.show('browser')\n","    if write_data is True:\n","        fig.write_image('./Figures/PCAF{}F{}.pdf'.format(a1 + 1, a2 + 1),\n","                        width=1100,\n","                        height=1100)\n"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Modèle de prédiction sur la consommation énergétique (SiteEnergyUse)\n","## 1.1 Consommation énergétique brute"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["r2 : 0.2985810392343702\n","rmse : 15549884.692595655\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predLR=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[6052352,4034944,-2539392,1082176,5977600,2591104,8059904,3110720,3851904,935680,3348928,3048896,1225280,2985728,1564864,1619712,901376,2361920,696320,1290240,1420032,122176,6060096,2003904,1364160,462272,4047232,1359104,1059328,7272704,2739328,2776640,1635904,4479232,36734080,416512,-68096,2028224,5812224,3435264,1382848,1148288,4660800,7954048,6567360,10781696,1827136,12493504,11208640,1229760,5575936,2264448,1038976,19582144,3163008,2605824,1814592,3105344,3407040,1337216,397056,2151168,5582400,1280320,2716096,4933888,6715072,2302912,222400,324288,3884288,4615104,273600,9631360,2232320,7219584,203584,609344,4123840,27888512,1431936,1293120,1294528,479744,4837376,4776448,105768320,1689152,-43200,892096,1024896,2288704,426816,1746944,672768,1448256,1058752,12300544,4849664,2560832,485056,16868800,5112704,4130816,16690112,8630656,675008,445376,1519040,4841728,2253248,3276800,1882944,332608,699968,1510208,7102144,541248,33553152,2376384,410304,3936768,2032000,-292096,12886336,563200,4298112,2338368,-227904,3932736,13986112,1001408,8089536,770816,532352,1993984,236352,605760,971328,2997184,11142848,19891712,9216384,11119552,1387776,1011968,2037120,1348224,3394624,11013440,5214400,2608704,2107776,2699008,1322432,1010880,1758848,19529216,8246592,1608512,54358528,10920640,2358720,1725120,2059136,1301568,2509120,998976,2177024,1078976,4696576,1156032,1271808,4389696,875904,3462144,10175232,1436992,-59392,752960,660224,834304,1113792,5188800,2079872,-119616,1537600,158635776,1497088,736640,2674944,957376,2937792,947776,5367552,-455040,4013888,3883776,15682112,1211712,5716736,1792384,10322688,1881344,1307776,602176,1818432,11367168,1622656,1430272,304512,1076160,2723456,494272,2275200,7275712,1407872,365568,1347840,5315776,271744,752896,1674048,1196352,2186624,4276352,9641856,14436608,2590208,8338816,11541568,1862016,968256,715712,2022528,1600128,3841792,16116736,31829184,19736128,4107776,8900416,1015360,2119744,434048,1450880,2261696,8684800,8470272,659840,1975488,20468096,931456,2631616,1610816,3741632,4780288,1726144,7235008,2455552,3105664,9325248,1161216,659136,556160,8927296,14312000,2236992,29332160,790976,703552,480256,815424,2210816,1690240,595712,7911872,4168256,20407040,7802944,2365056,1320000,3993792,6218368,258496,1724928,1327936,534400,22151040,2408000,22346432,-200960,958016,5461440,1427264,3556288,761664,2850368,1849216,2036864,4934400,543936,3143104,2251008,-1347840,2369856,1561600,2991424,961472,2325760,4063040,2491840,13078720,4001600,2368256,42240,7714048,1631936,1176704,1436544,1552960,3149696,9305280,2842048,9707072,1052224,2273792,-146432,2734272,3162432,14864448,1780736,2604928,-318528,2779904,3329152,736512,615808,1114368,2720576,5556608,1473536,1081728,3900672,21806784,1906816,11118016,11876160,14837440,14115648,4947264,7574272,5619328,610944,23569216,1222016,826496,10521856,620544,1054144,1696768,51200,5855232,5956800,1042048,1023680,8020608,791104,4482880,6436928,5299456,15896064,26391360,1065536,361664,730816,6630592,7966848,1801600,10715840,5811008,9139904,544064,3000896,323200,850048,1119616,3196672,1801600,2016768,5240000,596352,2124160,907456,1768512,2901184,10742720,18208384,1517376,1934016,1177920,962752,313024,2383360,4620032,2309248,8592128,6301056,2137152,1282752,2425536,11263104,1197120,1320192,1848960,1096576,1221504,2989120,2004096,10712832,1149056,5427776,2448448,4095424,5660160,1930112,987520,27609152,1785344,1361984,2756928,4422208,409984,6739776,2402688,774400,428032,1849472,1246656,192192,11697728,-98624,1484480,1587712,1051776,7330304,761728,59520128,2989376,2724352,225408,1039872,3614848,1244288,1949312,2244928,2338304,1045888,3078272,2216704,1574848,832384,63531840,1992512,955008,15828416,19996288,1162176,992256,9463744,30147200,942464,380032,1060864,971904,1144704,2053504,956288,882752,2084672,3396032,19275776,43664064,27699776,6276928,9556160,670656,10959488,1554752,8566400,2282368,1881408,6213184,951360,2618816,1090112,526208,10795328,66487936,9545472,13055424,1089792,3093952,9743680,22115712,1478848,9384960,21247424,1359040,69398720,1340096,2391296,47284736,5041472,1147072,538432,1705216,333824,4578496,968576,1427712,1435328,1495616,-187456,319616,4074048,1968064,846528,2603328,4221760,15066048,872960,1663680,2210880,7079680,753600,332288,1356992,2149312,1400768,1072768,5461760,6229312,1396480,-764480,4644480,5611392,4127488,1591616,529280,4932736,5970752,2282880,2588032,13652544,2079488,1203776,9520512,732032,831872,1170176,1390272,432000,3172864,28224,3277888,1216640,2123776,4221824,2605312,3420992,6534656,8248320,4539200,714048,1213760,886464,1905984,16960,-425280,511104,1024512,6908352,2966336,891968,1979776,1020480,2153600,1456256,180736,2849728,1759936,12328192,1424832,5156736,1229632,802944,6008000,2454912,3095616,772736,78424320,2330240,6368960,4442368,4954880,3323072,3038272,775552,1043136,5511360,2230016,981248,1738176,6057344,4616512,3674240,934080,1532608,1184704,2474624,7124992,536896,1566080,4573440,5974912,52011008,5931392,597632,388160,3366784,981952,1595840,529728,3210880,6581632,2312256,1780608,1407232,388288,7921344,3918848,1491392,1502592,6898048,2804800,10700032,398592,5373632,1900160,488384,2246272,6688704,1788928,788224,164288,3115712,35962496,8861120,25109120,516864,2013568,1637760,1623296,6790144,512768,1219072,8213056,738176,3174976,754176,3391424,495744,1436224,971264,20096,-225088,1506624,5854720,7384384,9862080,2790656,3029632,1080064,2943616,3143424,945536,179392,692416,2132800,142144,1445184,234368,9362816,361408,45652736,1685696,1584704,8296192,1109568,1288128,947392,-221184,3147968,2894080,3370304,6953984,9183872,815936,2208320,822592,1496960,1461632,1290624,1410816,1603456,16844480,16899904,1451648,21649728,54529792,8726720,8114752,926080,1892288,1507072,2672256,871232,6656832,51008,3910848,5692672,4670208,25434752,79488,6412672,4278976,1077376,26112,3147072,1434304,-431680,9960384,6166272,2584896,6510976,2915904,15112896,1588416,1747904,1484608,1039744,738048,129024,400704,855232,1357440,5608448,4764416,2003136,5612672,2021696,2061120,4255680,-1895744,99712,9918720,5410560,7872704,1503680,1290752,1233344,4166848,132078400,5303680,1219840,386112,1542016,17490112,743296,7683456,39506432,41658624,2380544,4072640,204800,1124416,964544,-12864,1351744,89344,159744,2796416,2718464,1142272,2155584,1159360,657408,2065536,5049088,810240,847104,4249216,389312,12633664,1269952,829120,13015168,786752,892480,64463104,1404224,370432,258816,-49024,1824320,5492160,26899264,2548288,981312,1767808,4948736,12180864,2125248,1608576,4248896,1323392,8207744,812416,128192,196800,1671296,5404352,-901888,1733888,66489920,5037440,8100480,3517952,15691136,3299776,17686400,1912320,11050496,4423744,987520,224000,2499200,2113536,65088,1303488,5965696,8257472,595584,597504,1155584,1449216,18260672,10401216,2008640,421568,3614976,585728,4836992,916864,2654528,2569088,626176,2533888,561728,862592,10420992,7618432,3470848,884928,2810176,-146432,14768704,1355520,3968832,1653184,1627584,7327488,1158336,13919744,11991680,2025792,1004096,3155584,215872,12326016,1027328,1379200,18193088,14564096,2736512,1831104,1070656,2218048,20023168,1481216,1137024,2998784,54720,3390784,3454848,1167872,1880192,1060480,334976,13823296,3656512,2078144,2422464,1056704,1505088,55238016,4183168,3644288,710016,5825152,802688,4568384,1800064,4020992,-77440,2269568,570560,1703808,1045056,2466688,1148352,15845504,1621696,1886976,5638848,17983104,6487424,1201088,4503488,4579904,3839232,1780672,2160576,1772352,11888576,448064,6089920,3306176,4848448,11770560,948736,7731200,1530112,2087872,1333376,16567936,18167168,5279424,1209984,4897024,9355904,414592,1399680,3689664,3449152,1537856,1426432,51945472,425216,842304,16094528,5460992,-267008,1664640,2788224,21382464,1737920,2064000,3241024,744576,1328000,1416768,780416,2318400,1310464,508096,15573632,9814208,1494400,4943872,26809536,438528,801856,3021760,726016,27328832,2038592,5018752,3591872,118784,4327808,9254720,15754176,894144,632192,635008,5200064,4049792,6620864,446784,1193664,2906944,14744832,789312,5217472,32965248,2621888,637184,9310976,913664,904128,1657408,3729984,23513792,1923968,969536,3076672,623488,364032,5208256,1923648,798080,3579136,8175296,2832192,2571840,5278272,2713152,4188992,2061888,1938880,2042496,545920,5160960,4940608,14540864,7420416,4945856,1692672,5756992,34816,615808,1543552,2376576,8740416,836352,2253056,502144,1539392,614784,1597888,-130432,1977600,1109376,3067648,1981440,1714496,288064,344256,1452992,3058304,5968768,7068864,2610368,-134592,2779072,1469632,747264,1708672,1575360,3666816,733440,1242560,10137728,2313856,737408,3298624,672320,2359680,3682752,5301440,5104768,913152,2215040,1784448,7828096,4705344,1512512,4277376,681664,2710976,7137152,1458304,8547520,825920,2501120,1241024,5495936,2034112,1372992,13351808,1485952,4191936,1630720,2539520,379968,1297152,8768064,5590144,2162624,5504768,10001344,708672,6202752,642432,963520,664768,1329408,14611456,4713408,1136960,647168,-180416,1081728,1017344,4638272,522304,508160,1307264,934272,1115520,2536768,2115456,17015232,2101888,1664256,1487296,1044032,15211200,1384832,5867008,3977408,3460480,9621184,17327104,2161408,9608576,642496,844288,4367616,804544,183552,2992256,25769536,4246080,1593600,1013696,4674112,1255936,1867520,1317120,5800320,1487488,11062912,2941696,2217984,795072,8699264,3878208,2720000,5036864,2200832,1056512,691840,10477696,1169280,1338112,185152,955584,299008,1756544,8373696,3650496,721088,1082176,8259648,3802880,843200,1948864,855552,1406720,1898112,3999232,1076288,19571520,10538048,4444992,819904,537856,3416832,5233536,611136,1012224,6323520,1952896,493888,5748800,8237888,157824,2901376,954880,3841536,2103040,1116096,968384,10936448,4855808,549760,1132736,2377344,-155456,7173888,645568,2615872,1795200,5731776,438656,4183360,4956288,450112,1131072,1558400,850432,7041088,3941504,138112,5678144,827776,2058752,4438720,2434432,3327296,7674560,1262912,4876224,3832320,2366720,20837632,11509888,2219136,1273152,9636736,4132544,61643456,576128,687808,9861760,949696,416896,1723520,2715840,781696,2541248,40230720,1197376,3896064,1226688,2393984,1030656,1226496,2248896,6004608,3900736,2288384,1418880,870848,3732544,2807168,14407360,2043328,8164992,971200,2377600,110144,3152640,1922624,3709312,8710528,853312,16192832,1642816,5422400,1016384,6857984,8951232,1536576,2796224,3745152],"xaxis":"x","y":[7506587,1972078.625,7768294,706505.6875,2274873.5,2547582,4571388,288191,6252842,1126157.25,1153276,4103800.75,1963054,3292049,724259,819803.6875,1589550,967432.8125,984178,968026.3125,1688745,678663.3125,19236770,1359948,342493,497620,2682044,4420650.5,866343,1250669,1770027.5,14084606,1981115.25,1986094.875,32141070,5415438,932083.5,293217,3543985.5,2443065.25,992794.6875,885865.875,552729,3580091,2993610,284867168,1222509,5672838.5,0,965786,1429146.375,1648774.375,726124.625,30648154,1836671,876425,2192111.5,1426823,1748132,1791071.375,646377,1487660,1882845.75,893053.1875,4247457,1796889,6426022,2493240.25,650008,385719,1851260.5,75073888,804002,25453690,1997488,2712983.75,913899,9546165,5122039,25307744,1313366,761504.875,1302636,234144,5968153.5,2966309,58761304,1160989,2217620,142873,2051501,1069490.125,348061,545426.375,1113457.75,2665558.25,628023,4759256,0,2406996.75,501194,6508595,14829099,2491238.25,4135650.5,1023418,454296,1682454.875,813038,11651200,6797528,1145535.75,1348497.125,493000,766628,8204897,3960898.75,609101,17463744,3348280.5,863728.8125,1692323,2112095,1878742.875,6711975,675611.375,2196718,1199742,748775,1545356,19645206,122156,8393893,866825.375,1357116,2496807.5,615894,845044.125,526752.8125,2991674.75,3093944,25471308,7221482.5,3894584.5,2502081,630578,1569569,1435730,2512443,8971926,19248732,1678495.25,586589.1875,1832808.75,1132489.625,811907.625,702385,20361158,5647146,2323265,41655852,0,962971,1200811,1107290.625,1761137.5,14029480,248607,1037555.125,659450,3528086.75,820705,2433777,4549892,1358652,2558573,4287439.5,1239315.375,786979,730765,2781314,567072,967082,3228330,1243282,7606084.5,2103293,114648520,1977299,798970,4700552,1813864,1482554.375,759060.8125,4946798.5,678351,3289053,3326999,6935723,1386331,2740128,888031.875,1809101.5,581289,975431,4475650,1441445.625,8287440.5,659652.625,1538750.625,1306087.75,1058986,1060846,131810,3040978,1593187,791564,1085183,818317,3022275,520738,1224685.125,2114860.25,1088016,4520838,3626431.5,4448582.5,9212950,2392466,6689184,13586070,2074152,567608.625,594848.625,3009463.75,2446842,2161828,7794869,15206762,10084950,3906967,25631512,1022114.5,1182575,1218112,1249939.25,1788148,6102883,3093378,1493131.75,832168,7791957,839362,338923,2092251,1929828,1061458.25,451738,5820965,2284507.5,3740373.75,5417598,1632498.625,992139,1232822.875,3590731,9851937,2069880,27139746,514555,948676.5,665200.1875,468973.5938,13140574,227509.5,4185406,11509942,3185509,20630168,2928099,1391916,111969.7031,4842910,2278160,2094410,1138520,2470566,851651.1875,19397324,1244824,35383012,777587.125,898115.375,3471845,2190460,17686444,696605,2396523,651293.3125,1187261,5661689.5,517029,4673827,1255834,1970956,644415,1290284,2656729,2041912,4700396,29195468,1403869,42709624,1986384,1240875,509485.6875,6856153.5,369291.5,1464856,5420552,1140620,1424157.25,3795938,498745,4028943,873648,1234573.875,936336,2503690,2756214.75,6884914,766381,1384558.625,1260416,1214929,5448975,832986.875,659761.875,209644.9063,1269847.75,2526375,3467070,600318,2338852,11765682,1232210,13901935,7539963.5,4425832.5,24906132,3579008.75,11353672,7475577.5,608491,11289052,698367.6875,1525624,28731502,1656420.25,557082.1875,934357,1020039,5288304,9049598,899266.125,588969.8125,16016644,387810,6056481,3437441,1441665.375,8800292,8386493,497991,738990,2159932,2436403,5747645,1150582,1912690,8787353,3024478.5,863773,2274920.5,1187315,3158151,847298.8125,7613116,456397,1026921,5974993.5,666385,2559461.75,1170616,1967129,3928695.25,5464762,20781390,629027.3125,6129780,964236,929950.125,1815371.25,1777652,1242811.75,562479,8964061,7577723,1537570.25,403800,855946,10416934,1736775,684739.6875,786324,827042,892086,1735816,2404644,3568616,1046995.188,276107,243518.2031,2212058.5,6858533.5,318364.8125,639407.375,10126597,2227908.75,849504,8381744,4397572,1676426.5,3761647,1228837.25,646505.1875,717766,1195351.25,934621.1875,1152896.25,4103027.5,3392129,841891,1692199.25,980356,4374646,2332996.5,35891484,2845946,2674034,627854.125,2181833.5,5424718,298063,3560614.25,978463,2130673.75,1466382.25,3706100.25,33268410,5040382,508051,55813996,1075019.375,2100722,6227419,11356990,653139,204499.0938,5722325.5,10564729,548293,619545.125,1162120.375,901015.1875,1074937,1353337,807185.5,576405.125,1662723,4329966,7133222,27282662,8435093,8058913.5,3567977,609218,6915293,509124,3869754,363808.1875,2204235,4307238.5,1123416,2799595,1997115,1943772,3467698,448385312,2417401.25,5249190,664088,415975,16917964,13609040,3537841.5,13458948,9157150,2204161.25,47952868,847097.8125,4571746.5,65047284,3833162,1211947.375,1458617.125,3014332,705948.3125,2022990.625,1567040.75,352136,2168281,1160087.875,588965,628609,3481727.5,1677002.25,619804,13943008,1053290.75,1990444,565458,3494929.5,3155391,4089677.5,1998725.25,712057,935396.6875,8201974,782202,848497,4368113,1165598.875,2420605.5,592739.3125,2184912,3600897.25,4486477,980603.375,958798.3125,1886382,2503948,4751090,1651120,6335315.5,956481,397624,3063969.75,1841154.25,1295685.75,2586116,727703.5,1636649.125,1855556,719471.1875,2267094,1787119,1331122.125,971308,902694,5024837,17186624,55073120,1819329,512166,772379.3125,1049436.25,887403,416566.1875,673834,609257,759405.6875,3155496,1070085.375,2006626,2804534,747384.5,1057256,1054581.375,507428.9063,2664566.5,2065156,9697866,847514.5,6522551.5,720005,573624,12783255,0,2136086,15480681,253832464,2339234,4212149,3021669,3181045.5,2601369,1656025.625,646994,1565475,7802205.5,1187723,1356874.75,2298648,1136698,1586388.75,1018111,9085108,1734533,1703504.875,1868497.125,11047668,433443,5459657,1841968,9444484,38400928,151754.2969,536642.875,3717064,2159170.25,1625308.625,682632.3125,785144.8125,2553763.5,4750287,9617898,1025205,874707,618876.1875,3162976,1520326,2114053,1109924,7546235,3542542,10444602,462465.5,4023479,1761048,869835,1189942.75,2211671,1735538.25,789630.125,1962351.25,827937,28450436,6074249,9861998,967697,2107743.75,696054,1152890,3791402,965651.125,2090895.75,12480984,552015.625,2904225,466985,1268459,862744,313528.0938,1769998,1547116,2023858.75,863703,4656777,11061916,5233553.5,1168017.75,2300803,727775.8125,1443367.75,4608963,2342494.25,706879.5,574575.375,855670,666740.3125,1981360,1193790,8773058,520348.9063,17666646,1607590,1227772,14332778,410433.0938,926261.1875,2796425,1234813.25,2701833.75,4120963,4742869,37276920,18622292,2157144,1776411.75,666967,1080203,3031811,433871,908834,1076644.75,12448381,15843743,709979.6875,13239050,143423024,12196872,10992344,1291170,835545,1050982,1513700.625,1829122.375,2549649.75,1844825,3989399,1857329,1495686,32381836,1109851,6897247.5,1863784,675772.875,1018068,2378077,972608,1181428.75,5653656,7771933.5,1827424.125,6419090,1218114,16701055,771512.6875,162960,1213444.125,2777052,1185469,490599,541542.6875,1103762,811537,2932696,1848656,775007.6875,2172841,1788999,837792,784762,2737534.5,1695067,3060149,4178233.5,2924362,1271753.25,683274,712149,4020660,59757440,3298570,760090,505601,858897,15660628,323739.4063,5702077,9387211,37022180,1712279.875,1742351.25,1001964,1612797,638507.125,530700.125,2327611,1261634.5,1072487.875,1162561.125,6454983,1063960.125,151376,2534873.5,645756,1200381,2855545,2219744,688145,1362993,456197.5938,9523142,502898.6875,611997,13171370,1009476,739210.5,58041572,1459199.75,607579.6875,839011.3125,862826,1128179.125,2306723.5,8437401,2147062,646691,1619177.125,2236167,4058492,966764,1795462.75,2165371,4322873,2167888.25,1251906,669217.6875,471113.3125,664539,1983509,792015,794724.625,53079164,3629002.5,11924933,1375366,14641502,14469228,11813861,8878186,9273278,1985171,599591.625,489557.0938,3646053.75,695606,518107,699217,2499140,3187950.75,1543282,634056,1117984,687953.6875,7564604,4230514,1859725.375,569115,2564004,435985,1972491,116486.6016,1904641.75,1743198,347437.6875,2147013,5071974,713821.875,1869575,13659926,10721568,968343,6960396,886599,22055140,872555,31803568,757167.3125,1274049.75,4054582,601590,7969653,8259071,2427919.75,588856.125,1642471.5,668915.8125,2244317,656345,317794.4063,18558162,666392,3682374,7654264,623107.6875,3039889,18011944,1483856,807358,5762597,779503.625,10077532,5284343.5,522791.0938,1696446,811703.6875,1419453,53166156,2075891,3441643.25,3220827,1308348.5,127374,25210246,2828098.5,1437220,698177,1786986,571384,2152312.5,1910465,1204956.5,552007.5,1947717,456747,5903142,1259456,2436063,563454,15036617,2050600.75,1256456.625,2334260,18829136,4988493.5,2085349,5587347,2759071.5,2137686,841242,1587454.75,1591124.875,7996408.5,2655387.5,2303386,15324777,2584082,3850963,1037696,9765777,3314418,1929698,1987073,17750994,6914839.5,3841879.25,557863.6875,1951856.25,1555420.25,620104,604591,27119380,7202141.5,752441,2314835,39523852,1084654,769148,4545148,2773647.5,938908,335245,1400053.875,9722237,3035962.25,1817550,10005787,1581036.75,1773864.5,1242764,1992158.25,28229320,696709,849745.6875,6219841.5,5950438.5,731315,2033074,24882924,442375.8125,721723,1929851.5,842923,14776743,938096.1875,11224871,8387933,1645327,3371455,17871658,2290681,737570,492421.9063,2014475.875,10040121,8611054,6298131.5,1140137,1965529,1363296.875,6994740.5,632517,13253979,10614234,4345698,448600.5,4498367,652425,948157,938833.625,2759674,8233527.5,952879,830739.8125,786253.6875,919739,724153,5669431,1427974.625,521341.9063,3936546.75,5214621.5,1415355,8839812,7456330,2527198,1472548,2222097.5,2003759,1280245,1083467.125,2407904.5,12037792,10053489,3983784,1541897.875,2798967,6057734,507625.5,1038815.188,738983,360045,7753080,573395,1152368.25,680395.875,1929011,1612392,1297041,1354000.625,5381105,1100499.75,6136327.5,1357701,1724560,601897,591840,1148202.25,1618916.375,41092076,3548309,3641620.75,458746,1489363.875,4527521,756196,1314357,808829,2971013.5,647321.5,564988.8125,9436408,469499.8125,674961,405062,2375104,1361794.25,3518459.5,1971236,13320624,671079,662012,230365,5792926,1562708,529806,2964206,829440.625,161311.0938,2539565,2156407,3970754,3460275.5,2235765.5,1084553.75,3897738,1398253,1453543,3780078,790039.8125,1655612,1128208.125,7240274,488181,819314,7011508,4045980,1076693,4742636,5231373,151798,8751704,768755.125,1221806,577290,882759.1875,6310254,84980760,759508.6875,1417852,981009.125,639878,844076.8125,7231928,1256360,1054995,1415494,3577120,763693,1177583.375,1063480.5,59107620,1657201,885843.1875,731092,722951,18868768,437619,3091387.5,1878442.25,3118304,8765978,15725581,1529309,465533.3125,3275779.25,1287159.25,3986851,484875,1261963,1644969,7545909.5,1633399.375,3294645,687049,2775717,1098965,1688739,1641546,686019.1875,2852102.75,4079837,2077039.375,1136405,1372475,3000864,11566123,4519115,2295176.5,2185530,991283,574650.375,4039667,585559,776996.875,635516.375,730211.1875,1067793,1437569,8651835,19061780,1372834,1136676,2634166,2009324,964015,1237252.75,507277,814403,1594733.625,7008252,1302192.875,25377650,9097980,1808401.375,1000599,929628,2520310,5917000,658297.5,705262.125,1189402,1034827.313,522880.0938,6361236.5,1118392,1082920.125,2330029,393493,1697909.625,6015128,666909,524053,9686734,2471623,2225265.5,1785906,1245181,669787.8125,4863803,1243200.75,419978.6875,2531163,2805926,2166711,3660395,614768,2213115.5,10975751,427697,784329.5,12187231,4099807,843267,1074180,474359,801264,1955013.5,1365778,1615569,2892428,5826745,2890379,1247362,915417,8050977,18661606,4341706,1039782,8070420,3132453,37797084,679574.875,1022614.313,19018762,4841347.5,585077.125,550324.1875,6881405.5,4644346,1248599,32552428,788027,3529643,3996541.5,3226884,676499.875,559805,123767.2031,1835804.875,1451145.875,1040593,1567746.75,558709,2853189.5,2568377.75,4309177,1188444,9343428,657385.8125,2979413.5,1646088.5,1696009,1524620,2438430,23612824,818484,4222682,2001936.5,3739029.75,2245568.25,5928881,6268627,3027867.5,2277238.25,2017423],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données de consommation prédites par le modèle de régression linéaire vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predLR"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle régression linéaire\n","pipeLR = make_pipeline(scaler, LinearRegression())\n","\n","pipeLR.fit(BEBM_train, SiteEnergyUse_train)\n","\n","SiteEnergyUse_predLR = pipeLR.predict(BEBM_test)\n","\n","LRr2 = metrics.r2_score(SiteEnergyUse_test, SiteEnergyUse_predLR)\n","print(\"r2 :\", LRr2)\n","LRrmse = metrics.mean_squared_error(SiteEnergyUse_test,\n","                                    SiteEnergyUse_predLR,\n","                                    squared=False)\n","print(\"rmse :\", LRrmse)\n","\n","fig = px.scatter(\n","        x=SiteEnergyUse_predLR.squeeze(),\n","        y=SiteEnergyUse_test.squeeze(),\n","        labels={\n","            'x': f'{SiteEnergyUse_predLR=}'.partition('=')[0],\n","            'y': f'{SiteEnergyUse_test=}'.partition('=')[0]\n","        },\n","        title=\n","        'Visualisation des données de consommation prédites par le modèle de régression linéaire vs les données test')\n","fig.show()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre    Ridge()\n","0  ridge__alpha  34.716868\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predRidge=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[6014115.300146965,3976235.784124896,-2383305.013471051,1049587.833218929,5898075.654942687,2590799.749424072,7991432.756961379,3068487.3597653303,3815703.3631973085,908773.1001907634,3336916.970485987,3195710.8073197454,1268168.417184303,2989758.395204543,1569924.8876624042,1607382.974094795,919125.6547084951,2405901.331357268,715491.7710232108,1260259.796062551,1457346.3650388212,91254.37804497872,6190880.7911838535,2042282.9371748692,1376939.612988034,523733.67442527856,4028476.9571405416,1329401.4427393805,1074288.1751229467,7521510.483486905,2569475.576951005,2845720.1183397523,1658306.1187966915,4411350.951925313,36674555.25284561,680121.0572937739,-22250.789124112576,2038116.6652139146,5771860.927548563,3372423.440900766,1345994.989184732,1118006.1468981507,4637428.596356755,8014299.251861317,6693664.656471894,11714181.582439195,1829341.457569987,12366323.046405938,11060322.201936573,1242629.1294993008,5603881.896403644,2221384.0698453644,1000035.7650465087,19383062.733426493,3151320.247374997,2565194.321002725,1864601.51662176,3096184.8925654846,3394504.9216345735,1283109.6001393506,415457.9673012411,2190255.248752028,5510624.046865725,1250872.4184458535,2829239.4604420112,5160928.290130706,6649132.929128321,2233850.2140534916,233400.6805663756,359920.90928166313,3822293.006477545,4766416.441765763,334304.5194206536,9501386.90186768,2308241.022878817,7332706.287421691,177281.62580750114,915975.1607719744,4101403.4222793216,27759135.421563584,1388438.5139373676,1286635.9976488426,1397104.1196695063,574775.9632128011,4776314.354470907,4826574.819079846,104567539.32562877,1699540.455315696,16946.974430975504,908294.0651507669,1045762.0382991704,2319062.4465694902,445696.15481656464,1715997.699676859,673077.499033967,1449368.8540754747,1076453.2660074,12174235.272251906,4778000.258454402,2504633.817861226,545968.3869362925,17192425.073218144,4976653.074302183,4136270.2329186457,17298039.85197767,8619698.66352265,690309.5523285861,433796.2947263357,1525515.37047572,4969059.4384937305,2390574.5636804975,3225019.8785880744,2003026.0366069954,308797.3707166561,723423.809599543,1662917.296092557,7134426.626887833,565171.9682783519,33804570.86200512,2322392.3235524544,385069.8138523232,3907564.894766895,2031711.6558068697,-242084.710761338,12734506.604287937,535989.9616899984,4284190.032359839,2334815.6369306217,-158689.8790821773,3920646.9303470203,13531563.453544509,1019406.8943366995,8480828.244861998,735744.9798379485,574975.418768038,1890287.4920513052,278690.6645592991,586525.0254996344,945602.119410556,3001822.1675612084,11008825.859356886,20079054.357187994,9103999.87148863,10970771.893608628,1446837.7011931716,1026719.4443152584,2033698.6742528789,1348733.3343599257,3375642.357209049,10711649.965758484,5214038.747981643,2554999.482277568,2175043.061010275,2657714.148474984,1421416.947956427,983333.3833970672,1771749.825340123,19491755.211616848,8322746.08296719,1609851.7129409616,54642761.708752036,10784746.056884527,2365228.927401944,1728607.6746831178,2029520.405016644,1277047.5328499367,2550550.678700302,1038792.2585629993,2149776.7853609826,1093734.0822119536,4814220.287307337,1172222.570346977,1313755.263576167,4354079.323306591,930359.5427012672,3495675.209288204,10018240.385399392,1453148.038548183,-42715.81693646312,739811.0047386792,676807.048654926,846280.4841764609,1132057.314522341,5339381.1240548175,2075941.8735280177,73472.02665736526,1596759.5819601673,159379623.69995695,1571221.2840640044,776713.7742807199,2667525.9628454964,1084824.0599642468,2883917.5248201457,919156.2152675295,5446665.736316323,-422822.38236925984,4001071.4828131497,3866130.3836273085,15530581.369152335,1227463.6770913894,5793127.436125912,1761368.9073285772,10240904.81627578,1847511.2495569382,1356122.9604138816,653052.3927859401,1867810.501111961,11487382.332393544,1583585.1984149134,1390242.5518365633,335720.16440527793,1108018.5356087773,2696810.994356837,556879.7573037089,2269707.9497365765,7225274.641163483,1418461.9333197502,373941.34406850673,1365745.7434179564,5357260.440086836,294674.89835551335,727346.8168494317,1648562.89186854,1246247.0423817975,2250071.327979458,4476509.64483386,9412411.645341193,15133480.690225648,2810757.6518061175,8198322.959900233,11161035.731660578,1874050.282285802,939737.9243657705,690429.1491068427,2185167.1372841103,1617253.0378725159,3797724.707570319,15445608.795884565,31642574.531370826,20104884.33422171,4039828.657122339,8894503.702855943,980357.1503228773,2168109.957450871,475922.5104869257,1471106.0899963076,2225082.4088900695,8577296.518157326,8318988.147587953,634917.2526234873,1979357.7459174257,20220779.244426765,904172.5163793394,2647749.4665123853,1616017.6467685043,3713770.931956973,4677966.660504229,1768982.6495720306,7308636.367259981,2406966.531294861,3056413.5108589735,9127732.811591463,1147044.6508991728,623830.1289405702,580647.4579400828,9449909.407335708,14321519.533961156,2241300.459694719,29953132.68653041,804673.0854089293,681851.1093360975,531241.6026532021,788725.8243432231,2178911.2344977176,1683056.7618255697,681608.978993318,7774648.660937321,4077021.2706484343,20366858.245629787,7737159.965066625,2317425.3909509773,1311242.208384023,4114754.9524012646,6115375.7509795185,273875.10394117935,1728427.148190489,1309703.0177758813,557582.5312258326,22120128.18466994,2402918.6103035538,21885163.11113783,-155699.54250416486,920062.5288119514,5658598.878285401,1444881.5858814514,3417192.832401241,791352.9228527336,2837402.619436288,1817002.2723026716,2071573.608420914,5139264.956736114,583020.9962646447,3332609.591419654,2251411.1703090244,-1254843.5296444087,2282101.1720925085,1569743.134142683,2976769.0717457198,976894.1323497104,2328211.135853941,4046475.122736385,2548931.388224426,13128197.039285827,3985637.69472048,2380764.933587698,18150.944611383835,8677352.205311423,1632083.8654721219,1218283.41653026,1494403.5383335482,1586108.0676000456,3088042.067402732,9315542.18768993,2878766.113641864,9794329.346414384,1056961.31291186,2243866.1494877893,-27858.378484073095,2731018.822909452,3214194.468780698,14777183.419429418,1786555.4517898695,2554637.202265218,-296079.9217895013,2716970.73699812,3361558.9532603114,744060.1117468518,641704.2375339293,1088398.5750799356,2678474.695127296,5466261.192682972,1491488.145625791,1096763.723455232,3878948.991631046,21547374.49442821,1870439.3411478885,11603686.69727845,11893903.60013727,14572769.405413123,14369639.925976858,4785962.730314145,7593815.378541123,5539581.530006684,585723.7833821396,23636817.28304173,1187329.5470947018,961851.358039205,10480870.262946218,593036.3333606985,1025576.582672206,1700445.5418330254,163346.29449802404,5573617.359072035,6442946.802903516,1037733.3409333379,996330.0638392484,7926076.513189834,769768.0885630138,4455731.135580055,6775788.7151003145,5067979.299762832,15693443.144981269,26859017.724886134,1082500.9035104646,402521.3039656521,702435.3878937499,6648855.741231474,7843661.904165179,1810203.1952971495,10624678.760610063,5987145.409630087,9028139.813049957,554195.0309406572,2942860.651690541,357494.3213038312,946091.4522990121,1083437.7776139209,3224256.1970850066,1811505.3454800262,2027822.7545939179,5829098.182240894,609254.384165348,2129839.1911014374,1044570.6722173041,1785461.6759684486,2947798.0528647657,10611237.260746147,17871065.193501078,1482669.6620083228,1901217.8811140058,1148063.8079208457,933869.1244167765,285420.4169483192,2372728.654436854,4452529.9328527,2266968.1267201435,8417819.048952851,6583452.17468564,2051403.5567842799,1323539.4509568417,2653559.7432781197,11013903.27295345,1209412.254855602,1287881.5764618497,1810144.7196369623,1116242.7221480417,1233963.8578531186,3023479.1037061587,2066389.5031837588,11006017.913931802,1112231.7531882143,5398707.588745304,2370315.3841690035,4016628.648848259,5753217.485535828,2331536.6904386533,959864.9393553236,27235285.560762256,1729316.842530913,1363314.77090293,2748582.3580358266,4415011.105681457,427017.4698728472,6825606.920664316,2354613.8484325097,745039.6401541466,475157.923237758,1875188.0639071746,1258602.5460665473,232190.1749777901,12355809.139986627,-8341.932669412112,1490312.7044131043,1535389.0938804874,1044604.5062596349,7271311.054259995,809078.0873020198,59504425.854757324,3020051.9830655726,2759000.472042148,279822.0182427645,1011619.3534286295,3725671.6843614792,1258514.2338235425,1883243.9970545326,2242205.5462120483,2316920.93525297,1016911.3110260237,3056259.8185601695,2216355.8719017045,1726831.9358007105,849803.4815390366,63767424.638719335,1955647.845668681,967327.4552674764,15639009.448062781,19788118.171309907,1187290.3545803616,966543.1925554967,9338821.568228355,29508324.630445447,955232.1802613472,349190.03379648575,1020217.7114794061,963130.1873932004,1160307.821833477,2113820.1387219233,951991.7688419595,872397.949249981,2104683.767887829,3485954.69650374,19550721.011755385,42541771.80579004,26901573.006569274,6173023.638351809,9352028.007903248,695057.6427925583,10733144.925988689,1541131.4794733394,8497337.8287611,2269337.7452014536,2016389.0458355057,6010795.6668311665,968409.3871961567,2568357.2157050064,1086729.4509409338,586635.8737089429,10702361.648461163,64323547.400449574,9576878.467471085,12971734.684415735,1097036.9672483604,3091217.9599640076,9802574.191827746,21878522.23865089,1447646.2954112436,9588567.58134199,20989187.80624706,1356613.5037517708,69237600.49759938,1307739.4906510175,2375408.422868911,49059682.8530622,4922861.514414053,1155834.9279110723,535575.3581071543,1655618.9500550814,338259.13913784944,4693871.50210461,1026465.2874421865,1390602.3777947281,1451626.0397086414,1446335.4884583605,-131447.25391622027,416720.80234267307,3940122.4865485257,1981420.2618872167,875615.7958746462,2687449.4628002103,4159154.21469579,14701646.303377733,865747.6399389757,1630857.3195813906,2179212.843528431,6979522.0630896855,806930.5164897502,319189.0898276828,1321099.6781326411,2293161.960179981,1404383.320948889,1083496.253274108,5583564.520416232,6116129.306649802,1350348.651019784,-738647.379833376,4623484.483049834,5516719.574399813,4049491.73258635,1555873.1547693003,503585.45626312913,5021227.136909068,5976184.343514575,2377385.928784939,3092583.539857048,13756659.918892622,2067462.2508590904,1219159.4584304648,9313541.309381858,824952.5739392964,860404.4470786327,1221948.3266924238,1353413.9203131958,449737.08960903436,3076522.5669845333,55121.71213018289,3696080.322516593,1201854.3113881545,2099403.985815776,4198356.0600167215,2605190.2644878337,3411382.9216682734,6706044.739775193,8673283.760012923,4552559.259080484,728129.852534319,1175957.60508618,852995.7097732602,1907328.9023856279,36044.7613817628,-367349.27580519253,537493.9093617385,992180.4085629708,6855918.633017755,2909440.2196668102,890060.7699227105,1936726.2232412666,988479.6154640806,2166419.1891165525,1448801.8871493072,196427.47658621403,2799790.952228033,1882414.1771034352,12553108.616936507,1388616.5863758114,5086788.3700417755,1243531.761962445,826765.7898783204,5941561.795323933,2576755.1318039955,3079759.4357904494,758659.648853533,77893156.06169431,2298228.1304125236,6342544.133523031,4754742.797253383,4911497.882013768,3264902.4346783916,3100706.6279739635,790592.0189838833,926958.579369237,5449848.056213146,2237459.7049073847,958404.1681007561,1747831.2920938996,6004617.808041049,4531187.552175863,3614507.0134617984,958908.4148311792,1557372.3949798513,1128305.9731947326,2467905.8590963325,7091519.748173301,557417.6550439121,1789536.2551426897,4548249.736025222,5918194.065647035,51303883.6608183,5853394.120989966,575039.3223982388,437433.76949217357,3320515.9391345233,971119.7515088688,1560296.0538387059,526637.2728227908,3160030.341241707,6431356.370745396,2305611.982025492,1794814.9208295126,1406749.2300033192,368513.76649416215,7844667.1725083515,3900791.4702120377,1478240.357758246,1491735.9560789627,7043158.883561108,2744274.290942902,10543389.160349883,396640.10710754176,5226953.204642462,1911657.8571274416,573788.8878890849,2200083.1707422584,6639558.889883532,1778064.6619428545,827521.5444352732,136976.31984127057,3112881.1390794637,35478179.60895702,8908871.631978814,24960318.28836996,576949.263987645,1979476.1156685897,1660042.4788534022,1637979.6943021836,6783135.563558234,524046.954832925,1177944.6235321893,10295958.46852834,712904.6974391281,3286075.7496981504,791559.1914630635,3348699.5662744027,479167.5162226523,1406125.2021066197,1076325.1293135483,30164.231467531994,-185914.9668782549,1476927.1105957762,5817723.98437701,7362638.075984071,9506884.506668352,2815003.0853868853,3024424.384718445,1070373.3461205345,2970510.24285804,3438664.238848416,900577.7156494509,197405.73532074504,701422.4107873396,2141558.3454902153,141791.88045939058,1418309.513331971,337033.77843913203,9370416.04098925,337952.39927620674,46941844.838185735,1743376.4531850205,1597660.2642877058,8302448.563237497,1082260.6743305563,1331126.2504033477,926692.1089535533,-105685.17327980231,3140229.2079100264,2857900.9655393045,3311223.7328725103,6932053.454382106,9436668.111930445,834252.9175591725,2262378.9478407125,836806.8010968557,1508014.9244450554,1526198.626917731,1278059.7645209217,1417741.0543763277,1567246.3238049145,16788593.969843626,17092220.562035736,1416867.7554451257,21721775.96043504,54342981.345382996,8804272.2393028,10159059.684428431,997303.2311785412,1887292.9157580163,1519911.004889809,2662529.7496714955,840267.3650158374,6685639.831936234,153845.75853088405,3871319.0256911414,5650076.421350297,4600181.663215036,25718648.567536704,177787.06740868697,6829406.287144012,4257150.7666612845,1067181.4552824032,114647.99497579597,3134186.946290939,1444638.274750926,-360460.19368715957,9643783.29076826,6086667.541964879,2569639.762836594,6966305.07124392,3007392.680257318,14925935.988597024,1570667.5489891148,1715907.4364305446,1447648.7494654283,1037476.4997493594,753313.2982560366,168821.9812515499,374960.33026511036,884367.6951900783,1391411.3975887306,5523277.057695308,4689007.133156557,1916621.7816819327,5570753.017301826,2052297.606047566,2063663.6179750743,4239817.769314735,-1561742.0044833939,187406.08674326446,9827749.467008606,5356771.857648091,7882120.027252339,1401966.5970417159,1264591.2048585494,1248439.5021330093,4389550.604767664,129428085.80938885,5262274.686724248,1233241.7518826034,381081.32926203543,1553043.2065474265,16493879.09838633,717237.3332622193,7640534.679693737,38166699.947657116,40940263.03462175,2568183.001584207,4088379.3873171993,234742.56631511683,1173681.9522763495,939554.4819074913,-6612.830055410042,1378160.6122397066,135738.15581470262,124213.94011208043,3009832.7137629404,2716897.5045252875,1153749.0047710636,2159053.879867508,1128566.9467169354,672888.7457899069,2065018.793696883,5048396.6661136765,840121.2955340161,864245.6009493403,4221759.633149071,371886.5923664747,12636313.933123095,1240040.8288881255,842583.648860977,13223880.949583774,760744.2179857595,865359.320464148,66210771.439009815,1370833.4998247824,378422.33492031065,223265.73090718454,-6728.29603901878,1796293.316152304,5389454.065210593,27493084.248692553,2714405.929602697,992781.690728137,1737151.4981858935,4951157.979641201,12302787.137000645,2098358.0386750097,1563278.4220483587,4245006.846492248,1366368.0530980018,8090484.552781495,861255.822676976,96501.8964664673,202043.31237164978,1688024.5774011267,5364772.289212368,-813752.8349644514,1715294.9296678707,66808450.459169574,4912636.296283476,8413257.171475016,3516718.2290084143,15975331.55716267,3497444.5032367026,18331641.813761372,1939044.6080277935,11157683.72171683,4723827.213863512,959864.9393553236,205120.41743163904,2470018.170208781,2082799.6694904196,109974.80049860058,1312388.243305212,5880656.0754311,8099608.064220201,658134.9401218002,573718.7716223246,1157116.4955793147,1458077.2017267016,18073925.179471876,10248228.682954296,1949518.5341089496,440190.09679138614,3562693.4560231483,636916.543338178,4963352.744627753,891624.6981145456,2603740.408260251,2604804.643196241,649218.2930135254,2537182.1540862266,649996.16777181,831241.0403843976,10377803.600375688,7261547.044526804,3463824.6407498466,894666.7690114779,2800935.040898178,-86727.41824002052,14564350.013051191,1369989.3430218254,3909576.3386364765,1613733.1226839225,1583670.0991196295,7329570.252333843,1171862.7443888122,13886762.471742926,11964421.890869806,1985165.154240582,983267.7309378481,3108488.800569093,188748.48014559783,12173897.680196742,1044500.0768121026,1352146.5537835164,18165335.578211684,14454395.433008239,2748485.0245627468,1909796.0047531023,1038034.1376906857,2224690.795345777,20233286.830749385,1477860.0575029948,1185069.1633164897,2993523.3433997924,114730.67027411307,3716802.2997288983,3477767.690103641,1158862.608923541,1909292.1150513305,1058021.876321481,351054.8457446166,13616679.80616212,3603943.759588829,2152774.986017073,2406835.5110457465,1054020.878942963,1476890.3601900162,53476404.98245984,4106446.6139836432,3549671.2229674393,778603.0216338378,5783232.699125921,841407.6358766428,4490875.325982484,1817071.2809637387,3957326.822963886,-119929.15578040294,2263508.899369339,621737.9651627974,2206701.2309733974,1072291.6234533996,2518225.7618636037,1162475.3667721145,15781045.383111816,1655899.524164623,1844897.2964950984,5556611.0211624075,17879819.273994662,6294457.586645488,1400313.2205965107,4323511.342687307,4608034.5124429725,3832809.442570229,1789262.1221522088,2177147.8584071845,1962028.1295005474,12314419.827198159,620957.7999226069,6150174.9165674895,3354349.389507361,4902936.215744054,11666500.830652134,1061344.8452381897,8012142.91827348,1583566.9259119472,2059398.641865359,1406049.8899284098,16519391.01500456,18009159.326765634,5090054.420542553,1185159.5481018785,5137517.717091294,9256545.164002478,489160.80109114526,1415932.108368762,3962649.0294303605,3435167.6430265144,1583643.8230051273,1596221.7823875262,51166984.876718394,474655.2450156552,902744.347003445,16354312.03167745,5377710.493994361,-139560.47875741543,1634851.0412402144,2862905.01966837,21529974.422530208,1750896.7522780653,2073048.541537587,3084331.8018028056,736990.8550332861,1374871.1830237913,1558315.7320563355,845625.1777917736,2296190.329670614,1320977.6273001991,489590.97200311953,15369493.262067258,9850022.8453742,1504031.4908258005,4914899.0425543105,26394429.517005377,445773.71399661666,816046.2544445437,2973095.1581245866,762015.7608966089,27358738.5797258,1997802.0087245977,5196809.797599625,3565751.002974659,244076.84821220185,4408394.382284367,9665766.43376102,15605541.96647281,921210.0796619402,608018.8052217963,609914.3333943987,5293704.112046318,4036013.3912075413,6512234.145028412,519678.3099614531,1218075.4547406575,2850144.140587307,14564973.266585745,797250.0652980304,5094957.194810187,32562386.330829505,2692340.6763421027,613254.0735080317,9113657.142327936,956624.6828842913,910464.0641435902,1625624.3089578156,3713675.7606022884,23477589.58226937,1923688.0327127625,963424.8599661202,3030591.618999766,640484.2403630377,387983.90742418636,5085981.404534493,1881002.595020858,771575.807543487,3943287.905887919,8064665.583254299,2827974.399201401,2519397.0763501674,5334193.6629113555,2705255.999804915,4152141.6203426667,2208861.9456885243,2152903.6329280287,2038933.942539114,518930.2081365769,5173951.938311768,4902691.663505841,14435714.802925352,7408209.838359904,4871512.981636119,1699992.9985743607,5725274.986620441,31923.19715164136,670168.727486508,1523143.6947534936,2406236.529301362,8873455.515115414,850165.7615513864,2211429.874870697,472355.6000654395,1555208.2974318797,678773.5145475536,1538865.3811416244,-61319.28608353902,2090465.5028740892,1098734.5122863278,3090786.0898161656,1981796.080595007,1764398.2671102344,330761.4379896589,380835.2784411046,1454106.3052444253,3033829.480485758,6255105.567942022,7208748.83183556,2555059.818086865,-95480.74988107476,2733175.449420914,1454603.0251900712,758098.477337792,1712081.351640718,1543719.5512222897,3701306.4030779926,704150.3895737238,1239336.390442098,10330521.65137911,2297743.293378686,751598.2965760629,3252807.5574610345,762621.9919904575,2321573.819200162,3655462.6314931596,5339371.436202022,5074645.695871348,943269.4430695192,2219276.227594006,1819955.5459254785,7605640.423080943,4602180.527350606,1528039.0602912456,4243345.398083552,652971.1289134594,2669448.370495856,7088255.033258225,1475478.2431244103,8482528.47233957,813386.0901662298,2458594.6541325124,1211788.4327917187,5294769.971075235,2021246.0273427693,1396867.4764478293,13450919.635099672,1451303.7257351242,4173836.563286002,1591528.3640905805,2492452.0814540125,409006.70222968026,1321109.1194084915,8493408.682737095,5431668.7551751435,2146966.502860903,5493670.641748,10038211.817700174,726323.3605809386,6446553.390585732,639814.9901808114,1062012.398849435,724612.569764625,1293749.914499378,14880755.530019602,4903315.225865671,1107174.5573404226,774037.1835314296,-133954.39249424217,1096763.723455232,1016498.8582932029,4596726.007404424,584724.0729927684,599115.7862825494,1335914.8319595975,950447.0011795913,1148437.0668446452,2538687.7911600806,2185761.4326047567,17134651.030670825,2104101.5523239253,1643845.4096467602,1502584.8248305852,1061018.2508876375,15062834.772696935,1414958.5486208305,6152760.888038607,4049173.2500627735,3453624.89391632,9498289.345165458,17752775.642609276,2169991.2680792515,9395475.62300425,782472.0761050908,816887.9571933157,4350390.2460498735,821641.348688944,272636.93991461117,2977671.704208864,25542130.67767149,4179284.145650993,1607831.7700331742,1030678.1025391276,4744933.296542881,1276547.022066241,1875644.0488750888,1331807.9898308348,5714933.982224956,1463696.4505566868,10970075.498655701,2890776.3045129473,2236302.769915685,811712.3915943599,8453465.84524935,3861484.6624981775,2759412.8370969556,4959329.9523159675,2198698.661488508,1074106.4216032254,662539.0330227858,10510559.370851893,1186159.9607410328,1334706.7823583898,168453.4680903973,961978.2562967779,371641.3504150254,1825823.8667074929,8472648.649110194,3554091.6165011497,740404.4270059848,1098928.8143396853,8302620.379744393,3816754.5039681606,904862.2222998242,1971812.1738260617,869301.5697700393,1402833.3495885986,1875516.7713899799,3990956.7751806825,1050488.0116278874,19638899.99672437,10051900.836218435,4415660.474980575,836715.3108234487,548924.1966412487,3379936.670707144,5158348.482676355,607560.5292934196,983694.4363823249,6253028.090749711,1912864.2939427483,567629.6583505154,5655272.430465324,8169883.194924946,175386.97603514697,2893668.6381418854,971027.0213392745,3770666.1106669866,2121256.099335121,1130702.7040694463,984115.1920548622,10996295.190592121,4894368.081029282,548429.5910501876,1216737.5469947644,2393282.3042454575,-157778.05179291405,7116420.847189594,591709.0113357019,2575213.5413436233,2014831.764832718,5813525.36818278,540586.7076575479,4365191.67046385,4925649.266514018,504308.4261553902,1220213.0157890287,1572990.1569558163,831514.403650074,6916872.218903529,3922454.6493274933,170783.49993586983,5570487.647416068,845019.529484373,2061497.3000635288,4370913.017576462,2385797.9272046997,3312547.1210081913,7293619.754776116,1405577.109191251,4668380.779464804,3804717.524283706,2396368.03883117,20628893.691364523,11566253.150798012,2268093.716753369,1158162.3982671062,9555430.288046235,4124949.4561012494,60911172.92908892,542491.3694788204,664247.7916403073,9935586.826833788,912326.0284990626,400595.12526182993,1692438.9923888007,2664036.256798269,793460.7586013586,2491633.456337767,40043762.02218668,1203096.2816677792,3855939.225983429,1296081.1536178323,2520087.5299456315,1001927.6121378336,1226097.941202863,2310864.7640309404,5911125.060874019,3793773.1560385507,2371809.420670908,1470326.8957319104,884285.2686582294,3713637.368763741,2756769.854041977,14129583.47067542,2108710.95792597,8080872.812648084,941687.249559293,2320136.969421687,135730.4557360215,3170728.264585666,1903188.2739332959,3699592.2401230573,8642497.87154586,873218.4889106974,16491210.815777358,1910529.466726008,5331048.076731095,984237.2428873039,6826276.291220557,9045770.41607872,1518476.3867299072,2896215.742624077,3725230.683157624],"xaxis":"x","y":[7506587,1972078.625,7768294,706505.6875,2274873.5,2547582,4571388,288191,6252842,1126157.25,1153276,4103800.75,1963054,3292049,724259,819803.6875,1589550,967432.8125,984178,968026.3125,1688745,678663.3125,19236770,1359948,342493,497620,2682044,4420650.5,866343,1250669,1770027.5,14084606,1981115.25,1986094.875,32141070,5415438,932083.5,293217,3543985.5,2443065.25,992794.6875,885865.875,552729,3580091,2993610,284867168,1222509,5672838.5,0,965786,1429146.375,1648774.375,726124.625,30648154,1836671,876425,2192111.5,1426823,1748132,1791071.375,646377,1487660,1882845.75,893053.1875,4247457,1796889,6426022,2493240.25,650008,385719,1851260.5,75073888,804002,25453690,1997488,2712983.75,913899,9546165,5122039,25307744,1313366,761504.875,1302636,234144,5968153.5,2966309,58761304,1160989,2217620,142873,2051501,1069490.125,348061,545426.375,1113457.75,2665558.25,628023,4759256,0,2406996.75,501194,6508595,14829099,2491238.25,4135650.5,1023418,454296,1682454.875,813038,11651200,6797528,1145535.75,1348497.125,493000,766628,8204897,3960898.75,609101,17463744,3348280.5,863728.8125,1692323,2112095,1878742.875,6711975,675611.375,2196718,1199742,748775,1545356,19645206,122156,8393893,866825.375,1357116,2496807.5,615894,845044.125,526752.8125,2991674.75,3093944,25471308,7221482.5,3894584.5,2502081,630578,1569569,1435730,2512443,8971926,19248732,1678495.25,586589.1875,1832808.75,1132489.625,811907.625,702385,20361158,5647146,2323265,41655852,0,962971,1200811,1107290.625,1761137.5,14029480,248607,1037555.125,659450,3528086.75,820705,2433777,4549892,1358652,2558573,4287439.5,1239315.375,786979,730765,2781314,567072,967082,3228330,1243282,7606084.5,2103293,114648520,1977299,798970,4700552,1813864,1482554.375,759060.8125,4946798.5,678351,3289053,3326999,6935723,1386331,2740128,888031.875,1809101.5,581289,975431,4475650,1441445.625,8287440.5,659652.625,1538750.625,1306087.75,1058986,1060846,131810,3040978,1593187,791564,1085183,818317,3022275,520738,1224685.125,2114860.25,1088016,4520838,3626431.5,4448582.5,9212950,2392466,6689184,13586070,2074152,567608.625,594848.625,3009463.75,2446842,2161828,7794869,15206762,10084950,3906967,25631512,1022114.5,1182575,1218112,1249939.25,1788148,6102883,3093378,1493131.75,832168,7791957,839362,338923,2092251,1929828,1061458.25,451738,5820965,2284507.5,3740373.75,5417598,1632498.625,992139,1232822.875,3590731,9851937,2069880,27139746,514555,948676.5,665200.1875,468973.5938,13140574,227509.5,4185406,11509942,3185509,20630168,2928099,1391916,111969.7031,4842910,2278160,2094410,1138520,2470566,851651.1875,19397324,1244824,35383012,777587.125,898115.375,3471845,2190460,17686444,696605,2396523,651293.3125,1187261,5661689.5,517029,4673827,1255834,1970956,644415,1290284,2656729,2041912,4700396,29195468,1403869,42709624,1986384,1240875,509485.6875,6856153.5,369291.5,1464856,5420552,1140620,1424157.25,3795938,498745,4028943,873648,1234573.875,936336,2503690,2756214.75,6884914,766381,1384558.625,1260416,1214929,5448975,832986.875,659761.875,209644.9063,1269847.75,2526375,3467070,600318,2338852,11765682,1232210,13901935,7539963.5,4425832.5,24906132,3579008.75,11353672,7475577.5,608491,11289052,698367.6875,1525624,28731502,1656420.25,557082.1875,934357,1020039,5288304,9049598,899266.125,588969.8125,16016644,387810,6056481,3437441,1441665.375,8800292,8386493,497991,738990,2159932,2436403,5747645,1150582,1912690,8787353,3024478.5,863773,2274920.5,1187315,3158151,847298.8125,7613116,456397,1026921,5974993.5,666385,2559461.75,1170616,1967129,3928695.25,5464762,20781390,629027.3125,6129780,964236,929950.125,1815371.25,1777652,1242811.75,562479,8964061,7577723,1537570.25,403800,855946,10416934,1736775,684739.6875,786324,827042,892086,1735816,2404644,3568616,1046995.188,276107,243518.2031,2212058.5,6858533.5,318364.8125,639407.375,10126597,2227908.75,849504,8381744,4397572,1676426.5,3761647,1228837.25,646505.1875,717766,1195351.25,934621.1875,1152896.25,4103027.5,3392129,841891,1692199.25,980356,4374646,2332996.5,35891484,2845946,2674034,627854.125,2181833.5,5424718,298063,3560614.25,978463,2130673.75,1466382.25,3706100.25,33268410,5040382,508051,55813996,1075019.375,2100722,6227419,11356990,653139,204499.0938,5722325.5,10564729,548293,619545.125,1162120.375,901015.1875,1074937,1353337,807185.5,576405.125,1662723,4329966,7133222,27282662,8435093,8058913.5,3567977,609218,6915293,509124,3869754,363808.1875,2204235,4307238.5,1123416,2799595,1997115,1943772,3467698,448385312,2417401.25,5249190,664088,415975,16917964,13609040,3537841.5,13458948,9157150,2204161.25,47952868,847097.8125,4571746.5,65047284,3833162,1211947.375,1458617.125,3014332,705948.3125,2022990.625,1567040.75,352136,2168281,1160087.875,588965,628609,3481727.5,1677002.25,619804,13943008,1053290.75,1990444,565458,3494929.5,3155391,4089677.5,1998725.25,712057,935396.6875,8201974,782202,848497,4368113,1165598.875,2420605.5,592739.3125,2184912,3600897.25,4486477,980603.375,958798.3125,1886382,2503948,4751090,1651120,6335315.5,956481,397624,3063969.75,1841154.25,1295685.75,2586116,727703.5,1636649.125,1855556,719471.1875,2267094,1787119,1331122.125,971308,902694,5024837,17186624,55073120,1819329,512166,772379.3125,1049436.25,887403,416566.1875,673834,609257,759405.6875,3155496,1070085.375,2006626,2804534,747384.5,1057256,1054581.375,507428.9063,2664566.5,2065156,9697866,847514.5,6522551.5,720005,573624,12783255,0,2136086,15480681,253832464,2339234,4212149,3021669,3181045.5,2601369,1656025.625,646994,1565475,7802205.5,1187723,1356874.75,2298648,1136698,1586388.75,1018111,9085108,1734533,1703504.875,1868497.125,11047668,433443,5459657,1841968,9444484,38400928,151754.2969,536642.875,3717064,2159170.25,1625308.625,682632.3125,785144.8125,2553763.5,4750287,9617898,1025205,874707,618876.1875,3162976,1520326,2114053,1109924,7546235,3542542,10444602,462465.5,4023479,1761048,869835,1189942.75,2211671,1735538.25,789630.125,1962351.25,827937,28450436,6074249,9861998,967697,2107743.75,696054,1152890,3791402,965651.125,2090895.75,12480984,552015.625,2904225,466985,1268459,862744,313528.0938,1769998,1547116,2023858.75,863703,4656777,11061916,5233553.5,1168017.75,2300803,727775.8125,1443367.75,4608963,2342494.25,706879.5,574575.375,855670,666740.3125,1981360,1193790,8773058,520348.9063,17666646,1607590,1227772,14332778,410433.0938,926261.1875,2796425,1234813.25,2701833.75,4120963,4742869,37276920,18622292,2157144,1776411.75,666967,1080203,3031811,433871,908834,1076644.75,12448381,15843743,709979.6875,13239050,143423024,12196872,10992344,1291170,835545,1050982,1513700.625,1829122.375,2549649.75,1844825,3989399,1857329,1495686,32381836,1109851,6897247.5,1863784,675772.875,1018068,2378077,972608,1181428.75,5653656,7771933.5,1827424.125,6419090,1218114,16701055,771512.6875,162960,1213444.125,2777052,1185469,490599,541542.6875,1103762,811537,2932696,1848656,775007.6875,2172841,1788999,837792,784762,2737534.5,1695067,3060149,4178233.5,2924362,1271753.25,683274,712149,4020660,59757440,3298570,760090,505601,858897,15660628,323739.4063,5702077,9387211,37022180,1712279.875,1742351.25,1001964,1612797,638507.125,530700.125,2327611,1261634.5,1072487.875,1162561.125,6454983,1063960.125,151376,2534873.5,645756,1200381,2855545,2219744,688145,1362993,456197.5938,9523142,502898.6875,611997,13171370,1009476,739210.5,58041572,1459199.75,607579.6875,839011.3125,862826,1128179.125,2306723.5,8437401,2147062,646691,1619177.125,2236167,4058492,966764,1795462.75,2165371,4322873,2167888.25,1251906,669217.6875,471113.3125,664539,1983509,792015,794724.625,53079164,3629002.5,11924933,1375366,14641502,14469228,11813861,8878186,9273278,1985171,599591.625,489557.0938,3646053.75,695606,518107,699217,2499140,3187950.75,1543282,634056,1117984,687953.6875,7564604,4230514,1859725.375,569115,2564004,435985,1972491,116486.6016,1904641.75,1743198,347437.6875,2147013,5071974,713821.875,1869575,13659926,10721568,968343,6960396,886599,22055140,872555,31803568,757167.3125,1274049.75,4054582,601590,7969653,8259071,2427919.75,588856.125,1642471.5,668915.8125,2244317,656345,317794.4063,18558162,666392,3682374,7654264,623107.6875,3039889,18011944,1483856,807358,5762597,779503.625,10077532,5284343.5,522791.0938,1696446,811703.6875,1419453,53166156,2075891,3441643.25,3220827,1308348.5,127374,25210246,2828098.5,1437220,698177,1786986,571384,2152312.5,1910465,1204956.5,552007.5,1947717,456747,5903142,1259456,2436063,563454,15036617,2050600.75,1256456.625,2334260,18829136,4988493.5,2085349,5587347,2759071.5,2137686,841242,1587454.75,1591124.875,7996408.5,2655387.5,2303386,15324777,2584082,3850963,1037696,9765777,3314418,1929698,1987073,17750994,6914839.5,3841879.25,557863.6875,1951856.25,1555420.25,620104,604591,27119380,7202141.5,752441,2314835,39523852,1084654,769148,4545148,2773647.5,938908,335245,1400053.875,9722237,3035962.25,1817550,10005787,1581036.75,1773864.5,1242764,1992158.25,28229320,696709,849745.6875,6219841.5,5950438.5,731315,2033074,24882924,442375.8125,721723,1929851.5,842923,14776743,938096.1875,11224871,8387933,1645327,3371455,17871658,2290681,737570,492421.9063,2014475.875,10040121,8611054,6298131.5,1140137,1965529,1363296.875,6994740.5,632517,13253979,10614234,4345698,448600.5,4498367,652425,948157,938833.625,2759674,8233527.5,952879,830739.8125,786253.6875,919739,724153,5669431,1427974.625,521341.9063,3936546.75,5214621.5,1415355,8839812,7456330,2527198,1472548,2222097.5,2003759,1280245,1083467.125,2407904.5,12037792,10053489,3983784,1541897.875,2798967,6057734,507625.5,1038815.188,738983,360045,7753080,573395,1152368.25,680395.875,1929011,1612392,1297041,1354000.625,5381105,1100499.75,6136327.5,1357701,1724560,601897,591840,1148202.25,1618916.375,41092076,3548309,3641620.75,458746,1489363.875,4527521,756196,1314357,808829,2971013.5,647321.5,564988.8125,9436408,469499.8125,674961,405062,2375104,1361794.25,3518459.5,1971236,13320624,671079,662012,230365,5792926,1562708,529806,2964206,829440.625,161311.0938,2539565,2156407,3970754,3460275.5,2235765.5,1084553.75,3897738,1398253,1453543,3780078,790039.8125,1655612,1128208.125,7240274,488181,819314,7011508,4045980,1076693,4742636,5231373,151798,8751704,768755.125,1221806,577290,882759.1875,6310254,84980760,759508.6875,1417852,981009.125,639878,844076.8125,7231928,1256360,1054995,1415494,3577120,763693,1177583.375,1063480.5,59107620,1657201,885843.1875,731092,722951,18868768,437619,3091387.5,1878442.25,3118304,8765978,15725581,1529309,465533.3125,3275779.25,1287159.25,3986851,484875,1261963,1644969,7545909.5,1633399.375,3294645,687049,2775717,1098965,1688739,1641546,686019.1875,2852102.75,4079837,2077039.375,1136405,1372475,3000864,11566123,4519115,2295176.5,2185530,991283,574650.375,4039667,585559,776996.875,635516.375,730211.1875,1067793,1437569,8651835,19061780,1372834,1136676,2634166,2009324,964015,1237252.75,507277,814403,1594733.625,7008252,1302192.875,25377650,9097980,1808401.375,1000599,929628,2520310,5917000,658297.5,705262.125,1189402,1034827.313,522880.0938,6361236.5,1118392,1082920.125,2330029,393493,1697909.625,6015128,666909,524053,9686734,2471623,2225265.5,1785906,1245181,669787.8125,4863803,1243200.75,419978.6875,2531163,2805926,2166711,3660395,614768,2213115.5,10975751,427697,784329.5,12187231,4099807,843267,1074180,474359,801264,1955013.5,1365778,1615569,2892428,5826745,2890379,1247362,915417,8050977,18661606,4341706,1039782,8070420,3132453,37797084,679574.875,1022614.313,19018762,4841347.5,585077.125,550324.1875,6881405.5,4644346,1248599,32552428,788027,3529643,3996541.5,3226884,676499.875,559805,123767.2031,1835804.875,1451145.875,1040593,1567746.75,558709,2853189.5,2568377.75,4309177,1188444,9343428,657385.8125,2979413.5,1646088.5,1696009,1524620,2438430,23612824,818484,4222682,2001936.5,3739029.75,2245568.25,5928881,6268627,3027867.5,2277238.25,2017423],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Ridge() vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predRidge"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["alphasridge = np.logspace(-3, 5, 1000)\n","param_gridRidge = {'ridge__alpha': alphasridge}\n","\n","GridRidge, \\\n","BestParametresRidge, \\\n","ScoresRidge, \\\n","SiteEnergyUse_predRidge, \\\n","figRidge = reg_modelGrid(model=Ridge(),\n","                            scaler=scaler,\n","                            X_train=BEBM_train,\n","                            X_test=BEBM_test,\n","                            y_train=SiteEnergyUse_train,\n","                            y_test=SiteEnergyUse_test,\n","                            y_test_name='SiteEnergyUse_test',\n","                            y_pred_name='SiteEnergyUse_predRidge',\n","                            score=score,\n","                            param_grid=param_gridRidge)\n","\n","print(BestParametresRidge)\n","ScoresRidge\n","figRidge.show()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[9090271.774252882,9090271.766686272,9090271.758978855,9090271.751128007,9090271.74313106,9090271.734985294,9090271.726687944,9090271.718236184,9090271.709627142,9090271.700857894,9090271.691925453,9090271.682826785,9090271.673558805,9090271.664118351,9090271.654502219,9090271.644707138,9090271.634729777,9090271.624566747,9090271.614214592,9090271.603669794,9090271.592928763,9090271.581987856,9090271.57084335,9090271.55949145,9090271.547928303,9090271.536149982,9090271.524152478,9090271.511931712,9090271.49948353,9090271.486803707,9090271.47388792,9090271.46073179,9090271.447330836,9090271.433680508,9090271.419776164,9090271.405613076,9090271.391186435,9090271.376491332,9090271.361522775,9090271.346275674,9090271.33074484,9090271.314925002,9090271.29881078,9090271.282396697,9090271.265677175,9090271.248646522,9090271.231298964,9090271.213628592,9090271.195629403,9090271.177295279,9090271.15861999,9090271.139597189,9090271.120220404,9090271.100483054,9090271.080378426,9090271.059899697,9090271.039039891,9090271.01779193,9090270.996148583,9090270.974102503,9090270.951646186,9090270.92877201,9090270.905472193,9090270.881738817,9090270.85756382,9090270.832938988,9090270.807855938,9090270.782306155,9090270.756280951,9090270.729771486,9090270.702768752,9090270.67526356,9090270.647246579,9090270.618708272,9090270.589638948,9090270.560028723,9090270.529867541,9090270.499145146,9090270.467851102,9090270.43597477,9090270.40350532,9090270.370431716,9090270.33674272,9090270.302426878,9090270.26747254,9090270.231867822,9090270.195600623,9090270.158658622,9090270.121029267,9090270.082699765,9090270.043657098,9090270.003887998,9090269.96337895,9090269.922116194,9090269.880085705,9090269.837273205,9090269.793664144,9090269.74924371,9090269.703996804,9090269.657908062,9090269.610961813,9090269.563142123,9090269.514432732,9090269.464817094,9090269.414278356,9090269.362799348,9090269.31036258,9090269.256950239,9090269.202544179,9090269.147125918,9090269.090676632,9090269.033177149,9090268.974607926,9090268.914949076,9090268.854180332,9090268.79228105,9090268.72923021,9090268.66500639,9090268.599587787,9090268.53295217,9090268.465076912,9090268.395938959,9090268.32551483,9090268.253780609,9090268.180711936,9090268.106283993,9090268.030471506,9090267.953248724,9090267.874589432,9090267.794466916,9090267.71285397,9090267.62972288,9090267.545045422,9090267.458792841,9090267.370935852,9090267.281444628,9090267.190288782,9090267.097437372,9090267.002858873,9090266.906521175,9090266.80839157,9090266.708436754,9090266.60662279,9090266.502915127,9090266.397278558,9090266.289677225,9090266.18007461,9090266.068433512,9090265.95471604,9090265.838883605,9090265.72089689,9090265.600715864,9090265.478299746,9090265.35360698,9090265.226595273,9090265.097221524,9090264.965441834,9090264.831211498,9090264.69448497,9090264.555215875,9090264.413356962,9090264.268860105,9090264.1216763,9090263.971755613,9090263.8190472,9090263.663499268,9090263.505059063,9090263.343672851,9090263.17928591,9090263.011842491,9090262.841285815,9090262.667558063,9090262.490600321,9090262.310352601,9090262.126753803,9090261.939741684,9090261.749252852,9090261.55522274,9090261.357585587,9090261.15627441,9090260.951220987,9090260.742355827,9090260.529608168,9090260.312905911,9090260.092175651,9090259.867342597,9090259.638330586,9090259.405062048,9090259.167457968,9090258.925437873,9090258.678919801,9090258.427820267,9090258.17205425,9090257.911535148,9090257.646174762,9090257.37588326,9090257.10056914,9090256.820139214,9090256.534498578,9090256.24355056,9090255.94719671,9090255.645336742,9090255.33786854,9090255.024688082,9090254.705689432,9090254.38076469,9090254.049803969,9090253.712695345,9090253.369324833,9090253.019576337,9090252.663331628,9090252.300470274,9090251.930869639,9090251.554404812,9090251.170948582,9090250.780371387,9090250.38254128,9090249.977323871,9090249.5645823,9090249.144177172,9090248.715966538,9090248.279805822,9090247.835547779,9090247.383042462,9090246.922137152,9090246.45267632,9090245.974501574,9090245.487451602,9090244.991362121,9090244.486065838,9090243.97139236,9090243.447168168,9090242.913216555,9090242.36935756,9090241.815407917,9090241.251180988,9090240.6764867,9090240.091131499,9090239.494918268,9090238.88764626,9090238.269111065,9090237.639104506,9090236.997414585,9090236.343825419,9090235.678117165,9090235.000065947,9090234.309443785,9090233.606018525,9090232.889553752,9090232.15980873,9090231.416538307,9090230.659492852,9090229.888418155,9090229.103055365,9090228.3031409,9090227.488406345,9090226.658578392,9090225.813378746,9090224.952524008,9090224.075725626,9090223.182689777,9090222.273117272,9090221.346703473,9090220.403138187,9090219.442105565,9090218.463284014,9090217.466346081,9090216.450958358,9090215.416781366,9090214.363469452,9090213.290670691,9090212.198026767,9090211.085172843,9090209.951737467,9090208.797342453,9090207.621602759,9090206.42412636,9090205.204514131,9090203.96235972,9090202.697249435,9090201.40876208,9090200.096468873,9090198.759933263,9090197.398710832,9090196.012349138,9090194.600387584,9090193.162357269,9090191.697780853,9090190.206172395,9090188.687037218,9090187.139871761,9090185.564163407,9090183.959390342,9090182.325021388,9090180.660515849,9090178.96532335,9090177.23888365,9090175.480626505,9090173.689971462,9090171.86632772,9090170.009093931,9090168.117658019,9090166.191397015,9090164.22967686,9090162.231852211,9090160.197266273,9090158.125250587,9090156.015124835,9090153.866196658,9090151.677761432,9090149.449102083,9090147.179488858,9090144.868179142,9090142.514417214,9090140.117434058,9090137.676447133,9090135.190660138,9090132.65926281,9090130.081430674,9090127.456324825,9090124.78309169,9090122.06086279,9090119.288754495,9090116.465867776,9090113.591287982,9090110.664084544,9090107.683310788,9090104.648003608,9090101.557183247,9090098.409853028,9090095.204999086,9090091.94159009,9090088.618576976,9090085.234892685,9090081.789451856,9090078.281150576,9090074.708866064,9090071.071456406,9090067.367760254,9090063.596596528,9090059.756764138,9090055.847041655,9090051.866187032,9090047.812937286,9090043.686008187,9090039.48409396,9090035.205866963,9090030.849977361,9090026.415052824,9090021.899698194,9090017.302495155,9090012.622001925,9090007.85675291,9090003.005258381,9089998.066004131,9089993.03745116,9089987.91803531,9089982.70616696,9089977.400230644,9089971.998584751,9089966.499561144,9089960.90146486,9089955.202573719,9089949.401137998,9089943.495380083,9089937.483494146,9089931.363645747,9089925.13397153,9089918.79257885,9089912.337545449,9089905.766919086,9089899.078717215,9089892.270926626,9089885.341503104,9089878.288371103,9089871.109423388,9089863.802520713,9089856.36549149,9089848.79613145,9089841.092203338,9089833.251436554,9089825.271526882,9089817.150136147,9089808.884891925,9089800.47338723,9089791.91318024,9089783.201793985,9089774.336716091,9089765.315398503,9089756.135257224,9089746.793672044,9089737.287986336,9089727.615506798,9089717.77350323,9089707.759208355,9089697.569817591,9089687.202488897,9089676.654342605,9089665.922461253,9089655.003889475,9089643.895633874,9089632.59466293,9089621.097906925,9089609.402257884,9089597.504569542,9089585.401657347,9089573.090298457,9089560.567231793,9089547.829158088,9089534.87274002,9089521.694602296,9089508.291331839,9089494.659477953,9089480.795552587,9089466.696030546,9089452.357349824,9089437.775911946,9089422.948082317,9089407.870190682,9089392.538531568,9089376.949364817,9089361.098916154,9089344.983377792,9089328.598909108,9089311.941637395,9089295.007658618,9089277.79303829,9089260.29381238,9089242.505988274,9089224.425545875,9089206.04843868,9089187.370595004,9089168.387919255,9089149.096293284,9089129.491577875,9089109.569614211,9089089.326225568,9089068.757219005,9089047.858387198,9089026.625510365,9089005.054358311,9088983.140692558,9088960.880268645,9088938.268838461,9088915.302152814,9088891.97596402,9088868.28602871,9088844.228110738,9088819.797984237,9088794.991436828,9088769.804273004,9088744.23231761,9088718.2714196,9088691.917455852,9088665.16633524,9088638.014002847,9088610.456444407,9088582.489690896,9088554.109823389,9088525.312978065,9088496.095351482,9088466.453206044,9088436.38287573,9088405.880772015,9088374.94339012,9088343.56731543,9088311.749230245,9088279.485920768,9088246.774284385,9088213.611337233,9088179.994222073,9088145.920216473,9088111.3867413,9088076.391369546,9088040.931835476,9088005.00604417,9087968.612081345,9087931.748223629,9087894.412949134,9087856.604948485,9087818.32313621,9087779.566662533,9087740.334925618,9087700.627584234,9087660.444570841,9087619.786105175,9087578.652708253,9087537.0452169,9087494.964798722,9087452.412967622,9087409.391599823,9087365.902950384,9087321.949670296,9087277.53482411,9087232.661908146,9087187.334869232,9087141.558124116,9087095.336579375,9087048.67565203,9087001.581290735,9086954.059997601,9086906.118850727,9086857.765527312,9086809.008327506,9086759.856198918,9086710.318761826,9086660.406335106,9086610.129962867,9086559.501441823,9086508.533349408,9086457.239072656,9086405.632837806,9086353.72974071,9086301.545778025,9086249.09787916,9086196.40393905,9086143.482851718,9086090.354544654,9086037.040013995,9085983.561360542,9085929.941826608,9085876.20583365,9085822.379020799,9085768.488284158,9085714.561816972,9085660.629150623,9085606.721196432,9085552.870288316,9085499.110226262,9085445.476320602,9085392.005437117,9085338.736042919,9085285.708253164,9085232.963878486,9085180.546473263,9085128.501384573,9085076.875801915,9085025.718807656,9084975.081428139,9084925.01668548,9084875.579650015,9084826.827493338,9084778.819541987,9084731.617331592,9084685.284661647,9084639.887650697,9084595.494791996,9084552.17700957,9084510.007714648,9084469.062862374,9084429.42100882,9084391.163368199,9084354.373870205,9084319.139217505,9084285.548943214,9084253.695468364,9084223.674159305,9084195.583384896,9084169.524573484,9084145.602269586,9084123.924190167,9084104.601280438,9084087.747769121,9084073.48122309,9084061.922601234,9084053.196307559,9084047.430243336,9084044.755858283,9084045.308200605,9084049.225965854,9084056.651544461,9084067.731067901,9084082.614453292,9084101.455446402,9084124.411662936,9084151.644627972,9084183.319813447,9084219.60667361,9084260.678678308,9084306.713343982,9084357.892262304,9084414.401126329,9084476.42975404,9084544.172109202,9084617.826319426,9084697.594691338,9084783.683722714,9084876.304111596,9084975.670762155,9085082.002787367,9085195.52350827,9085316.46044986,9085445.045333486,9085581.514065666,9085726.106723337,9085879.06753541,9086040.644860666,9086211.091161892,9086390.66297629,9086579.620882079,9086778.22946136,9086986.757259216,9087205.476739034,9087434.66423413,9087674.599895686,9087925.56763706,9088187.855074551,9088461.75346465,9088747.557637926,9089045.565929636,9089356.080107156,9089679.405294383,9090015.849893289,9090365.72550275,9090729.34683485,9091107.031628843,9091499.10056302,9091905.877164658,9092327.68771834,9092764.861172874,9093217.729047094,9093686.62533483,9094171.886409346,9094673.850927567,9095192.859734444,9095729.255767727,9096283.383963624,9096855.591163578,9097446.226022664,9098055.638919894,9098684.181870902,9099332.208443372,9100000.07367566,9100688.133998994,9101396.747163706,9102126.272169916,9102877.06920311,9103649.49957505,9104443.925670436,9105260.710899767,9106100.219658826,9106962.817295264,9107848.870082604,9108758.745202182,9109692.810733404,9110651.435652671,9111634.989841413,9112643.84410357,9113678.370192904,9114738.940850455,9115825.929852461,9116939.712069092,9118080.66353419,9119249.16152637,9120445.584661648,9121670.312997822,9122923.728150796,9124206.213422999,9125518.153944004,9126859.936823457,9128231.951316375,9129634.58900083,9131068.24396804,9132533.313024808,9134030.19590823,9135559.29551258,9137121.018128242,9138715.77369248,9140343.976051837,9142006.043235939,9143702.397742387,9145433.46683242,9147199.68283702,9149001.483473025,9150839.31216884,9152713.61839931,9154624.858029205,9156573.493664864,9158559.995013345,9160584.839248609,9162648.511384007,9164751.504650503,9166894.32087995,9169077.47089266,9171301.474888654,9173566.862841737,9175874.17489574,9178223.961762054,9180616.785117729,9183053.218003273,9185533.845219376,9188059.263721645,9190630.083012585,9193246.92552991,9195910.427030323,9198621.236967953,9201380.01886649,9204187.450684233,9207044.225171106,9209951.050216842,9212908.649189431,9215917.761262964,9218979.141734073,9222093.562326094,9225261.811480109,9228484.694632107,9231763.034475425,9235097.671207653,9238489.462761326,9241939.285017563,9245448.032001965,9249016.616062053,9252645.968025576,9256337.037338976,9260090.792185418,9263908.219581753,9267790.325453779,9271738.134689325,9275752.691168496,9279835.057770688,9283986.316357795,9288207.567733206,9292499.931576122,9296864.54635086,9301302.569190653,9305815.175755773,9310403.560065502,9315068.934303785,9319812.528598245,9324635.590772402,9329539.38607084,9334525.196857218,9339594.322284967,9344748.077940589,9349987.795459492,9355314.822114315,9360730.520375757,9366236.26744592,9371833.454764256,9377523.487486156,9383307.783934373,9389187.775023382,9395164.903656883,9401240.624098675,9407416.401317144,9413693.710303668,9420074.035365252,9426558.869391734,9433149.71309799,9439848.07424154,9446655.466816025,9453573.410221066,9460603.428409003,9467747.049009178,9475005.802430283,9482381.220941486,9489874.837733012,9497488.185956946,9505222.797749002,9513080.2032321,9521061.929502644,9529169.499600375,9537404.431462798,9545768.236865154,9554262.420347037,9562888.478126701,9571647.897004269,9580542.153254997,9589572.711513858,9598741.023652736,9608048.527651595,9617496.646464976,9627086.78688531,9636820.338404499,9646698.672075309,9656723.139374178,9666895.071067015,9677215.776079705,9687686.540375004,9698308.625837546,9709083.269168798,9720011.680793725,9731095.04378102,9742334.512778793,9753731.212967582,9765286.239032622,9777000.65415727,9788875.489039546,9800911.740933739,9813110.372718997,9825472.311996838,9837998.45021955,9850689.641851323,9863546.70356409,9876570.413469838,9889761.510391282,9903120.69317268,9916648.620032474,9930345.907959502,9944213.132154351,9958250.8255174,9972459.478185087,9986839.53711568,10001391.405725982,10016115.443580061,10031011.966131188,10046081.244517922,10061323.505415292,10076738.930941744,10092327.658622567,10108089.781410253,10124025.347762205,10140134.361775944,10156416.783381995,10172872.528594289,10189501.469817948,10206303.43621401,10223278.214120634,10240425.547530035,10257745.138620349,10275236.648341414,10292899.697053285,10310733.86521617,10328738.694130337,10346913.686724273,10365258.308389395,10383771.987859292,10402454.118131462,10421304.057429265,10440321.13020172,10459504.62815866,10478853.811338566,10498367.909206327,10518046.121778052,10537887.620769894,10557891.550767867,10578057.030415352,10598383.153615102,10618868.990742346,10639513.589865535,10660315.977971256,10681275.162189793,10702390.131017689,10723659.855533719,10745083.290604603,10766659.376076853,10788387.037950987,10810265.189534567,10832292.732570361,10854468.558336016,10876791.54871169,10899260.577212125,10921874.509979714,10944632.206735099,10967532.5216821,10990574.304363698,11013756.400465919,11037077.652566735,11060536.90082697,11084132.983620564,11107864.738101546,11131731.000705224,11155730.607581422,11179862.394957483,11204125.199429207,11228517.85817792,11253039.209112126,11277688.090932341,11302463.343118032,11327363.805835659,11352388.319767118,11377535.725858163,11402804.864986498,11428194.577549545,11453703.702972185,11479331.079134854,11505075.541722896,11530935.923498029,11556911.053493235,11582999.756132651,11609200.85027816,11635513.148204748,11661935.454506993,11688466.564939145,11715105.265191697,11741850.329607483,11768700.519840663,11795654.5834621,11822711.252515005,11849869.242024839,11877127.248467755,11904483.94820207,11931937.995867409,11959488.022756454,11987132.635164296,12014870.412720673,12042699.906710433,12070619.638387792,12098628.097289996,12126723.739556162,12154904.98625714,12183170.221742352,12211517.792009521,12239946.003103342,12268453.119549125,12297037.362827372,12325696.909895338,12354429.891761476,12383234.392118614,12412108.446041688,12441050.038755605,12470057.104478845,12499127.525348049,12528259.130428858,12557449.694817917,12586696.938840745,12615998.527350137,12645352.06912908,12674755.11640238,12704205.164460456,12733699.65139876,12763235.957975779,12792811.40759227,12822423.266394034,12852068.743500141,12881744.99135818,12911449.106227662,12941178.128792387,12970929.044902053,13000698.786443148,13030484.232338669,13060282.209675778,13090089.494960202,13119902.815495681,13149718.85088646,13179534.234660408,13209345.556009935,13239149.361647546,13268942.157772575,13298720.412145127,13328480.556263218,13358218.98763842,13387932.072165431,13417616.146580368,13447267.52100258,13476882.481554398,13506457.293053117,13535988.201769209,13565471.438244794,13594903.220165983,13624279.755282873,13653597.24437064,13682851.884225274,13712039.87068735,13741157.401687223,13770200.680305103,13799165.91783936,13828049.336876597,13856847.174356932,13885555.684628168,13914171.142482549,13942689.8461699,13971108.120381232,13999422.319196781,14027628.828992998,14055724.07130284,14083704.505624155,14111566.632171024,14139306.994563293,14166922.182449618,14194408.834059691,14221763.63868158,14248983.339060253,14276064.733713862,14303004.679164315,14329800.092079256,14356447.951322656,14382945.299911562,14409289.246876786,14435476.969025671,14461505.712605331,14487372.794864934,14513075.605515976,14538611.608089764,14563978.341191467,14589173.419650529,14614194.53556726,14639039.45925588,14663706.040084371,14688192.207211778,14712495.97022379,14736615.419667676,14760548.727487762,14784294.147362897,14807850.014947552,14831214.748018181,14854386.846526915,14877364.892564496,14900147.550234739,14922733.565442782]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[10901621.750080697,10901621.744581098,10901621.738979165,10901621.733272983,10901621.72746062,10901621.721540097,10901621.7155094,10901621.709366482,10901621.703109253,10901621.696735587,10901621.690243313,10901621.683630228,10901621.67689409,10901621.670032598,10901621.663043428,10901621.6559242,10901621.648672495,10901621.641285848,10901621.63376175,10901621.626097646,10901621.618290918,10901621.61033893,10901621.60223897,10901621.59398828,10901621.585584065,10901621.577023467,10901621.568303572,10901621.55942142,10901621.550373986,10901621.54115821,10901621.531770943,10901621.522209,10901621.512469135,10901621.502548032,10901621.49244232,10901621.482148567,10901621.47166328,10901621.460982883,10901621.450103756,10901621.439022198,10901621.42773444,10901621.416236648,10901621.404524917,10901621.392595265,10901621.380443644,10901621.368065909,10901621.355457872,10901621.342615236,10901621.329533638,10901621.316208636,10901621.3026357,10901621.288810221,10901621.2747275,10901621.260382744,10901621.245771088,10901621.230887566,10901621.215727115,10901621.200284591,10901621.184554731,10901621.168532215,10901621.152211577,10901621.135587286,10901621.118653683,10901621.101405019,10901621.083835438,10901621.065938966,10901621.04770952,10901621.029140914,10901621.01022683,10901620.990960848,10901620.971336428,10901620.951346887,10901620.930985453,10901620.910245195,10901620.889119074,10901620.867599906,10901620.845680391,10901620.823353078,10901620.800610382,10901620.777444577,10901620.7538478,10901620.729812028,10901620.705329102,10901620.680390697,10901620.65498836,10901620.62911345,10901620.602757182,10901620.575910602,10901620.5485646,10901620.520709882,10901620.492336987,10901620.463436283,10901620.433997953,10901620.404012008,10901620.373468257,10901620.342356328,10901620.31066566,10901620.27838549,10901620.245504858,10901620.212012604,10901620.177897349,10901620.143147517,10901620.107751306,10901620.071696697,10901620.034971455,10901619.997563109,10901619.95945896,10901619.920646075,10901619.881111277,10901619.840841144,10901619.799822014,10901619.75803996,10901619.7154808,10901619.67213009,10901619.627973115,10901619.582994891,10901619.537180157,10901619.49051336,10901619.44297867,10901619.394559955,10901619.345240783,10901619.295004422,10901619.243833827,10901619.191711638,10901619.138620172,10901619.084541414,10901619.02945703,10901618.973348318,10901618.916196253,10901618.857981455,10901618.79868417,10901618.738284294,10901618.676761338,10901618.61409444,10901618.550262349,10901618.48524342,10901618.419015603,10901618.35155645,10901618.282843087,10901618.212852217,10901618.141560106,10901618.068942599,10901617.994975073,10901617.91963246,10901617.842889227,10901617.764719363,10901617.685096374,10901617.603993282,10901617.521382598,10901617.437236346,10901617.351526001,10901617.26422254,10901617.175296387,10901617.084717404,10901616.992454927,10901616.898477705,10901616.802753909,10901616.705251127,10901616.605936335,10901616.504775919,10901616.401735624,10901616.296780568,10901616.189875226,10901616.080983406,10901615.970068263,10901615.85709226,10901615.74201716,10901615.624804035,10901615.505413227,10901615.383804347,10901615.259936254,10901615.133767061,10901615.00525409,10901614.874353882,10901614.741022188,10901614.60521392,10901614.46688317,10901614.325983178,10901614.182466323,10901614.0362841,10901613.887387117,10901613.735725068,10901613.581246717,10901613.423899874,10901613.263631416,10901613.1003872,10901612.93411211,10901612.764750011,10901612.592243733,10901612.41653505,10901612.23756466,10901612.055272173,10901611.869596088,10901611.680473771,10901611.487841431,10901611.29163411,10901611.091785645,10901610.888228659,10901610.68089454,10901610.469713418,10901610.254614128,10901610.035524188,10901609.812369816,10901609.585075846,10901609.35356574,10901609.117761556,10901608.877583925,10901608.632952007,10901608.38378349,10901608.129994553,10901607.871499836,10901607.608212404,10901607.340043744,10901607.066903709,10901606.788700508,10901606.505340662,10901606.216728993,10901605.922768565,10901605.623360677,10901605.31840482,10901605.007798653,10901604.69143796,10901604.369216619,10901604.041026574,10901603.706757786,10901603.366298223,10901603.019533796,10901602.66634834,10901602.306623572,10901601.940239059,10901601.567072157,10901601.18699801,10901600.799889468,10901600.405617082,10901600.004049048,10901599.59505116,10901599.178486781,10901598.754216777,10901598.32209951,10901597.88199075,10901597.433743663,10901596.977208758,10901596.512233824,10901596.038663898,10901595.556341214,10901595.065105148,10901594.564792177,10901594.055235822,10901593.53626659,10901593.007711938,10901592.469396207,10901591.921140574,10901591.362762984,10901590.79407812,10901590.214897314,10901589.625028517,10901589.024276223,10901588.412441425,10901587.789321523,10901587.15471031,10901586.508397866,10901585.850170523,10901585.179810777,10901584.497097244,10901583.801804591,10901583.093703445,10901582.372560358,10901581.638137717,10901580.890193675,10901580.128482081,10901579.352752415,10901578.56274971,10901577.758214463,10901576.93888258,10901576.10448529,10901575.254749067,10901574.389395554,10901573.508141467,10901572.610698538,10901571.696773423,10901570.766067602,10901569.818277322,10901568.853093483,10901567.870201575,10901566.869281575,10901565.85000786,10901564.812049117,10901563.755068256,10901562.678722307,10901561.582662323,10901560.466533309,10901559.329974098,10901558.172617264,10901556.994089017,10901555.794009106,10901554.571990736,10901553.327640425,10901552.06055794,10901550.770336157,10901549.456560982,10901548.11881124,10901546.75665854,10901545.3696672,10901543.957394114,10901542.519388646,10901541.05519251,10901539.564339675,10901538.046356224,10901536.50076025,10901534.92706175,10901533.324762478,10901531.693355847,10901530.032326803,10901528.341151692,10901526.619298158,10901524.866225012,10901523.081382092,10901521.264210152,10901519.414140737,10901517.530596046,10901515.612988815,10901513.660722189,10901511.673189575,10901509.649774533,10901507.589850636,10901505.49278134,10901503.357919864,10901501.184609042,10901498.972181195,10901496.71995801,10901494.427250398,10901492.093358375,10901489.717570905,10901487.299165798,10901484.837409556,10901482.331557266,10901479.780852435,10901477.184526902,10901474.54180067,10901471.851881804,10901469.113966294,10901466.327237934,10901463.490868188,10901460.604016077,10901457.665828042,10901454.675437847,10901451.631966444,10901448.534521867,10901445.382199101,10901442.174080005,10901438.909233153,10901435.586713778,10901432.205563653,10901428.764810981,10901425.2634703,10901421.70054242,10901418.075014314,10901414.385859031,10901410.632035626,10901406.812489104,10901402.926150318,10901398.97193596,10901394.94874845,10901390.855475925,10901386.690992178,10901382.454156661,10901378.143814415,10901373.758796073,10901369.297917854,10901364.759981563,10901360.143774597,10901355.448069971,10901350.671626354,10901345.81318809,10901340.871485293,10901335.84523387,10901330.733135644,10901325.53387842,10901320.246136103,10901314.86856884,10901309.399823125,10901303.838532,10901298.183315203,10901292.432779368,10901286.585518245,10901280.640112935,10901274.595132133,10901268.449132422,10901262.20065857,10901255.848243855,10901249.3904104,10901242.825669594,10901236.152522447,10901229.369460057,10901222.47496407,10901215.467507165,10901208.3455536,10901201.107559774,10901193.751974812,10901186.277241217,10901178.681795541,10901170.964069115,10901163.122488793,10901155.155477757,10901147.061456379,10901138.838843115,10901130.486055447,10901122.001510894,10901113.383628044,10901104.6308277,10901095.741534015,10901086.714175742,10901077.547187507,10901068.239011206,10901058.788097376,10901049.192906741,10901039.45191177,10901029.563598301,10901019.526467312,10901009.339036692,10900998.999843154,10900988.507444238,10900977.860420348,10900967.057376955,10900956.09694687,10900944.977792611,10900933.698608885,10900922.258125192,10900910.655108523,10900898.888366204,10900886.956748841,10900874.859153394,10900862.594526395,10900850.161867304,10900837.560232013,10900824.78873645,10900811.846560417,10900798.732951514,10900785.447229268,10900771.988789413,10900758.357108349,10900744.551747793,10900730.572359605,10900716.418690799,10900702.090588793,10900687.588006813,10900672.91100956,10900658.059779067,10900643.034620797,10900627.835969983,10900612.464398202,10900596.920620184,10900581.205500938,10900565.320063086,10900549.265494509,10900533.04315625,10900516.654590767,10900500.101530414,10900483.385906318,10900466.509857504,10900449.475740407,10900432.28613871,10900414.943873532,10900397.45201398,10900379.813888097,10900362.033094171,10900344.113512464,10900326.059317334,10900307.874989796,10900289.565330504,10900271.135473171,10900252.590898503,10900233.93744852,10900215.181341438,10900196.329186989,10900177.388002323,10900158.365228359,10900139.26874674,10900120.106897283,10900100.888496075,10900081.622854073,10900062.319796346,10900042.989681918,10900023.643424252,10900004.292512357,10899984.949032566,10899965.625690993,10899946.33583667,10899927.093485396,10899907.913344296,10899888.810837165,10899869.802130487,10899850.904160304,10899832.134659808,10899813.512187801,10899795.05615789,10899776.78686863,10899758.725534396,10899740.894317212,10899723.316359423,10899706.015817285,10899689.017895455,10899672.348882414,10899656.036186855,10899640.108374981,10899624.595208844,10899609.527685627,10899594.93807793,10899580.859975088,10899567.328325508,10899554.379480058,10899542.051236473,10899530.38288486,10899519.41525426,10899509.190760296,10899499.753453888,10899491.149071114,10899483.425084116,10899476.630753158,10899470.817179777,10899466.03736107,10899462.346245067,10899459.800787264,10899458.460008243,10899458.385052422,10899459.639247924,10899462.28816753,10899466.399690766,10899472.044067053,10899479.29397996,10899488.224612508,10899498.91371353,10899511.44166509,10899525.89155089,10899542.349225692,10899560.90338571,10899581.645639937,10899604.67058242,10899630.07586539,10899657.96227327,10899688.433797479,10899721.59771202,10899757.564649843,10899796.448679809,10899838.367384393,10899883.441937912,10899931.79718532,10899983.56172146,10900038.867970772,10900097.85226728,10900160.654934952,10900227.420368237,10900298.2971127,10900373.437945807,10900452.999957625,10900537.14463144,10900626.03792421,10900719.850346712,10900818.757043317,10900922.937871333,10901032.57747974,10901147.86538727,10901268.99605972,10901396.168986391,10901529.588755505,10901669.465128541,10901816.01311335,10901969.45303594,10902130.010610793,10902297.917009618,10902473.408928411,10902656.72865272,10902848.12412092,10903047.848985504,10903256.162672129,10903473.330436388,10903699.623418171,10903935.318693444,10904180.699323427,10904436.054400919,10904701.679093804,10904977.874685526,10905264.948612459,10905563.21449807,10905872.992183793,10906194.607756497,10906528.393572452,10906874.688277785,10907233.836825244,10907606.190487323,10907992.10686558,10908391.949896183,10908806.089851648,10909234.903338628,10909678.773291927,10910138.088964542,10910613.24591389,10911104.645984152,10911612.697284821,10912137.81416544,10912680.41718666,10913240.933087708,10913819.794750249,10914417.441158881,10915034.317358332,10915670.874407493,10916327.569330497,10917004.865064988,10917703.230407776,10918423.139958154,10919165.074059043,10919929.518736254,10920716.965636184,10921527.911962174,10922362.860409878,10923222.319101997,10924106.801522695,10925016.826452095,10925952.917901251,10926915.605047988,10927905.422174055,10928922.908604017,10929968.60864637,10931043.071537353,10932146.851387948,10933280.507134546,10934444.602493864,10935639.705922572,10936866.390582254,10938125.23431016,10939416.819596404,10940741.733568124,10942100.567981195,10943493.919220058,10944922.38830629,10946386.580916425,10947887.1074097,10949424.582866222,10950999.627136188,10952612.864900699,10954264.92574473,10955956.444242854,10957688.060058177,10959460.41805509,10961274.168426331,10963129.966834815,10965028.47457076,10966970.358724525,10968956.29237564,10970986.95479839,10973063.031684358,10975185.215382311,10977354.2051557,10979570.707458114,10981835.436226958,10984149.113195557,10986512.468223896,10988926.239648182,10991391.17464933,10993908.029640459,10996477.57067346,10999100.57386466,11001777.825839514,11004510.124196332,11007298.277988825,11010143.108227413,11013045.448399084,11016006.145005545,11019026.058119433,11022106.061958237,11025247.045475636,11028449.912969783,11031715.58470822,11035044.997568835,11038439.105696449,11041898.881174477,11045425.314711029,11049019.416338965,11052682.216129102,11056414.764916075,11060218.135036001,11064093.421075307,11068041.740629934,11072064.235074116,11076162.070337951,11080336.437692896,11084588.554544395,11088919.665230652,11093331.041826779,11097823.9849533,11102399.824588157,11107059.920881204,11111805.664970275,11116638.479797848,11121559.820927259,11126571.177357579,11131674.07233601,11136870.0641669,11142160.747016247,11147547.751710758,11153032.746530307,11158617.43799284,11164303.57163056,11170092.932756444,11175987.347219877,11181988.682150425,11188098.846688613,11194319.792702537,11200653.51548931,11207102.054460129,11213667.493807815,11220351.963155726,11227157.638186835,11234086.741251752,11241141.541954573,11248324.35771523,11255637.554307127,11263083.546368854,11270664.797888527,11278383.822659604,11286243.18470669,11294245.498679982,11302393.430216935,11310689.696269713,11319137.065396834,11327738.35801759,11336496.446627568,11345414.25597369,11354494.763187053,11363740.997871941,11373156.042149123,11382743.030651687,11392505.150471544,11402445.641054645,11412567.794042964,11422874.953061206,11433370.51344619,11444057.921916738,11454940.676181989,11466022.324485775,11477306.465084968,11488796.745659374,11500496.862650884,11512410.56052953,11524541.630984012,11536893.912034366,11549471.28706424,11562277.683770433,11575317.073027253,11588593.467663262,11602110.921148049,11615873.526186641,11629885.413219247,11644150.748824155,11658673.734021466,11673458.60247568,11688509.618595097,11703831.07552614,11719427.29304087,11735302.615316069,11751461.408602482,11767908.058782926,11784646.968818232,11801682.55608018,11819019.24957082,11836661.487027828,11854613.711915826,11872880.370303907,11891465.907629818,11910374.765351726,11929611.377488714,11949180.167051531,11969085.542365573,11989331.89328831,12009923.587323904,12030864.965638101,12052160.338976888,12073813.983492846,12095830.136483576,12118212.992046941,12140966.696658347,12164095.344675712,12187602.973778144,12211493.560344819,12235771.014780957,12260439.176798122,12285501.810656514,12310962.60037728,12336825.144933224,12363092.953426503,12389769.440262396,12416857.920328321,12444361.604187584,12472283.593297629,12500626.87526254,12529394.319129888,12558588.670742046,12588212.548152033,12618268.437114265,12648758.686660226,12679685.5047693,12711050.95414477,12742856.948104821,12775105.246598369,12807797.45235508,12840935.007178904,12874519.188393971,12908551.105451524,12943031.696705995,12977961.726368101,13013341.781642258,13049172.270055236,13085453.416982397,13122185.263377383,13159367.663710501,13197000.284120616,13235082.600784576,13273613.898507811,13312593.26953892,13352019.6126106,13391891.632208593,13432207.83806964,13472966.5449089,13514165.872376638,13555803.74524329,13597877.893811595,13640385.854553638,13683324.970970385,13726692.394670408,13770485.086664224,13814699.81887002,13859333.17582608,13904381.556604808,13949841.176922727,13995708.07144049,14041978.096246593,14088646.931517998,14135710.084350765,14183162.891753338,14231000.52379494,14279217.986901369,14327810.127290254,14376771.634537661,14426097.045267912,14475780.746958205,14525816.981849924,14576199.85095796,14626923.318170022,14677981.214427339,14729367.241978623,14781074.978699043,14833097.882466175,14885429.295584954,14938062.449253788,14990990.468064275,15044206.37452705,15097703.093616456,15151473.457327176,15205510.209235875,15259806.009061422,15314353.437217385,15369144.999350693,15424173.130860921,15479430.20139448,15534908.519308705,15590600.336100858,15646497.85079747,15702593.214299656,15758878.533680394,15815345.876430035,15871987.274646478,15928794.72916698,15985760.21363857,16042875.678524414,16100133.05504391,16157524.25904415,16215041.194801174,16272675.75874916,16330419.84313624,16388265.339605888,16446204.14270276,16504228.153302412,16562329.28196433,16620499.452207845,16678730.603710927,16737014.695431758,16795343.708653353,16853709.649951488,16912104.554086454,16970520.486819237,17028949.54765287,17087383.872499794,17145815.636276178,17204237.055424273,17262640.390363906,17321017.947874382,17379362.08340801,17437665.20333671,17495919.767132994,17554118.28948684,17612253.34235993,17670317.55697883,17728303.62576851,17786204.30422795,17844012.412749298,17901720.83838212,17959322.536544472,18016810.532682173,18074177.923877925,18131417.88041185,18188523.647274777,18245488.545636,18302305.974266626,18358969.410920233,18415472.41367192,18471808.62221707,18527971.759131335,18583955.63109259,18639754.130066313,18695361.234455258,18750771.010214467,18805977.61193252,18860975.28387984,18915758.361024786,18970321.270018283,19024658.530147493,19078764.754259102,19132634.649652712,19186263.01894453,19239644.760901764,19292774.871247914,19345648.44343901,19398260.66941087,19450606.84029732,19502682.347119253,19554482.68144444,19606003.436017647,19657240.305360902,19708189.086343464,19758845.678721003,19809206.085643653,19859266.414132144,19909022.875521656,19958471.785872605,20007609.566347744,20056432.743554924,20104937.949854687,20153121.92363201,20200981.509531483,20248513.65865501,20295715.428721435,20342583.984187216,20389116.596327398,20435310.643276233,20481163.61002653,20526673.088387303,20571836.77689881,20616652.480704486,20661118.111379154,20705231.68671294,20748991.330450423,20792395.27198458,20835441.84600516,20878129.49210105,20920456.754316602,20962422.280661475,21004024.82257409,21045263.234338485,21086136.472454697,21126643.594962765,21166783.76072046,21206556.22863513,21245960.35684981,21284995.601884257,21323661.51773115,21361957.754908178,21399884.059466656,21437440.271957327,21474626.326354194,21511442.24893719,21547888.157134734,21583964.25832711,21619670.848611675,21655008.311531175,21689977.11676623,21724577.818793375,21758811.055509787,21792677.546826206,21826178.093229365,21859313.5743154,21892084.94729572,21924493.245476853,21956539.576715745,21988225.121852245,22019551.133120228,22050518.932538956,22081129.91028652,22111385.52305667,22141287.292401023,22170836.803058043]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[7278921.798425067,7278921.788791445,7278921.778978545,7278921.768983031,7278921.758801499,7278921.74843049,7278921.737866486,7278921.727105885,7278921.716145031,7278921.704980202,7278921.693607593,7278921.682023343,7278921.670223521,7278921.658204105,7278921.645961009,7278921.633490076,7278921.620787058,7278921.607847646,7278921.594667433,7278921.581241944,7278921.567566609,7278921.553636782,7278921.53944773,7278921.524994619,7278921.510272542,7278921.495276498,7278921.480001384,7278921.4644420035,7278921.448593073,7278921.432449204,7278921.416004899,7278921.399254578,7278921.382192537,7278921.364812985,7278921.347110008,7278921.329077586,7278921.310709591,7278921.2919997815,7278921.272941795,7278921.25352915,7278921.23375524,7278921.2136133555,7278921.193096642,7278921.172198127,7278921.150910705,7278921.129227135,7278921.107140057,7278921.084641949,7278921.061725168,7278921.03838192,7278921.014604282,7278920.990384156,7278920.965713309,7278920.940583365,7278920.914985766,7278920.888911828,7278920.862352667,7278920.83529927,7278920.807742434,7278920.77967279,7278920.751080795,7278920.721956734,7278920.692290703,7278920.662072616,7278920.6312922025,7278920.599939009,7278920.568002354,7278920.535471396,7278920.502335072,7278920.4685821235,7278920.434201078,7278920.399180232,7278920.363507705,7278920.327171348,7278920.290158822,7278920.252457538,7278920.214054691,7278920.174937215,7278920.135091823,7278920.094504962,7278920.053162839,7278920.011051403,7278919.968156338,7278919.924463059,7278919.879956719,7278919.834622192,7278919.7884440655,7278919.741406642,7278919.693493935,7278919.6446896475,7278919.594977209,7278919.544339713,7278919.492759945,7278919.44022038,7278919.386703153,7278919.332190082,7278919.27666263,7278919.22010193,7278919.162488749,7278919.103803519,7278919.044026278,7278918.983136728,7278918.921114159,7278918.85793749,7278918.793585258,7278918.728035587,7278918.661266198,7278918.593254402,7278918.523977081,7278918.453410692,7278918.38153125,7278918.308314336,7278918.233735051,7278918.157768063,7278918.08038755,7278918.001567209,7278917.921280263,7278917.83949942,7278917.756196903,7278917.671344386,7278917.584913041,7278917.496873496,7278917.407195834,7278917.31584958,7278917.2228037,7278917.12802657,7278917.03148598,7278916.933149131,7278916.832982611,7278916.730952379,7278916.627023769,7278916.521161467,7278916.413329506,7278916.303491242,7278916.191609355,7278916.077645836,7278915.961561961,7278915.843318294,7278915.722874661,7278915.600190134,7278915.475223033,7278915.347930908,7278915.218270509,7278915.0861977935,7278914.951667889,7278914.814635087,7278914.675052847,7278914.532873743,7278914.3880494805,7278914.240530864,7278914.090267778,7278913.937209188,7278913.781303106,7278913.622496556,7278913.460735619,7278913.295965342,7278913.12812976,7278912.957171869,7278912.783033603,7278912.605655831,7278912.424978299,7278912.240939644,7278912.053477374,7278911.86252782,7278911.668026137,7278911.469906276,7278911.268100967,7278911.062541667,7278910.853158592,7278910.639880635,7278910.422635376,7278910.201349066,7278909.975946553,7278909.746351319,7278909.512485419,7278909.274269448,7278909.031622536,7278908.784462303,7278908.532704851,7278908.276264721,7278908.015054857,7278907.748986588,7278907.4779696185,7278907.201911948,7278906.920719886,7278906.634297993,7278906.342549063,7278906.045374084,7278905.742672202,7278905.434340696,7278905.120274942,7278904.800368362,7278904.474512411,7278904.142596525,7278903.804508094,7278903.460132412,7278903.109352635,7278902.75204977,7278902.388102615,7278902.017387701,7278901.639779293,7278901.255149296,7278900.863367263,7278900.464300318,7278900.057813124,7278899.643767822,7278899.222024012,7278898.792438684,7278898.354866177,7278897.909158121,7278897.45516342,7278896.992728144,7278896.521695534,7278896.041905915,7278895.5531966565,7278895.055402111,7278894.548353568,7278894.031879177,7278893.505803922,7278892.969949525,7278892.424134422,7278891.868173684,7278891.301878938,7278890.725058351,7278890.137516518,7278889.539054419,7278888.929469352,7278888.308554864,7278887.676100669,7278887.031892618,7278886.375712561,7278885.707338326,7278885.026543643,7278884.333098037,7278883.626766785,7278882.907310816,7278882.174486621,7278881.428046221,7278880.667737026,7278879.89330177,7278879.104478467,7278878.301000254,7278877.482595348,7278876.64898694,7278875.7998931175,7278874.935026745,7278874.054095393,7278873.1568012275,7278872.242840914,7278871.311905523,7278870.363680407,7278869.39784513,7278868.414073326,7278867.412032612,7278866.391384485,7278865.351784174,7278864.292880562,7278863.214316067,7278862.115726493,7278860.996740942,7278859.856981686,7278858.696064022,7278857.513596171,7278856.309179128,7278855.08240654,7278853.8328645835,7278852.560131805,7278851.2637789985,7278849.943369057,7278848.598456822,7278847.228588967,7278845.833303824,7278844.412131224,7278842.964592354,7278841.490199615,7278839.98845645,7278838.4588571675,7278836.900886795,7278835.314020903,7278833.697725447,7278832.051456559,7278830.374660426,7278828.666773043,7278826.927220088,7278825.155416701,7278823.350767309,7278821.512665421,7278819.640493451,7278817.733622484,7278815.791412113,7278813.813210214,7278811.798352716,7278809.746163421,7278807.65595376,7278805.527022591,7278803.358655963,7278801.150126874,7278798.90069507,7278796.609606767,7278794.276094457,7278791.899376622,7278789.478657499,7278787.013126831,7278784.501959605,7278781.944315776,7278779.339340036,7278776.686161499,7278773.983893448,7278771.231633066,7278768.428461113,7278765.573441689,7278762.665621869,7278759.704031483,7278756.687682736,7278753.615569958,7278750.486669254,7278747.299938184,7278744.054315468,7278740.7487206105,7278737.382053604,7278733.953194566,7278730.461003394,7278726.904319415,7278723.28196102,7278719.592725327,7278715.83538775,7278712.008701711,7278708.1113981735,7278704.142185299,7278700.099748047,7278695.982747774,7278691.789821804,7278687.519583046,7278683.1706195725,7278678.7414941555,7278674.230743885,7278669.636879692,7278664.958385911,7278660.193719838,7278655.341311252,7278650.399561983,7278645.366845377,7278640.241505876,7278635.021858496,7278629.706188333,7278624.2927500745,7278618.779767481,7278613.1654328555,7278607.447906545,7278601.625316383,7278595.695757159,7278589.657290073,7278583.507942166,7278577.245705782,7278570.868537962,7278564.374359897,7278557.761056306,7278551.02647489,7278544.168425662,7278537.184680399,7278530.07297197,7278522.830993762,7278515.456398987,7278507.946800072,7278500.299767988,7278492.512831629,7278484.58347708,7278476.509146985,7278468.287239846,7278459.9151093345,7278451.390063574,7278442.70936446,7278433.870226899,7278424.869818118,7278415.705256914,7278406.373612904,7278396.871905781,7278387.197104561,7278377.346126799,7278367.315837836,7278357.103049981,7278346.704521764,7278336.116957091,7278325.337004484,7278314.361256215,7278303.186247544,7278291.808455839,7278280.224299761,7278268.430138437,7278256.422270593,7278244.196933686,7278231.750303079,7278219.0784911495,7278206.177546403,7278193.043452641,7278179.672128017,7278166.059424195,7278152.2011254355,7278138.092947694,7278123.730537733,7278109.109472208,7278094.225256747,7278079.073325057,7278063.649038011,7278047.947682705,7278031.964471579,7278015.694541467,7277999.132952692,7277982.274688132,7277965.1146523375,7277947.647670576,7277929.868487936,7277911.771768398,7277893.3520939695,7277874.603963715,7277855.521792906,7277836.099912122,7277816.332566331,7277796.213914052,7277775.738026445,7277754.898886479,7277733.69038807,7277712.106335236,7277690.14044126,7277667.786327919,7277645.037524624,7277621.887467695,7277598.329499566,7277574.356868024,7277549.962725546,7277525.1401285175,7277499.882036616,7277474.181312113,7277448.0307192635,7277421.422923736,7277394.350491973,7277366.805890719,7277338.781486496,7277310.269545129,7277281.262231316,7277251.751608272,7277221.729637323,7277191.188177684,7277160.118986123,7277128.5137168355,7277096.363921225,7277063.661047859,7277030.396442411,7276996.561347676,7276962.146903673,7276927.144147806,7276891.544015036,7276855.337338261,7276818.514848618,7276781.067175971,7276742.984849444,7276704.258298047,7276664.877851378,7276624.83374046,7276584.116098626,7276542.714962557,7276500.620273379,7276457.821877927,7276414.30953005,7276370.072892142,7276325.1015366865,7276279.384948026,7276232.912524203,7276185.673578973,7276137.657343963,7276088.8529709745,7276039.249534443,7275988.836034078,7275937.601397654,7275885.534483964,7275832.624086017,7275778.858934332,7275724.227700518,7275668.719000984,7275612.321400896,7275555.023418348,7275496.81352872,7275437.680169318,7275377.611744216,7275316.596629325,7275254.623177784,7275191.679725514,7275127.754597128,7275062.836112049,7274996.912590949,7274929.97236248,7274862.003770281,7274792.995180288,7274722.934988413,7274651.811628491,7274579.613580573,7274506.329379601,7274431.947624353,7274356.456986851,7274279.846222047,7274202.104177918,7274123.219805999,7274043.182172208,7273961.980468158,7273879.604022854,7273796.042314806,7273711.284984585,7273625.321847804,7273538.142908558,7273449.7383733075,7273360.098665255,7273269.214439139,7273177.076596562,7273083.67630179,7272989.004998025,7272893.05442421,7272795.816632321,7272697.284005193,7272597.449274832,7272496.305541307,7272393.846292146,7272290.065422231,7272184.957254333,7272078.516560072,7271970.738581522,7271861.619053323,7271751.154225333,7271639.3408858655,7271526.176385471,7271411.6586612435,7271295.786261725,7271178.5583723085,7271059.974841236,7270940.036206082,7270818.743720832,7270696.099383436,7270572.105963893,7270446.767032893,7270320.086990886,7270192.071097691,7270062.725502551,7269932.057274656,7269800.074434132,7269666.785983377,7269532.201938901,7269396.333363482,7269259.192398671,7269120.792297678,7268981.147458524,7268840.273457467,7268698.187082687,7268554.906368161,7268410.450627711,7268264.840489203,7268118.097928803,7267970.246305289,7267821.310394402,7267671.316423079,7267520.292103651,7267368.266667839,7267215.270900594,7267061.337173604,7266906.499478523,7266750.793459789,7266594.256446964,7266436.927486576,7266278.847373322,7266120.058680627,7265960.605790418,7265800.534922089,7265639.89416051,7265478.733483084,7265317.104785661,7265155.061907299,7264992.660653745,7264829.958819555,7264667.016208722,7264503.894653774,7264340.658033188,7264177.372287044,7264014.105430804,7263850.927567132,7263687.9108956205,7263525.129720333,7263362.660455059,7263200.581626179,7263038.973872977,7262877.919945408,7262717.504699067,7262557.81508741,7262398.940150959,7262240.971003535,7262084.0008153245,7261928.124792703,7261773.440154747,7261620.046106278,7261468.043807444,7261317.536339631,7261168.628667757,7261021.4275987195,7260876.04173606,7260732.581430724,7260591.158727819,7260451.887309378,7260314.88243304,7260180.260866628,7260048.140818605,7259918.641864313,7259791.884868076,7259667.991901119,7259547.08615527,7259429.291852511,7259314.734150392,7259203.539043327,7259095.833259822,7258991.744155689,7258891.3996033445,7258794.927877221,7258702.457535427,7258614.117297759,7258530.035920132,7258450.342065643,7258375.164172322,7258304.630317781,7258238.868080939,7258178.004400908,7258122.1654333845,7258071.476404582,7258026.061463075,7257986.043529628,7257951.544145399,7257922.683318618,7257899.579370126,7257882.348777929,7257871.106021122,7257865.963423406,7257867.03099652,7257874.416283879,7257888.224204685,7257908.556898834,7257935.513572922,7257969.190347675,7258009.680107032,7258057.0723492745,7258111.453040478,7258172.904470527,7258241.505112065,7258317.329482613,7258400.448010168,7258490.926902521,7258588.828020565,7258694.208755873,7258807.121912681,7258927.615594627,7259055.733096337,7259191.512800086,7259334.988077697,7259486.187197816,7259645.133238678,7259811.8440064555,7259986.33195929,7260168.6041370025,7260358.662096566,7260556.501853284,7260762.113827634,7260975.482797746,7261196.587857402,7261425.402379414,7261661.893984241,7261906.024513641,7262157.750009139,7262417.020695057,7262683.780965823,7262957.969377217,7263239.518641232,7263528.355624145,7263824.401347382,7264127.570990765,7264437.773897588,7264754.913581142,7265078.887732012,7265409.588225701,7265746.901129966,7266090.706711205,7266440.879439359,7266797.2879905775,7267159.795247084,7267528.2582934555,7267902.528408678,7268282.451053247,7268667.865850595,7269058.606562086,7269454.501054895,7269855.371261973,7270261.033133388,7270671.296578327,7271085.9653969705,7271504.8372015655,7271927.703325964,7272354.348722928,7272784.551848554,7273218.08453309,7273654.711837587,7274094.191895744,7274536.27574034,7274980.70711379,7275427.222262237,7275875.54971277,7276325.4100333415,7276776.515574997,7277228.570196114,7277681.268968379,7278134.297864318,7278587.3334262,7279040.042416262,7279492.081448276,7279943.096600429,7280392.723009797,7280840.584448467,7281286.292881771,7281729.4480089,7282169.636786429,7282606.432935309,7283039.396432005,7283468.072984472,7283891.993493955,7284310.673503436,7284723.612633881,7285130.294009438,7285530.183672862,7285922.729992557,7286307.36306275,7286683.4940983895,7287050.514826533,7287407.796876021,7287754.6911674235,7288090.527305325,7288414.612975101,7288726.233346524,7289024.650486537,7289309.102783779,7289578.80438739,7289832.944662882,7290070.6876678215,7290291.171650275,7290493.5085729705,7290676.783666264,7290840.055013035,7290982.353168727,7291102.680819817,7291200.012484001,7291273.294255492,7291321.443598758,7291343.349194201,7291337.870839101,7291303.839407291,7291240.056870925,7291145.296387751,7291018.302457133,7290857.791148132,7290662.450402806,7290430.940417824,7290161.894107364,7289853.917650128,7289505.591123253,7289115.469225574,7288682.082092712,7288203.9362060875,7287679.515397898,7287107.281953746,7286485.677814477,7285813.12587842,7285088.031405047,7284308.783520687,7283473.756826714,7282581.313110255,7281629.803157141,7280617.568666564,7279542.94426643,7278404.259628151,7277199.84167925,7275928.016911737,7274587.113783896,7273175.465212766,7271691.411154206,7270133.301267123,7268499.497658026,7266788.377701811,7264998.336934254,7263127.792011491,7261175.18373128,7259138.980110778,7257017.679515062,7254809.813830553,7252513.951677137,7250128.701652675,7247652.715603314,7245084.691912916,7242423.37880474,7239667.577648476,7236816.146265499,7233868.002225405,7230822.126126539,7227677.564853596,7224433.434805043,7221088.925083471,7217643.300641874,7214095.905379062,7210446.165177492,7206693.590877032,7202837.781178247,7198878.425469173,7194815.30656949,7190648.303386606,7186377.393478076,7182002.65551528,7177524.271643443,7172942.529733457,7168257.82552115,7163470.664630097,7158581.664474233,7153591.55603693,7148501.185523439,7143311.515884036,7138023.628205257,7132638.722967196,7127158.121164946,7121583.265292539,7115915.720188122,7110157.173739215,7104309.437447301,7098374.446851051,7092354.261807846,7086251.066633377,7080067.170099298,7073805.005289141,7067467.129312707,7061056.2228794405,7054575.089731354,7048026.655936131,7041413.969041142,7034740.197089331,7028008.627497716,7021222.6657995265,7014385.834251,7007501.770303782,7000574.224944014,6993607.060899112,6986604.250713384,6979569.87469341,6972508.118724335,6965423.271958075,6958319.724374473,6951201.964216409,6944074.575299925,6936942.234200345,6929809.707315382,6922681.847806293,6915563.592418065,6908459.958179705,6901376.038985708,6894317.002059755,6887288.0843019225,6880294.588520486,6873341.879549708,6866435.380254984,6859580.567426783,6852782.967565027,6846048.1525556715,6839381.735241283,6832789.364887757,6826276.722549345,6819849.516334427,6813513.476574676,6807274.350900459,6801137.899225557,6795109.888644617,6789196.088246897,6783402.26385023,6777734.172659414,6772197.557853559,6766798.143107083,6761541.627049657,6756433.677670446,6751479.926672467,6746685.963783208,6742057.331027973,6737599.516972711,6733317.950943558,6729217.997230439,6725304.949282638,6721584.023904344,6718060.355458639,6714738.990088633,6711624.879964686,6708722.877566964,6706037.7300128555,6703574.0734388735,6701336.427447,6699329.189625483,6697556.630154347,6696022.886505874,6694731.958250534,6693687.701978734,6692893.826348954,6692353.887272623,6692071.283246203,6692049.250840781,6692290.860359305,6692799.011671528,6693576.430236431,6694625.663321694,6695949.076429473,6697548.849937484,6699426.975963917,6701585.255464421,6704025.295568938,6706748.507165571,6709756.102738449,6713049.094465654,6716628.292583053,6720494.304019037,6724647.531304703,6729088.171763282,6733816.216981999,6738831.452568866,6744133.458196221,6749721.607932059,6755595.070859574,6761752.81198453,6768193.593429424,6774915.975912546,6781918.320509536,6789198.790694044,6796755.354653667,6804585.787876376,6812687.676002223,6821058.4179341905,6829695.229201685,6838595.14556925,6847755.026882782,6857171.561144694,6866841.268809206,6876760.507288131,6886925.475657415,6897332.219553874,6907976.636251546,6918854.479906404,6929961.366958102,6941292.7816769555,6952844.08184431,6964610.504554065,6976587.172123148,6988769.098098469,7001151.193347916,7013728.272222901,7026495.058779912,7039446.19304871,7052576.237334708,7065879.682543396,7079350.954514673,7092984.420355218,7106774.394757305,7120715.1462925095,7134800.903669395,7149025.861944206,7163384.188674219,7177870.0300035635,7192477.5166718885,7207200.769936471,7222033.90739892,7236971.04872803,7252006.321270697,7267133.865543465,7282347.8405974815,7297642.429250334,7313011.843178657,7328450.327865797,7343952.167399377,7359511.689114155,7375123.268075926,7390781.331402754,7406480.362420276,7422214.9046483515,7437979.565616704,7453769.020507683,7469578.015624732,7485401.371685555,7501233.986939379,7517070.840108154,7532906.9931518575,7548737.5938585,7564557.878259777,7580363.172873549,7596148.896774878,7611910.563497404,7627643.782767311,7643344.262072319,7659007.808068454,7674630.32782752]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[9062028.287070712,9062028.29782247,9062028.308774322,9062028.319929998,9062028.33129328,9062028.342868036,9062028.354658201,9062028.366667788,9062028.378900878,9062028.39136163,9062028.404054277,9062028.416983142,9062028.430152616,9062028.443567181,9062028.457231393,9062028.471149905,9062028.48532744,9062028.499768829,9062028.514478974,9062028.529462883,9062028.544725647,9062028.560272459,9062028.576108601,9062028.592239464,9062028.608670525,9062028.625407375,9062028.642455705,9062028.659821311,9062028.677510098,9062028.695528083,9062028.713881388,9062028.732576257,9062028.751619047,9062028.771016229,9062028.790774403,9062028.810900286,9062028.83140072,9062028.852282677,9062028.873553256,9062028.89521969,9062028.91728935,9062028.939769737,9062028.962668492,9062028.985993404,9062029.009752406,9062029.033953577,9062029.05860514,9062029.083715485,9062029.109293146,9062029.135346819,9062029.161885368,9062029.188917814,9062029.216453345,9062029.24450133,9062029.273071306,9062029.302172983,9062029.331816262,9062029.36201122,9062029.39276812,9062029.424097426,9062029.456009796,9062029.488516068,9062029.521627307,9062029.555354763,9062029.589709915,9062029.624704432,9062029.660350226,9062029.69665941,9062029.733644335,9062029.771317573,9062029.809691938,9062029.848780477,9062029.888596484,9062029.929153493,9062029.970465302,9062030.012545953,9062030.055409756,9062030.09907129,9062030.143545404,9062030.188847208,9062030.234992124,9062030.281995833,9062030.32987432,9062030.378643863,9062030.428321056,9062030.47892278,9062030.53046625,9062030.582968991,9062030.636448856,9062030.690924032,9062030.74641304,9062030.802934755,9062030.860508395,9062030.919153534,9062030.978890121,9062031.039738467,9062031.101719264,9062031.16485359,9062031.229162911,9062031.2946691,9062031.361394435,9062031.429361599,9062031.498593712,9062031.569114313,9062031.640947388,9062031.714117363,9062031.788649118,9062031.864568003,9062031.941899834,9062032.02067091,9062032.100908015,9062032.182638438,9062032.265889978,9062032.35069094,9062032.437070165,9062032.52505703,9062032.614681458,9062032.705973923,9062032.79896548,9062032.893687747,9062032.99017294,9062033.088453872,9062033.188563969,9062033.290537275,9062033.39440847,9062033.50021288,9062033.60798649,9062033.71776595,9062033.829588594,9062033.94349246,9062034.05951628,9062034.177699517,9062034.298082365,9062034.420705765,9062034.545611423,9062034.672841817,9062034.802440226,9062034.934450723,9062035.068918206,9062035.205888413,9062035.345407927,9062035.4875242,9062035.63228557,9062035.779741276,9062035.929941468,9062036.082937233,9062036.238780614,9062036.397524612,9062036.559223223,9062036.723931449,9062036.89170531,9062037.062601876,9062037.236679275,9062037.413996715,9062037.594614515,9062037.77859411,9062037.96599808,9062038.15689017,9062038.35133532,9062038.54939967,9062038.751150588,9062038.956656713,9062039.165987944,9062039.37921549,9062039.596411893,9062039.817651032,9062040.043008165,9062040.272559965,9062040.506384512,9062040.744561363,9062040.98717153,9062041.234297562,9062041.486023523,9062041.742435055,9062042.00361939,9062042.269665385,9062042.540663555,9062042.816706099,9062043.097886935,9062043.38430172,9062043.67604791,9062043.973224765,9062044.275933392,9062044.584276794,9062044.898359878,9062045.218289511,9062045.544174552,9062045.876125893,9062046.214256478,9062046.558681369,9062046.909517761,9062047.26688504,9062047.630904814,9062048.00170095,9062048.379399635,9062048.764129397,9062049.15602116,9062049.555208292,9062049.961826645,9062050.3760146,9062050.797913117,9062051.227665778,9062051.665418856,9062052.11132133,9062052.56552497,9062053.028184365,9062053.499456992,9062053.979503257,9062054.46848656,9062054.966573345,9062055.473933151,9062055.990738694,9062056.517165892,9062057.05339395,9062057.599605415,9062058.15598623,9062058.722725812,9062059.300017104,9062059.888056645,9062060.487044645,9062061.097185042,9062061.71868557,9062062.351757847,9062062.996617433,9062063.653483907,9062064.32258094,9062065.004136374,9062065.698382307,9062066.405555157,9062067.125895755,9062067.859649425,9062068.60706606,9062069.368400224,9062070.14391122,9062070.933863191,9062071.73852521,9062072.558171365,9062073.393080868,9062074.243538124,9062075.109832855,9062075.992260182,9062076.891120743,9062077.806720778,9062078.739372242,9062079.689392917,9062080.657106511,9062081.642842771,9062082.646937609,9062083.669733195,9062084.711578093,9062085.772827378,9062086.853842745,9062087.954992646,9062089.076652413,9062090.219204389,9062091.383038046,9062092.568550142,9062093.776144844,9062095.006233852,9062096.25923658,9062097.535580253,9062098.835700097,9062100.160039453,9062101.509049954,9062102.88319167,9062104.282933269,9062105.708752174,9062107.161134735,9062108.640576389,9062110.147581832,9062111.682665197,9062113.246350233,9062114.839170473,9062116.461669432,9062118.114400784,9062119.797928566,9062121.512827354,9062123.259682478,9062125.03909021,9062126.851657987,9062128.698004603,9062130.57876044,9062132.494567666,9062134.446080474,9062136.433965303,9062138.458901064,9062140.521579374,9062142.622704811,9062144.762995137,9062146.943181558,9062149.16400898,9062151.426236259,9062153.730636463,9062156.077997154,9062158.46912064,9062160.90482428,9062163.385940742,9062165.913318304,9062168.487821149,9062171.110329669,9062173.781740764,9062176.502968144,9062179.27494268,9062182.09861269,9062184.974944292,9062187.904921737,9062190.88954775,9062193.929843873,9062197.026850838,9062200.181628909,9062203.395258274,9062206.668839399,9062210.003493438,9062213.400362603,9062216.860610576,9062220.385422913,9062223.976007462,9062227.633594776,9062231.359438553,9062235.154816085,9062239.021028679,9062242.95940214,9062246.97128723,9062251.058060126,9062255.221122935,9062259.461904159,9062263.781859212,9062268.182470929,9062272.665250089,9062277.23173595,9062281.883496793,9062286.62213047,9062291.44926497,9062296.366559008,9062301.375702584,9062306.478417618,9062311.676458523,9062316.971612856,9062322.365701938,9062327.860581506,9062333.458142364,9062339.160311071,9062344.96905061,9062350.886361102,9062356.914280502,9062363.054885348,9062369.310291486,9062375.682654822,9062382.174172113,9062388.787081733,9062395.523664491,9062402.386244433,9062409.377189694,9062416.498913327,9062423.753874192,9062431.144577825,9062438.67357735,9062446.34347439,9062454.156920016,9062462.116615698,9062470.225314286,9062478.485820996,9062486.90099444,9062495.473747654,9062504.207049156,9062513.103924023,9062522.167455003,9062531.400783623,9062540.807111338,9062550.3897007,9062560.151876558,9062570.097027255,9062580.228605887,9062590.550131567,9062601.065190697,9062611.777438307,9062622.690599393,9062633.808470285,9062645.134920048,9062656.673891913,9062668.429404737,9062680.405554477,9062692.60651573,9062705.03654325,9062717.69997356,9062730.601226551,9062743.744807126,9062757.135306891,9062770.777405847,9062784.67587417,9062798.835573973,9062813.261461142,9062827.958587194,9062842.932101177,9062858.187251601,9062873.729388434,9062889.563965108,9062905.696540575,9062922.132781448,9062938.878464095,9062955.939476883,9062973.321822392,9062991.031619709,9063009.07510676,9063027.458642703,9063046.188710338,9063065.271918619,9063084.715005185,9063104.524838932,9063124.708422681,9063145.272895869,9063166.225537298,9063187.573767973,9063209.325153958,9063231.487409316,9063254.068399118,9063277.076142501,9063300.518815782,9063324.404755667,9063348.742462514,9063373.540603647,9063398.808016777,9063424.553713461,9063450.78688266,9063477.51689436,9063504.753303267,9063532.505852595,9063560.784477921,9063589.599311136,9063618.960684458,9063648.879134564,9063679.365406765,9063710.430459328,9063742.085467836,9063774.341829667,9063807.21116858,9063840.70533937,9063874.83643264,9063909.61677968,9063945.058957431,9063981.175793579,9064017.980371734,9064055.486036748,9064093.706400106,9064132.655345472,9064172.347034333,9064212.79591177,9064254.01671234,9064296.024466097,9064338.834504738,9064382.462467877,9064426.924309444,9064472.236304227,9064518.415054576,9064565.477497194,9064613.440910114,9064662.322919816,9064712.141508486,9064762.915021425,9064814.662174618,9064867.402062459,9064921.154165642,9064975.938359207,9065031.77492076,9065088.684538852,9065146.688321546,9065205.807805147,9065266.06496311,9065327.482215147,9065390.082436489,9065453.88896736,9065518.925622627,9065585.216701671,9065652.786998387,9065721.661811478,9065791.866954869,9065863.42876837,9065936.374128532,9066010.73045972,9066086.525745386,9066163.788539583,9066242.547978664,9066322.83379323,9066404.676320294,9066488.106515657,9066573.155966528,9066659.856904373,9066748.24221798,9066838.345466789,9066930.200894414,9067023.843442446,9067119.308764465,9067216.63324031,9067315.85399057,9067417.008891337,9067520.136589197,9067625.276516447,9067732.468906574,9067841.75480997,9067953.176109886,9068066.77553863,9068182.596694011,9068300.684056,9068421.083003655,9068543.839832263,9068669.001770724,9068796.616999151,9068926.73466671,9069059.404909665,9069194.678869652,9069332.608712161,9069473.247645224,9069616.649938293,9069762.870941324,9069911.96710404,9070063.995995365,9070219.01632304,9070377.087953374,9070538.271931173,9070702.630499767,9070870.227121199,9071041.126496503,9071215.394586079,9071393.098630155,9071574.307169314,9071759.090065083,9071947.518520517,9072139.66510084,9072335.60375405,9072535.409831496,9072739.160108421,9072946.932804406,9073158.807603732,9073374.865675615,9073595.189694269,9073819.863858795,9074048.973912872,9074282.607164184,9074520.85250357,9074763.800423866,9075011.543038398,9075264.174099082,9075521.78901411,9075784.48486515,9076052.360424047,9076325.516168987,9076604.05430002,9076888.07875398,9077177.695218688,9077473.011146404,9077774.135766514,9078081.180097317,9078394.256956955,9078713.480973346,9079038.968593122,9079370.83808946,9079709.2095688,9080054.204976348,9080405.948100302,9080764.564574772,9081130.181881279,9081502.92934879,9081882.938152244,9082270.341309454,9082665.27367635,9083067.87194047,9083478.27461264,9083896.622016769,9084323.05627772,9084757.721307091,9085200.762786949,9085652.328151366,9086112.566565724,9086581.628903715,9087059.66772193,9087546.837232029,9088043.293270381,9088549.193265116,9089064.696200537,9089589.962578816,9090125.15437894,9090670.4350128,9091225.96927845,9091791.923310373,9092368.46452685,9092955.761574233,9093553.984268222,9094163.303532023,9094783.891331382,9095415.920606516,9096059.565200808,9096714.999786424,9097382.399786677,9098061.94129525,9098753.800992236,9099458.156057045,9100175.184078157,9100905.062959796,9101647.970825542,9102404.085918946,9103173.58650116,9103956.650745746,9104753.456630621,9105564.181827333,9106389.00358768,9107228.098627865,9108081.64301021,9108949.812022645,9109832.780056069,9110730.720479736,9111643.805514848,9112572.206106512,9113516.091794247,9114475.630581275,9115450.988802718,9116442.330993047,9117449.819752887,9118473.61561549,9119513.876913147,9120570.759643696,9121644.417337554,9122735.000925386,9123842.65860686,9124967.535720628,9126109.774616,9127269.514526494,9128446.89144564,9129642.03800538,9130855.083357355,9132086.153057417,9133335.36895376,9134602.849078935,9135888.707546128,9137193.054450091,9138515.995772941,9139857.633295322,9141218.064513158,9142597.382560385,9143995.676137956,9145413.02944947,9146849.52214377,9148305.2292647,9149780.22120848,9151274.563688852,9152788.317710355,9154321.539549936,9155874.28074717,9157446.588103322,9159038.50368945,9160650.064863747,9162281.304298291,9163932.250015395,9165602.925433628,9167293.34942369,9169003.536374135,9170733.496267138,9172483.234764203,9174252.753301948,9176042.049197854,9177851.115766037,9179679.942442888,9181528.514922563,9183396.815302147,9185284.822236389,9187192.511101807,9189119.854169961,9191066.820789699,9193033.377578061,9195019.488619633,9197025.11567396,9199050.218390763,9201094.754532551,9203158.680204263,9205241.95008951,9207344.51769307,9209466.335589074,9211607.35567448,9213767.529427387,9215946.808169568,9218145.14333284,9220362.486728627,9222598.79082024,9224854.008997293,9227128.095851671,9229421.007454522,9231732.70163365,9234063.138250757,9236412.279477924,9238780.090072768,9241166.53765168,9243571.592960574,9245995.230142541,9248437.427001895,9250898.165264001,9253377.430830363,9255875.214028433,9258391.509855632,9260926.318217037,9263479.64415631,9266051.498079337,9268641.895970196,9271250.859598957,9273878.41672098,9276524.601267299,9279189.453525737,9281873.020312475,9284575.355133722,9287296.518337244,9290036.577253576,9292795.606326599,9295573.687233415,9298370.90899334,9301187.368065892,9304023.168437738,9306878.421698537,9309753.24710569,9312647.771637987,9315562.130038276,9318496.464845182,9321450.926414056,9324425.672927262,9327420.870394073,9330436.692640277,9333473.321287885,9336530.945725126,9339609.763067067,9342709.978107225,9345831.803260501,9348975.458497846,9352141.171273086,9355329.176442316,9358539.716176352,9361773.0398667,9365029.404025575,9368309.072180424,9371612.314763557,9374939.408997385,9378290.63877581,9381666.29454239,9385066.673165778,9388492.077813074,9391942.81782164,9395419.208569976,9398921.571348231,9402450.233228954,9406005.52693864,9409587.79073066,9413197.368260158,9416834.608461417,9420499.865428323,9424193.4982984,9427915.871140955,9431667.35284985,9435448.317041392,9439259.14195776,9443100.21037651,9446971.909526499,9450874.631010672,9454808.770736119,9458774.728851689,9462772.90969358,9466803.72173912,9470867.577569116,9474964.893838909,9479096.091258466,9483261.594581636,9487461.832604738,9491697.238174662,9495968.248206548,9500275.303711116,9504618.84983175,9508999.335891291,9513417.215448592,9517872.94636475,9522366.990878988,9526899.815694092,9531471.892071275,9536083.695934348,9540735.707983037,9545428.413815217,9550162.304057948,9554937.874506962,9559755.626274463,9564616.065944895,9569519.705738435,9574467.063681869,9579458.663786568,9584495.036233194,9589576.717562808,9594704.250873994,9599878.18602564,9605099.079844972,9610367.496340446,9615684.00691908,9621049.190607812,9626463.634278446,9631927.932875749,9637442.689648237,9643008.516381213,9648626.033631586,9654295.87096396,9660018.667187592,9665795.070593614,9671625.739192145,9677511.340948684,9683452.554019323,9689450.066984223,9695504.57907887,9701616.800422477,9707787.452243062,9714017.267098583,9720306.98909356,9726657.374090591,9733069.189916166,9739543.21656012,9746080.246368134,9752681.084226582,9759346.547739098,9766077.467394125,9772874.686722772,9779739.062446252,9786671.464612111,9793672.776718536,9800743.895825928,9807885.732654894,9815099.211669901,9822385.271147676,9829744.863229502,9837178.953956505,9844688.523287026,9852274.56509513,9859938.087149287,9867680.11107027,9875501.672267249,9883403.81985107,9891387.616523711,9899454.138442846,9907604.475060472,9915839.728934532,9924161.015512455,9932569.462885536,9941066.211513048,9949652.41391506,9958329.23433279,9967097.848355493,9975959.442512767,9984915.213831246,9993966.369354626,10003114.125626002,10012359.708131552,10021704.350704541,10031149.29488878,10040695.789260602,10050345.08870852,10060098.453669766,10069957.149322938,10079922.444736116,10089995.611969745,10100177.925133808,10110470.659398735,10120875.089959683,10131392.490953865,10142024.134330675,10152771.288674522,10163635.21798028,10174617.18038151,10185718.426831556,10196940.199737893,10208283.731550116,10219750.243302124,10231340.943109209,10243057.024620857,10254899.665430203,10266870.025441274,10278969.245195266,10291198.444157219,10303558.718964696,10316051.14164015,10328676.757768797,10341436.584644096,10354331.60938296,10367362.787013033,10380531.038534576,10393837.248959538,10407282.265330687,10420866.894723738,10434591.90223557,10448458.008961828,10462465.889967289,10476616.172252528,10490909.43272058,10505346.196147377,10519926.933159914,10534652.058226122,10549521.92766064,10564536.83765068,10579697.022306353,10595002.651739748,10610453.830177365,10626050.594110278,10641792.91048665,10657680.67495117,10673713.710135976,10689891.764007693,10706214.508275144,10722681.536862273,10739292.364450831,10756046.425097233,10772943.070927989,10789981.570917998,10807161.109755864,10824480.786800366,10841939.615131918,10859536.520702912,10877270.341590453,10895139.827355009,10913143.63850819,10931280.346092654,10949548.43137704,10967946.285668408,10986472.21024459,11005124.41640852,11023901.025666311,11042800.07003064,11061819.492450673,11080957.14736948,11100210.801409572,11119578.134186903,11139056.739253355,11158644.125167431,11178337.716692487,11198134.856121661,11218032.804728154,11238028.744339354,11258119.779032873,11278302.936952338,11298575.172240388,11318933.367086092,11339374.333883677,11359894.817499157,11380491.497641249,11401160.991332566,11421899.85547698,11442704.589518653,11463571.638188148,11484497.394330695,11505478.201811576,11526510.358493354,11547590.119279567,11568713.699219245,11589877.276666662,11611076.996490374,11632308.973325778,11653569.294865055,11674854.025178596,11696159.208061714,11717480.870400583,11738815.025551297,11760157.676725913,11781504.820379464,11802852.449591884,11824196.557438929,11845533.140346197,11866858.201420529,11888167.753753092,11909457.823688684,11930724.454055835,11951963.707352532,11973171.668882508,11994344.44983721,12015478.190318858,12036569.06230003,12057613.27251559,12078607.0652829,12099546.725246536,12120428.580043906,12141249.00288846,12162004.415067393,12182691.288351027,12203306.147311201,12223845.57154645,12244306.197811773,12264684.722051155,12284977.901331369,12305182.555675514,12325295.569795394,12345313.894721724,12365234.54933162,12385054.62177297,12404771.270785471,12424381.726918498,12443883.293645956,12463273.34837867,12482549.343374956,12501708.80655029,12520749.342187038,12539668.631545587,12558464.433378188,12577134.584347112,12595676.999348795],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[11559257.172565335,11559257.128671704,11559257.083961222,11559257.038418688,11559256.99202862,11559256.944775244,11559256.896642495,11559256.847614009,11559256.79767312,11559256.746802842,11559256.694985883,11559256.642204626,11559256.58844113,11559256.53367711,11559256.477893954,11559256.42107269,11559256.363194004,11559256.304238215,11559256.24418528,11559256.183014786,11559256.12070593,11559256.057237534,11559255.992588017,11559255.926735401,11559255.859657293,11559255.791330894,11559255.721732974,11559255.65083987,11559255.578627478,11559255.50507125,11559255.43014618,11559255.353826791,11559255.276087143,11559255.196900798,11559255.11624084,11559255.03407985,11559254.950389892,11559254.865142515,11559254.778308734,11559254.689859033,11559254.59976334,11559254.507991027,11559254.414510893,11559254.319291158,11559254.222299453,11559254.123502802,11559254.02286762,11559253.920359693,11559253.815944172,11559253.709585564,11559253.601247707,11559253.490893776,11559253.37848625,11559253.263986917,11559253.147356853,11559253.028556412,11559252.907545205,11559252.784282094,11559252.658725174,11559252.53083177,11559252.400558393,11559252.267860765,11559252.132693779,11559251.995011477,11559251.854767062,11559251.711912857,11559251.566400297,11559251.418179924,11559251.267201345,11559251.113413239,11559250.956763333,11559250.79719837,11559250.634664113,11559250.46910531,11559250.300465684,11559250.128687909,11559249.953713592,11559249.77548326,11559249.593936322,11559249.409011073,11559249.220644651,11559249.028773025,11559248.833330974,11559248.634252068,11559248.43146864,11559248.224911757,11559248.01451121,11559247.800195483,11559247.58189173,11559247.359525748,11559247.133021953,11559246.902303364,11559246.667291552,11559246.427906644,11559246.184067274,11559245.935690556,11559245.682692084,11559245.424985858,11559245.16248429,11559244.895098161,11559244.622736594,11559244.345307019,11559244.062715145,11559243.774864923,11559243.481658537,11559243.18299632,11559242.878776774,11559242.568896512,11559242.253250211,11559241.931730604,11559241.60422842,11559241.270632358,11559240.930829048,11559240.58470301,11559240.232136616,11559239.873010054,11559239.50720128,11559239.134585984,11559238.755037535,11559238.368426966,11559237.974622892,11559237.57349149,11559237.164896457,11559236.748698946,11559236.324757535,11559235.892928164,11559235.45306411,11559235.005015898,11559234.548631292,11559234.083755223,11559233.610229734,11559233.127893928,11559232.63658393,11559232.136132805,11559231.62637052,11559231.107123878,11559230.578216463,11559230.039468586,11559229.49069721,11559228.931715894,11559228.362334736,11559227.7823603,11559227.191595558,11559226.589839816,11559225.976888655,11559225.352533849,11559224.716563305,11559224.068760987,11559223.408906847,11559222.736776745,11559222.05214237,11559221.354771178,11559220.644426296,11559219.92086645,11559219.18384588,11559218.433114266,11559217.668416634,11559216.889493266,11559216.096079623,11559215.287906248,11559214.464698683,11559213.626177367,11559212.77205754,11559211.902049158,11559211.015856784,11559210.1131795,11559209.193710785,11559208.257138435,11559207.303144446,11559206.331404895,11559205.341589862,11559204.33336328,11559203.306382848,11559202.260299912,11559201.194759334,11559200.109399386,11559199.003851624,11559197.877740758,11559196.730684526,11559195.562293576,11559194.372171327,11559193.159913827,11559191.925109629,11559190.667339643,11559189.38617701,11559188.081186932,11559186.751926553,11559185.397944788,11559184.018782185,11559182.613970762,11559181.183033848,11559179.725485938,11559178.240832504,11559176.72856985,11559175.188184928,11559173.619155174,11559172.020948336,11559170.393022269,11559168.73482479,11559167.045793464,11559165.325355424,11559163.572927179,11559161.787914416,11559159.969711805,11559158.117702778,11559156.23125935,11559154.309741879,11559152.352498878,11559150.35886677,11559148.328169681,11559146.259719206,11559144.152814189,11559142.006740471,11559139.820770664,11559137.594163897,11559135.32616557,11559133.016007112,11559130.662905699,11559128.266064022,11559125.824669985,11559123.33789647,11559120.80490103,11559118.22482562,11559115.596796308,11559112.919922978,11559110.193299046,11559107.41600114,11559104.587088788,11559101.705604132,11559098.77057158,11559095.78099748,11559092.735869814,11559089.634157823,11559086.474811701,11559083.256762214,11559079.978920355,11559076.640176995,11559073.239402477,11559069.77544628,11559066.24713661,11559062.653280014,11559058.992660992,11559055.26404158,11559051.46616095,11559047.597734977,11559043.657455832,11559039.643991517,11559035.555985464,11559031.392056044,11559027.15079613,11559022.830772635,11559018.430526014,11559013.948569812,11559009.383390136,11559004.733445188,11558999.997164736,11558995.172949588,11558990.259171085,11558985.254170543,11558980.156258725,11558974.96371526,11558969.674788095,11558964.287692904,11558958.800612524,11558953.211696316,11558947.519059593,11558941.720782988,11558935.814911818,11558929.79945545,11558923.672386639,11558917.431640875,11558911.075115696,11558904.60067,11558898.006123355,11558891.289255276,11558884.447804498,11558877.47946825,11558870.381901488,11558863.15271614,11558855.789480329,11558848.289717574,11558840.650905993,11558832.870477477,11558824.945816863,11558816.874261076,11558808.653098272,11558800.279566968,11558791.750855131,11558783.064099286,11558774.216383573,11558765.204738831,11558756.026141625,11558746.677513268,11558737.155718852,11558727.457566213,11558717.579804935,11558707.519125288,11558697.272157187,11558686.835469086,11558676.205566915,11558665.378892945,11558654.351824656,11558643.1206736,11558631.681684203,11558620.0310326,11558608.164825404,11558596.07909848,11558583.769815696,11558571.232867645,11558558.464070357,11558545.459163964,11558532.213811392,11558518.723596957,11558504.984025039,11558490.990518615,11558476.738417884,11558462.222978782,11558447.439371508,11558432.382679043,11558417.047895595,11558401.429925092,11558385.523579566,11558369.323577577,11558352.824542584,11558336.021001285,11558318.907381956,11558301.478012733,11558283.72711988,11558265.648826042,11558247.237148447,11558228.485997103,11558209.389172949,11558189.940365981,11558170.133153357,11558149.960997468,11558129.417243982,11558108.495119836,11558087.187731255,11558065.488061663,11558043.38896962,11558020.883186718,11557997.963315431,11557974.62182693,11557950.851058891,11557926.643213248,11557901.99035393,11557876.884404544,11557851.317146048,11557825.280214375,11557798.765098026,11557771.763135633,11557744.26551349,11557716.263263047,11557687.747258354,11557658.708213497,11557629.136679983,11557599.023044098,11557568.357524216,11557537.130168084,11557505.33085007,11557472.949268376,11557439.974942204,11557406.397208901,11557372.205221074,11557337.387943622,11557301.934150811,11557265.832423232,11557229.071144784,11557191.638499571,11557153.522468816,11557114.710827699,11557075.19114216,11557034.9507657,11556993.976836104,11556952.256272167,11556909.775770353,11556866.521801446,11556822.480607133,11556777.638196604,11556731.980343072,11556685.492580267,11556638.160198925,11556589.968243219,11556540.901507162,11556490.944530988,11556440.081597507,11556388.2967284,11556335.57368052,11556281.895942157,11556227.246729264,11556171.608981661,11556114.965359233,11556057.298238073,11555998.589706628,11555938.82156181,11555877.975305103,11555816.032138621,11555752.972961199,11555688.778364405,11555623.428628607,11555556.903718961,11555489.183281463,11555420.246638916,11555350.072786951,11555278.640390027,11555205.927777411,11555131.912939185,11555056.573522257,11554979.88682636,11554901.82980008,11554822.379036877,11554741.510771165,11554659.200874347,11554575.424850939,11554490.157834671,11554403.374584656,11554315.049481565,11554225.156523876,11554133.669324107,11554040.56110518,11553945.804696767,11553849.372531727,11553751.236642605,11553651.368658196,11553549.73980018,11553446.320879852,11553341.082294911,11553233.99402638,11553125.025635593,11553014.14626131,11552901.324616935,11552786.528987857,11552669.727228932,11552550.886762086,11552429.97457408,11552306.957214413,11552181.800793413,11552054.47098047,11551924.93300249,11551793.151642514,11551659.091238543,11551522.715682602,11551383.988420006,11551242.87244886,11551099.330319861,11550953.324136285,11550804.815554304,11550653.765783582,11550500.135588173,11550343.885287728,11550184.974759048,11550023.363437973,11549859.01032165,11549691.873971162,11549521.912514586,11549349.083650416,11549173.344651474,11548994.652369238,11548812.963238655,11548628.233283425,11548440.418121833,11548249.472973071,11548055.35266415,11547858.011637358,11547657.403958336,11547453.483324759,11547246.203075703,11547035.516201615,11546821.375355046,11546603.73286207,11546382.540734462,11546157.750682654,11545929.3141295,11545697.182224864,11545461.305861091,11545221.635689354,11544978.122136936,11544730.71542548,11544479.365590196,11544224.022500157,11543964.63587957,11543701.155330213,11543433.530354952,11543161.710382456,11542885.644793108,11542605.28294613,11542320.57420803,11542031.467982335,11541737.913740696,11541439.861055395,11541137.259633284,11540830.059351215,11540518.210292997,11540201.6627879,11539880.367450815,11539554.275224028,11539223.33742073,11538887.505770242,11538546.732465066,11538200.970209772,11537850.172271738,11537494.292533876,11537133.285549315,11536767.106598098,11536395.711745998,11536019.057905423,11535637.102898523,11535249.805522507,11534857.125617236,11534459.024135133,11534055.463213481,11533646.406249098,11533231.817975529,11532811.664542694,11532385.913599148,11531954.534376899,11531517.497778907,11531074.776469262,11530626.344966114,11530172.179737348,11529712.25929914,11529246.564317318,11528775.07771165,11528297.784763077,11527814.673223898,11527325.733430978,11526830.958421987,11526330.344054708,11525823.889129447,11525311.59551455,11524793.46827508,11524269.51580462,11523739.749960313,11523204.186201027,11522662.84372877,11522115.745633285,11521562.919039873,11521004.395260397,11520440.209947525,11519870.40325214,11519295.019983945,11518714.10977521,11518127.727247689,11517535.9321826,11516938.789693743,11516336.370403571,11515728.75062235,11515116.012530176,11514498.24436192,11513875.540594997,11513248.002139853,11512615.736533202,11511978.858133763,11511337.488320595,11510691.755693791,11510041.796277514,11509387.753725214,11508729.77952695,11508068.033218667,11507402.6825933,11506733.903913599,11506061.882126478,11505386.811078781,11504708.893734274,11504028.34239173,11503345.378903901,11502660.234897217,11501973.151992016,11501284.382023124,11500594.187260559,11499902.840630202,11499210.625934148,11498517.838070607,11497824.783253063,11497131.779228505,11496439.155494468,11495747.253514696,11495056.426933104,11494367.04178592,11493679.476711636,11492994.123158595,11492311.38558998,11491631.681685898,11490955.442542363,11490283.112866905,11489615.15117059,11488952.029956184,11488294.235902231,11487642.270042837,11486996.647942893,11486357.89986858,11485726.570952872,11485103.221355893,11484488.426419925,11483882.776818875,11483286.878702028,11482701.353831979,11482126.839716544,11481563.989734573,11481013.473255554,11480475.975752894,11479952.198910855,11479442.860725068,11478948.695596624,11478470.454419719,11478008.9046629,11477564.830443973,11477139.032598617,11476732.328742843,11476345.553329416,11475979.557698427,11475635.210122176,11475313.395844633,11475015.017115718,11474740.993220698,11474492.260505008,11474269.772394914,11474074.499414349,11473907.429198375,11473769.56650378,11473661.93321723,11473585.568361573,11473541.528100844,11473530.88574458,11473554.731752055,11473614.173737163,11473710.336474575,11473844.36190797,11474017.40916107,11474230.654552264,11474485.291613674,11474782.531115457,11475123.60109627,11475509.746900735,11475942.231224848,11476422.334170248,11476951.353308294,11477530.6037549,11478161.418257102,11478845.147292338,11479583.159181397,11480376.840216042,11481227.594802292,11482136.84562031,11483106.033801898,11484136.619126543,11485230.08023695,11486387.914875016,11487611.640139118,11488902.79276363,11490262.9294215,11491693.627050703,11493196.483205412,11494773.116432551,11496425.166674528,11498154.295698741,11499962.18755452,11501850.549058037,11503821.110305732,11505875.625216668,11508015.872104222,11510243.654277459,11512560.800672429,11514969.166513586,11517470.634005487,11520067.113054737,11522760.54202229,11525552.888505861,11528446.150152383,11531442.355500188,11534543.564850599,11537751.871168483,11541069.401011307,11544498.315486046,11548040.811233299,11551699.12143786,11555475.516864872,11559372.306920625,11563391.840737019,11567536.508278528,11571808.741470512,11576211.015347593,11580745.849220714,11585415.807861451,11590223.502702054,11595171.593049588,11600262.787312496,11605499.844237808,11610885.574157154,11616422.840239642,11622114.559749616,11627963.705307215,11633973.306149572,11640146.449390477,11646486.281276155,11652996.008434905,11659678.899118068,11666538.284429953,11673577.559544113,11680800.184903417,11688209.687401237,11695809.661541065,11703603.770571774,11711595.74759575,11719789.396646999,11728188.593736347,11736797.287860809,11745619.501974061,11754659.333915066,11763920.957291776,11773408.622316768,11783126.65659171,11793079.465837533,11803271.534567026,11813707.426696787,11824391.786095122,11835329.337062802,11846524.884743322,11857983.315459397,11869709.596972441,11881708.778661696,11893985.991619755,11906546.448661147,11919395.444240762,11932538.354278771,11945980.635888835,11959727.827006353,11973785.545913525,11988159.490658052,12002855.438362343,12017879.244420115,12033236.84157729,12048934.238894304,12064977.520586716,12081372.84474142,12098126.441905526,12115244.613545293,12132733.730372418,12150600.230535252,12168850.61767242,12187491.458826631,12206529.382216513,12225971.074864363,12245823.280078044,12266092.794785175,12286786.466718195,12307911.191448793,12329473.909270545,12351481.601928864,12373941.2891973,12396860.025299797,12420244.895178473,12444103.010606851,12468441.506148806,12493267.5349635,12518588.26445716,12544410.871782653,12570742.53918811,12597590.449216293,12624961.779756553,12652863.69895168,12681303.359962199,12710287.895591037,12739824.412771923,12769919.98692501,12800581.65618386,12831816.415498057,12863631.210616268,12896032.931954784,12929028.408357091,12962624.400750298,12996827.595704656,13031644.598902795,13067081.928525664,13103146.008562453,13139843.162052305,13177179.604265742,13215161.435834257,13253794.635836823,13293085.054852251,13333038.407986803,13373660.267886674,13414956.057745138,13456931.044314668,13499590.33093419,13542938.850582175,13586981.358966287,13631722.427660413,13677166.437300315,13723317.57084878,13770179.806941722,13817756.91332637,13866052.440402843,13915069.714880472,13964811.833559927,14015281.657252433,14066481.804847075,14118414.647536969,14171082.303215211,14224486.631050894,14278629.226255603,14333511.41505027,14389134.249842115,14445498.504621008,14502604.670584217,14560452.95199808,14619043.262304796,14678375.22048206,14738448.147662692,14799261.064021038,14860812.68593237,14923101.423410889,14986125.377831569,15049882.339940254,15114369.78815604,15179584.887169335,15245524.486838294,15312185.12138587,15379563.008898942,15447654.051130496,15516453.833605189,15585957.626027945,15656160.382994793,15727056.745004322,15798641.039767774,15870907.283815136,15943849.184393901,16017460.141656961,16091733.251135143,16166661.30648981,16242236.802540144,16318451.938559469,16395298.62183454,16472768.471481167,16550852.822509304,16629542.730130397,16708828.974299347,16788702.064483326,16869152.244649258,16950169.498461604,17031743.554682083,17113863.892762404,17196519.748621378,17279700.12059743,17363393.775567356,17447589.255222578,17532274.882493474,17617438.76811312,17703068.817311082,17789152.736628495,17875678.04084555,17962632.060012564,18050001.946576074,18137774.6825914,18225937.087013446,18314475.823057495,18403377.40562213,18492628.208766546,18582214.473234687,18672122.314018928,18762337.72795626,18852846.6013501,18943634.717611235,19034687.76491145,19125991.34384381,19217530.97508368,19309292.107044946,19401260.12352598,19493420.351340167,19585758.06792619,19678258.508933127,19770906.8757761,19863688.343157973,19956588.066552926,20049591.18964818,20142682.851739712,20235848.195078664,20329072.372164655,20422340.552982684,20515637.932180427,20608949.73618253,20702261.23023906,20795557.72540488,20888824.585446995,20982047.233677097,21075211.159706235,21168301.92611896,21261305.175063945,21354206.63475833,21446992.12590305,21539647.568006147,21632158.98561148,21724512.5144298,21816694.407369412,21908691.04046353,22000488.9186915,22092074.681690842,22183435.109357294,22274557.127329826,22365427.812357664,22456034.39754631,22546364.277479693,22636405.01321517,22726144.337148685,22815570.157746863,22904670.564143144,22993433.830595087,23081848.420799732,23169902.992064323,23257586.399329495,23344887.699042115,23431796.15287526,23518301.231292512,23604392.61695423,23690060.20796337,23775294.1209484,23860084.693981532,23944422.48932986,24028298.29603773,24111703.13233865,24194628.247895088,24277065.125864953,24359005.484793395,24440441.28032907,24521364.706764057,24601768.19839676,24681644.43071758,24760986.32141709,24839787.0312168,24918039.964522827,24995738.76990298,25072877.340388097,25149449.8135984,25225450.57169638,25300874.241167292,25375715.692429367,25449970.03927524,25523632.638146907,25596699.087246545,25669165.225485653,25741027.13127546,25812281.1211613,25882923.748304326,25952951.800813828,26022362.29993363,26091152.498086452,26159319.87677982,26226862.144377783,26293777.233742494,26360063.299749937,26425718.716684274,26490742.07551531,26555132.18106372,26618888.04905877,26682008.903093316,26744494.171481036,26806343.48402076,26867556.668672863,26928133.748152886,26988074.936447237,27047380.635256242,27106051.43036945,27164088.087978397,27221491.55093172,27278262.93493785,27334403.52472013,27389914.770129357,27444798.282218672,27499055.829285577,27552689.332885936,27605700.863824606,27658092.638127323,27709867.012998372,27761026.482768487,27811573.674837265,27861511.345614444,27910842.376463965,27959569.769654986,28007696.644323714,28055226.232449606,28102161.874849897,28148507.01719569,28194265.206053067,28239440.084952533,28284035.390489686,28328054.94846037,28371502.670032855,28414382.54795989,28456698.652833164,28498455.129382543,28539656.192822423,28580306.12524729,28620409.27207846,28659970.038564004,28698992.88633353,28737482.330009244,28775442.93387522,28812879.308605768,28849796.108054373,28886198.026104376],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[9724111.816135798,9724111.801112823,9724111.785810271,9724111.770222945,9724111.754345546,9724111.738172669,9724111.721698826,9724111.704918405,9724111.687825706,9724111.67041492,9724111.652680123,9724111.634615287,9724111.616214277,9724111.597470826,9724111.578378567,9724111.55893101,9724111.539121544,9724111.51894343,9724111.49838981,9724111.477453697,9724111.45612797,9724111.434405385,9724111.412278553,9724111.389739947,9724111.366781913,9724111.343396638,9724111.319576181,9724111.295312436,9724111.270597154,9724111.245421939,9724111.219778229,9724111.193657301,9724111.167050276,9724111.139948115,9724111.112341601,9724111.084221343,9724111.05557779,9724111.0264012,9724110.996681657,9724110.96640905,9724110.935573094,9724110.904163303,9724110.872169001,9724110.839579312,9724110.806383152,9724110.772569237,9724110.738126082,9724110.703041965,9724110.667304963,9724110.630902927,9724110.593823485,9724110.556054028,9724110.517581718,9724110.478393476,9724110.438475983,9724110.397815663,9724110.356398696,9724110.31421101,9724110.271238249,9724110.227465821,9724110.182878833,9724110.137462135,9724110.091200288,9724110.044077562,9724109.996077942,9724109.947185112,9724109.897382446,9724109.846653022,9724109.794979589,9724109.742344584,9724109.688730119,9724109.634117965,9724109.578489559,9724109.521825993,9724109.464108003,9724109.405315971,9724109.345429914,9724109.284429478,9724109.22229392,9724109.159002129,9724109.094532585,9724109.028863376,9724108.961972184,9724108.893836267,9724108.82443247,9724108.7537372,9724108.681726431,9724108.608375685,9724108.533660028,9724108.457554068,9724108.38003194,9724108.30106729,9724108.220633287,9724108.138702579,9724108.055247337,9724107.970239183,9724107.88364923,9724107.79544805,9724107.70560566,9724107.614091538,9724107.52087457,9724107.425923077,9724107.329204794,9724107.230686849,9724107.13033576,9724107.02811742,9724106.923997095,9724106.817939399,9724106.709908286,9724106.599867048,9724106.48777829,9724106.373603916,9724106.257305132,9724106.138842411,9724106.018175501,9724105.895263398,9724105.770064333,9724105.64253576,9724105.51263435,9724105.380315958,9724105.245535623,9724105.108247554,9724104.968405092,9724104.825960726,9724104.680866057,9724104.533071782,9724104.382527687,9724104.229182625,9724104.07298449,9724103.913880212,9724103.751815736,9724103.586736005,9724103.41858493,9724103.247305384,9724103.072839176,9724102.895127038,9724102.714108594,9724102.529722355,9724102.341905676,9724102.150594752,9724101.9557246,9724101.757229025,9724101.555040596,9724101.349090638,9724101.139309183,9724100.925624987,9724100.70796546,9724100.48625668,9724100.260423332,9724100.030388724,9724099.796074722,9724099.557401747,9724099.31428874,9724099.066653129,9724098.814410822,9724098.557476152,9724098.295761872,9724098.029179098,9724097.757637313,9724097.481044302,9724097.199306149,9724096.912327185,9724096.620009964,9724096.32225524,9724096.018961918,9724095.710027017,9724095.395345652,9724095.074810985,9724094.748314207,9724094.41574447,9724094.076988876,9724093.731932431,9724093.380458005,9724093.022446292,9724092.65777577,9724092.286322668,9724091.907960905,9724091.522562064,9724091.129995354,9724090.73012754,9724090.322822921,9724089.90794328,9724089.485347832,9724089.054893179,9724088.61643326,9724088.169819307,9724087.714899784,9724087.25152035,9724086.779523805,9724086.29875002,9724085.8090359,9724085.310215319,9724084.80211909,9724084.284574855,9724083.75740709,9724083.220436996,9724082.673482465,9724082.116358008,9724081.548874702,9724080.970840119,9724080.38205826,9724079.782329485,9724079.171450472,9724078.54921412,9724077.91540948,9724077.269821702,9724076.612231959,9724075.942417365,9724075.26015089,9724074.565201322,9724073.857333144,9724073.136306489,9724072.401877034,9724071.653795939,9724070.891809763,9724070.115660353,9724069.325084787,9724068.519815272,9724067.699579058,9724066.86409834,9724066.01309018,9724065.146266393,9724064.26333347,9724063.363992466,9724062.4479389,9724061.51486266,9724060.56444791,9724059.596372947,9724058.610310143,9724057.605925793,9724056.58288003,9724055.540826699,9724054.479413249,9724053.398280613,9724052.297063082,9724051.175388204,9724050.032876622,9724048.869141988,9724047.68379081,9724046.47642233,9724045.246628394,9724043.993993305,9724042.718093688,9724041.418498363,9724040.09476819,9724038.746455923,9724037.373106064,9724035.974254712,9724034.549429415,9724033.098148998,9724031.619923428,9724030.114253625,9724028.580631323,9724027.018538872,9724025.427449109,9724023.806825139,9724022.156120187,9724020.47477741,9724018.762229716,9724017.017899567,9724015.2411988,9724013.431528436,9724011.58827847,9724009.71082769,9724007.798543444,9724005.850781463,9724003.866885638,9724001.8461878,9723999.788007505,9723997.691651816,9723995.556415074,9723993.381578667,9723991.16641079,9723988.910166219,9723986.612086061,9723984.271397501,9723981.887313575,9723979.459032873,9723976.985739317,9723974.466601884,9723971.900774332,9723969.287394924,9723966.625586165,9723963.914454512,9723961.15309006,9723958.340566294,9723955.475939758,9723952.558249766,9723949.58651808,9723946.559748618,9723943.476927109,9723940.337020786,9723937.13897805,9723933.881728133,9723930.564180769,9723927.185225813,9723923.743732926,9723920.238551203,9723916.668508792,9723913.032412538,9723909.329047613,9723905.557177106,9723901.715541655,9723897.802859044,9723893.817823792,9723889.759106753,9723885.625354694,9723881.41518987,9723877.12720959,9723872.759985803,9723868.312064601,9723863.781965822,9723859.168182561,9723854.469180703,9723849.68339846,9723844.809245877,9723839.845104342,9723834.7893261,9723829.640233735,9723824.396119654,9723819.055245578,9723813.615841994,9723808.07610763,9723802.434208894,9723796.688279338,9723790.83641906,9723784.876694174,9723778.80713619,9723772.625741428,9723766.330470445,9723759.919247393,9723753.389959414,9723746.740456013,9723739.96854841,9723733.072008913,9723726.048570227,9723718.895924818,9723711.611724233,9723704.193578389,9723696.639054906,9723688.945678376,9723681.110929664,9723673.132245174,9723665.007016107,9723656.732587721,9723648.30625856,9723639.725279698,9723630.98685397,9723622.088135133,9723613.026227124,9723603.798183216,9723594.401005194,9723584.83164254,9723575.086991567,9723565.163894573,9723555.059138982,9723544.769456457,9723534.291522002,9723523.62195309,9723512.757308718,9723501.694088513,9723490.42873179,9723478.957616594,9723467.277058763,9723455.38331096,9723443.272561686,9723430.940934291,9723418.384485994,9723405.59920684,9723392.581018718,9723379.325774306,9723365.829256026,9723352.087175006,9723338.095170014,9723323.848806368,9723309.343574887,9723294.574890766,9723279.538092494,9723264.228440724,9723248.641117178,9723232.771223491,9723216.61378009,9723200.163725033,9723183.41591287,9723166.365113463,9723149.006010829,9723131.33320195,9723113.341195596,9723095.024411134,9723076.377177328,9723057.393731134,9723038.068216499,9723018.394683145,9722998.36708535,9722977.97928073,9722957.225029029,9722936.097990867,9722914.591726543,9722892.69969479,9722870.415251553,9722847.731648773,9722824.64203314,9722801.139444893,9722777.216816587,9722752.866971895,9722728.082624378,9722702.856376298,9722677.180717424,9722651.04802383,9722624.450556733,9722597.38046133,9722569.829765629,9722541.790379325,9722513.254092677,9722484.212575383,9722454.657375515,9722424.579918437,9722393.971505763,9722362.823314333,9722331.126395207,9722298.871672716,9722266.049943503,9722232.651875617,9722198.66800764,9722164.088747848,9722128.904373411,9722093.105029628,9722056.68072921,9722019.621351616,9721981.916642435,9721943.556212785,9721904.529538844,9721864.82596135,9721824.434685232,9721783.344779277,9721741.545175834,9721699.024670666,9721655.771922808,9721611.775454516,9721567.023651341,9721521.504762243,9721475.206899812,9721428.118040597,9721380.226025522,9721331.5185604,9721281.983216584,9721231.607431708,9721180.378510552,9721128.283626053,9721075.30982042,9721021.444006404,9720966.672968697,9720910.983365508,9720854.361730255,9720796.79447345,9720738.26788474,9720678.768135134,9720618.28127939,9720556.793258615,9720494.289903061,9720430.756935118,9720366.179972516,9720300.544531764,9720233.836031815,9720166.03979795,9720097.141065959,9720027.124986509,9719955.976629842,9719883.680990724,9719810.22299367,9719735.587498479,9719659.759306077,9719582.723164674,9719504.46377624,9719424.965803342,9719344.213876309,9719262.192600783,9719178.886565616,9719094.280351188,9719008.358538093,9718921.105716266,9718832.506494509,9718742.545510465,9718651.207441064,9718558.477013405,9718464.339016125,9718368.778311279,9718271.77984671,9718173.328668948,9718073.409936659,9717972.008934623,9717869.111088293,9717764.701978952,9717658.767359437,9717551.293170527,9717442.26555789,9717331.670889767,9717219.495775238,9717105.727083223,9716990.351962147,9716873.357860355,9716754.7325472,9716634.464134939,9716512.541101355,9716388.952313159,9716263.687050201,9716136.735030513,9716008.08643611,9715877.731939724,9715745.662732348,9715611.87055166,9715476.347711353,9715339.087131372,9715200.082369072,9715059.327651298,9714916.817907447,9714772.54880348,9714626.5167769,9714478.719072746,9714329.153780574,9714177.81987248,9714024.71724209,9713869.84674466,9713713.210238172,9713554.810625492,9713394.651897606,9713232.739177894,9713069.078767506,9712903.67819179,9712736.546247803,9712567.693052914,9712397.130094467,9712224.870280534,9712050.927991716,9711875.319134068,9711698.061193015,9711519.173288379,9711338.676230405,9711156.592576843,9710972.946691046,9710787.764801018,9710601.075059533,9710412.907605093,9710223.294623924,9710032.270412799,9709839.871442785,9709646.136423804,9709451.106370019,9709254.824665967,9709057.337133456,9708858.692099094,9708658.940462515,9708458.135765124,9708256.334259432,9708053.594978817,9707849.979807729,9707645.55355222,9707440.384010766,9707234.542045284,9707028.101652287,9706821.14003409,9706613.737669973,9706405.978387244,9706197.949432068,9705989.741540002,9705781.44900613,9705573.1697547,9705365.005408116,9705157.061355244,9704949.44681887,9704742.274922203,9704535.66275432,9704329.731434397,9704124.606174637,9703920.416341748,9703717.29551681,9703515.381553467,9703314.816634225,9703115.747324778,9702918.32462618,9702722.704024741,9702529.0455395,9702337.513767116,9702148.27792403,9701961.511885766,9701777.39422322,9701596.108235748,9701417.841980968,9701242.788301075,9701071.144845558,9700903.11409016,9700738.90335192,9700578.724800197,9700422.79546351,9700271.337232068,9700124.576855857,9699982.745938173,9699846.080924463,9699714.823086383,9699589.218500929,9699469.518024588,9699355.977262378,9699248.856531708,9699148.42082098,9699054.939742843,9698968.687482104,9698889.942738166,9698818.988661999,9698756.112787629,9698701.6069581,9698655.767245932,9698618.89386805,9698591.291095255,9698573.267156236,9698565.134136181,9698567.207870059,9698579.807830648,9698603.257011402,9698637.881804246,9698684.011872465,9698741.980018778,9698812.122048773,9698894.77662987,9698990.285145994,9699098.991548143,9699221.242201068,9699357.385726318,9699507.772841848,9699672.756198468,9699852.690213392,9700047.930901185,9700258.835702367,9700485.763310011,9700729.07349462,9700989.12692764,9701266.285003899,9701560.909663364,9701873.363212522,9702204.008145746,9702553.206967052,9702921.322012529,9703308.715273894,9703715.748223472,9704142.781641006,9704590.175442666,9705058.288512588,9705547.478537353,9706058.101843715,9706590.513239967,9707145.06586125,9707722.111019174,9708321.998056045,9708945.074204015,9709591.684449462,9710262.171402887,9710956.875174537,9711676.133256115,9712420.280408686,9713189.648557061,9713984.566690857,9714805.360772355,9715652.353651337,9716525.864987027,9717426.211177211,9718353.705294663,9719308.657030841,9720291.372646991,9721302.15493257,9722341.303170977,9723409.113112597,9724505.876954997,9725631.883330243,9726787.417299137,9727972.760352306,9729188.19041786,9730433.981875516,9731710.405576868,9733017.728871623,9734356.215639496,9735726.12632746,9737127.71799208,9738561.244346535,9740026.95581205,9741525.09957332,9743055.919637557,9744619.656896798,9746216.549193026,9747846.831385724,9749510.735421432,9751208.490404882,9752940.32267128,9754706.455859305,9756507.110984392,9758342.506511876,9760212.858429572,9762118.38031935,9764059.28342733,9766035.776732257,9768048.067011707,9770096.35890568,9772180.854977323,9774301.755770314,9776459.259862678,9778653.563916689,9780884.862724561,9783153.3492497,9785459.214663265,9787802.648375794,9790183.838063791,9792602.969691033,9795060.227524519,9797555.794144968,9800089.850451766,9802662.575662356,9805274.147306068,9807924.7412124,9810614.531493839,9813343.690523293,9816112.388906263,9818920.79544794,9821769.0771154,9824657.398995105,9827585.924245995,9830554.814048452,9833564.227549411,9836614.321804037,9839705.25171424,9842837.169964544,9846010.2269556,9849224.570735907,9852480.346932158,9855777.69867867,9859116.766546488,9862497.688472606,9865920.599689912,9869385.632658372,9872892.916998038,9876442.57942447,9880034.743687134,9883669.530511381,9887347.05754461,9891067.43930721,9894830.787148852,9898637.209210811,9902486.810394775,9906379.692338862,9910315.953401357,9914295.688652718,9918318.98987647,9922385.945579471,9926496.641012106,9930651.158198932,9934849.575980198,9939091.970064823,9943378.413095165,9947708.97472408,9952083.721704643,9956502.717992919,9960966.024864126,9965473.701042522,9970025.802845277,9974622.38434065,9979263.497520661,9983949.192488486,9988679.51766073,9993454.519984758,9998274.24517115,10003138.737941427,10008048.042291027,10013002.201767623,10018001.259764723,10023045.25983056,10028134.24599217,10033268.263094578,10038447.357154975,10043671.57573173,10048940.968308061,10054255.586690158,10059615.485419516,10065020.722199278,10070471.358334238,10075967.459184248,10081509.094630692,10087096.339555703,10092729.274333684,10098407.985334871,10104132.565440398,10109903.114568522,10115719.74021153,10121582.557982875,10127491.69217404,10133447.276320659,10139449.453777328,10145498.378300643,10151594.214639815,10157737.13913438,10163927.34031835,10170165.019530226,10176450.391528238,10182783.685110163,10189165.143737044,10195595.026160158,10202073.607050452,10208601.177629828,10215178.046303386,10221804.539291978,10228481.001264155,10235207.795966811,10241985.306853514,10248813.937709767,10255694.11327424,10262626.279855032,10269610.905939978,10276648.482800053,10283739.52508475,10290884.571408449,10298084.184926614,10305338.953900708,10312649.492250634,10320016.440093486,10327440.464267392,10334922.258839108,10342462.545594094,10350062.074507669,10357721.624195855,10365442.002344476,10373224.046115039,10381068.622525875,10388976.628806997,10396948.992727086,10404986.672891011,10413090.65900622,10421261.97211629,10429501.664800039,10437810.821334323,10446190.557818891,10454642.022261444,10463166.394621154,10471764.886808807,10480438.742641784,10489189.23775202,10498017.679445166,10506925.406509092,10515913.788969945,10524984.227793934,10534138.1545331,10543377.030913254,10552702.348362425,10562115.627478022,10571618.417431174,10581212.29530654,10590898.865376085,10600679.758305361,10610556.63029079,10620531.162126677,10630605.058200685,10640780.045416577,10651057.872043177,10661440.3064886,10671929.135998884,10682526.16528034,10693233.215045001,10704052.120478762,10714984.729631871,10726032.90173169,10737198.50541769,10748483.41689893,10759889.518034384,10771418.69433666,10783072.832899924,10794853.820252927,10806763.540138375,10818803.87121999,10830976.68471886,10843283.841980953,10855727.191977784,10868308.568742635,10881029.788744744,10893892.648204302,10906898.92035127,10920050.352631234,10933348.663861811,10946795.541343374,10960392.637928007,10974141.569050973,10988043.909729106,11002101.19153083,11016314.899522679,11030686.469197476,11045217.28338949,11059908.66918206,11074761.894813528,11089778.166587226,11104958.62579173,11120304.345637534,11135816.328216523,11151495.50149077,11167342.716317223,11183358.743515,11199544.270982062,11215899.90086806,11232426.146810275,11249123.431239458,11265992.082762463,11283032.33362855,11300244.317286046,11317628.066036178,11335183.508790657,11352910.468939435,11370808.662335118,11388877.695400089,11407117.063362416,11425526.148626251,11444104.21928234,11462850.42776382,11481763.809652383,11500843.282639477,11520087.645646889,11539495.578110758,11559065.639432708,11578796.26860134,11598685.783987049,11618732.383312592,11638934.143801505,11659289.022506036,11679794.856815705,11700449.36514735,11721250.147816839,11742194.688092332,11763280.353428448,11784504.39688019,11805863.958695138,11827356.068081765,11848977.645151481,11870725.503031388,11892596.350144403,11914586.792652883,11936693.337061524,11958912.39297489,11981240.276004449,12003673.210819796,12026207.334338136,12048838.699045995,12071563.276446635,12094376.960626459,12117275.571933324,12140254.860759512,12163310.511421824,12186438.146131054,12209633.32904296,12232891.570382705,12256208.330634503,12279579.024788277,12302999.026634935,12326463.67310183,12349968.268620085,12373508.08951523,12397078.3884129,12420674.398651183,12444291.338691387,12467924.416519154,12491568.834027816,12515219.79137619,12538872.491313133,12562522.143461319,12586163.968553009,12609793.202610707,12633405.101065928,12656994.942809517,12680558.034167225,12704089.712794619,12727585.351485526,12751040.361888764,12774450.198127985,12797810.360319918,12821116.397986695,12844363.913358016,12867548.564559573,12890666.068684246,12913712.204743031,12936682.81649302,12959573.815140003,12982381.1819137,13005100.970513847,13027729.30942583,13050262.40410476,13072696.53902723,13095028.079610351,13117253.473997902,13139369.254713733,13161372.040182896,13183258.536121117,13205025.536793686,13226669.926144825,13248188.67879912,13269578.860936584,13290837.631043268,13311962.24053951,13332950.034288036,13353798.450984458],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[9162324.886407474,9162324.939046742,9162324.992665635,9162325.047282377,9162325.10291555,9162325.15958404,9162325.217307145,9162325.276104463,9162325.33599601,9162325.39700214,9162325.459143588,9162325.52244148,9162325.586917363,9162325.652593138,9162325.719491128,9162325.787634086,9162325.857045189,9162325.927748036,9162325.99976666,9162326.07312555,9162326.147849636,9162326.22396434,9162326.30149554,9162326.380469574,9162326.460913321,9162326.542854136,9162326.62631985,9162326.711338852,9162326.797940055,9162326.886152918,9162326.976007389,9162327.067534061,9162327.160764033,9162327.255728997,9162327.35246125,9162327.450993687,9162327.551359799,9162327.653593719,9162327.757730195,9162327.863804653,9162327.97185312,9162328.081912352,9162328.19401978,9162328.308213513,9162328.424532395,9162328.543015908,9162328.663704412,9162328.7866389,9162328.911861176,9162329.03941382,9162329.169340184,9162329.301684463,9162329.436491631,9162329.573807541,9162329.71367886,9162329.856153172,9162330.001278883,9162330.149105355,9162330.299682828,9162330.453062525,9162330.609296566,9162330.768438097,9162330.930541178,9162331.09566097,9162331.26385359,9162331.435176233,9162331.609687123,9162331.787445614,9162331.968512118,9162332.152948212,9162332.340816608,9162332.532181151,9162332.727106934,9162332.925660199,9162333.127908472,9162333.333920492,9162333.543766322,9162333.757517286,9162333.975246063,9162334.197026681,9162334.42293453,9162334.653046422,9162334.887440588,9162335.126196701,9162335.369395956,9162335.617121026,9162335.869456118,9162336.126487033,9162336.388301156,9162336.654987486,9162336.926636688,9162337.203341125,9162337.485194853,9162337.772293724,9162338.064735299,9162338.362619024,9162338.666046165,9162338.975119876,9162339.289945235,9162339.61062926,9162339.937280986,9162340.270011459,9162340.608933782,9162340.954163186,9162341.30581702,9162341.664014855,9162342.028878437,9162342.40053182,9162342.779101348,9162343.164715717,9162343.557506008,9162343.957605774,9162344.365150992,9162344.780280229,9162345.203134611,9162345.633857867,9162346.072596444,9162346.519499462,9162346.97471887,9162347.438409401,9162347.910728676,9162348.391837263,9162348.881898709,9162349.381079605,9162349.88954964,9162350.407481652,9162350.93505172,9162351.472439151,9162352.019826645,9162352.57740026,9162353.145349542,9162353.723867545,9162354.313150924,9162354.913399976,9162355.52481874,9162356.147615058,9162356.782000633,9162357.42819109,9162358.08640607,9162358.756869335,9162359.439808745,9162360.13545647,9162360.844048934,9162361.565827008,9162362.30103603,9162363.049925877,9162363.81275111,9162364.589771008,9162365.381249666,9162366.187456101,9162367.008664334,9162367.845153492,9162368.697207881,9162369.565117072,9162370.449176082,9162371.349685377,9162372.266950998,9162373.201284714,9162374.153004056,9162375.122432508,9162376.109899534,9162377.115740718,9162378.140297925,9162379.183919339,9162380.24695964,9162381.329780119,9162382.432748744,9162383.556240354,9162384.700636785,9162385.866326915,9162387.053706884,9162388.263180215,9162389.4951579,9162390.750058597,9162392.02830878,9162393.330342792,9162394.656603066,9162396.007540306,9162397.383613547,9162398.785290385,9162400.213047113,9162401.66736886,9162403.148749815,9162404.657693287,9162406.194712035,9162407.760328278,9162409.355073957,9162410.979490941,9162412.634131137,9162414.319556706,9162416.03634031,9162417.785065206,9162419.56632551,9162421.380726397,9162423.228884287,9162425.111427043,9162427.028994221,9162428.982237222,9162430.97181963,9162432.998417297,9162435.062718688,9162437.165424991,9162439.307250522,9162441.48892281,9162443.711182926,9162445.974785708,9162448.280500026,9162450.62910903,9162453.021410435,9162455.458216788,9162457.940355731,9162460.468670255,9162463.044019064,9162465.667276777,9162468.339334292,9162471.061099047,9162473.833495365,9162476.657464681,9162479.53396599,9162482.463976057,9162485.448489815,9162488.488520687,9162491.58510088,9162494.73928185,9162497.9521345,9162501.224749666,9162504.558238465,9162507.9537326,9162511.412384842,9162514.935369337,9162518.523882058,9162522.179141156,9162525.902387453,9162529.694884751,9162533.557920376,9162537.492805528,9162541.50087574,9162545.583491355,9162549.742037972,9162553.977926908,9162558.292595685,9162562.6875085,9162567.164156718,9162571.72405941,9162576.368763836,9162581.09984593,9162585.918910932,9162590.82759382,9162595.827559942,9162600.920505544,9162606.1081583,9162611.392278,9162616.774657033,9162622.257121075,9162627.841529636,9162633.529776715,9162639.323791457,9162645.225538805,9162651.237020075,9162657.360273747,9162663.597376095,9162669.95044188,9162676.421625052,9162683.013119506,9162689.727159819,9162696.566021964,9162703.532024115,9162710.627527397,9162717.854936678,9162725.216701426,9162732.715316446,9162740.353322806,9162748.133308621,9162756.057909952,9162764.129811672,9162772.35174843,9162780.72650545,9162789.256919568,9162797.94588014,9162806.796330018,9162815.811266512,9162824.993742438,9162834.346867077,9162843.873807281,9162853.5777885,9162863.462095851,9162873.530075217,9162883.785134412,9162894.230744233,9162904.870439688,9162915.707821148,9162926.74655558,9162937.990377706,9162949.443091279,9162961.108570375,9162972.990760699,9162985.093680797,9162997.421423486,9163009.978157192,9163022.768127339,9163035.79565775,9163049.065152073,9163062.581095276,9163076.348055111,9163090.370683629,9163104.65371872,9163119.201985732,9163134.020398976,9163149.113963464,9163164.487776456,9163180.147029242,9163196.097008796,9163212.343099546,9163228.890785126,9163245.745650247,9163262.91338249,9163280.399774179,9163298.210724322,9163316.352240529,9163334.830440985,9163353.651556483,9163372.82193248,9163392.348031135,9163412.236433467,9163432.493841534,9163453.12708055,9163474.143101232,9163495.548981994,9163517.351931283,9163539.559289936,9163562.178533616,9163585.217275193,9163608.683267225,9163632.584404593,9163656.9287269,9163681.724421266,9163706.979824824,9163732.703427574,9163758.903875047,9163785.589971116,9163812.770680906,9163840.455133637,9163868.6526256,9163897.37262315,9163926.62476577,9163956.418869196,9163986.76492855,9164017.67312156,9164049.153811872,9164081.217552315,9164113.875088323,9164147.137361385,9164181.015512517,9164215.520885864,9164250.665032268,9164286.459713021,9164322.916903537,9164360.04879724,9164397.867809348,9164436.386580897,9164475.61798268,9164515.575119402,9164556.271333726,9164597.720210534,9164639.935581177,9164682.931527942,9164726.722388292,9164771.322759485,9164816.747503152,9164863.011749875,9164910.130903952,9164958.120648209,9165006.996948875,9165056.776060486,9165107.474531041,9165159.109206976,9165211.69723851,9165265.25608486,9165319.803519595,9165375.35763618,9165431.93685343,9165489.559921196,9165548.245926095,9165608.014297327,9165668.884812519,9165730.877603834,9165794.013163963,9165858.312352367,9165923.79640157,9165990.486923508,9166058.405916035,9166127.575769478,9166198.019273354,9166269.75962312,9166342.820427064,9166417.22571329,9166492.999936841,9166570.167986868,9166648.75519392,9166728.78733741,9166810.290653067,9166893.291840637,9166977.818071589,9167063.896996943,9167151.55675529,9167240.825980837,9167331.733811626,9167424.30989781,9167518.584410062,9167614.588048212,9167712.352049803,9167811.90819891,9167913.288835024,9168016.526862096,9168121.65575762,9168228.709581932,9168337.722987588,9168448.731228821,9168561.770171203,9168676.876301365,9168794.086736843,9168913.439236153,9169034.972208785,9169158.72472548,9169284.736528648,9169413.048042735,9169543.700384947,9169676.735375876,9169812.19555035,9169950.124168502,9170090.565226771,9170233.563469112,9170379.164398368,9170527.41428768,9170678.360192139,9170832.049960293,9170988.53224618,9171147.856521066,9171310.073085563,9171475.233081713,9171643.388505306,9171814.592218177,9171988.897960717,9172166.360364402,9172347.034964515,9172530.978212839,9172718.247490613,9172908.901121423,9173102.998384269,9173300.599526722,9173501.765778128,9173706.559362877,9173915.043513827,9174127.282485718,9174343.3415687,9174563.287101857,9174787.18648694,9175015.108201891,9175247.121814752,9175483.297997281,9175723.708538808,9175968.426360112,9176217.525527177,9176471.081265083,9176729.169971911,9176991.86923258,9177259.257832676,9177531.415772336,9177808.424279993,9178090.365826203,9178377.3241373,9178669.384209124,9178966.632320575,9179269.156047158,9179577.044274319,9179890.387210907,9180209.276402224,9180533.804743249,9180864.066491405,9181200.157279488,9181542.174128145,9181890.215458393,9182244.381103735,9182604.772322247,9182971.491808211,9183344.643703798,9183724.333610116,9184110.668598277,9184503.757219952,9184903.709517725,9185310.637035035,9185724.652825726,9186145.871463258,9186574.409049451,9187010.383222856,9187453.913166538,9187905.119615559,9188364.124863716,9188831.052769953,9189306.02876403,9189789.17985171,9190280.634619303,9190780.523237485,9191288.977464495,9191806.13064862,9192332.11772987,9192867.07524098,9193411.141307471,9193964.455646928,9194527.159567432,9195099.395964999,9195681.309320075,9196273.04569312,9196874.752719045,9197486.579600658,9198108.677100994,9198741.197534453,9199384.294756776,9200038.124153841,9200702.842629062,9201378.608589634,9202065.58193124,9202763.924021529,9203473.797682049,9204195.367168665,9204928.798150545,9205674.257687451,9206431.914205462,9207201.93747108,9207984.498563437,9208779.769844946,9209587.92492998,9210409.138651777,9211243.587027417,9212091.447220802,9212952.897503719,9213828.117214745,9214717.286716204,9215620.587348854,9216538.201384442,9217470.311976064,9218417.103106186,9219378.759532444,9220355.466730973,9221347.41083746,9222354.77858572,9223377.757243717,9224416.534547217,9225471.29863089,9226542.237956643,9227629.541239616,9228733.397371413,9229853.995340638,9230991.524150841,9232146.172735741,9233318.129871655,9234507.584087336,9235714.723570952,9236939.736074312,9238182.808814451,9239444.128372325,9240723.880588872,9242022.250458207,9243339.42201823,9244675.578238403,9246030.900904901,9247405.570503063,9248799.766097309,9250213.665208321,9251647.443687817,9253101.27559073,9254575.333045077,9256069.786119271,9257584.802687338,9259120.548291756,9260677.18600425,9262254.876284465,9263853.776836729,9265474.042465024,9267115.824926091,9268779.272781102,9270464.531245679,9272171.742038758,9273901.043230055,9275652.56908673,9277426.449918985,9279222.81192501,9281041.7770355,9282883.46275767,9284747.98201934,9286635.443012895,9288545.949039577,9290479.598354451,9292436.48401178,9294416.693711687,9296420.309647795,9298447.408356497,9300498.060567852,9302572.33105853,9304670.278506977,9306791.955351098,9308937.407648945,9311106.674942274,9313299.790123682,9315516.779307466,9317757.661704356,9320022.449500743,9322311.147742487,9324623.754223559,9326960.259380156,9329320.64619021,9331704.89007882,9334112.958830016,9336544.812504817,9339000.40336631,9341479.67581174,9343982.566312006,9346509.003358988,9349058.907420775,9351632.190905225,9354228.758132124,9356848.505314063,9359491.32054652,9362157.083807167,9364845.666964848,9367556.93379813,9370290.740024045,9373046.933336861,9375825.353457175,9378625.832191559,9381448.193502782,9384292.253590725,9387157.820984228,9390044.69664372,9392952.674074855,9395881.539453164,9398831.071759595,9401801.042927114,9404791.21799822,9407801.355293216,9410831.206589354,9413880.517310627,9416949.026727842,9420036.46816931,9423142.569241386,9426267.052059067,9429409.633486016,9432570.025384126,9435747.934871914,9438943.064591683,9442155.112984965,9445383.774575854,9448628.740261806,9451889.697611487,9455166.331169123,9458458.322764983,9461765.351831306,9465087.095723206,9468423.23004405,9471773.428974556,9475137.365605142,9478514.71227085,9481905.140888117,9485308.32329292,9488723.93157935,9492151.638438173,9495591.117494494,9499042.043643927,9502504.093386471,9505976.945157358,9509460.279654197,9512953.78015962,9516457.13285872,9519970.027150488,9523492.155952586,9527023.215998614,9530562.908127273,9534110.937562594,9537667.014184507,9541230.852789173,9544802.173338259,9548380.701196538,9551966.167357145,9555558.30865389,9559156.86795991,9562761.594372146,9566372.243381139,9569988.577025345,9573610.364029786,9577237.379928261,9580869.40716885,9584506.235202128,9588147.660551852,9591793.48686765,9595443.524959372,9599097.592812965,9602755.515587434,9606417.125592792,9610082.262248823,9613750.772024518,9617422.508358054,9621097.331557356,9624775.108681204,9628455.713400893,9632139.025842613,9635824.932410631,9639513.325591495,9643204.10373942,9646897.170843225,9650592.436275104,9654289.814521533,9657989.22489683,9661690.591239776,9665393.841593789,9669098.907871256,9672805.725502532,9676514.233070347,9680224.37193024,9683936.085817736,9687649.320443116,9691364.023074476,9695080.142110046,9698797.626640536,9702516.426002556,9706236.489323977,9709957.765062276,9713680.200536905,9717403.741456717,9721128.331443576,9724853.911553223,9728580.419794606,9732307.790648814,9736035.954588795,9739764.837601116,9743494.360710936,9747224.439511517,9750954.983699467,9754685.896617,9758417.074802537,9762148.407550877,9765879.776484262,9769611.055135611,9773342.108545218,9777072.792872183,9780802.95502187,9784532.43229066,9788261.052029217,9791988.631325534,9795714.976709004,9799439.883876627,9803163.137442647,9806884.510712648,9810603.765483344,9814320.651869072,9818034.908156076,9821746.260685619,9825454.423766876,9829159.099620612,9832859.978354454,9836556.73797077,9840249.044407753,9843936.551614761,9847618.901662366,9851295.724887956,9854966.64007746,9858631.254683781,9862289.165082388,9865939.956864674,9869583.205169322,9873218.475052183,9876845.321894838,9880463.291852199,9884071.92233926,9887670.742557177,9891259.274058694,9894837.031353025,9898403.522550076,9901958.250043936,9905500.711235529,9909030.399294171,9912546.803957827,9916049.412371706,9919537.709964907,9923011.181364609,9926469.311347459,9929911.585827516,9933337.492880294,9936746.523802243,9940138.174204992,9943511.945143709,9946867.34427875,9950203.887069857,9953521.098002024,9956818.511842165,9960095.674925638,9963352.14647163,9966587.499926444,9969801.324333549,9972993.225729335,9976162.828563409,9979309.777142245,9982433.737094935,9985534.396859787,9988611.469190437,9991664.69268011,9994693.833302584,9997698.685968483,10000679.076095313,10003634.861189714,10006565.932440355,10009472.216319809,10012353.676193666,10015210.313935224,10018042.171543857,10020849.332765277,10023631.924711715,10026390.119480092,10029124.13576614,10031834.240472354,10034520.75030766,10037184.033376547,10039824.510755405,10042442.658053713,10045039.00695765,10047614.146753663,10050168.725829417,10052703.453149533,10055219.099703362,10057716.499922095,10060196.553062312,10062660.224553093,10065108.54730367,10067542.622968586,10069963.623167252,10072372.790654637,10074771.440439884,10077160.96084949,10079542.814531647,10081918.53939829,10084289.74950133,10086658.135839477,10089025.467092037,10091393.59027601,10093764.431322757,10096139.995570494,10098522.368168805,10100913.714391386,10103316.279853208,10105732.390628219,10108164.453263802,10110614.95468816,10113086.462006811,10115581.62218444,10118103.161608402,10120653.885530178,10123236.677381212,10125854.49795961,10128510.384484267,10131207.449513135,10133948.879722465,10136737.934543915,10139577.944656769,10142472.310332434,10145424.499628829,10148438.046432279,10151516.548344905,10154663.664415684,10157883.11271363,10161178.66774185,10164554.15769149,10168013.461534983,10171560.505958298,10175199.26213227,10178933.742323438,10182767.996345324,10186706.107851334,10190752.190471033,10194910.383791946,10199184.849189479,10203579.765508013,10208099.324596822,10212747.72670481,10217529.175738655,10222447.874389485,10227508.019133657,10232713.795113767,10238069.370906582,10243578.893185059,10249246.48128212,10255076.22166448,10261072.162325164,10267238.307104036,10273578.609945973,10280096.96910695,10286797.22131859,10293683.135922318,10300758.4089846,10308026.65740506,10315491.413029773,10323156.116782214,10331024.112824718,10339098.642763512,10347382.839910587,10355879.723615907,10364592.193683477,10373523.02488497,10382674.861584611,10392050.212488944,10401651.445535135,10411480.782931322,10421540.296362275,10431831.902373526,10442357.357946787,10453118.256279135,10464116.02277815,10475351.911284627,10486827.000534173,10498542.190868292,10510498.20120514,10522695.56627942,10535134.634160226,10547815.564055003,10560738.324406981,10573902.691292683,10587308.247125281,10600954.379668824,10614840.281367345,10628964.948992105,10643327.183609247,10657925.590869296,10672758.58161892,10687824.372834546,10703120.988876402,10718646.263060719,10734397.839546831,10750373.1755351,10766569.543770611,10782984.035346793,10799613.562802259,10816454.863503296,10833504.50330381,10850758.88047353,10868214.22988486,10885866.627447862,10903711.994782386,10921746.104115712,10939964.583393527,10958362.921591703,10976936.474215697,10995680.468974251,11014590.011613593,11033660.091898112,11052885.589723283,11072261.281346427,11091781.845720785,11111441.870918376,11131235.860626997,11151158.240706868,11171203.365792478,11191365.525925305,11211638.953203335,11232017.828433426,11252496.287772972,11273068.429347487,11293728.319831206,11314470.000978075,11335287.496090995,11356174.81641756,11377125.967461053,11398134.955195894,11419195.792177333,11440302.503535632,11461449.132845515,11482629.747862343,11503838.446116861,11525069.360361133,11546316.663858691,11567574.575512575,11588837.364825603,11610099.356687617,11631354.935985217,11652598.552029893,11673824.722801171,11695028.039001843,11716203.16792292,11737344.85711655,11758447.937875483,11779507.328518357,11800518.037480423,11821475.166209863,11842373.91187026,11863209.569850225,11883977.536081605,11904673.309167989,11925292.492325805,11945830.795140339,11966284.035139613,11986648.139189193,12006919.144711316,12027093.200732032],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[5943636.709085091,5943636.666777627,5943636.623682821,5943636.579786028,5943636.535072313,5943636.4895264795,5943636.443133057,5943636.395876248,5943636.347739998,5943636.298707941,5943636.248763397,5943636.197889393,5943636.146068638,5943636.093283507,5943636.0395160485,5943635.984747989,5943635.928960701,5943635.872135227,5943635.814252234,5943635.755292053,5943635.695234637,5943635.6340595605,5943635.5717460355,5943635.508272862,5943635.443618464,5943635.377760868,5943635.310677683,5943635.242346089,5943635.172742868,5943635.101844343,5943635.029626417,5943634.956064537,5943634.881133685,5943634.8048084015,5943634.727062725,5943634.647870224,5943634.567203971,5943634.485036556,5943634.401340031,5943634.316085939,5943634.229245297,5943634.140788587,5943634.050685731,5943633.958906095,5943633.865418472,5943633.770191088,5943633.673191562,5943633.574386915,5943633.473743557,5943633.371227268,5943633.266803213,5943633.160435867,5943633.052089073,5943632.94172601,5943632.829309135,5943632.714800251,5943632.598160408,5943632.479349969,5943632.358328544,5943632.235054967,5943632.109487346,5943631.981582982,5943631.851298413,5943631.71858932,5943631.583410597,5943631.445716303,5943631.305459594,5943631.162592809,5943631.017067376,5943630.868833826,5943630.717841759,5943630.56403984,5943630.407375801,5943630.247796358,5943630.0852472745,5943629.91967329,5943629.75101812,5943629.57922442,5943629.4042337965,5943629.225986759,5943629.044422704,5943628.859479922,5943628.671095531,5943628.479205498,5943628.283744582,5943628.084646344,5943627.881843107,5943627.675265921,5943627.464844561,5943627.250507485,5943627.032181859,5943626.809793453,5943626.583266663,5943626.352524491,5943626.117488494,5943625.878078787,5943625.6342139775,5943625.385811173,5943625.1327859275,5943624.87505224,5943624.612522492,5943624.345107459,5943624.072716226,5943623.795256197,5943623.512633078,5943623.224750786,5943622.93151147,5943622.632815457,5943622.32856121,5943622.018645316,5943621.70296243,5943621.381405255,5943621.053864475,5943620.720228784,5943620.3803847665,5943620.034216901,5943619.681607539,5943619.322436827,5943618.956582695,5943618.583920782,5943618.204324434,5943617.817664619,5943617.423809931,5943617.022626498,5943616.61397798,5943616.197725481,5943615.773727514,5943615.341839997,5943614.901916141,5943614.453806421,5943613.997358551,5943613.532417402,5943613.058824966,5943612.57642028,5943612.0850394005,5943611.584515344,5943611.074677999,5943610.555354111,5943610.026367197,5943609.487537471,5943608.938681839,5943608.379613776,5943607.810143301,5943607.230076904,5943606.639217459,5943606.0373641765,5943605.424312559,5943604.799854272,5943604.163777136,5943603.515865004,5943602.855897721,5943602.1836510375,5943601.4988965355,5943600.801401541,5943600.09092907,5943599.367237716,5943598.630081592,5943597.879210238,5943597.114368539,5943596.335296645,5943595.541729852,5943594.733398548,5943593.910028117,5943593.071338836,5943592.217045761,5943591.346858674,5943590.460481972,5943589.557614509,5943588.637949597,5943587.701174808,5943586.746971927,5943585.775016827,5943584.784979327,5943583.776523152,5943582.7493057465,5943581.702978197,5943580.637185112,5943579.551564477,5943578.445747571,5943577.319358826,5943576.172015665,5943575.003328412,5943573.812900173,5943572.60032666,5943571.365196066,5943570.107088951,5943568.825578083,5943567.520228267,5943566.1905962415,5943564.836230505,5943563.456671184,5943562.051449837,5943560.620089337,5943559.162103692,5943557.67699788,5943556.164267686,5943554.623399523,5943553.053870281,5943551.455147129,5943549.826687319,5943548.167938059,5943546.478336276,5943544.757308428,5943543.004270347,5943541.218627009,5943539.399772326,5943537.547088982,5943535.659948198,5943533.73770951,5943531.779720559,5943529.785316906,5943527.753821747,5943525.684545736,5943523.576786726,5943521.429829541,5943519.242945729,5943517.015393326,5943514.74641659,5943512.4352457775,5943510.0810968345,5943507.683171187,5943505.240655431,5943502.752721068,5943500.218524254,5943497.637205478,5943495.007889282,5943492.329683984,5943489.601681361,5943486.822956339,5943483.992566723,5943481.109552805,5943478.172937101,5943475.181724027,5943472.134899516,5943469.03143072,5943465.8702656515,5943462.650332788,5943459.370540815,5943456.029778143,5943452.626912595,5943449.160791056,5943445.630238997,5943442.034060162,5943438.371036124,5943434.639925906,5943430.83946554,5943426.968367644,5943423.025321011,5943419.008990169,5943414.918014894,5943410.751009801,5943406.50656387,5943402.18323996,5943397.779574325,5943393.294076174,5943388.72522709,5943384.071480587,5943379.331261582,5943374.5029658545,5943369.584959513,5943364.575578472,5943359.473127855,5943354.275881482,5943348.982081258,5943343.589936576,5943338.09762378,5943332.5032854965,5943326.80503004,5943321.000930798,5943315.089025548,5943309.06731587,5943302.933766438,5943296.686304335,5943290.322818385,5943283.841158474,5943277.239134805,5943270.514517175,5943263.6650342485,5943256.688372798,5943249.582176952,5943242.344047385,5943234.971540569,5943227.462167922,5943219.813395028,5943212.022640753,5943204.087276446,5943196.004625029,5943187.771960149,5943179.386505238,5943170.8454326615,5943162.145862736,5943153.284862789,5943144.259446225,5943135.066571521,5943125.703141243,5943116.166001018,5943106.451938501,5943096.557682341,5943086.479901087,5943076.215202139,5943065.760130584,5943055.111168116,5943044.264731869,5943033.21717325,5943021.964776745,5943010.503758754,5942998.830266282,5942986.940375762,5942974.83009176,5942962.495345641,5942949.931994331,5942937.135818887,5942924.102523207,5942910.8277326,5942897.306992399,5942883.535766501,5942869.509435927,5942855.223297339,5942840.672561513,5942825.85235181,5942810.757702626,5942795.383557776,5942779.724768899,5942763.7760938015,5942747.532194805,5942730.987637,5942714.136886588,5942696.974309045,5942679.494167396,5942661.690620358,5942643.5577205205,5942625.089412418,5942606.279530681,5942587.1217980515,5942567.609823413,5942547.737099791,5942527.497002311,5942506.882786106,5942485.88758423,5942464.504405495,5942442.726132324,5942420.545518474,5942397.95518685,5942374.94762718,5942351.515193693,5942327.650102771,5942303.344430535,5942278.590110401,5942253.378930631,5942227.702531761,5942201.55240411,5942174.919885124,5942147.796156749,5942120.172242789,5942092.039006125,5942063.387145988,5942034.20719511,5942004.489516948,5941974.224302687,5941943.401568364,5941912.011151874,5941880.042709918,5941847.485714926,5941814.3294519605,5941780.563015498,5941746.17530627,5941711.15502792,5941675.490683756,5941639.170573348,5941602.182789129,5941564.515212916,5941526.155512428,5941487.091137662,5941447.309317354,5941406.797055247,5941365.541126413,5941323.52807345,5941280.744202685,5941237.1755802855,5941192.8080283385,5941147.627120847,5941101.618179725,5941054.766270655,5941007.056199022,5940958.472505612,5940908.999462468,5940858.621068486,5940807.321045086,5940755.082831817,5940701.889581844,5940647.724157424,5940592.569125332,5940536.406752218,5940479.218999879,5940420.987520563,5940361.693652094,5940301.318413054,5940239.84249784,5940177.246271699,5940113.509765712,5940048.612671687,5939982.534337019,5939915.253759517,5939846.749582169,5939777.000087797,5939705.983193771,5939633.676446557,5939560.057016289,5939485.101691263,5939408.786872392,5939331.088567602,5939251.982386188,5939171.443533102,5939089.44680327,5939005.966575737,5938920.976807901,5938834.451029626,5938746.362337342,5938656.683388099,5938565.386393581,5938472.443114119,5938377.8248526165,5938281.502448495,5938183.446271569,5938083.626215943,5937982.011693821,5937878.571629371,5937773.274452508,5937666.088092674,5937556.9799726745,5937445.917002351,5937332.865572446,5937217.7915482875,5937100.660263589,5936981.436514216,5936860.0845519295,5936736.568078209,5936610.850238038,5936482.893613744,5936352.660218824,5936220.111491884,5936085.2082904745,5935947.910885162,5935808.178953421,5935665.971573822,5935521.247220051,5935373.963755153,5935224.078425795,5935071.547856592,5934916.328044538,5934758.374353555,5934597.641509063,5934434.083592791,5934267.654037547,5934098.3056222685,5933925.990467104,5933750.660028665,5933572.26509547,5933390.7557834955,5933206.08153194,5933018.191099187,5932827.0325588845,5932632.553296347,5932434.700005064,5932233.418683545,5932028.654632298,5931820.352451191,5931608.456036942,5931392.908581009,5931173.65256773,5930950.629772769,5930723.781261882,5930493.047390082,5930258.367801079,5930019.681427154,5929776.926489414,5929530.040498411,5929278.960255264,5929023.621853154,5928763.960679311,5928499.911417486,5928231.408050886,5927958.383865707,5927680.771455111,5927398.502723818,5927111.508893293,5926819.720507473,5926523.067439199,5926221.478897218,5925914.883433922,5925603.208953726,5925286.3827222185,5924964.331375965,5924636.980933195,5924304.256805187,5923966.083808509,5923622.386178079,5923273.087581103,5922918.111131945,5922557.379407836,5922190.814465574,5921818.337859239,5921439.870658824,5921055.333469926,5920664.646454555,5920267.729352909,5919864.501506363,5919454.881881536,5919038.789095555,5918616.141442479,5918186.856920988,5917750.853263272,5917308.047965235,5916858.358317989,5916401.701440649,5915937.994314543,5915467.153818721,5914989.096766961,5914503.739946107,5914011.000155958,5913510.794250583,5913003.0391811,5912487.652040068,5911964.550107336,5911433.650897495,5910894.872208909,5910348.132174316,5909793.349313058,5909230.442584948,5908659.331445723,5908079.935904217,5907492.176581122,5906895.974769454,5906291.252496659,5905677.932588405,5905055.938734047,5904425.195553703,5903785.628667091,5903137.164763903,5902479.73167591,5901813.2584506385,5901137.675426687,5900452.914310598,5899758.9082553135,5899055.591940179,5898342.901652421,5897620.775370089,5896889.152846457,5896147.975695774,5895397.187480368,5894636.733799022,5893866.56237654,5893086.623154549,5892296.8683833135,5891497.252714598,5890687.733295465,5889868.269862955,5889038.824839455,5888199.363428855,5887349.853713129,5886490.26674954,5885620.576668067,5884740.760769136,5883850.799621427,5882950.677159687,5882040.380782389,5881119.901449065,5880189.233777213,5879248.376138623,5878297.330754928,5877336.103792222,5876364.705454604,5875383.15007646,5874391.45621326,5873389.646730747,5872377.7488923,5871355.794444225,5870323.819698875,5869281.865615278,5868229.977877174,5867168.20696814,5866096.608243678,5865015.241999979,5863924.173539185,5862823.473230883,5861713.216569655,5860593.484228404,5859464.362107255,5858325.941377813,5857178.318522518,5856021.595368903,5854855.879118506,5853681.282370216,5852497.923137868,5851305.92486182,5850105.416414319,5848896.532098522,5847679.411640811,5846454.200176415,5845221.04822799,5843980.111677058,5842731.551728159,5841475.5348654995,5840212.232802024,5838941.822420716,5837664.485708078,5836380.4096796615,5835089.786297502,5833792.812379534,5832489.68950078,5831180.623886369,5829865.826296353,5828545.511902295,5827219.900155712,5825889.214648347,5824553.68296437,5823213.536524641,5821869.010423057,5820520.343255154,5819167.776939198,5817811.556529789,5816451.930024357,5815089.148162636,5813723.464219468,5812355.133791172,5810984.414575785,5809611.566147532,5808236.849725829,5806860.527939256,5805482.864584834,5804104.124383106,5802724.572729373,5801344.475441665,5799964.098505813,5798583.7078182455,5797203.568926943,5795823.946771136,5794445.1054203315,5793067.307813164,5791690.815496748,5790315.888367072,5788942.784411058,5787571.759450925,5786203.066891432,5784836.957470712,5783473.679015229,5782113.476199571,5780756.590311664,5779403.259024073,5778053.716171962,5776708.191538384,5775366.910647465,5774030.094566087,5772697.959714669,5771370.717687571,5770048.575083702,5768731.733347835,5767420.388623122,5766114.731615329,5764814.947469174,5763521.215657262,5762233.709881936,5760952.597990483,5759678.041903937,5758410.197559841,5757149.2148691835,5755895.237687736,5754648.403801974,5753408.8449297175,5752176.6867355965,5750952.048861381,5749735.044971205,5748525.782811683,5747324.364286774,5746130.885547388,5744945.437095486,5743768.103902539,5742598.965542093,5741438.096336144,5740285.565515047,5739141.43739055,5738005.771541614,5736878.623012543,5735760.04252297,5734650.076689226,5733548.768256501,5732456.156341286,5731372.276683474,5730297.1619075015,5729230.8417919,5728173.343546557,5727124.692097034,5726084.910375214,5725054.019615545,5724032.039656158,5723018.989244112,5722014.886343997,5721019.748449126,5720033.592894575,5719056.437171263,5718088.299240345,5717129.197847124,5716179.152833732,5715238.185449836,5714306.318660629,5713383.577451382,5712469.9891278315,5711565.583611751,5710670.393730978,5709784.455503309,5708907.808413589,5708040.495683422,5707182.56453292,5706334.066433964,5705495.057354443,5704665.5979930125,5703845.754003921,5703035.596211492,5702235.200813901,5701444.649575886,5700664.030010136,5699893.435547045,5699132.9656926645,5698382.726174635,5697642.829075978,5696913.392956651,5696194.54296279,5695486.410923632,5694789.135436152,5694102.86193745,5693427.742765009,5692763.937204963,5692111.611528542,5691470.939016931,5690842.099974768,5690225.281732601,5689620.678638603,5689028.492039907,5688448.930253964,5687882.208530323,5687328.54900329,5686788.180635943,5686261.339156009,5685748.266984108,5685249.213154943,5684764.433231982,5684294.189216235,5683838.749449708,5683398.388514188,5682973.387125974,5682564.03202718,5682170.615874321,5681793.437124764,5681432.799921797,5681089.013978917,5680762.394464056,5680453.2618843885,5680161.941972422,5679888.765574009,5679634.068538962,5679398.191614923,5679181.480345148,5678984.284970812,5678806.960338506,5678649.865813513,5678513.36519947,5678397.826665011,5678303.622677956,5678231.129947604,5678180.729375667,5678152.806016363,5678147.749046172,5678165.95174373,5678207.811480327,5678273.729721437,5678364.112039731,5678479.368139913,5678619.911895825,5678786.161400109,5678978.539026804,5679197.471507173,5679443.390019038,5679716.730289899,5680017.932714087,5680347.442484147,5680705.709736681,5681093.189712801,5681510.342933353,5681957.635389054,5682435.538745628,5682944.530564039,5683485.094535886,5684057.720733971,5684662.90587809,5685301.153615995,5685972.97481952,5686678.887895784,5687419.4191133985,5688195.102943528,5689006.482415702,5689854.109488149,5690738.545432475,5691660.36123243,5692620.1379964845,5693618.467383904,5694655.95204397,5695733.206067953,5696850.855453408,5698009.538580327,5699209.906698603,5700452.624426267,5701738.370257874,5703067.837082354,5704441.732709636,5705860.780405254,5707325.719432104,5708837.3055984685,5710396.311811353,5712003.528634117,5713659.76484733,5715365.848011697,5717122.62503185,5718930.962719717,5720791.748356122,5722705.890249186,5724674.318288047,5726697.984490311,5728777.863541617,5730914.953325584,5733110.27544236,5735364.875713923,5737679.824674192,5740056.218041955,5742495.177174543,5744997.849500134,5747565.408926449,5750199.0562236225,5752900.019378911,5755669.553920853,5758508.943210494,5761419.498697163,5764402.560136317,5767459.495766909,5770591.70244567,5773800.605735745,5777087.6599470265,5780454.348125566,5783902.181989452,5787432.7018084815,5791047.476225052,5794748.10201365,5798536.203776393,5802413.433572093,5806381.470476375,5810442.020070444,5814596.813856161,5818847.608595173,5823196.185569939,5827644.3497646125,5832193.928963813,5836846.77276752,5841604.751520386,5846469.755154005,5851443.691940737,5856528.487157972,5861726.081661825,5867038.43036951,5872467.500649818,5878015.270621372,5883683.727358568,5889474.865005341,5895390.682797178,5901433.182992054,5907604.368711232,5913906.2416911945,5920340.799948217,5926910.035357453,5933615.931148638,5940460.459320901,5947445.577979455,5954573.22859725,5961845.333205023,5969263.791513484,5976830.477971701,5984547.238766068,5992415.888764572,6000438.20841137,6008615.940576991,6016950.787369801,6025444.406914644,6034098.410104823,6042914.357333918,6051893.755214123,6061038.053288054,6070348.640741194,6079826.843122338,6089473.919079595,6099291.057119632,6109279.372398014,6119439.903548591,6129773.60955996,6140281.366707116,6150963.965546416,6161822.107981967,6172856.404411601,6184067.370960426,6195455.426809981,6207020.89163081,6218763.983126186,6230684.814694505,6242783.393217684,6255059.616982656,6267513.273742776,6280144.0389256785,6292951.473993784,6305935.02496331,6319094.021087301,6332427.673707711,6345935.075281234,6359615.198583098,6373466.896092552,6387488.8995633675,6401679.819782081,6416038.146516278,6430562.248654666,6445250.374540124,6460100.652496467,6475111.091549004,6490279.58233852,6505603.898227742,6521081.69659876,6536710.520339452,6552487.799516281,6568410.853230471,6584476.891653919,6600683.01824084,6617026.232110551,6633503.430596416,6650111.411955501,6666846.878233083,6683706.438275768,6700686.610886568,6717783.828114994,6734994.438674867,6752314.711482269,6769740.839305815,6787268.942521142,6804895.072961382,6822615.217855162,6840425.303843522,6858321.201067124,6876298.7273148885,6894353.652225314,6912481.701531575,6930678.561341597,6948939.882444296,6967261.284633234,6985638.361039059,7004066.682462208,7022541.801697454,7041059.257842123,7059614.580579895,7078203.29443241,7096820.922971045,7115462.992981527,7134125.038574314,7152802.605233875,7171491.253800421,7190186.564377822,7208884.140161828,7227579.611183045,7246268.6379593685,7264946.915053018,7283610.174527605,7302254.189300983,7320874.776390086,7339467.80004416,7358029.174763286,7376554.86819933,7395040.90393689,7413483.364152073,7431878.392147314,7450222.194760783,7468511.044649223,7486741.282443384,7504909.318775568,7523011.636179008,7541044.790859184,7559005.414337411,7576890.214967282,7594695.979324875,7612419.5734737925,7630057.944106381,7647608.119562702,7665067.21072901,7682432.411817704,7699701.001030881,7716870.341109821,7733937.879772855,7750901.150044248],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Ridge en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Ridge pour tout les paramètres de GridSearchCV\n","fig1 = go.Figure([\n","    go.Scatter(name='RMSE moyenne',\n","               x=alphasridge,\n","               y=GridRidge.ScoresMean,\n","               mode='lines',\n","               marker=dict(color='red', size=2),\n","               showlegend=True),\n","    go.Scatter(name='SDup RMSE',\n","               x=alphasridge,\n","               y=GridRidge.ScoresMean + GridRidge.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               showlegend=False),\n","    go.Scatter(name='SDdown RMSE',\n","               x=alphasridge,\n","               y=GridRidge.ScoresMean - GridRidge.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               fillcolor='rgba(68, 68, 68, .3)',\n","               fill='tonexty',\n","               showlegend=False)\n","])\n","\n","fig2 = px.line(GridRidge,\n","               x=alphasridge,\n","               y=[\n","                   'ScoresSplit0', 'ScoresSplit1', 'ScoresSplit2',\n","                   'ScoresSplit3', 'ScoresSplit4'\n","               ])\n","\n","fig3 = go.Figure(data=fig1.data + fig2.data)\n","fig3.update_xaxes(type='log', title='alpha')\n","fig3.update_yaxes(title='RMSE')\n","fig3.update_layout(\n","    title=\"RMSE du modèle Ridge en fonction de l'hyperparamètre alpha\")\n","fig3.show()\n","if write_data is True:\n","    fig3.write_image('./Figures/graphRMSERidge.pdf')"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"name":"stdout","output_type":"stream","text":["      paramètre  Lasso()\n","0  lasso__alpha      1.0\n"]},{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning:\n","\n","Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.429e+15, tolerance: 1.735e+14\n","\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predLasso=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[6053540.947729835,4036637.548376533,-2583803.5514774234,1068219.309584301,5983605.250238145,2624483.2974906326,8085302.397006949,3145248.0087765437,3825223.639407725,926161.5857352847,3376462.425311761,3047749.186791042,1250778.450074524,3006493.811976231,1584170.8922751155,1612199.1766281123,928691.6501614009,2342453.2788045374,722532.8034228357,1288282.6862868029,1437475.817228585,87567.13916289527,6056985.6567456955,2017861.2440436403,1395571.874201525,478268.33851966425,4048350.5188637706,1358281.6564116883,1085778.1118385259,7291027.702673618,2734032.201064498,2816768.4392899415,1609346.9256138268,4475056.278869605,36581994.8874121,408954.44805442914,-77615.47081112675,2068336.7975776365,5835188.106181212,3416467.3801488956,1388405.3424025131,1140875.5687174345,4696489.366174245,7999788.21337074,6531589.788628252,10632063.576601982,1846804.103944724,12498755.018685509,11206471.601766858,1256206.4137744408,5578937.86064833,2257934.8517547674,1011273.6374156375,19497062.869225156,3183417.979757238,2612787.3680580603,1814408.6649553692,3132745.384420026,3434764.439593323,1336183.3715189067,415389.13261808944,2161378.8420104515,5572114.360365847,1278778.909664103,2755798.80666473,4973487.952545773,6712800.327434577,2243438.8422857034,220905.6493728608,349195.6188593751,3878694.295795191,4559222.297165116,298321.9735815837,9624816.65637755,2227633.2835025527,7210156.46736929,205560.57625299622,544957.5779782292,4090165.507764359,27753303.432843648,1404492.3951798426,1265534.1179402159,1308902.6583112867,517580.1989419805,4839858.616444947,4764975.371050142,105873133.62814215,1722172.8131393052,-47423.67654904118,917725.7540582856,1048340.1938555685,2275462.674182897,446002.25923928595,1753061.1135738299,651101.024074161,1414332.48279294,1091359.6398076047,12309800.89591456,4846251.861960053,2534527.988605862,515636.75549724465,16871911.251419976,5042272.338333879,4125135.6398580065,16811018.779833477,8672441.941499852,693648.7462346368,410587.67872409686,1539210.7182523431,4785625.907826824,2186192.4069120376,3277404.8569243704,1869721.1298598803,318243.39859874896,717777.2090784372,1488839.9910744228,7094974.991774678,559002.6987508077,33533222.594098248,2350026.786670949,395461.58365818486,3944017.3387655113,2051683.596137926,-301866.30778468587,12856681.779357458,548253.0693615898,4339637.830796042,2358545.922090099,-219741.4560285746,3974209.5402990547,13984638.274748592,1033605.9203311978,7912814.689584732,743705.7724996274,557173.0429174071,1929683.1357976156,258494.1622884767,590018.3484379004,969723.404491307,2977733.9200632153,11174608.108462986,19875059.39712686,9231463.858102534,11123986.297747774,1386500.3559689163,1037619.5514523451,2050305.661675041,1315157.362031693,3408888.806383486,10968439.124232465,5187871.627168775,2588907.754233803,2106995.51703677,2706454.3972721687,1321035.8941221365,1004532.9271687025,1798667.1359085292,19452127.211486448,8239036.121615874,1648178.8806709305,54154609.796368055,10934257.625907013,2377133.8560368,1744821.2701857528,2052197.7162581421,1308668.1739950874,2503115.5212627435,1018229.1037502538,2170334.50739036,1097720.6778252735,4731807.918027971,1188316.4378526479,1296700.8762204242,4373498.491046004,901607.5562642587,3465095.9047795394,10131130.921688916,1422638.828136215,-55244.97004059423,737429.1538265008,674340.6240253351,853513.8961177215,1092596.1198797307,5224858.2787173595,2093072.6564771903,-142435.69728826312,1551930.7303092724,158802526.0125536,1504756.6874609157,761412.8578379273,2695381.697390787,995212.2838415033,2925293.814646857,939559.9927577449,5341263.863795327,-457641.2900342401,4055631.318864685,3912237.832416463,15714543.265896762,1244242.5079785353,5733398.218531685,1799101.7283335468,10332396.644257607,1886205.3687591534,1313561.512668337,593242.0562829669,1810996.6153068407,11348662.620085303,1608837.9874678794,1409708.3932788176,276227.7654014728,1108483.5457112473,2748787.239139722,505500.91192732425,2281194.120728846,7319813.058891971,1434219.4605150106,374665.74383578636,1387628.796976762,5358147.389000418,283394.7347146678,745372.2492650803,1660509.6892855135,1235220.2278702552,2192546.809046612,4282030.2604469275,9626100.130174562,14272963.803977039,2609960.866546786,8391948.949145203,11569889.975190343,1888155.3464644258,959804.0222177836,707996.8200469629,2001051.7785903676,1617868.6556849766,3854855.64543314,16100130.497131532,31868124.49617119,19761454.51340678,4095537.6937842984,8876137.999419902,994740.6082434342,2152598.891114515,431166.0823924893,1436631.980724506,2268458.2304202435,8729556.362170711,8486551.115812646,651796.6025184977,2002070.113721325,20477124.35235796,924390.5031484356,2671641.404968666,1630851.2034797468,3744430.8322388195,4788563.895694428,1765292.1134689366,7235493.883840062,2442428.856239685,3112917.641921363,9337057.891061861,1153801.602782126,664721.0801612935,553437.7686795164,8913401.05444709,14276625.420571461,2270650.383230116,29363522.200960044,809430.332590028,689139.3893688903,451670.72806402855,807512.327182733,2216683.0617751656,1696210.0546799614,633317.1814656546,7787869.68879511,4210105.062394572,20376730.74008623,7831266.334734777,2351777.448453933,1326034.485549338,3980353.1896340474,6214674.381940991,270690.8078831001,1744638.5052507007,1333267.9704783969,526494.2879220082,22018194.106163066,2448157.744605781,22279339.052817676,-208782.39551917696,930308.7711876372,5470059.878372217,1408002.760378251,3557576.176077096,753468.8824172718,2863975.1525261877,1855318.0947353793,2053154.3376425705,4898602.08679229,569046.6010671738,3175508.047431579,2277496.8708445677,-1397826.7380178357,2356972.225858671,1587376.4760885192,3005069.6823862693,987176.4293780152,2359146.1327311583,4098208.3512817863,2511963.5844497723,13014388.77453939,4036616.5681692897,2394743.4716875954,20603.504416702548,7647276.422831539,1637533.2528283997,1202673.137708492,1468343.5641148598,1585050.062549299,3156988.3241970763,9300445.553762829,2882234.5307765477,9713706.321418973,1058067.631828174,2266984.6110470584,-133206.980731023,2714188.1649227464,3169322.4423863213,14862357.438965974,1806877.1630858749,2591930.5731121553,-353928.7677294398,2796585.369094449,3329911.636107021,721097.550504481,620852.5076278043,1114290.4681173756,2727472.3648031396,5532878.13060013,1492919.935576317,1108532.3462524898,3921825.794056694,21830337.523832355,1906028.1667622915,11129779.812338214,11850999.754677476,14836530.566970171,14007795.536646584,4941270.785842067,7440142.73953261,5629412.872395758,601993.1577168494,23608911.68645253,1207668.9550289158,858052.7113369233,10499677.372000339,606006.7888379963,1047299.9219708516,1716309.9403176534,74875.59823484393,5832370.552955097,5951014.763126634,1034027.9879141415,1021080.3512408966,7961444.784263911,795098.7064991933,4505759.761547577,6435410.924727704,5309659.199188488,15900430.626155306,26405298.010369647,1097482.2651318437,383335.6375377241,716762.3394794597,6662477.217917712,7901277.586708365,1834207.718326132,10761368.168387143,5803946.215476938,9134151.536562797,549067.2877085777,2981578.2171928524,343006.3602106611,881927.3781530692,1099099.3861580803,3223883.3398384857,1834869.0360726253,2034865.4472469902,5210595.042970868,608198.9416478691,2130371.992342447,932286.998026948,1788268.5499260505,2901622.6069887597,10758335.680302152,17982424.676459566,1510060.7375223122,1940577.9369370996,1171305.9304035793,954455.3349644765,291186.99076106935,2390149.8609540905,4614850.241211804,2316354.1800869135,8604109.635021374,6334915.940629795,2117102.794365629,1321381.4736288532,2458630.8986749435,11309979.742570877,1222577.6657248875,1312857.372601291,1841596.3297064945,1128918.4091277719,1247433.6968919488,2990497.008516446,2010190.927861476,10738915.807955202,1128250.3932988616,5470597.446036646,2454520.819943933,4076768.501327499,5690040.331782011,1923474.965840501,980773.4856119531,27577188.499795705,1758592.1822248003,1368219.7266112817,2777443.1532290988,4450623.980439613,407148.61230379716,6733359.222542807,2389427.025074628,759894.8641517125,446938.3103858535,1822018.6533861854,1219099.5100103114,197511.63551669242,11765161.71559155,-125253.32029254409,1503571.5559172188,1560550.609667343,1076613.1801462413,7316048.5536968,765833.2709910786,59659200.3152091,2957533.638935933,2740019.309786529,222047.60387507174,1032909.0680630351,3611478.17719867,1275678.0768074654,1935810.2246292075,2264787.5104084644,2338443.3911634265,1038527.2050883593,3044816.1049702438,2203172.4512604577,1573630.9317204116,858509.9151014639,63103964.72193846,1992293.2161067976,974101.5390718076,15873454.614685193,20024871.494161233,1186189.6433068428,990924.1369573297,9463625.181111751,30131406.193519,961856.2884233289,363895.99943628674,1039667.5642522757,954548.7904717647,1176253.9521392214,2069205.0847448704,942418.2287895537,880720.0253672756,2111259.7712991973,3429192.9785953937,19303694.04126537,43514985.486964546,27647552.533719763,6245014.502579939,9560847.185325202,696398.2283822461,10965104.551972521,1561284.6795408954,8568007.542520048,2288884.6713734013,1907753.6093479046,6188965.992978214,978586.4774305748,2605820.708176101,1117645.0222234577,557011.4686738595,10833390.002388366,66530578.49154285,9539467.33722569,13040929.14718071,1102029.796158156,3134496.04620301,9716060.643506952,22147974.75709185,1477992.6888706957,9274848.342198672,21261839.995903365,1350997.3779875448,69194102.50407802,1332961.515457002,2371596.9896661835,47343652.62832165,5069029.646482604,1119362.7008752138,530482.6928825174,1671596.1503082279,319179.4497453165,4555863.346336639,953644.9442289327,1413462.2718973774,1453648.1546591623,1487964.5807474218,-179576.1328322473,349806.4645156085,4082346.201754036,1960824.259681811,870099.7193856577,2630874.3819032805,4223122.013350529,15076834.161198677,891690.3303274631,1662257.6286113756,2170507.4093392417,7071681.8961697575,751835.5858932147,318465.965965471,1343097.7719023884,2026335.0661286698,1413187.0980840493,1091710.774777718,5469022.724435864,6217070.184197424,1362540.6451354316,-795237.9491854622,4685783.308655525,5580572.581198685,4091762.2228157525,1584171.9186858656,515446.76351977,4929895.750436168,5989386.735465027,2305430.395511749,2593457.1392657617,13702793.812856665,2099350.569082819,1235835.3209661474,9492152.560931355,730293.8506963863,836666.8252003149,1129265.8506823282,1375812.6952766818,429252.60803332273,3206710.2724853717,31531.39250827115,3286244.9334197976,1215265.6515182042,2129897.9233448217,4263347.35696171,2632228.2086240733,3455241.3097691336,6480350.299913958,8172118.104989497,4559808.338618582,731938.000128014,1192766.4153721891,865799.9465643046,1925758.5558871531,1520.3361420358997,-421132.50939890416,528957.8339075963,1010100.0602377907,6913537.76692161,2953648.7255925965,904238.0272445767,1966250.9668352755,1006353.3790692263,2179956.576898679,1442109.0045559886,164901.22404979705,2843513.7036611177,1798809.0214026454,12308039.842510803,1411451.8576118061,5165647.980863273,1257120.2384497006,827943.1678707856,5989676.467001667,2488424.4027803694,3109337.0778333894,766762.7716500519,78449542.93006533,2336528.680960389,6390169.10522051,4443200.031845425,4963090.601296866,3344241.6647901875,3024463.8185545383,795174.6676559784,1040093.1739999981,5505107.828595905,2270039.428847842,973974.6782074813,1771062.433265694,6073939.477930491,4618911.564565379,3668331.855550431,966129.3462118213,1545169.7188624935,1124186.3800277472,2467886.374724908,7000495.3583771065,555113.769043545,1461947.187384849,4602815.139510142,5951937.973003197,51859580.36572309,5941759.277561251,590054.5439400156,386207.17373711243,3380864.2050637477,980724.6850707103,1588649.6595946378,514903.51101196,3211609.480305679,6543402.138155157,2351786.417201608,1820309.0152834794,1399548.8112126507,360100.79591710446,7929937.807312828,3947328.699946432,1477462.0968482173,1534844.6380979307,6836531.350083126,2802019.0689256396,10693017.71342768,391747.5748600778,5379731.826350119,1940309.9605737668,527097.8812867571,2232980.240670185,6720055.529619105,1748655.0627505232,760122.3897435297,142931.94724256312,3156427.8384092404,35859306.97698534,8801347.774046877,25014857.635103885,540708.3456875919,2019806.5362821068,1663839.881991011,1663238.3190350567,6804164.971291492,510893.42627475597,1191388.4809093042,8171847.187751789,730751.0544609269,3174430.7487207484,779882.1408673967,3398893.1085528284,474244.96371828904,1435956.753808754,996231.8853123474,-2395.951477392111,-254241.58354161447,1511827.3564563307,5863734.225310842,7325276.159274785,9796781.50424765,2783292.498330676,3063484.6714853435,1073466.2358597156,2930971.4635930485,3131200.7240314065,916885.5637350883,170563.11294104694,678540.069266974,2173061.1053623864,130735.30164793995,1451681.7356732143,239036.3665547052,9377105.330678368,347759.9356096338,45633903.65982255,1681924.0929053095,1610298.476898163,8262295.600342311,1108076.4603256104,1273594.0344318738,945819.8309249473,-257188.2160671139,3154892.7500780523,2861000.53141539,3357898.4159147507,6970065.187649164,9228620.5894046,798556.2105736681,2194029.8465579445,841962.4910292698,1501342.7363422937,1442442.037070679,1296339.7357644157,1430100.0520263473,1595686.1095941365,16779265.9467069,16870254.41024962,1443442.9186958873,21527196.03900339,54646183.14974461,8721274.91004612,8044088.327111252,964425.6858408519,1898694.9505994786,1492841.5134695666,2646849.9040715233,856303.3673916003,6646178.394547328,80335.18409149582,3918522.695652917,5719906.336849544,4666228.400933912,25318746.88529231,97856.6734139882,6395632.471690984,4308106.6817389205,1081628.8222867548,50143.38982941001,3167829.054499999,1460720.376097539,-448549.54637447186,9924540.364888187,6149870.556940459,2571952.115213462,6523391.332331521,2941024.189911681,15120017.156033613,1593590.5620306632,1752969.731106304,1471215.991373784,1044644.4243756251,757433.7085677569,147497.2082479382,385226.7472952772,857747.3213762073,1380007.139489585,5651630.64470559,4769982.550436707,1985801.2357277744,5634607.382895758,2041326.3365864186,2087421.3383905715,4297337.945470849,-1948479.359843052,112050.60562729952,9940998.96760662,5384642.1609649435,7869515.486838145,1472779.9558088263,1296057.3934765048,1258884.0591960808,4166494.4572667936,132020528.8259906,5333550.694399781,1246702.637151741,378335.3599238759,1573859.0683446727,17334591.67690216,735137.4129021729,7653433.537361665,39552387.48446304,41692123.771178775,2353443.630814757,4060308.7551591927,202341.14360006643,1151408.779830132,963600.779167068,-27794.863017306197,1371334.3217531154,87073.39867123868,117812.8575143381,2836598.4250276657,2752144.6046243985,1147498.6764407738,2188986.3715639776,1151567.3174179718,676011.9300021266,2085403.726655005,5090849.461937403,798926.8811141425,873131.1099056173,4277062.509486956,374375.3695676932,12605662.597077712,1267813.013560988,847810.9689509312,12981110.745509788,779183.7622496856,885096.0421122729,64173459.236587875,1396837.860257648,361045.6540228445,231548.63580771908,-40831.78222554503,1823559.7406406451,5461089.175485957,26900171.79396143,2574526.144997018,999871.3949141284,1760396.4858238888,4977624.630063364,12182879.085437752,2145280.6041165553,1581500.2347776268,4276342.815483229,1298436.7324590078,8189640.60898236,830789.1627932026,93146.7887323238,188675.51028884435,1712472.2695191025,5426072.592001518,-901845.1733853626,1738996.408460165,66161621.54616156,5019790.357193956,8019854.1741912365,3544364.297754817,15668305.392266413,3284245.7508280138,17670886.99518345,1800219.2059832378,11023338.656302586,4443905.07006328,980773.4856119531,207536.5156584438,2505748.501506417,2118146.360211501,75109.93160073902,1323494.3292206314,5962580.085257178,8232702.880914452,594048.3958005048,589839.2895358969,1167426.4248928127,1434887.9215730783,18299592.0996211,10370750.146441972,1962360.7670758413,440427.92872020206,3622655.016687442,604004.6845400606,4865411.757977329,915076.6889107833,2641642.6354462774,2595645.235365445,631226.5212224089,2573590.4605286685,519504.00793166086,847165.1206390043,10460155.504898144,7644394.320691051,3511722.872150172,897150.3039649541,2830444.9843941554,-132116.3642317541,14730220.54988543,1388535.4242020259,3970449.49064125,1639359.73162155,1620330.8058343963,7358079.199496757,1184562.5592340883,13876788.306326762,11998885.51100993,2018786.9342393305,1000784.1857502377,3162818.4400968123,193316.36804076633,12351140.618611025,1059010.246303415,1384698.386976686,18261120.56660638,14607453.154860523,2733858.9922774425,1826625.4936975655,1056522.3537409785,2257224.3579537957,19961641.755728755,1480795.729153269,1158522.7838560909,3026140.2367813075,32856.062174815685,3382462.214668989,3424247.889041898,1160350.0313757225,1899704.3471611957,1034809.645436553,330375.6269487839,13798570.129871244,3664416.804346806,2077561.066449049,2429160.542061552,1049267.6221340634,1510988.9570975625,55127023.444388166,4152812.908573089,3663090.0421754727,748611.3682683413,5849721.711451868,820982.1894703442,4548787.535263063,1824374.4550718023,4015402.4672140265,-112224.77020799695,2282965.4239961347,602163.1285090479,1705374.2396894516,1070692.0123642413,2486290.2882573456,1175058.7826113885,15848455.196727108,1621009.819072973,1873390.2309555332,5634517.0268389825,17969769.50715447,6415643.27751462,1233294.3703409426,4490259.757746538,4544270.401822325,3873747.8537037987,1813006.9858601093,2173967.764707398,1752926.1936193104,11865533.21929725,427017.8281881516,6111503.940354746,3321599.1760396017,4860031.110877097,11805939.562326405,960532.8531532851,7623084.961897874,1566137.0067640014,2105615.6928184433,1332888.0391892288,16467820.505690236,18141044.041408967,5251972.965709582,1215640.8220536602,4903365.890623987,9372056.019873273,399369.45178222004,1438437.4489211955,3689762.9871792477,3415989.679111969,1576485.4316652468,1451541.2140483062,51731821.505082555,449434.4174161614,857817.8921108139,16064416.34930469,5450008.627454074,-249165.49997123936,1670908.2752679922,2774948.5293789357,21374631.302190393,1730625.77638871,2103701.812510183,3248189.4789001807,742501.5946036545,1313389.359781453,1448794.7547626733,785623.8966321726,2299104.1334982533,1335526.395586974,511447.5272986139,15558905.555210812,9795173.141823392,1520850.0397296206,4974010.72260059,26700860.87149976,430818.52074835496,820944.5234982988,3022356.3900594166,751580.113669374,27229762.982679937,2031580.4796929648,4887194.156855416,3532160.853764118,116710.9150764402,4355954.192275606,9181107.313875696,15796909.154371826,910996.74713174,624564.6271957615,626483.6590138068,5166741.091468679,4090910.347068983,6585041.46679404,472892.130192979,1218255.7888549003,2887674.6765454784,14754578.6683973,807268.2298449506,5224340.576294739,32961592.708469555,2628039.946024193,629864.8103122672,9293460.419912584,940130.8100283449,909753.8870335412,1642823.4052926197,3757892.844765117,23495148.716968942,1950298.2893865672,962179.1527979281,3083955.370621909,643205.624160307,383094.0502276425,5191348.3325484805,1909943.217965917,790149.6583528006,3535418.393265061,8173670.269515302,2859270.3211339945,2578279.9197400035,5287449.7416115645,2733579.568816638,4224328.240827529,2094144.3753586663,1958803.8946798553,2055605.844791547,530981.7829991833,5020211.819960089,4983514.958398472,14541744.70778779,7443807.414337337,4940924.098316948,1719241.3767284793,5765918.843188735,19738.3476074012,600595.2257392444,1536081.021350474,2416643.5320139383,8761950.315632356,855487.0962231115,2229494.8318359545,480440.0810073321,1579440.5963137513,613228.3251192989,1600538.8164493633,-132777.77048188308,1971678.7123535527,1111868.0043435628,2997395.363557543,2001149.09159607,1725055.7267298095,305251.05258416734,359752.59291135566,1445004.0511926631,3065334.0974559607,5968734.103937291,7066454.647408783,2583955.4955035155,-119543.8165468867,2779461.7913754154,1402528.0209583421,758888.6305981767,1721426.2278165536,1592024.484021821,3639036.7119846963,718498.606362453,1235102.3085572182,10141534.086167432,2320115.451348803,755697.4416847636,3306063.750152624,611211.4757888159,2366146.0882054945,3670122.340374944,5279941.615918929,5146715.592933737,937265.3287549524,2248353.0611537816,1823968.9153204984,7798156.100518335,4727916.4401734965,1551934.4735884378,4280571.769213626,666684.7472752337,2718334.1180505436,7181094.473187562,1490610.6373846522,8577373.724871896,801227.8462996488,2501476.3251614454,1239210.3012253626,5476261.156468799,2027529.8349110913,1405464.8534248646,13376762.56774765,1478829.324863213,4227149.012960916,1616879.6446101638,2531701.998522187,405078.42952330434,1321019.346550438,8792732.643579086,5604020.017109336,2180047.643997182,5512401.02160548,10031756.53284035,733498.6995259507,6207157.079989258,633653.9971973903,996086.2573893354,683654.4966489433,1315408.8842420224,14558490.61561077,4686981.157805653,1129909.6726143192,678736.5222870545,-188994.93329656962,1108532.3462524898,1022962.8310859823,4638334.12957776,540883.0722101585,526332.5313251817,1339670.3395566698,960401.3663929091,1138914.043123803,2522851.2285074713,2121156.262991433,16902952.39066483,2128360.6838422013,1650098.6532330604,1526164.617746117,1075733.2378606657,15247776.01667415,1424098.5964049527,5929053.518063525,3949960.8301041764,3501396.653319739,9609171.393930621,17252375.767605502,2201846.582633064,9588200.497408886,646802.8627174494,836023.6570508326,4409281.468190149,829998.5852333643,214832.25931177568,3005983.507061529,25775343.09995081,4240111.954860361,1632716.574881386,1032232.9099740644,4672029.262462862,1287204.4395950865,1900460.007282453,1349880.640438545,5791413.52613106,1493358.4185270597,11094480.024761967,2935627.2309272857,2257656.7839853587,819946.5138055086,8695739.724215573,3905825.2942609442,2720299.195241343,5038005.342986373,2220741.1610609517,1088983.6956519298,676371.2888329856,10481830.934913965,1173252.0982578197,1330672.4507828038,162925.71292512584,947281.2715146733,330100.0042406034,1789131.3904350335,8336911.926865764,3685335.0367176323,747754.3644600005,1114113.8742215685,8266512.973775586,3831510.6004417557,861972.5422909572,1934840.2810763419,874860.1793386152,1426582.0708503802,1897494.8563202857,4033989.670035742,1075909.8317564724,19652354.02553003,10540218.526759526,4432049.477978045,845259.4573101995,554168.7709279219,3398013.624139595,5249325.036161352,595895.4517975161,1004898.4570388063,6344829.044837961,1945589.5777510367,478522.8558775352,5724233.01759949,8265965.535305772,159489.25002671196,2894543.235791927,981236.568988828,3816258.4781249743,2142347.990429699,1142892.154042251,994487.0267800919,10870481.934011377,4876809.500446687,540836.3505429381,1170776.0100500998,2397301.7786406325,-179588.0245377766,7199440.756810294,609705.209587639,2622930.8219534415,1780354.3431306675,5716578.258456604,450228.08218476176,4209931.038329482,4985481.785939105,447859.3644544948,1163368.4240258613,1597442.9424163657,835691.0389181355,7072658.423730057,3969260.492152662,129718.08330863062,5714571.303111307,853666.6443225876,2085228.1591699484,4434116.933417974,2451143.39713291,3348400.8103312957,7619512.280848909,1240980.0059207731,4879266.821501014,3833946.6173845064,2388959.026547444,20889440.43447798,11432299.464347864,2227442.063377998,1281941.7684862185,9612260.914069286,4142342.724753775,61648055.671459936,548055.9095265472,681538.8978285585,9823105.598722836,824977.3333405871,401198.9732670593,1729210.2895495545,2702686.1237536184,773848.6956237217,2528145.6107790354,40023042.615042865,1209404.195501159,3878193.1723468387,1195175.4192643333,2398726.4981544814,1023357.7154790503,1251615.4896271536,2262371.405684816,5980023.317850821,3888509.8921246906,2308476.058784639,1424224.782259897,890029.6689479246,3708373.2498847945,2766784.37515139,14426564.52632154,2062599.1417168789,8119703.121435124,969830.0539094708,2344353.876234344,103080.91301559308,3193092.9580964977,1934244.9248078507,3750413.877327979,8754609.181517065,880420.5844841804,16192175.209945226,1610988.0166652878,5392598.845497786,1002058.4030955064,6886876.905948631,8929106.634687414,1542856.7218912567,2781437.0596762104,3766201.451859984],"xaxis":"x","y":[7506587,1972078.625,7768294,706505.6875,2274873.5,2547582,4571388,288191,6252842,1126157.25,1153276,4103800.75,1963054,3292049,724259,819803.6875,1589550,967432.8125,984178,968026.3125,1688745,678663.3125,19236770,1359948,342493,497620,2682044,4420650.5,866343,1250669,1770027.5,14084606,1981115.25,1986094.875,32141070,5415438,932083.5,293217,3543985.5,2443065.25,992794.6875,885865.875,552729,3580091,2993610,284867168,1222509,5672838.5,0,965786,1429146.375,1648774.375,726124.625,30648154,1836671,876425,2192111.5,1426823,1748132,1791071.375,646377,1487660,1882845.75,893053.1875,4247457,1796889,6426022,2493240.25,650008,385719,1851260.5,75073888,804002,25453690,1997488,2712983.75,913899,9546165,5122039,25307744,1313366,761504.875,1302636,234144,5968153.5,2966309,58761304,1160989,2217620,142873,2051501,1069490.125,348061,545426.375,1113457.75,2665558.25,628023,4759256,0,2406996.75,501194,6508595,14829099,2491238.25,4135650.5,1023418,454296,1682454.875,813038,11651200,6797528,1145535.75,1348497.125,493000,766628,8204897,3960898.75,609101,17463744,3348280.5,863728.8125,1692323,2112095,1878742.875,6711975,675611.375,2196718,1199742,748775,1545356,19645206,122156,8393893,866825.375,1357116,2496807.5,615894,845044.125,526752.8125,2991674.75,3093944,25471308,7221482.5,3894584.5,2502081,630578,1569569,1435730,2512443,8971926,19248732,1678495.25,586589.1875,1832808.75,1132489.625,811907.625,702385,20361158,5647146,2323265,41655852,0,962971,1200811,1107290.625,1761137.5,14029480,248607,1037555.125,659450,3528086.75,820705,2433777,4549892,1358652,2558573,4287439.5,1239315.375,786979,730765,2781314,567072,967082,3228330,1243282,7606084.5,2103293,114648520,1977299,798970,4700552,1813864,1482554.375,759060.8125,4946798.5,678351,3289053,3326999,6935723,1386331,2740128,888031.875,1809101.5,581289,975431,4475650,1441445.625,8287440.5,659652.625,1538750.625,1306087.75,1058986,1060846,131810,3040978,1593187,791564,1085183,818317,3022275,520738,1224685.125,2114860.25,1088016,4520838,3626431.5,4448582.5,9212950,2392466,6689184,13586070,2074152,567608.625,594848.625,3009463.75,2446842,2161828,7794869,15206762,10084950,3906967,25631512,1022114.5,1182575,1218112,1249939.25,1788148,6102883,3093378,1493131.75,832168,7791957,839362,338923,2092251,1929828,1061458.25,451738,5820965,2284507.5,3740373.75,5417598,1632498.625,992139,1232822.875,3590731,9851937,2069880,27139746,514555,948676.5,665200.1875,468973.5938,13140574,227509.5,4185406,11509942,3185509,20630168,2928099,1391916,111969.7031,4842910,2278160,2094410,1138520,2470566,851651.1875,19397324,1244824,35383012,777587.125,898115.375,3471845,2190460,17686444,696605,2396523,651293.3125,1187261,5661689.5,517029,4673827,1255834,1970956,644415,1290284,2656729,2041912,4700396,29195468,1403869,42709624,1986384,1240875,509485.6875,6856153.5,369291.5,1464856,5420552,1140620,1424157.25,3795938,498745,4028943,873648,1234573.875,936336,2503690,2756214.75,6884914,766381,1384558.625,1260416,1214929,5448975,832986.875,659761.875,209644.9063,1269847.75,2526375,3467070,600318,2338852,11765682,1232210,13901935,7539963.5,4425832.5,24906132,3579008.75,11353672,7475577.5,608491,11289052,698367.6875,1525624,28731502,1656420.25,557082.1875,934357,1020039,5288304,9049598,899266.125,588969.8125,16016644,387810,6056481,3437441,1441665.375,8800292,8386493,497991,738990,2159932,2436403,5747645,1150582,1912690,8787353,3024478.5,863773,2274920.5,1187315,3158151,847298.8125,7613116,456397,1026921,5974993.5,666385,2559461.75,1170616,1967129,3928695.25,5464762,20781390,629027.3125,6129780,964236,929950.125,1815371.25,1777652,1242811.75,562479,8964061,7577723,1537570.25,403800,855946,10416934,1736775,684739.6875,786324,827042,892086,1735816,2404644,3568616,1046995.188,276107,243518.2031,2212058.5,6858533.5,318364.8125,639407.375,10126597,2227908.75,849504,8381744,4397572,1676426.5,3761647,1228837.25,646505.1875,717766,1195351.25,934621.1875,1152896.25,4103027.5,3392129,841891,1692199.25,980356,4374646,2332996.5,35891484,2845946,2674034,627854.125,2181833.5,5424718,298063,3560614.25,978463,2130673.75,1466382.25,3706100.25,33268410,5040382,508051,55813996,1075019.375,2100722,6227419,11356990,653139,204499.0938,5722325.5,10564729,548293,619545.125,1162120.375,901015.1875,1074937,1353337,807185.5,576405.125,1662723,4329966,7133222,27282662,8435093,8058913.5,3567977,609218,6915293,509124,3869754,363808.1875,2204235,4307238.5,1123416,2799595,1997115,1943772,3467698,448385312,2417401.25,5249190,664088,415975,16917964,13609040,3537841.5,13458948,9157150,2204161.25,47952868,847097.8125,4571746.5,65047284,3833162,1211947.375,1458617.125,3014332,705948.3125,2022990.625,1567040.75,352136,2168281,1160087.875,588965,628609,3481727.5,1677002.25,619804,13943008,1053290.75,1990444,565458,3494929.5,3155391,4089677.5,1998725.25,712057,935396.6875,8201974,782202,848497,4368113,1165598.875,2420605.5,592739.3125,2184912,3600897.25,4486477,980603.375,958798.3125,1886382,2503948,4751090,1651120,6335315.5,956481,397624,3063969.75,1841154.25,1295685.75,2586116,727703.5,1636649.125,1855556,719471.1875,2267094,1787119,1331122.125,971308,902694,5024837,17186624,55073120,1819329,512166,772379.3125,1049436.25,887403,416566.1875,673834,609257,759405.6875,3155496,1070085.375,2006626,2804534,747384.5,1057256,1054581.375,507428.9063,2664566.5,2065156,9697866,847514.5,6522551.5,720005,573624,12783255,0,2136086,15480681,253832464,2339234,4212149,3021669,3181045.5,2601369,1656025.625,646994,1565475,7802205.5,1187723,1356874.75,2298648,1136698,1586388.75,1018111,9085108,1734533,1703504.875,1868497.125,11047668,433443,5459657,1841968,9444484,38400928,151754.2969,536642.875,3717064,2159170.25,1625308.625,682632.3125,785144.8125,2553763.5,4750287,9617898,1025205,874707,618876.1875,3162976,1520326,2114053,1109924,7546235,3542542,10444602,462465.5,4023479,1761048,869835,1189942.75,2211671,1735538.25,789630.125,1962351.25,827937,28450436,6074249,9861998,967697,2107743.75,696054,1152890,3791402,965651.125,2090895.75,12480984,552015.625,2904225,466985,1268459,862744,313528.0938,1769998,1547116,2023858.75,863703,4656777,11061916,5233553.5,1168017.75,2300803,727775.8125,1443367.75,4608963,2342494.25,706879.5,574575.375,855670,666740.3125,1981360,1193790,8773058,520348.9063,17666646,1607590,1227772,14332778,410433.0938,926261.1875,2796425,1234813.25,2701833.75,4120963,4742869,37276920,18622292,2157144,1776411.75,666967,1080203,3031811,433871,908834,1076644.75,12448381,15843743,709979.6875,13239050,143423024,12196872,10992344,1291170,835545,1050982,1513700.625,1829122.375,2549649.75,1844825,3989399,1857329,1495686,32381836,1109851,6897247.5,1863784,675772.875,1018068,2378077,972608,1181428.75,5653656,7771933.5,1827424.125,6419090,1218114,16701055,771512.6875,162960,1213444.125,2777052,1185469,490599,541542.6875,1103762,811537,2932696,1848656,775007.6875,2172841,1788999,837792,784762,2737534.5,1695067,3060149,4178233.5,2924362,1271753.25,683274,712149,4020660,59757440,3298570,760090,505601,858897,15660628,323739.4063,5702077,9387211,37022180,1712279.875,1742351.25,1001964,1612797,638507.125,530700.125,2327611,1261634.5,1072487.875,1162561.125,6454983,1063960.125,151376,2534873.5,645756,1200381,2855545,2219744,688145,1362993,456197.5938,9523142,502898.6875,611997,13171370,1009476,739210.5,58041572,1459199.75,607579.6875,839011.3125,862826,1128179.125,2306723.5,8437401,2147062,646691,1619177.125,2236167,4058492,966764,1795462.75,2165371,4322873,2167888.25,1251906,669217.6875,471113.3125,664539,1983509,792015,794724.625,53079164,3629002.5,11924933,1375366,14641502,14469228,11813861,8878186,9273278,1985171,599591.625,489557.0938,3646053.75,695606,518107,699217,2499140,3187950.75,1543282,634056,1117984,687953.6875,7564604,4230514,1859725.375,569115,2564004,435985,1972491,116486.6016,1904641.75,1743198,347437.6875,2147013,5071974,713821.875,1869575,13659926,10721568,968343,6960396,886599,22055140,872555,31803568,757167.3125,1274049.75,4054582,601590,7969653,8259071,2427919.75,588856.125,1642471.5,668915.8125,2244317,656345,317794.4063,18558162,666392,3682374,7654264,623107.6875,3039889,18011944,1483856,807358,5762597,779503.625,10077532,5284343.5,522791.0938,1696446,811703.6875,1419453,53166156,2075891,3441643.25,3220827,1308348.5,127374,25210246,2828098.5,1437220,698177,1786986,571384,2152312.5,1910465,1204956.5,552007.5,1947717,456747,5903142,1259456,2436063,563454,15036617,2050600.75,1256456.625,2334260,18829136,4988493.5,2085349,5587347,2759071.5,2137686,841242,1587454.75,1591124.875,7996408.5,2655387.5,2303386,15324777,2584082,3850963,1037696,9765777,3314418,1929698,1987073,17750994,6914839.5,3841879.25,557863.6875,1951856.25,1555420.25,620104,604591,27119380,7202141.5,752441,2314835,39523852,1084654,769148,4545148,2773647.5,938908,335245,1400053.875,9722237,3035962.25,1817550,10005787,1581036.75,1773864.5,1242764,1992158.25,28229320,696709,849745.6875,6219841.5,5950438.5,731315,2033074,24882924,442375.8125,721723,1929851.5,842923,14776743,938096.1875,11224871,8387933,1645327,3371455,17871658,2290681,737570,492421.9063,2014475.875,10040121,8611054,6298131.5,1140137,1965529,1363296.875,6994740.5,632517,13253979,10614234,4345698,448600.5,4498367,652425,948157,938833.625,2759674,8233527.5,952879,830739.8125,786253.6875,919739,724153,5669431,1427974.625,521341.9063,3936546.75,5214621.5,1415355,8839812,7456330,2527198,1472548,2222097.5,2003759,1280245,1083467.125,2407904.5,12037792,10053489,3983784,1541897.875,2798967,6057734,507625.5,1038815.188,738983,360045,7753080,573395,1152368.25,680395.875,1929011,1612392,1297041,1354000.625,5381105,1100499.75,6136327.5,1357701,1724560,601897,591840,1148202.25,1618916.375,41092076,3548309,3641620.75,458746,1489363.875,4527521,756196,1314357,808829,2971013.5,647321.5,564988.8125,9436408,469499.8125,674961,405062,2375104,1361794.25,3518459.5,1971236,13320624,671079,662012,230365,5792926,1562708,529806,2964206,829440.625,161311.0938,2539565,2156407,3970754,3460275.5,2235765.5,1084553.75,3897738,1398253,1453543,3780078,790039.8125,1655612,1128208.125,7240274,488181,819314,7011508,4045980,1076693,4742636,5231373,151798,8751704,768755.125,1221806,577290,882759.1875,6310254,84980760,759508.6875,1417852,981009.125,639878,844076.8125,7231928,1256360,1054995,1415494,3577120,763693,1177583.375,1063480.5,59107620,1657201,885843.1875,731092,722951,18868768,437619,3091387.5,1878442.25,3118304,8765978,15725581,1529309,465533.3125,3275779.25,1287159.25,3986851,484875,1261963,1644969,7545909.5,1633399.375,3294645,687049,2775717,1098965,1688739,1641546,686019.1875,2852102.75,4079837,2077039.375,1136405,1372475,3000864,11566123,4519115,2295176.5,2185530,991283,574650.375,4039667,585559,776996.875,635516.375,730211.1875,1067793,1437569,8651835,19061780,1372834,1136676,2634166,2009324,964015,1237252.75,507277,814403,1594733.625,7008252,1302192.875,25377650,9097980,1808401.375,1000599,929628,2520310,5917000,658297.5,705262.125,1189402,1034827.313,522880.0938,6361236.5,1118392,1082920.125,2330029,393493,1697909.625,6015128,666909,524053,9686734,2471623,2225265.5,1785906,1245181,669787.8125,4863803,1243200.75,419978.6875,2531163,2805926,2166711,3660395,614768,2213115.5,10975751,427697,784329.5,12187231,4099807,843267,1074180,474359,801264,1955013.5,1365778,1615569,2892428,5826745,2890379,1247362,915417,8050977,18661606,4341706,1039782,8070420,3132453,37797084,679574.875,1022614.313,19018762,4841347.5,585077.125,550324.1875,6881405.5,4644346,1248599,32552428,788027,3529643,3996541.5,3226884,676499.875,559805,123767.2031,1835804.875,1451145.875,1040593,1567746.75,558709,2853189.5,2568377.75,4309177,1188444,9343428,657385.8125,2979413.5,1646088.5,1696009,1524620,2438430,23612824,818484,4222682,2001936.5,3739029.75,2245568.25,5928881,6268627,3027867.5,2277238.25,2017423],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Lasso() vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predLasso"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["alphaslasso = np.linspace(0.1, 1, 5)\n","param_gridLasso = {'lasso__alpha': alphaslasso}\n","\n","GridLasso, \\\n","BestParametresLasso, \\\n","ScoresLasso, \\\n","SiteEnergyUse_predLasso, \\\n","figLasso = reg_modelGrid(model=Lasso(),\n","                            scaler=RobustScaler(quantile_range=(10, 90)),\n","                            X_train=BEBM_train,\n","                            X_test=BEBM_test,\n","                            y_train=SiteEnergyUse_train,\n","                            y_test=SiteEnergyUse_test,\n","                            y_test_name='SiteEnergyUse_test',\n","                            y_pred_name='SiteEnergyUse_predLasso',\n","                            score=score,\n","                            param_grid=param_gridLasso)\n","\n","print(BestParametresLasso)\n","ScoresLasso\n","figLasso.show()\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[9090272.136699114,9090272.037197085,9090271.934887495,9090271.84376998,9090271.73691992]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[10901622.012787886,10901621.93889909,10901621.861987393,10901621.795605093,10901621.713994522]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[7278922.260610342,7278922.13549508,7278922.0077875955,7278921.891934869,7278921.759845321]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[9062027.720249284,9062027.744801199,9062027.769353155,9062027.79390516,9062027.818457209],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[11559259.44580465,11559259.25690975,11559259.054268539,11559258.877761854,11559258.672985388],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[9724112.572800327,9724112.453829894,9724112.343078408,9724112.238496313,9724112.11164087],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[9162322.07174529,9162322.102955233,9162322.134165248,9162322.165375281,9162322.196585393],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[5943638.8728960175,5943638.6274893535,5943638.373572129,5943638.143311298,5943637.884930735],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Lasso en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Lasso pour tout les paramètres de GridSearchCV\n","fig1 = go.Figure([\n","    go.Scatter(name='RMSE moyenne',\n","               x=alphaslasso,\n","               y=GridLasso.ScoresMean,\n","               mode='lines',\n","               marker=dict(color='red', size=2),\n","               showlegend=True),\n","    go.Scatter(name='SDup RMSE',\n","               x=alphaslasso,\n","               y=GridLasso.ScoresMean + GridLasso.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               showlegend=False),\n","    go.Scatter(name='SDdown RMSE',\n","               x=alphaslasso,\n","               y=GridLasso.ScoresMean - GridLasso.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               fillcolor='rgba(68, 68, 68, .3)',\n","               fill='tonexty',\n","               showlegend=False)\n","])\n","\n","fig2 = px.line(GridLasso,\n","               x=alphaslasso,\n","               y=[\n","                   'ScoresSplit0', 'ScoresSplit1', 'ScoresSplit2',\n","                   'ScoresSplit3', 'ScoresSplit4'\n","               ])\n","\n","fig3 = go.Figure(data=fig1.data + fig2.data)\n","fig3.update_xaxes(type='log', title='alpha')\n","fig3.update_yaxes(title='RMSE')\n","fig3.update_layout(\n","    title=\"RMSE du modèle Lasso en fonction de l'hyperparamètre alpha\")\n","fig3.show()\n","if write_data is True:\n","    fig3.write_image('./Figures/graphRMSELasso.pdf')\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.583e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.544e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.239e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.953e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.601e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.632e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.007e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.593e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.683e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.653e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.645e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.271e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.303e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.063e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.708e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.700e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.738e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.122e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.767e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.335e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.795e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.185e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.369e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.828e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.818e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.856e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.250e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.893e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.403e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.919e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.882e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.319e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.961e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.437e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.986e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.950e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.391e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.033e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.472e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.056e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.467e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.020e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.508e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.129e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.107e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.094e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.546e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.544e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.186e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.171e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.628e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.206e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.268e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.580e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.286e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.251e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.714e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.616e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.369e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.353e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.335e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.804e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.652e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.442e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.422e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.455e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.896e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.689e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.534e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.993e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.512e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.545e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.629e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.638e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.605e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.725e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.092e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.727e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.761e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.734e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.194e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.701e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.797e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.829e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.833e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.300e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.800e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.832e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.933e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.935e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.902e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.408e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.868e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.040e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.039e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.006e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.518e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.149e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.145e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.631e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.112e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.260e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.936e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.745e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.254e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.372e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.364e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.969e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.862e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.329e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.486e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.439e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.601e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.098e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.475e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.588e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.979e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.033e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.064e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.663e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.833e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.551e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.336e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.701e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.717e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.217e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.815e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.094e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.122e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.929e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.042e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.775e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.177e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.109e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.150e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.949e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.998e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.294e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.887e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.180e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.808e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.155e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.692e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.065e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.202e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.574e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.266e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.455e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.227e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.218e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.485e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.536e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.377e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.406e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.273e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.251e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.731e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.432e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.254e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.146e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.326e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.625e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.516e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.035e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.923e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.592e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.798e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.696e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.294e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.315e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.925e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.897e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.334e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.126e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.637e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.736e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.352e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.560e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.934e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.834e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.832e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.656e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.359e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.992e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.032e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.085e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.369e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.461e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.386e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.015e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.175e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.261e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.216e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.401e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.415e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.264e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.101e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.184e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.467e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.002e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.303e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.921e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.387e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.748e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.423e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.836e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.442e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.343e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.340e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.498e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.570e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.429e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.544e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.465e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.453e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.547e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.482e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.751e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.413e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.224e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.686e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.616e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.290e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.080e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.154e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.704e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.639e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.485e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.824e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.475e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.766e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.610e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.777e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.502e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.724e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.814e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.494e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.980e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.928e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.469e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.669e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.413e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.522e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.872e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.879e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.353e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.510e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.931e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.980e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.026e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.518e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.525e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.531e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.827e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.873e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.958e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.917e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.030e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.076e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.120e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.160e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.619e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.704e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.572e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.069e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.662e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.537e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.110e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.183e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.542e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.148e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.552e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.098e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.997e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.547e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.299e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.066e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.199e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.033e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.843e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.812e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.268e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.234e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.216e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.778e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.762e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.247e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.742e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.557e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.561e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.276e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.564e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.303e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.127e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.568e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.204e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.180e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.355e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.404e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.762e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.762e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.872e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.380e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.328e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.762e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.925e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.949e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.328e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.900e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.352e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.762e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.574e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.373e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.571e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.226e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.394e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.577e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.425e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.762e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.247e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.580e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.446e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.283e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.762e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.266e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.028e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.762e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.971e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.482e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.648e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.412e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.464e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.010e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.991e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.582e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.430e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.763e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.584e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.648e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.446e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.461e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.300e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.648e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.586e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.588e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.648e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.342e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.498e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.329e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.315e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.763e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.540e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.763e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.527e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.073e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.648e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.086e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.513e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.059e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.475e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.763e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.763e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.044e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.590e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.648e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.488e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.500e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.648e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.591e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.511e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.593e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.386e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.355e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.649e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.583e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.594e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.366e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.552e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.120e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.764e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.376e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.764e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.563e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.129e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.573e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.649e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.098e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.521e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.764e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.109e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.595e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.765e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.530e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.649e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.547e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.649e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.539e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.649e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.418e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.598e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.597e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.765e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.599e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.614e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.403e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.765e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.395e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.650e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.161e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.154e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.411e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.600e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.592e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.607e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.562e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.555e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.138e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.601e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.146e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.600e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.424e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.650e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.621e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.650e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.574e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.568e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.602e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.441e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.601e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.436e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.650e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.637e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.430e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.632e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.184e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.179e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.173e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.627e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.651e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.167e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.584e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.651e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.652e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.579e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.589e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.603e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.593e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.604e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.603e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.450e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.446e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.604e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.652e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.646e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.458e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.454e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.642e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.189e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.654e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.650e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.652e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.770e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.201e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.193e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.771e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.197e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.771e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.597e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.653e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.604e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.605e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.601e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.606e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.653e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.608e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.605e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.465e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.654e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.470e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.606e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.660e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.462e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.772e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.666e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.204e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.773e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.657e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.468e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.655e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.213e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.774e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.610e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.663e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.775e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.613e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.207e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.655e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.606e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.618e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.607e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.656e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.607e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.607e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.657e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.473e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.479e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.477e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.475e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.777e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.668e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.778e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.675e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.671e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.673e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.222e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.658e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.218e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.215e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.220e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.779e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.620e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.781e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.608e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.622e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.625e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.658e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.659e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.624e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.608e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.660e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.608e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.486e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.608e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.783e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.481e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.483e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.784e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.681e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.484e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.677e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.678e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.786e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.680e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.661e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.228e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.627e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.227e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.224e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.225e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.608e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.628e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.663e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.788e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.631e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.609e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.630e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.664e+15, tolerance: 1.529e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.487e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.609e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.609e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.686e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.683e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.490e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.790e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.488e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.795e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.685e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.792e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.231e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.684e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.633e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.797e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.632e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.230e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.609e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.492e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.609e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.634e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.687e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.635e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.494e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.609e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.803e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.492e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.689e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.800e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.635e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.609e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.688e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.236e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.806e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.234e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.236e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.493e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.609e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.809e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.688e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.636e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.235e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.637e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.638e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.495e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.690e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.497e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.816e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.495e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.496e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.820e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.691e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.692e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.638e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.639e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.239e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.691e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.237e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.238e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.824e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.238e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.497e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.639e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.692e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.640e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.827e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.498e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.497e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.693e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.831e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.240e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.835e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.693e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.498e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.240e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.640e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.839e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.640e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.693e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.239e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.240e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.641e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.641e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.499e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.843e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.694e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.847e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.500e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.499e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.499e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.695e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.641e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.242e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.850e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.694e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.694e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.241e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.642e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.241e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.854e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.241e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.642e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.642e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.501e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.856e+15, tolerance: 1.540e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.500e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.696e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.642e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.500e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.695e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.500e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.242e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.695e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.243e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.695e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.643e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.242e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.242e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.643e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.643e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.501e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.696e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.501e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.501e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.696e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.243e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.696e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.501e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.243e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.643e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.643e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.696e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.243e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.643e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.243e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.643e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.501e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.501e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.502e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.696e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.696e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.502e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.697e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.244e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.644e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.243e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.244e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.697e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.644e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.244e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.644e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.644e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.502e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.502e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.697e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.502e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.697e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.502e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.244e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.644e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.697e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.244e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.644e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.697e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.244e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.244e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.644e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.644e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.502e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.502e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.502e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.697e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.502e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.644e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.697e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.697e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.697e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.244e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.644e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.244e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.244e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.244e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.644e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.644e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.502e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.697e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.502e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.502e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.697e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.697e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.644e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.502e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.244e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.644e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.697e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+17, tolerance: 1.529e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 7.221e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+17, tolerance: 1.501e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e+17, tolerance: 1.540e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"name":"stdout","output_type":"stream","text":["              paramètre  ElasticNet()\n","0     elasticnet__alpha  47686.116977\n","1  elasticnet__l1_ratio      1.000000\n"]},{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+17, tolerance: 1.649e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predEN=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[6003116.030979849,4053514.748802008,-2465863.213946097,1142726.6066255681,5983100.124426235,2527391.304178948,7970461.700014539,3044163.5179434167,3857053.931657475,994894.5910768213,3288182.8388539897,3069921.2095970428,1180943.7617132282,2936786.8634425416,1527448.9660461326,1665878.0058895275,862294.6280632089,2419399.753203582,657978.7987664118,1329752.7728753833,1389049.3020537707,217441.3319297782,6049727.816220777,1976648.7254151534,1309466.0947891765,439991.075708376,4015942.046415053,1399125.966688037,1017976.7692694115,7193198.8461178485,2792945.9134180257,2701187.859219933,1716543.6912668892,4503572.213482985,36715043.9967335,446226.9014676416,-9543.407288127113,1960681.6942304294,5866529.141160557,3485516.7699160567,1413622.6997448811,1199198.3891377142,4580874.067244362,7836986.528911928,6615163.729375256,10924913.110333612,1787734.2911343393,12455006.966223897,11174810.075921217,1186881.4774425172,5580144.019716721,2306271.3932707133,1117359.9063752135,19543164.315054763,3113164.052838614,2626881.712097113,1927319.6978181335,3046644.57789009,3345963.645188811,1377667.2249626766,369115.8735345935,2122152.5260744896,5606135.486046221,1320333.9580496445,2639973.0018429686,4835286.139975836,6672369.293793624,2416184.532685395,222975.86648099055,293795.1242890358,3912541.5831678715,4739372.709149981,236876.8966913866,9622829.743241517,2238563.458713909,7225846.36744857,206205.1359720528,726386.8498182101,4134789.0496613747,28017754.83441237,1507063.369790159,1368706.8145587414,1275001.2667915486,422552.36102138646,4849434.228277413,4772601.089392811,104995171.7274702,1633147.2888967793,12113.094639264047,851426.7648027407,988470.9381574728,2337786.0146783926,399455.325136733,1774841.2324764282,744291.3497506613,1531485.7098209239,1007973.4548383623,12203798.890347997,4871449.384849824,2626996.678781378,436707.5031011333,16715875.56061162,5186258.744534548,4155505.106406904,17134921.816694412,8504693.097819608,644887.9037689667,537149.9700646764,1482890.726678214,4915999.728866432,2322746.0539329983,3301092.6027970933,1934140.8151391002,399455.325136733,674615.6784784945,1551814.8470967659,7113221.392435551,514910.04264268326,33414244.80701996,2444144.8794240053,475983.1955958614,3897268.7628757367,1990782.2030507482,-227855.07520347508,12839000.439994557,627408.7570250477,4211630.964990152,2294901.243289509,-233467.52738655498,3849516.328899754,13940429.106269851,950736.0416665643,8343150.778670138,852184.0428197961,493685.07042273995,2117733.3467673045,206079.1445993802,674615.6784784945,1014041.3451587902,3064311.025270062,11034222.661724066,19804444.669091716,9186367.06168207,11077612.633594131,1425474.2803495307,970248.7364505234,2004951.5317316437,1439921.9496665827,3351389.2241764483,11148763.497161523,5281453.032012807,2665355.355859411,2139229.6374481614,2719711.377446943,1359886.0827575107,1064074.6225992297,1693422.8235500893,19519192.292689886,8215633.466330657,1544519.3407311733,54295041.3460665,10873959.747303193,2318636.0823035548,1686663.1628119876,2102168.3810978774,1334421.0738459658,2561826.1460215333,966062.9202853057,2219184.6188603514,1047243.6703200745,4817107.113393339,1104063.4791663329,1226451.5521999365,4436172.739676876,834766.3715297445,3434208.509953879,10202292.387235891,1493011.4505144283,-66240.96821691329,814587.1994022347,641004.5601167555,803358.8739502935,1181425.9863921045,5101266.461945164,2047336.1984474685,-45479.437049578875,1511091.5201897733,157390064.73142564,1480790.752530155,696099.0236489549,2628725.7764402153,895223.173208413,2983199.2497858107,999682.5327809572,5405575.492422497,-443327.83031444764,3930210.2136087283,3819168.524655018,15547033.678492038,1159489.581794719,5680534.088947848,1820472.1600224825,10276564.2265456,1906795.2055639427,1289483.1979866081,655088.029349729,1861740.7914114627,11357338.110742344,1678512.2653666367,1496697.7932504273,387769.9730445137,1024679.5787755232,2673592.133317981,473070.56046382734,2297673.878917491,7165210.8441413455,1363303.1243707798,357377.1557898368,1286059.0823368833,5219824.14928898,253723.5665516355,807230.7875435045,1729470.202984836,1134452.1638243431,2179366.8490671064,4276459.090601711,9665230.592258547,14520866.98195509,2539956.4809114076,8572861.448954446,11460193.215338787,1813050.6178451572,1019735.0045193315,770189.4869307431,2078945.7880060135,1567035.8054525855,3800212.443658366,16095191.717974745,31601828.359908663,19828810.57258409,4142981.390732079,8909638.865188528,1085439.734368887,2058941.075831878,479173.3863759618,1506804.777984874,2285630.80571842,8563082.231529342,8411891.955183227,714491.6877208452,1926077.310208677,20378120.05343643,984648.6552706433,2558345.056714006,1573712.2508790295,3715001.4631236745,4783864.083283035,1659800.5672846797,7210780.148819597,2504651.0042165397,3122591.2526217564,9292472.705381928,1211714.9067484601,648094.5519067934,600641.9461138127,8921603.957000239,14291363.634991385,2176721.5829745163,29056997.431857727,759634.426694074,766914.5287519437,561242.9376296196,868815.3460194895,2234228.7069146675,1718079.0115570915,537389.9349546176,8032991.706663128,4084687.4862298733,20309796.454443667,7703161.722891912,2414810.001263338,1351324.2573716294,3992892.034503072,6227896.611443134,241282.28738324507,1686482.0317576465,1358649.9696944407,589470.6571152639,22261892.415549107,2337294.420226329,22336262.028080814,-139467.47753630858,1037118.8493020919,5405266.255007828,1495049.039627473,3580287.0988298175,814243.143713346,2811346.9856583634,1876184.0573802916,2002663.3632023702,5116781.600040235,505519.0133085619,3069921.2095970428,2199041.809100763,-1274116.6661028988,2420689.2360397354,1515090.9479086483,2951180.1596097173,920256.5654523708,2264435.2539820108,3987941.560858666,2445816.817950923,13182322.984846579,3926900.3955457043,2314995.568554555,120011.21473065345,7751733.816095207,1659800.5672846797,1133311.2986223656,1380763.6742495112,1496852.7210789737,3166482.9892824604,9271512.697448798,2766639.937527094,9614918.687789313,1037118.8493020919,2314995.568554555,-160498.2132980358,2788571.375905196,3176647.785872235,14801767.739353409,1732629.3441723478,2652816.2066675858,-220988.52554060286,2747790.7295442745,3429274.0999660622,798033.3842129784,651958.5281551434,1157316.0091426256,2740541.4486961733,5598542.190774819,1436802.454253349,1040527.5855348827,3844205.7165963817,21672630.945012804,1941975.7365484093,11007981.432249056,11856804.382869,14789334.119075635,14248737.660654677,4980242.676978687,7699173.43615159,5616634.812322393,665133.4754128866,23318467.18744162,1280929.601087852,774659.2517395243,10544459.429054974,684646.1701968452,1106459.2893150544,1658406.718334771,14529.237980619539,5879739.263175028,5956264.135004519,1092849.5262114902,1064939.1714286483,8102579.600872498,825442.8111473848,4422919.435216298,6427254.909591906,5302553.836292533,15811271.132976057,26143924.276076622,1014041.3451587902,331543.6657807068,794411.5891275713,6543625.8619721085,8027030.433934197,1744180.6252078933,10575999.162912032,5798589.703309469,9120749.519251298,532668.8629620392,3054515.426111333,297135.6977419532,799031.26950478,1188865.5663976735,3136280.3461932167,1744824.3068096447,1979722.9343089133,5285254.290571941,575736.9474529414,2148035.550474194,863655.6096522203,1729470.202984836,2926972.2520733024,10699605.836922292,18434173.669904575,1565083.1189068009,1960681.6942304294,1229356.7096855126,1014444.7137097595,388175.74068269925,2357292.5388674326,4661333.516318544,2284750.406597323,8533380.050313925,6213031.668107591,2198446.9934494747,1219984.3116702405,2640395.9440714032,11122464.476996733,1153553.363443749,1369642.7112727195,1893654.8514816144,1045147.8444049265,1178187.186834143,2966060.645221251,1979484.364531328,10608839.019085035,1217755.969565084,5332526.836317772,2470726.5220300066,4140059.628230061,5782504.045881607,1951247.375157806,1040527.5855348827,27508258.38033316,1858158.0933241253,1344498.248518993,2710053.6198393833,4352432.877233865,455825.57980683004,6830657.084667694,2452122.9984576115,837158.5179520785,399867.4336939927,1919514.0005029987,1322365.1479379262,232246.10515471222,12090170.81636304,-11261.530124293175,1447570.1710816934,1661856.469730572,1016557.3925519476,7360965.643093277,795206.542216198,59750624.73350934,3075181.685929344,2685220.701673193,271735.463731328,1092828.625184736,3619629.2105044965,1190644.123141394,2002982.5528217638,2201981.012412508,2370175.4416281423,1097764.9987066803,3153700.0187693904,2266707.572456073,1560766.638238695,792740.3031962141,63666345.90739987,2027469.5941974237,922833.5066554339,15658614.997962726,19818828.464073297,1123582.8227144072,1035052.5474623614,9447544.341724183,29966096.2061303,910697.7260145782,444784.7294100041,1130046.6448018453,1041454.2641801694,1092108.829579818,2022985.2833818374,1017568.424835138,925567.8915450484,2033975.1419727409,3325572.2742946614,19111112.85725093,43617994.59203393,27667261.68130056,6319847.1556034945,9509257.514186874,634449.572201414,10896269.657300614,1584516.1627050266,8528609.626143457,2305439.607649345,1830251.8162163105,6250634.995775602,911743.4058983377,2666582.1667975117,1049902.7030273948,477715.9569810862,10678449.790224819,66029322.48411969,9535356.440989695,13037858.00402443,1065153.0562826802,3017309.699729423,9790794.290977946,21891881.733650148,1517766.8072814783,9565812.429963304,21124750.61207317,1406880.6164620717,69318584.72178002,1389567.127250244,2449353.917169945,46900506.04805114,4980242.676978687,1223596.7914660703,593892.1799605193,1787314.217298177,399867.4336939927,4626780.993052324,1027549.6265889895,1484883.1682759663,1402776.7987672747,1543446.6214094483,-194480.50253327843,271735.463731328,4084687.4862298733,2010850.1943522082,810558.479138898,2561826.1460215333,4238355.243485281,14949812.371911842,848654.3741856397,1700362.7241000512,2248358.2581957397,7092519.479623072,796718.4312683782,399492.12989865756,1415147.7123546307,2345010.417747648,1373528.6762984574,1039391.3401239524,5405575.492422497,6252697.72903054,1481021.604407323,-665321.1576706329,4554729.156241935,5661345.207546309,4201379.450471901,1638531.7614421297,594895.7327708143,4921915.756239343,5917526.346159596,2238204.878306247,2542389.2531719627,13768503.423185244,2038227.785953521,1151157.5532950272,9539164.244476568,774659.2517395243,865743.6172217161,1263584.547810418,1447570.1710816934,477715.9569810862,3106027.9755776403,67986.74678348121,3235198.5225676927,1257140.6687696006,2148107.125190161,4136211.8135460922,2550540.632698289,3350722.5116865635,6640606.317320367,8390473.770757845,4475699.981654606,682834.859653434,1281695.2317475036,957651.7755312184,1865982.9066097082,84839.38066552742,-429521.91438188637,486024.55731231463,1085126.9313450882,6904009.213797314,3011345.568421292,869339.2042316124,2032728.1402786311,1081413.744731095,2102168.3810978774,1512733.5432506548,246777.69231585553,2886615.2913797,1691566.6260672486,12311032.547333712,1482890.726678214,5156920.279134044,1187787.132714223,765600.6655539726,5986779.900469843,2390374.4513732903,3054515.426111333,828873.4557266599,78150858.0116833,2352985.9644717993,6301715.797090959,4401186.69581169,4955725.91717447,3272571.6332174987,3079335.7541416697,745506.2044554658,1061001.0751478232,5524034.806410791,2160579.1440275335,1033694.6214402565,1681599.8459330322,5992676.945987414,4630948.137319386,3704059.7396212285,883751.5534971696,1503922.6766232476,1306937.9559003026,2513638.829606622,7252180.471027248,507520.1028309325,1696482.3461635248,4503572.213482985,6019980.335163675,52014348.8662606,5926093.301393662,653281.5736158513,434509.45438685874,3372557.156377266,1024679.5787755232,1642969.4722734876,593931.1705264035,3235885.423234286,6583374.181447683,2241822.8550280915,1714840.7272745767,1454958.6436279817,471797.47687335545,7847555.927001484,3853945.6870885156,1547869.6215025838,1447905.181015853,6962041.116199313,2795363.1991099147,10633272.108403016,456431.6396347149,5334979.336952975,1833799.3906644664,432172.63818713673,2297074.8159416025,6601884.91249783,1878442.6288211697,867088.7560315793,241282.28738324507,3039045.426250359,35917953.09671125,8927495.600579245,25024931.35387525,481520.89341705735,2039202.0062873103,1590603.89667868,1559204.7122833105,6730935.742230477,558741.3046686209,1295864.560428399,8162409.806306932,792740.3031962141,3189886.319075764,714464.5798322661,3405832.866459446,569363.5890740205,1476106.664783018,927596.2702933035,96589.89152174024,-137999.3154840013,1535778.3529275577,5803693.862389371,7475344.182372967,10035053.053496243,2825582.9404314156,2962468.096706347,1132049.2998974859,2987363.0120604225,3136197.6936450507,1027571.4634086979,236876.8966913866,755867.2242937239,2064469.7883678982,206079.1445993802,1476156.123867901,223800.30211199215,9328492.294956962,428707.9904128257,45338556.52391554,1686342.7150724286,1537712.7128794724,8316621.726828417,1151157.5532950272,1344843.8481737706,990266.5363521136,-126826.24885017239,3163425.8999462565,2917143.569908083,3411936.45541127,6883793.697388852,9051064.909476917,836926.6880164845,2256652.0585300657,791875.7543667955,1482253.1899025354,1478820.5105925929,1321964.5545690546,1374755.4872365585,1649943.017865621,16819721.941843368,16825182.370407872,1499060.8495994583,21758976.752489343,54483011.042216234,8746459.35784638,8162409.806306932,865743.6172217161,1870231.1337441285,1562633.1488136267,2747590.081101318,932705.1491170258,6668536.074554067,12113.094639264047,3920648.073921521,5610705.441978833,4693035.29632381,25426994.410792883,53049.51116209314,6444352.339188945,4211498.38835791,1109151.3797609364,-9543.407288127113,3096949.5519120414,1389567.127250244,-362075.12558081467,10035053.053496243,6194225.195003297,2632388.779039055,6444352.339188945,2860265.680589932,15053369.342527878,1616576.1303405077,1774750.6669492573,1542120.5814477643,1027571.4634086979,708102.6417340222,103294.09948651586,465839.8565527578,850691.1559620909,1321255.9235133687,5512921.694221452,4728630.8394169845,2066707.6082984463,5541675.3923543,1979960.783441354,2010665.5125859855,4169756.0329748755,-1704941.4625120405,85757.17735312134,9825103.671855677,5450964.194788415,7865555.875332598,1594770.828807943,1321923.0310964275,1189477.994531939,4158623.2648496274,131318207.05757588,5227915.356300426,1177462.6626167784,443181.07636279124,1486159.4382989507,17677365.865866683,797087.4485004013,7714295.965655578,39203804.49486502,41392124.335578606,2443151.5111195343,4106561.559449668,252667.9435072313,1084199.0438603845,1007973.4548383623,56033.7938915384,1316181.4978080117,138162.34850255586,262956.3704054975,2999697.400756911,2653911.3456362286,1174043.8553115898,2095816.226046878,1209794.5558166704,627408.7570250477,2024200.882576687,4955725.91717447,823464.5835868721,807230.7875435045,4180817.149416306,454947.3722696176,12586623.30664645,1309466.0947891765,797671.9481057117,12988546.84487619,840740.032596614,945705.4785873,64412223.083936006,1452872.4307424696,425785.4330529813,344724.6323468012,-56318.79541070387,1860112.450495721,5546364.306730075,26851139.16995483,2490351.518667825,948372.9853175336,1813050.6178451572,4874440.395059641,12057828.803467253,2084008.672957348,1682488.7959195455,4185493.6423547715,1398896.3654785212,8231553.762204686,780161.2052223536,222975.86648099055,255066.8115021647,1607972.9762396086,5335004.859351069,-894885.4448905527,1760663.9681832762,66723336.335880466,5075291.790667781,8200600.732399909,3462422.956510219,15634367.586189842,3334263.455795746,17548171.080222,2050661.213496721,11002222.775295943,4355670.256597968,1040527.5855348827,294689.3225191557,2520609.115082404,2095616.8149123734,54686.69749815483,1269103.7507181377,5977797.86476546,8286642.1046026945,591768.1349717705,653088.2602992014,1130046.6448018453,1505280.5766071412,18062367.876327645,10408699.57630021,2106549.347931826,393930.82797932834,3627721.352859038,555410.138317184,4761007.86956121,959883.1599107918,2702083.8534483737,2513638.829606622,662289.1748830939,2461418.493956489,658725.6765872678,923648.596399969,10276564.2265456,7538813.703191379,3391164.1958895195,862105.1443662713,2762581.6255983114,-162489.54809658183,14775802.050264593,1302492.5491970428,3987941.560858666,1708761.1514416058,1674510.4677778166,7233124.354432398,1115878.1041407934,13847374.190275196,11916878.588647855,2069261.4086653418,1044698.2485678743,3171995.62013675,291180.06108302297,12340442.08782241,975913.2582199818,1409771.5924518765,17990711.37045465,14387145.623198552,2727970.418449248,1835645.8717499892,1131134.2191477357,2147880.6388919894,20047945.743679296,1471602.789581584,1107230.6083612414,2940944.799271131,130628.00704709184,3413796.08657961,3536276.701720479,1218108.1414011396,1839661.198179288,1141917.6190747751,341427.7893839823,13821785.102206647,3669109.798775987,2109935.3646770334,2397598.5753060677,1107930.6168670403,1534933.1510015987,55041797.8647923,4246349.411033385,3600475.534128451,651958.5281551434,5754866.643313812,770625.4864750279,4607714.217086541,1755803.3678500203,4048027.611815038,19685.539732248057,2235531.363907905,522635.5425260898,1669418.093574007,1002792.3888361752,2546589.718362035,1106459.2893150544,15739994.848506464,1658295.142632497,1940699.4665254252,5652667.622223128,17858243.687968828,6583374.181447683,1145321.111735806,4556847.174321196,4650252.469443642,3765342.33390737,1723169.422904322,2253243.3531116378,1832641.871998092,11879465.344851049,526322.4849457811,6012684.268086551,3282711.730662236,4816904.783887245,11657837.138764134,923040.8259105089,7945391.126831293,1476045.1122801611,2053735.8468375865,1332495.1313171443,16694009.240072407,18123573.872624584,5334979.336952975,1242225.3671863293,4891385.869819524,9325455.29463104,478045.2788474278,1336413.515443718,3677512.9274750133,3526968.6256730547,1472660.9035392157,1377567.59515513,52014348.8662606,386799.4184607549,817850.1892261112,16042754.669510806,5485343.6344542075,-291329.69271009695,1693422.8235500893,2831894.6633709176,21306753.30780589,1782766.4473430933,1995730.5532454385,3258563.3860391444,788564.8975657991,1384203.2491987864,1359886.0827575107,814513.1295054345,2329200.5201593647,1265492.3550265683,544327.4148099481,15534939.004293466,9851395.723619837,1449159.2441284764,4871449.384849824,26746826.172744665,494962.85419823276,771045.6831175652,3048324.2164640436,686444.2575884839,27314074.541528862,2081940.582469221,5185907.571535805,3708187.163368278,117660.32199043641,4257039.5316865025,9356937.394908944,15566397.062657502,867440.3691485671,687503.1606240161,689405.0366945982,5247857.428650307,3965172.13887697,6672369.293793624,409652.61742335325,1154970.7284386104,2961450.69646627,14691197.799372785,759455.7188812336,5191220.497870076,32797584.679190174,2639973.0018429686,692755.9611999092,9278136.6763852,873158.1755484629,890130.9987042632,1715927.4837873464,3666203.349263932,23395372.867343333,1874850.193343914,1021703.3410444064,3093837.570188551,594895.7327708143,339060.2953266818,5245245.823925352,1976925.6773936516,851607.8958570817,3641999.7523974217,8169121.360184474,2775579.7247007336,2592894.358951936,5240794.254720964,2666582.1667975117,4113275.58401243,2203023.7371421224,1894885.7967298434,2010204.3323075366,610291.8723898106,5367309.117536081,4850067.9430874605,14478803.68663131,7360965.643093277,4965275.270998531,1645777.0036157751,5702555.427782106,103294.09948651586,677792.8807243223,1590603.89667868,2305439.607649345,8652157.088602081,805279.4523880391,2324098.3014093176,575736.9474529414,1476156.123867901,610746.0026464066,1594770.828807943,-80086.55785300536,2019069.2966814481,1146467.058326486,3192564.370148395,1940699.4665254252,1688852.8051160914,266217.640369751,322939.4323091449,1499963.1050275469,3074991.963989359,5924905.904608434,7035076.83163682,2675953.337352823,-153346.05030565755,2807601.0452446793,1589632.2591021494,725079.501757205,1678953.7821078845,1547848.2074573257,3719721.8193033054,796132.3341438121,1292072.2387786675,10097978.252455147,2336435.323097147,706381.8967177812,3313909.2563629234,796001.3798477894,2382445.3542637555,3720721.6681263023,5345645.58802082,5011615.33984443,877701.3922201912,2154623.594344898,1718079.0115570915,7860937.809906857,4644408.706830906,1448895.900189561,4246349.411033385,744781.6802381012,2731484.8959791167,7027732.373896427,1403570.9700722983,8442447.321222037,905069.7685488453,2532100.7115918174,1281119.0847847892,5551803.07752404,2028984.1502315346,1319087.1458799187,13210410.742199061,1534140.2497695913,4115729.519696335,1686482.0317576465,2582755.181953299,346483.87218512245,1257489.4746427909,8675123.023954269,5551803.07752404,2128804.310728053,5450964.194788415,9881676.255674846,668846.6620268798,6213031.668107591,696099.0236489549,912112.4965585384,634184.9039851597,1387706.3576219492,14612421.752017003,4739372.709149981,1188330.5258772462,597139.32169466,-119896.65869411081,1040527.5855348827,1050878.597645979,4600913.449906152,492669.33319864445,477729.19798399205,1253807.9031663034,893720.8659913952,1080187.2524103264,2583409.8167890855,2137727.414800286,17077400.893525824,2051238.8687583993,1719051.679063366,1423356.4215274614,992486.7496921956,15053369.342527878,1321964.5545690546,6093798.042120066,4027130.1047186125,3380930.2913192455,9607101.111697856,17325196.0159299,2092997.9294266263,9612447.99879775,675596.7700497652,897071.7904967063,4280698.803758819,764483.8587189973,138162.34850255586,2952085.814881423,25720652.883893017,4270728.24312746,1528955.8262083414,983059.7431899973,4650252.469443642,1201947.3420843983,1809840.6324065537,1264183.3312038935,5823696.443963442,1517383.9714839233,10952623.71144955,2977905.3427676307,2148107.125190161,754521.6507302353,8654502.367422914,3812782.811538142,2825111.4682112164,5046039.089561606,2158328.428316295,1005618.7511319276,754381.6261181813,10593400.837489134,1158073.0343078712,1386770.465210953,260885.4212595648,1006698.9192358602,252667.9435072313,1698420.6463778142,8450564.07997968,3580287.0988298175,682974.884265488,1030524.2711038331,8194450.230610931,3738614.4229572904,811911.9856052303,2000403.8310295686,824479.3441481991,1371475.5099596572,1933274.6983292243,3924154.0044978615,1119278.4877309878,19553947.672625754,10520842.464337906,4475699.981654606,779608.3017564819,518374.8141756386,3466762.930665784,5189518.932847695,674182.3794847825,1064436.884707912,6252697.72903054,1996718.4214017184,556485.9207183465,5788186.404669985,8149509.747230748,210257.81834923103,2887957.0549072144,914369.8061862842,3897268.7628757367,2033849.9476957358,1074580.2237510155,927501.807626016,10977343.85021651,4789348.879969381,604105.8097523763,1069967.166573485,2332945.944239544,-68776.31067112833,7092519.479623072,740497.4541455526,2636934.4856130457,1843764.863874992,5712698.282800398,417599.8504783907,4114766.126971909,4882828.38422339,495502.49209993053,1078026.2539829663,1493997.532720503,912067.3912304405,6952617.825981138,3875681.4136094516,196149.0913333972,5591165.373946581,787940.3302561739,2008491.9399338919,4462998.857310572,2401708.0901703956,3275907.03360108,7717895.954400081,1326189.7167178998,4891143.40101487,3852117.494097774,2324874.056989272,20629120.59825094,11556818.693144845,2196494.0268999143,1269984.167147674,9616556.080249343,4100588.612487926,61336517.73926096,658283.2491476145,743968.9509246629,9828975.577517506,1159903.417381004,486024.55731231463,1751203.6298849103,2762581.6255983114,834766.3715297445,2589601.4687025305,40379103.65908119,1171567.550708095,3894172.829666745,1309938.6467231293,2363283.2591602947,1082731.1211963664,1186787.8333011842,2340018.4582717316,6043098.448784232,3938555.2891561124,2243717.1152018933,1447514.8615362532,839513.2216585132,3789453.0665561603,2887957.0549072144,14274597.818587571,2000403.8310295686,8217247.24308495,1014219.6733880308,2454057.6283278842,169907.99358180398,3074991.963989359,1895823.8738051725,3627721.352859038,8587180.186246403,814423.0689151078,16042754.669510806,1705965.4847977106,5475051.918156454,1077157.1649540784,6776885.934963077,8927495.600579245,1566333.3954866608,2837839.1616108883,3689972.6238249075],"xaxis":"x","y":[7506587,1972078.625,7768294,706505.6875,2274873.5,2547582,4571388,288191,6252842,1126157.25,1153276,4103800.75,1963054,3292049,724259,819803.6875,1589550,967432.8125,984178,968026.3125,1688745,678663.3125,19236770,1359948,342493,497620,2682044,4420650.5,866343,1250669,1770027.5,14084606,1981115.25,1986094.875,32141070,5415438,932083.5,293217,3543985.5,2443065.25,992794.6875,885865.875,552729,3580091,2993610,284867168,1222509,5672838.5,0,965786,1429146.375,1648774.375,726124.625,30648154,1836671,876425,2192111.5,1426823,1748132,1791071.375,646377,1487660,1882845.75,893053.1875,4247457,1796889,6426022,2493240.25,650008,385719,1851260.5,75073888,804002,25453690,1997488,2712983.75,913899,9546165,5122039,25307744,1313366,761504.875,1302636,234144,5968153.5,2966309,58761304,1160989,2217620,142873,2051501,1069490.125,348061,545426.375,1113457.75,2665558.25,628023,4759256,0,2406996.75,501194,6508595,14829099,2491238.25,4135650.5,1023418,454296,1682454.875,813038,11651200,6797528,1145535.75,1348497.125,493000,766628,8204897,3960898.75,609101,17463744,3348280.5,863728.8125,1692323,2112095,1878742.875,6711975,675611.375,2196718,1199742,748775,1545356,19645206,122156,8393893,866825.375,1357116,2496807.5,615894,845044.125,526752.8125,2991674.75,3093944,25471308,7221482.5,3894584.5,2502081,630578,1569569,1435730,2512443,8971926,19248732,1678495.25,586589.1875,1832808.75,1132489.625,811907.625,702385,20361158,5647146,2323265,41655852,0,962971,1200811,1107290.625,1761137.5,14029480,248607,1037555.125,659450,3528086.75,820705,2433777,4549892,1358652,2558573,4287439.5,1239315.375,786979,730765,2781314,567072,967082,3228330,1243282,7606084.5,2103293,114648520,1977299,798970,4700552,1813864,1482554.375,759060.8125,4946798.5,678351,3289053,3326999,6935723,1386331,2740128,888031.875,1809101.5,581289,975431,4475650,1441445.625,8287440.5,659652.625,1538750.625,1306087.75,1058986,1060846,131810,3040978,1593187,791564,1085183,818317,3022275,520738,1224685.125,2114860.25,1088016,4520838,3626431.5,4448582.5,9212950,2392466,6689184,13586070,2074152,567608.625,594848.625,3009463.75,2446842,2161828,7794869,15206762,10084950,3906967,25631512,1022114.5,1182575,1218112,1249939.25,1788148,6102883,3093378,1493131.75,832168,7791957,839362,338923,2092251,1929828,1061458.25,451738,5820965,2284507.5,3740373.75,5417598,1632498.625,992139,1232822.875,3590731,9851937,2069880,27139746,514555,948676.5,665200.1875,468973.5938,13140574,227509.5,4185406,11509942,3185509,20630168,2928099,1391916,111969.7031,4842910,2278160,2094410,1138520,2470566,851651.1875,19397324,1244824,35383012,777587.125,898115.375,3471845,2190460,17686444,696605,2396523,651293.3125,1187261,5661689.5,517029,4673827,1255834,1970956,644415,1290284,2656729,2041912,4700396,29195468,1403869,42709624,1986384,1240875,509485.6875,6856153.5,369291.5,1464856,5420552,1140620,1424157.25,3795938,498745,4028943,873648,1234573.875,936336,2503690,2756214.75,6884914,766381,1384558.625,1260416,1214929,5448975,832986.875,659761.875,209644.9063,1269847.75,2526375,3467070,600318,2338852,11765682,1232210,13901935,7539963.5,4425832.5,24906132,3579008.75,11353672,7475577.5,608491,11289052,698367.6875,1525624,28731502,1656420.25,557082.1875,934357,1020039,5288304,9049598,899266.125,588969.8125,16016644,387810,6056481,3437441,1441665.375,8800292,8386493,497991,738990,2159932,2436403,5747645,1150582,1912690,8787353,3024478.5,863773,2274920.5,1187315,3158151,847298.8125,7613116,456397,1026921,5974993.5,666385,2559461.75,1170616,1967129,3928695.25,5464762,20781390,629027.3125,6129780,964236,929950.125,1815371.25,1777652,1242811.75,562479,8964061,7577723,1537570.25,403800,855946,10416934,1736775,684739.6875,786324,827042,892086,1735816,2404644,3568616,1046995.188,276107,243518.2031,2212058.5,6858533.5,318364.8125,639407.375,10126597,2227908.75,849504,8381744,4397572,1676426.5,3761647,1228837.25,646505.1875,717766,1195351.25,934621.1875,1152896.25,4103027.5,3392129,841891,1692199.25,980356,4374646,2332996.5,35891484,2845946,2674034,627854.125,2181833.5,5424718,298063,3560614.25,978463,2130673.75,1466382.25,3706100.25,33268410,5040382,508051,55813996,1075019.375,2100722,6227419,11356990,653139,204499.0938,5722325.5,10564729,548293,619545.125,1162120.375,901015.1875,1074937,1353337,807185.5,576405.125,1662723,4329966,7133222,27282662,8435093,8058913.5,3567977,609218,6915293,509124,3869754,363808.1875,2204235,4307238.5,1123416,2799595,1997115,1943772,3467698,448385312,2417401.25,5249190,664088,415975,16917964,13609040,3537841.5,13458948,9157150,2204161.25,47952868,847097.8125,4571746.5,65047284,3833162,1211947.375,1458617.125,3014332,705948.3125,2022990.625,1567040.75,352136,2168281,1160087.875,588965,628609,3481727.5,1677002.25,619804,13943008,1053290.75,1990444,565458,3494929.5,3155391,4089677.5,1998725.25,712057,935396.6875,8201974,782202,848497,4368113,1165598.875,2420605.5,592739.3125,2184912,3600897.25,4486477,980603.375,958798.3125,1886382,2503948,4751090,1651120,6335315.5,956481,397624,3063969.75,1841154.25,1295685.75,2586116,727703.5,1636649.125,1855556,719471.1875,2267094,1787119,1331122.125,971308,902694,5024837,17186624,55073120,1819329,512166,772379.3125,1049436.25,887403,416566.1875,673834,609257,759405.6875,3155496,1070085.375,2006626,2804534,747384.5,1057256,1054581.375,507428.9063,2664566.5,2065156,9697866,847514.5,6522551.5,720005,573624,12783255,0,2136086,15480681,253832464,2339234,4212149,3021669,3181045.5,2601369,1656025.625,646994,1565475,7802205.5,1187723,1356874.75,2298648,1136698,1586388.75,1018111,9085108,1734533,1703504.875,1868497.125,11047668,433443,5459657,1841968,9444484,38400928,151754.2969,536642.875,3717064,2159170.25,1625308.625,682632.3125,785144.8125,2553763.5,4750287,9617898,1025205,874707,618876.1875,3162976,1520326,2114053,1109924,7546235,3542542,10444602,462465.5,4023479,1761048,869835,1189942.75,2211671,1735538.25,789630.125,1962351.25,827937,28450436,6074249,9861998,967697,2107743.75,696054,1152890,3791402,965651.125,2090895.75,12480984,552015.625,2904225,466985,1268459,862744,313528.0938,1769998,1547116,2023858.75,863703,4656777,11061916,5233553.5,1168017.75,2300803,727775.8125,1443367.75,4608963,2342494.25,706879.5,574575.375,855670,666740.3125,1981360,1193790,8773058,520348.9063,17666646,1607590,1227772,14332778,410433.0938,926261.1875,2796425,1234813.25,2701833.75,4120963,4742869,37276920,18622292,2157144,1776411.75,666967,1080203,3031811,433871,908834,1076644.75,12448381,15843743,709979.6875,13239050,143423024,12196872,10992344,1291170,835545,1050982,1513700.625,1829122.375,2549649.75,1844825,3989399,1857329,1495686,32381836,1109851,6897247.5,1863784,675772.875,1018068,2378077,972608,1181428.75,5653656,7771933.5,1827424.125,6419090,1218114,16701055,771512.6875,162960,1213444.125,2777052,1185469,490599,541542.6875,1103762,811537,2932696,1848656,775007.6875,2172841,1788999,837792,784762,2737534.5,1695067,3060149,4178233.5,2924362,1271753.25,683274,712149,4020660,59757440,3298570,760090,505601,858897,15660628,323739.4063,5702077,9387211,37022180,1712279.875,1742351.25,1001964,1612797,638507.125,530700.125,2327611,1261634.5,1072487.875,1162561.125,6454983,1063960.125,151376,2534873.5,645756,1200381,2855545,2219744,688145,1362993,456197.5938,9523142,502898.6875,611997,13171370,1009476,739210.5,58041572,1459199.75,607579.6875,839011.3125,862826,1128179.125,2306723.5,8437401,2147062,646691,1619177.125,2236167,4058492,966764,1795462.75,2165371,4322873,2167888.25,1251906,669217.6875,471113.3125,664539,1983509,792015,794724.625,53079164,3629002.5,11924933,1375366,14641502,14469228,11813861,8878186,9273278,1985171,599591.625,489557.0938,3646053.75,695606,518107,699217,2499140,3187950.75,1543282,634056,1117984,687953.6875,7564604,4230514,1859725.375,569115,2564004,435985,1972491,116486.6016,1904641.75,1743198,347437.6875,2147013,5071974,713821.875,1869575,13659926,10721568,968343,6960396,886599,22055140,872555,31803568,757167.3125,1274049.75,4054582,601590,7969653,8259071,2427919.75,588856.125,1642471.5,668915.8125,2244317,656345,317794.4063,18558162,666392,3682374,7654264,623107.6875,3039889,18011944,1483856,807358,5762597,779503.625,10077532,5284343.5,522791.0938,1696446,811703.6875,1419453,53166156,2075891,3441643.25,3220827,1308348.5,127374,25210246,2828098.5,1437220,698177,1786986,571384,2152312.5,1910465,1204956.5,552007.5,1947717,456747,5903142,1259456,2436063,563454,15036617,2050600.75,1256456.625,2334260,18829136,4988493.5,2085349,5587347,2759071.5,2137686,841242,1587454.75,1591124.875,7996408.5,2655387.5,2303386,15324777,2584082,3850963,1037696,9765777,3314418,1929698,1987073,17750994,6914839.5,3841879.25,557863.6875,1951856.25,1555420.25,620104,604591,27119380,7202141.5,752441,2314835,39523852,1084654,769148,4545148,2773647.5,938908,335245,1400053.875,9722237,3035962.25,1817550,10005787,1581036.75,1773864.5,1242764,1992158.25,28229320,696709,849745.6875,6219841.5,5950438.5,731315,2033074,24882924,442375.8125,721723,1929851.5,842923,14776743,938096.1875,11224871,8387933,1645327,3371455,17871658,2290681,737570,492421.9063,2014475.875,10040121,8611054,6298131.5,1140137,1965529,1363296.875,6994740.5,632517,13253979,10614234,4345698,448600.5,4498367,652425,948157,938833.625,2759674,8233527.5,952879,830739.8125,786253.6875,919739,724153,5669431,1427974.625,521341.9063,3936546.75,5214621.5,1415355,8839812,7456330,2527198,1472548,2222097.5,2003759,1280245,1083467.125,2407904.5,12037792,10053489,3983784,1541897.875,2798967,6057734,507625.5,1038815.188,738983,360045,7753080,573395,1152368.25,680395.875,1929011,1612392,1297041,1354000.625,5381105,1100499.75,6136327.5,1357701,1724560,601897,591840,1148202.25,1618916.375,41092076,3548309,3641620.75,458746,1489363.875,4527521,756196,1314357,808829,2971013.5,647321.5,564988.8125,9436408,469499.8125,674961,405062,2375104,1361794.25,3518459.5,1971236,13320624,671079,662012,230365,5792926,1562708,529806,2964206,829440.625,161311.0938,2539565,2156407,3970754,3460275.5,2235765.5,1084553.75,3897738,1398253,1453543,3780078,790039.8125,1655612,1128208.125,7240274,488181,819314,7011508,4045980,1076693,4742636,5231373,151798,8751704,768755.125,1221806,577290,882759.1875,6310254,84980760,759508.6875,1417852,981009.125,639878,844076.8125,7231928,1256360,1054995,1415494,3577120,763693,1177583.375,1063480.5,59107620,1657201,885843.1875,731092,722951,18868768,437619,3091387.5,1878442.25,3118304,8765978,15725581,1529309,465533.3125,3275779.25,1287159.25,3986851,484875,1261963,1644969,7545909.5,1633399.375,3294645,687049,2775717,1098965,1688739,1641546,686019.1875,2852102.75,4079837,2077039.375,1136405,1372475,3000864,11566123,4519115,2295176.5,2185530,991283,574650.375,4039667,585559,776996.875,635516.375,730211.1875,1067793,1437569,8651835,19061780,1372834,1136676,2634166,2009324,964015,1237252.75,507277,814403,1594733.625,7008252,1302192.875,25377650,9097980,1808401.375,1000599,929628,2520310,5917000,658297.5,705262.125,1189402,1034827.313,522880.0938,6361236.5,1118392,1082920.125,2330029,393493,1697909.625,6015128,666909,524053,9686734,2471623,2225265.5,1785906,1245181,669787.8125,4863803,1243200.75,419978.6875,2531163,2805926,2166711,3660395,614768,2213115.5,10975751,427697,784329.5,12187231,4099807,843267,1074180,474359,801264,1955013.5,1365778,1615569,2892428,5826745,2890379,1247362,915417,8050977,18661606,4341706,1039782,8070420,3132453,37797084,679574.875,1022614.313,19018762,4841347.5,585077.125,550324.1875,6881405.5,4644346,1248599,32552428,788027,3529643,3996541.5,3226884,676499.875,559805,123767.2031,1835804.875,1451145.875,1040593,1567746.75,558709,2853189.5,2568377.75,4309177,1188444,9343428,657385.8125,2979413.5,1646088.5,1696009,1524620,2438430,23612824,818484,4222682,2001936.5,3739029.75,2245568.25,5928881,6268627,3027867.5,2277238.25,2017423],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle ElasticNet() vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predEN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["alphasEN = np.logspace(0, 7, 200)\n","l1ratioEN = np.linspace(0, 1, 6)\n","param_gridEN = {\n","    'elasticnet__alpha': alphasEN,\n","    'elasticnet__l1_ratio': l1ratioEN\n","}\n","\n","GridEN, \\\n","BestParametresEN, \\\n","ScoresEN, \\\n","SiteEnergyUse_predEN, \\\n","figEN = reg_modelGrid(model=ElasticNet(),\n","                         scaler=scaler,\n","                         X_train=BEBM_train,\n","                         X_test=BEBM_test,\n","                         y_train=SiteEnergyUse_train,\n","                         y_test=SiteEnergyUse_test,\n","                         y_test_name='SiteEnergyUse_test',\n","                         y_pred_name='SiteEnergyUse_predEN',\n","                         score=score,\n","                         param_grid=param_gridEN)\n","\n","print(BestParametresEN)\n","ScoresEN\n","figEN.show()\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"y":[9090271.73691992,9090271.694388788,9090271.655173382,9090271.615412321,9090271.57332498,9090271.51514088,9090271.451374006,9090271.392702537,9090271.33313079,9090271.251883155,9090271.182450488,9090271.10233299,9090270.999043271,9090270.909776973,9090270.787977947,9090270.675906911,9090270.563431235,9090270.408559892,9090270.274777597,9090270.121770605,9090269.924498867,9090269.70829226,9090269.471330168,9090269.363135617,9090269.093279064,9090268.797530135,9090268.473400202,9090268.248779867,9090267.871923845,9090267.550546054,9090267.107110051,9090266.699278798,9090266.285481151,9090265.72057705,9090265.101443252,9090264.679270703,9090263.961002316,9090263.34091306,9090262.494032526,9090261.715137552,9090260.712340025,9090259.853536982,9090258.924190799,9090257.860737568,9090256.489141753,9090254.985938026,9090253.338478087,9090252.194831958,9090250.707472745,9090248.651198354,9090246.397674311,9090244.321939003,9090242.219348084,9090239.348280553,9090236.894970927,9090234.060824355,9090231.187390389,9090227.26083713,9090222.957558695,9090218.993785085,9090215.020235317,9090210.714880666,9090204.829148542,9090199.418711191,9090192.450527145,9090184.814037804,9090180.01296742,9090172.628227051,9090163.099782104,9090152.658185354,9090141.215316799,9090135.96048726,9090122.929276614,9090108.650096688,9090093.003407147,9090082.20761877,9090064.026499778,9090048.42523167,9090027.031058757,9090007.378537137,9089913.242768187,9089879.845224474,9089845.049616668,9089810.605033478,9089768.652454266,9089728.649984501,9089679.867739847,9089633.400319923,9089715.36562644,9089863.240037654,9089834.04447383,9089802.610025797,9089764.543895077,9089728.570126489,9089684.214362605,9089640.723632628,9089584.962653968,9089530.02131977,9089467.69312649,9089387.317956068,9089303.113726646,9089207.956023347,9089107.50851188,9088981.005930018,9088856.459127741,9088711.04758783,9088560.989395132,9088388.939369153,9088213.786430458,9088064.286043663,9087904.413076766,9087735.253322441,9087563.238645991,9087369.300407145,9087159.362082263,9086936.777535066,9086701.446424317,9086448.346899578,9086180.680604005,9085956.637943711,9085795.432154622,9085739.591908509,9085675.679459175,9085607.518330451,9085548.047135457,9085472.55787646,9085394.245920105,9085326.804925103,9085265.533171318,9085212.265550073,9085177.64386828,9085155.295516137,9085145.403722273,9085143.112300452,9085204.352730636,9085316.823898565,9085469.616967797,9085701.50941893,9086057.917541334,9086512.168524595,9087086.028685817,9087845.66980011,9088724.355853114,9089705.786781842,9090897.569747278,9092372.376228556,9094180.275338415,9096349.797579296,9098974.578078318,9102114.243173558,9105907.432223182,9110433.180878866,9115753.451039571,9122096.023496555,9129648.712076869,9138639.222162325,9149328.112309659,9162035.095677575,9177073.655313965,9194863.287574925,9215880.301989585,9240655.225091482,9269844.844194734,9304158.949660912,9344386.456169851,9390567.10883527,9443373.996941868,9493060.119882816,9527881.940371504,9559704.234721547,9583126.111482123,9609248.664522352,9635500.754605573,9663325.679717673,9694020.151081087,9728496.94109309,9767216.876257354,9810766.031256352,9859719.321405884,9914792.767394874,9976753.714344883,10046472.954261765,10124912.906829504,10213116.044047922,10312278.264227439,10423731.497657504,10548917.281775404,10689418.148997607,10847016.840754285,11023674.598239673,11221552.487706302,11442988.223428337,11690611.864188302,11967293.111762857,12279659.608943755,12628281.294566672,13016577.013201747,13448662.75211083,13785220.015671667,13913366.302134294]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"y":[10901621.713994522,10901621.681543428,10901621.655889636,10901621.609890115,10901621.59584842,10901621.551873548,10901621.503579922,10901621.465680895,10901621.39714897,10901621.333668796,10901621.310302699,10901621.25933663,10901621.182414524,10901621.080289867,10901620.985834885,10901620.911014354,10901620.874928549,10901620.760453071,10901620.608235061,10901620.50761982,10901620.357102426,10901620.191789787,10901620.010222046,10901619.960023269,10901619.756092407,10901619.532132192,10901619.286167854,10901619.037757982,10901618.743288489,10901618.636474356,10901618.303153716,10901618.04259555,10901617.568790747,10901617.130721837,10901616.649602829,10901616.61766648,10901616.086881436,10901615.38079549,10901614.728950372,10901614.214576725,10901613.447883803,10901613.173542012,10901612.121093953,10901611.429411668,10901610.390988315,10901609.250556171,10901607.998058874,10901606.729488097,10901606.241687914,10901604.696695011,10901603.000026017,10901601.668593762,10901599.256559163,10901597.02619533,10901596.215090247,10901594.425513884,10901591.151933735,10901588.131420653,10901584.814432638,10901582.187494205,10901580.937687865,10901576.060749209,10901571.568088338,10901568.03845961,10901562.757272087,10901556.95781241,10901553.591136705,10901548.847836656,10901541.699215628,10901533.850597939,10901525.232177094,10901522.820381423,10901513.143410267,10901502.519968415,10901490.857411277,10901479.096878657,10901465.147811327,10901460.04545019,10901444.266346006,10901432.061561165,10901234.155134361,10901198.879858563,10901164.437510813,10901130.726867996,10901089.112008143,10901032.412121527,10900983.745143376,10900939.578795806,10900972.381017545,10900777.630682325,10900711.69318131,10900644.760791551,10900563.95712998,10900490.220553797,10900398.228094233,10900313.65201082,10900205.879121741,10900103.232429884,10899988.812540183,10899841.406349361,10899697.023363233,10899533.52909454,10899367.369080076,10899165.693104258,10898974.906971099,10898763.398636658,10898538.220540727,10898245.14147033,10897899.943058906,10897575.573538052,10897234.466782903,10896866.893058736,10896483.544262893,10896060.867209518,10895600.22863795,10895111.706723588,10894595.346780075,10894037.952368539,10893447.176002832,10892962.28821083,10892637.278785257,10892549.328273058,10892455.471297324,10892357.185235756,10892277.871944118,10892182.323748022,10892086.122278085,10892010.239372188,10891943.600054618,10891902.673761131,10891903.555252697,10891921.714007344,10891967.91387677,10892063.677299522,10892202.949548826,10892429.700315852,10892744.97018975,10893198.673441265,10893849.710466184,10894686.619640531,10895749.68114836,10897129.42232102,10898779.9210202,10900713.43216524,10903077.9153122,10905989.829533912,10909554.442436364,10913850.86440471,10919050.533910276,10925295.20531568,10932825.725816442,10941838.26826015,10952689.28717519,10965686.289743582,10981163.82666761,10999579.257029237,11021470.029059358,11047478.67152485,11078292.843792047,11114803.147380836,11158007.431537684,11209055.281063383,11269375.323435057,11340518.536347348,11424287.227442533,11521969.354101447,11635296.12441982,11745012.533409446,11821428.901521213,11895665.779436816,11952188.307431981,12014941.318999894,12081718.626816988,12156309.106896684,12240167.39762783,12334193.721965712,12439670.941520661,12558093.130888708,12691063.780798107,12840421.334149513,13008191.526398147,13196625.589818379,13408195.085077548,13645597.993739603,13911815.199184705,14210091.06202665,14543927.183095893,14917108.297655182,15333739.377173712,15798236.299889632,16315344.664227718,16890126.737924848,17528056.953729115,18234994.362450577,19019240.982681274,19885835.513824556,20841847.134376224,21895088.853699777,22627768.35709385,22699279.180546105]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"y":[7278921.759845321,7278921.707234148,7278921.654457127,7278921.620934526,7278921.55080154,7278921.478408212,7278921.399168089,7278921.319724178,7278921.269112609,7278921.170097513,7278921.0545982765,7278920.945329351,7278920.815672017,7278920.739264078,7278920.590121008,7278920.440799469,7278920.251933922,7278920.056666712,7278919.941320132,7278919.73592139,7278919.491895309,7278919.224794735,7278918.93243829,7278918.766247965,7278918.4304657215,7278918.062928079,7278917.660632549,7278917.459801753,7278917.000559201,7278916.464617752,7278915.911066386,7278915.3559620455,7278915.0021715555,7278914.310432263,7278913.553283675,7278912.740874927,7278911.835123195,7278911.301030628,7278910.259114681,7278909.215698379,7278907.976796247,7278906.533531953,7278905.7272876445,7278904.292063467,7278902.587295191,7278900.72131988,7278898.678897299,7278897.66017582,7278895.173257576,7278892.605701696,7278889.795322604,7278886.975284243,7278885.182137006,7278881.670365775,7278877.574851608,7278873.696134826,7278871.222847043,7278866.390253607,7278861.100684754,7278855.800075964,7278849.102782769,7278845.369012122,7278838.090208746,7278830.798962773,7278822.143782201,7278812.670263196,7278806.434798136,7278796.408617446,7278784.500348578,7278771.4657727685,7278757.198456503,7278749.100593096,7278732.71514296,7278714.780224961,7278695.149403017,7278685.3183588805,7278662.905188228,7278636.805013149,7278609.795771508,7278582.695513108,7278592.330402013,7278560.8105903845,7278525.661722524,7278490.483198958,7278448.19290039,7278424.887847476,7278375.990336318,7278327.22184404,7278458.350235334,7278948.849392982,7278956.395766352,7278960.459260044,7278965.1306601735,7278966.91969918,7278970.200630975,7278967.795254435,7278964.046186196,7278956.810209655,7278946.573712798,7278933.229562775,7278909.204090059,7278882.382952154,7278847.647943685,7278796.318755777,7278738.011284383,7278658.696539003,7278583.758249538,7278532.737267978,7278527.629802009,7278552.998549273,7278574.359370628,7278603.613586146,7278642.93302909,7278677.733604772,7278718.495526576,7278761.848346544,7278807.5460685585,7278858.741430617,7278914.185205178,7278950.987676593,7278953.585523987,7278929.85554396,7278895.887621026,7278857.851425147,7278818.2223267965,7278762.792004898,7278702.369562126,7278643.370478017,7278587.46628802,7278521.857339015,7278451.732483863,7278388.87702493,7278322.893567777,7278222.547301383,7278205.755912445,7278203.947481277,7278194.263745843,7278204.345396597,7278266.124616485,7278337.717408659,7278422.376223275,7278561.917279201,7278668.790686029,7278698.141398443,7278717.224182355,7278754.922923201,7278806.108240466,7278848.730753883,7278898.62224636,7278933.281031438,7278989.138629922,7279028.093497583,7278817.614903954,7278505.757249529,7278133.597486127,7277699.187295413,7277186.19555996,7276591.5198303,7275854.4668358825,7274923.427769013,7273753.172441485,7272255.16911958,7270314.364954411,7267799.362974477,7264485.6848971695,7259164.863569094,7251451.869463917,7241107.706356187,7234334.979221795,7223742.690006278,7214063.915532264,7203556.010044809,7189282.882394156,7170342.2525386615,7147872.904534344,7122800.16022047,7094762.810994048,7063438.931623996,7028374.862013661,6989164.200640235,6945315.90229162,6896320.318705151,6841630.72858146,6780634.094356241,6712741.329270173,6637371.933288358,6553907.380454915,6461728.000340031,6360294.304334858,6249112.896589714,6127760.311184885,5995849.708931827,5853166.774647489,5699591.861075138,5540078.235206237,5370727.0753087895,5191306.892027271,5002236.650521882,4942671.674249483,5127453.423722485]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"xaxis":"x","y":[9062027.818457209,9062027.827663265,9062027.837645989,9062027.84847093,9062027.860209139,9062027.872937663,9062027.886740064,9062027.90170692,9062027.917936498,9062027.935535325,9062027.954618916,9062027.975312544,9062027.997752056,9062028.022084737,9062028.048470316,9062028.077082012,9062028.108107625,9062028.14175083,9062028.178232474,9062028.21779205,9062028.26068924,9062028.307205655,9062028.357646665,9062028.412343401,9062028.471654942,9062028.535970658,9062028.60571281,9062028.681339247,9062028.76334648,9062028.852272926,9062028.948702428,9062029.053268118,9062029.166656546,9062029.289612234,9062029.42294252,9062029.567522895,9062029.724302774,9062029.894311689,9062030.078666104,9062030.278576748,9062030.49535654,9062030.73042929,9062030.98533902,9062031.261760155,9062031.561508529,9062031.886553328,9062032.239030115,9062032.621254826,9062033.035739101,9062033.48520679,9062033.97261196,9062034.501158357,9062035.074320547,9062035.695866888,9062036.369884437,9062037.100805903,9062037.893439049,9062038.752998464,9062039.685140122,9062040.695998866,9062041.79222912,9062042.981049048,9062044.270288559,9062045.668441406,9062047.18472172,9062048.829125503,9062050.612497374,9062052.546603069,9062054.644208247,9062056.919164198,9062059.386500847,9062062.062528037,9062064.96494557,9062068.112962952,9062071.52742962,9062075.230976708,9062079.248171385,9062083.605684819,9062088.332475275,9062093.459987523,9062099.022370256,9062105.056713184,9062111.60330575,9062118.705919422,9062126.41211612,9062134.773585144,9062143.846511532,9062153.691979108,9062164.376411691,9062175.972056387,9062188.557513442,9062202.218317652,9062217.047576655,9062233.146672588,9062250.626033932,9062269.605985358,9062290.217684636,9062312.604156451,9062336.921434475,9062363.33982467,9062393.330053214,9062426.011650017,9062458.75731912,9062495.567484256,9062535.14847243,9062579.10521468,9062626.006481154,9062680.081092266,9062736.441325175,9062795.707760395,9062864.613930294,9062934.704509094,9063018.875214059,9063101.097952234,9063195.775393752,9063298.301331952,9063410.775012257,9063535.747447602,9063676.304280303,9063824.978213353,9063995.521036819,9064192.936218208,9064409.032034313,9064659.975552851,9064928.789194144,9065165.255983073,9065446.508108703,9065751.254495103,9066087.863124631,9066465.117775097,9066876.025785184,9067330.637033958,9067834.109744586,9068392.324456107,9069011.986007174,9069705.695671871,9070472.170984907,9071375.805242343,9072562.3040041,9073862.779408382,9075289.180950455,9076854.939276164,9078575.12382725,9080466.656490425,9082548.570399141,9084842.306023201,9087372.052180547,9090168.209430236,9093255.339228395,9096671.783589626,9100457.335057914,9104657.22723018,9109322.953451935,9114513.213271625,9120295.007663582,9126744.909813052,9133950.537143743,9142012.25668443,9151045.156756876,9161181.330231622,9172572.5143029,9185393.144009147,9199843.868162887,9216155.609360378,9234594.23032736,9255465.90163152,9279123.174028121,9294895.816045444,9305344.594416916,9317242.925746826,9330795.310073512,9346266.923648583,9357029.860143954,9365347.904201502,9374573.824204298,9384806.500843689,9396026.111858087,9408458.049653186,9422246.031755256,9437564.698907087,9454593.706676584,9473566.478742823,9494716.917361103,9518324.651921315,9544708.126516657,9574255.609019509,9607367.248398459,9644518.296361826,9686250.504524557,9733235.984403694,9786081.051658094,9845709.41598431,9912969.978110258,9988839.394260656,10074654.139173659,10171660.688190285,10281376.801980527,10405504.458542682,10546044.389997821,10704953.165703531],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"xaxis":"x","y":[11559258.672985388,11559258.593052452,11559258.505614117,11559258.409960125,11559258.365895139,11559258.25753002,11559258.139005478,11559258.009361524,11559257.867546778,11559257.712409705,11559257.640771754,11559257.465007473,11559257.272768125,11559257.06249724,11559256.8324902,11559256.580880035,11559256.464416739,11559256.179333607,11559255.867533756,11559255.52649366,11559255.153449629,11559254.745374814,11559254.298953954,11559254.093647955,11559253.587930758,11559253.034797648,11559252.42976801,11559251.767934673,11559251.043923015,11559250.710166242,11559249.889936605,11559248.992819162,11559248.011550067,11559246.938174175,11559245.763978759,11559245.22142629,11559243.891108554,11559242.436117386,11559240.844677819,11559239.10389463,11559237.19964507,11559236.317731338,11559234.16017857,11559231.800481463,11559229.219554815,11559226.39649862,11559223.30842438,11559219.93026474,11559218.376030562,11559214.549313609,11559210.363950778,11559205.786076209,11559200.778604342,11559195.300922079,11559192.77530049,11559186.570137238,11559179.78367056,11559172.361046221,11559164.24220069,11559155.361364037,11559151.258447858,11559141.198202979,11559130.196070297,11559118.163255857,11559105.002547676,11559090.607514964,11559083.945440724,11559067.639487697,11559049.808201239,11559030.30799711,11559008.98171529,11558999.009514527,11558974.848273298,11558948.430831758,11558919.545348026,11558887.960033398,11558853.4212783,11558837.25797842,11558798.13644224,11558755.368433192,11558337.446506776,11558255.478851518,11558174.774737343,11558082.041892309,11557985.96994244,11557870.114503592,11557759.213101048,11557636.847302502,11557499.015024915,11557340.963455578,11557178.315198403,11557009.596054923,11556808.529550917,11556619.1827459,11556388.056492252,11556167.452415898,11555891.91866476,11555622.73231513,11555322.422167124,11554943.730823118,11554560.817903226,11554129.70501671,11553685.07030559,11553146.200537672,11552621.827657983,11552033.125034165,11551411.392286887,11550717.638543079,11549981.49241416,11549166.171557778,11548329.094331196,11547403.57817508,11546408.091973554,11545339.339413017,11544172.957967333,11542930.63626841,11541610.434047818,11540182.071784902,11538659.430317178,11537338.925135659,11536348.790799022,11535812.50254688,11535252.794877373,11534665.93926267,11534081.360839855,11533456.878886722,11532800.456121251,11532133.058957757,11531429.179498209,11530741.98387274,11530087.441350764,11529395.523880092,11528703.005582271,11528039.3145919,11527427.18744724,11526892.173717486,11526463.015780374,11526175.186574344,11526071.98623369,11526205.836912276,11526640.494881513,11527451.871042693,11528730.692274531,11530586.423581112,11533149.899379883,11536577.317757303,11541067.611847088,11546813.628508087,11554094.461792141,11563221.038549291,11574561.874240598,11588552.886649473,11605708.177872615,11626623.54162156,11652029.458475567,11682747.831994986,11719758.789381154,11764202.628740525,11817377.14002207,11880877.06130685,11956459.32525896,12046124.683590611,12152323.130068155,12277697.276384376,12425213.40730233,12598466.502968837,12800191.274901655,13000144.11108265,13140532.3489038,13278283.440124467,13378443.229947181,13489812.996777548,13613743.90625007,13751751.357150283,13905532.49300132,14076984.728846693,14268225.13415214,14481610.46212349,14719757.568936056,14985563.92980666,15282227.940279864,15613268.699043954,15982545.011194838,16394273.434439404,16853045.31798226,17363842.952164724,17932055.147214245,18563492.775932234,19264405.026580576,20041497.29472502,20901951.77370351,21853451.86561432,22904211.520381898,24063010.522853844,25339236.601160318,26742935.046106666,28284866.337569244,29976572.09540897,31110426.16201454,31110426.16201454],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"xaxis":"x","y":[9724112.11164087,9724112.059965096,9724112.040096562,9724111.981120322,9724111.916222602,9724111.844812224,9724111.76623925,9724111.735863183,9724111.64618168,9724111.547500245,9724111.438920718,9724111.396952728,9724111.273022838,9724111.136655843,9724110.986610549,9724110.928627059,9724110.757369848,9724110.56892569,9724110.361579353,9724110.281468766,9724110.044810783,9724109.784402056,9724109.497872291,9724109.38719087,9724109.060156982,9724108.700301912,9724108.304350143,9724108.151432186,9724107.699510258,9724107.202232908,9724106.655073717,9724106.443802264,9724105.81930179,9724105.132125271,9724104.37601778,9724104.08412641,9724103.221147174,9724102.271557502,9724101.226714024,9724100.82344109,9724099.63092166,9724098.318718864,9724096.874889178,9724096.317737645,9724094.669851517,9724092.856582612,9724090.86142775,9724090.091690164,9724087.814578844,9724085.30894077,9724082.551973483,9724081.488552269,9724078.342000496,9724074.879679104,9724071.070083957,9724069.600959247,9724065.25308877,9724060.46891852,9724055.20492593,9724053.175378546,9724047.167709447,9724040.55723948,9724033.283850357,9724030.480211874,9724022.179441854,9724013.045898054,9724002.99655273,9723999.12379523,9723987.656662006,9723975.040612629,9723961.157548906,9723955.809078459,9723939.966206785,9723922.534751106,9723903.356515536,9723895.97023877,9723874.085897781,9723850.008103605,9723823.518625204,9723813.319858039,9723783.094896255,9723749.84228757,9723713.26119119,9723695.856638849,9723650.063785063,9723599.713862069,9723544.380305888,9723519.467323683,9723927.146578955,9723934.539400501,9723942.598303521,9723951.284573138,9723960.83974391,9723971.193380563,9723982.422840413,9723994.457802746,9724007.70863108,9724021.875393044,9724037.324177984,9724054.249493914,9724072.487848807,9724092.237322072,9724113.332306452,9724136.646969466,9724162.186556594,9724189.271653442,9724217.836182168,9724141.442541057,9723990.929566937,9724045.190004066,9724059.489619562,9724117.746389082,9724218.252611084,9724288.414733384,9724365.428991834,9724450.155619062,9724542.75151351,9724644.510253193,9724755.150296476,9724876.61876684,9725009.094463782,9725153.886276817,9725312.255556893,9725453.650233181,9725601.046737319,9725762.825599454,9725938.723575072,9726134.176802738,9726349.338893462,9726586.430987507,9726847.883319365,9727134.011411019,9727453.120395431,9727806.301362475,9728197.201793358,9728628.480565047,9729110.553750394,9729643.32809761,9730240.961126285,9730907.140029471,9731648.170293638,9732481.160651017,9733410.679354982,9734453.153179273,9735632.180811375,9736953.004704753,9738450.330477891,9740134.39561902,9742048.706289062,9744209.343250444,9746661.80163352,9749462.93899937,9752639.458614763,9756261.7571958,9760417.846043617,9765151.857447779,9770573.431886451,9776790.702087564,9783963.237575954,9792175.703984318,9801627.965308463,9812519.023128172,9825136.538962312,9839651.801211601,9856420.675817871,9875812.16615187,9898326.857983045,9910956.546871928,9920178.546027146,9930861.439767584,9943239.68381056,9955557.475213751,9959953.177788919,9964962.016545057,9970699.569539336,9977256.670194065,9984773.053129937,9993415.910974149,10003331.105776254,10014737.580258636,10027873.699035103,10043043.627830084,10060519.99323347,10080701.556503212,10104024.027924104,10131038.948745878,10162250.315359822,10198377.72794831,10240209.22692056,10288654.279376656,10344850.200683558,10409841.85316093,10485090.786000086,10572181.456320057,10672921.063808136,10789361.010369882,10924000.408024756,11079123.639967013,11257805.432954725,11463453.512210475],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"xaxis":"x","y":[9162322.196585393,9162322.208287947,9162322.220977757,9162322.234738238,9162322.24965961,9162322.265839847,9162322.283385195,9162322.302410752,9162322.323041447,9162322.34541271,9162322.369671348,9162322.395976633,9162322.424501235,9162322.45543236,9162322.488973098,9162322.525343576,9162322.564782549,9162322.607548948,9162322.653923465,9162322.704210533,9162322.75874022,9162322.81787058,9162322.881989654,9162322.95151847,9162323.026913373,9162323.108669395,9162323.19732319,9162323.293456776,9162323.397701276,9162323.510741018,9162323.633318238,9162323.766237577,9162323.910371764,9162324.066667076,9162324.236149758,9162324.419932555,9162324.619222222,9162324.83532734,9162325.069666905,9162325.32377974,9162325.599334577,9162325.898141056,9162326.222161468,9162326.573523883,9162326.954536105,9162327.367700677,9162327.815731639,9162328.301572056,9162328.82841382,9162329.399718182,9162330.019239035,9162330.691047287,9162331.419557884,9162332.209558962,9162333.066243311,9162333.995242633,9162335.002664948,9162336.09513481,9162337.279836971,9162338.564564185,9162339.95776863,9162341.468617912,9162343.10705581,9162344.88386841,9162346.810755493,9162348.900408454,9162351.166594815,9162353.624249747,9162356.289575927,9162359.180151459,9162362.315047763,9162365.714957064,9162369.402331524,9162373.401533863,9162377.739001714,9162382.443425938,9162387.545944914,9162393.080355437,9162399.083342977,9162405.594731519,9162412.657756288,9162420.319360463,9162428.6305186,9162437.646589354,9162447.427699951,9162458.039165689,9162469.55194804,9162482.04315487,9162495.59658698,9162510.303335808,9162526.262437299,9162543.581587832,9162562.377928425,9162582.77890473,9162604.92321054,9162628.961823888,9162655.05914609,9162683.39425479,9162714.162283823,9162747.575944636,9162783.8672053,9162823.289145466,9162866.11800857,9162912.65547466,9162963.231181249,9163018.205522258,9163077.97276089,9163142.964496233,9163213.653528832,9163290.55817904,9163374.247116968,9163465.344774365,9163564.537417326,9163672.579971906,9163790.303708348,9163918.62490587,9164058.55463925,9164211.209850315,9164377.825893689,9164559.770775385,9164712.229767907,9164919.239666225,9165099.39370494,9165291.410665898,9165526.103327537,9165765.533341207,9165992.463068979,9166271.886047738,9166598.90971303,9166920.145562507,9167278.949233465,9167704.580547102,9168149.037744416,9168604.395191018,9169119.895701293,9169733.19954135,9170363.853912406,9171108.134739798,9171992.06062639,9172997.00170824,9174151.513642257,9175681.42584485,9177143.681977885,9178314.411306668,9179557.924988039,9181053.51699204,9182791.397866393,9184677.002993077,9186825.871273903,9189163.152376868,9191917.969419073,9194934.027951142,9198357.911939425,9202341.10716934,9206758.216240656,9211783.391689068,9217506.898759276,9224127.083279135,9231577.92826141,9240095.798539054,9249845.877571898,9261014.069180125,9273820.908020712,9288587.320974015,9305544.46567425,9320376.690626621,9332540.462205378,9346964.578312594,9353332.924175357,9347563.452118773,9341459.11893979,9335035.30348478,9328305.31342667,9321286.990238398,9314005.979108488,9306499.049310356,9298821.237394199,9291118.455907952,9283322.666247705,9275626.398029702,9268173.192404099,9261142.755043438,9254831.410039175,9249366.474185321,9245159.63174407,9242621.585023493,9242319.229457768,9244714.44880149,9250600.838061782,9260843.35042056,9276534.907023951,9298770.593582023,9329057.226383716,9369088.53137657,9437928.434482208,9523290.51791414,9625426.510069862,9747031.7259439,9891336.857081138,10061716.700405424],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"xaxis":"x","y":[5943637.884930735,5943637.782975179,5943637.671532481,5943637.6027719965,5943637.474638413,5943637.334584649,5943637.181500043,5943637.014170306,5943636.910947546,5943636.718557791,5943636.508269706,5943636.278415584,5943636.0271721,5943635.872214681,5943635.58334558,5943635.267601874,5943634.922479412,5943634.545240393,5943634.312618936,5943633.8788880175,5943633.404804469,5943632.886608202,5943632.3201882765,5943631.970977389,5943631.319739265,5943630.607911066,5943629.829846851,5943629.349736445,5943628.455138199,5943627.477317178,5943626.408519266,5943625.240266874,5943624.519525579,5943623.17630649,5943621.7081274465,5943620.103345374,5943618.349230858,5943617.26725137,5943615.250437773,5943613.04599555,5943610.636442282,5943608.002664356,5943606.378385751,5943603.350184691,5943600.040257802,5943596.422354895,5943592.467776549,5943590.029378002,5943585.482601391,5943580.512812421,5943575.0805963045,5943569.142860888,5943565.482257157,5943558.6553757265,5943551.193342437,5943543.036976758,5943538.004088619,5943528.626087637,5943518.375689762,5943507.171619789,5943494.925021524,5943487.369293914,5943473.28847768,5943457.8977784095,5943441.075168979,5943422.6872420395,5943411.343751469,5943390.206999517,5943367.100263106,5943341.843001384,5943314.235771194,5943297.2063582195,5943265.464625888,5943230.770403767,5943192.84874085,5943169.433419026,5943125.831206505,5943078.174036066,5943026.084408091,5942969.149675418,5942933.992311356,5942868.528909636,5942796.978330455,5942718.774127455,5942633.388727751,5942580.608806016,5942482.346832728,5942374.951839444,5942490.69352966,5943354.42194,5943334.488916488,5943306.369595436,5943273.924675475,5943236.548928666,5943195.04323588,5943143.14013524,5943079.9091432765,5943009.500479433,5942927.635569055,5942827.693694,5942705.065622684,5942568.536982469,5942414.2646196745,5942213.959184032,5941999.901770453,5941735.530514596,5941471.739264566,5941262.570173127,5941146.415317187,5941023.802717028,5940894.620385808,5940754.892764591,5940606.436013935,5940445.069965181,5940272.344350056,5940086.169550032,5939884.716908751,5939668.195161877,5939434.692232382,5939182.896827323,5938911.524705591,5938619.394834415,5938304.9211223535,5937966.61593766,5937602.935578432,5937212.295571841,5936793.078726521,5936343.648322171,5935862.374627257,5935347.649552513,5934797.919652628,5934211.724708519,5933587.745144655,5932873.225900766,5932265.492704108,5931624.569997071,5930938.490410902,5930205.092440555,5929422.275716211,5928588.084564604,5927700.783661227,5926758.952185825,5925761.601830931,5924708.289351733,5923599.273157954,5922435.735665485,5921219.984320153,5919955.75134606,5918648.511808079,5917305.89810157,5915938.180764799,5914558.823564173,5912738.753319119,5910740.498224452,5908743.031960921,5906768.119866736,5904850.904377669,5903042.807596223,5901404.8139535105,5899986.543812773,5898895.827505704,5898225.205549351,5898099.775759607,5898702.7403741935,5900159.50172745,5902714.282797511,5906688.215591138,5912339.54710147,5920021.288334306,5924569.915850088,5921693.214639574,5919570.623487093,5918471.515418249,5913280.130453125,5905288.889551987,5896937.756270654,5888238.8447524,5879227.277622975,5869939.23431415,5860471.22997229,5850900.033328768,5841343.210648532,5831951.202318932,5822914.103190362,5814454.216970099,5806898.393333915,5800594.468446732,5795987.49594417,5793618.607683953,5794142.082272436,5798344.505462396,5807167.388800104,5821729.810065549,5843345.654003163,5873557.806094453,5914159.2102524005,5967215.008364339,6035081.840691582,6120487.236310115,6226281.970337488],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle EN pour le paramètre l1=1.0 en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE ElasticNet pour tout le meilleur paramètre l1 ratio\n","for i in BestParametresEN['ElasticNet()'][BestParametresEN['paramètre'] ==\n","                                          'elasticnet__l1_ratio']:\n","    fig1 = go.Figure([\n","        go.Scatter(name='RMSE moyenne',\n","                   x=alphasEN,\n","                   y=GridEN.ScoresMean.where(\n","                       GridEN.elasticnet__l1_ratio == i).dropna(),\n","                   mode='lines',\n","                   marker=dict(color='red', size=2),\n","                   showlegend=True),\n","        go.Scatter(\n","            name='SDup RMSE',\n","            x=alphasEN,\n","            y=GridEN.ScoresMean.where(\n","                GridEN.elasticnet__l1_ratio == i).dropna() +\n","            GridEN.ScoresSD.where(GridEN.elasticnet__l1_ratio == i).dropna(),\n","            mode='lines',\n","            marker=dict(color=\"#444\"),\n","            line=dict(width=1),\n","            showlegend=False),\n","        go.Scatter(\n","            name='SDdown RMSE',\n","            x=alphasEN,\n","            y=GridEN.ScoresMean.where(\n","                GridEN.elasticnet__l1_ratio == i).dropna() -\n","            GridEN.ScoresSD.where(GridEN.elasticnet__l1_ratio == i).dropna(),\n","            mode='lines',\n","            marker=dict(color=\"#444\"),\n","            line=dict(width=1),\n","            fillcolor='rgba(68, 68, 68, .3)',\n","            fill='tonexty',\n","            showlegend=False)\n","    ])\n","\n","    fig2 = px.line(GridEN.where(GridEN.elasticnet__l1_ratio == i).dropna(),\n","                   x=alphasEN,\n","                   y=[\n","                       'ScoresSplit0', 'ScoresSplit1', 'ScoresSplit2',\n","                       'ScoresSplit3', 'ScoresSplit4'\n","                   ])\n","\n","    fig3 = go.Figure(data=fig1.data + fig2.data)\n","    fig3.update_xaxes(type='log', title='alpha')\n","    fig3.update_yaxes(title='RMSE')\n","    fig3.update_layout(\n","        title=\n","        \"RMSE du modèle EN pour le paramètre l1={:.2} en fonction de l'hyperparamètre alpha\"\n","        .format(i))\n","    fig3.show()\n","    if write_data is True:\n","        fig3.write_image('./Figures/graphRMSEEN{:.2}.pdf'.format(i))\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                          paramètre  KNeighborsRegressor(n_jobs=-1)\n","0  kneighborsregressor__n_neighbors                               6\n"]},{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning:\n","\n","\n","5 fits failed out of a total of 250.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n","    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n","  File \"/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/neighbors/_regression.py\", line 213, in fit\n","    return self._fit(X, y)\n","  File \"/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/neighbors/_base.py\", line 566, in _fit\n","    raise ValueError(\"Expected n_neighbors > 0. Got %d\" % self.n_neighbors)\n","ValueError: Expected n_neighbors > 0. Got 0\n","\n","\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning:\n","\n","One or more of the test scores are non-finite: [               nan -12670752.92479431 -12028919.27712639\n"," -11927193.50975769 -12214388.17542299 -12266748.44296878\n"," -12215731.27337505 -12251488.85422829 -12334126.32398735\n"," -12387910.69512736 -12500746.42978987 -12519269.63921909\n"," -12593778.69126618 -12648094.80524051 -12654672.53442047\n"," -12689843.15070828 -12741386.67049728 -12741884.62511984\n"," -12778270.86146582 -12839656.56086856 -12875960.21025426\n"," -12920383.68453314 -12961354.14937447 -13002960.11719805\n"," -13033514.11322536 -13081099.50119891 -13109226.98823911\n"," -13127903.76406802 -13149354.98300733 -13183839.13584398\n"," -13228895.05234365 -13253623.14395287 -13271942.39253906\n"," -13299346.68032679 -13323473.51790466 -13356205.73957579\n"," -13387827.58684056 -13411317.39031699 -13435094.50334769\n"," -13463052.95164448 -13484325.68495863 -13513410.4951283\n"," -13534448.13427251 -13554032.1453893  -13573121.55958576\n"," -13592920.32012857 -13613599.58257607 -13631612.09791476\n"," -13654059.93262793 -13681315.42778024]\n","\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predkNN=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[5118989.166666667,3537822.9583333335,2613802.6666666665,755815.5416666666,5006556.15625,2158928.8333333335,3085167.3333333335,2567730.5,4617944.333333333,1331787.5,1790205.8333333333,2621727.5208333335,1125546.3333333333,2505603.8333333335,2273068.1666666665,1407268.4791666667,1087007.6666666667,1641485.2291666667,717645.8333333334,961223.1823,1089194.8333333333,1650715.125,7488967,1372927.8333333333,722351.5,813635.1666666666,2721015.3333333335,1086084.3645833333,948029,19764073,10070297.083333334,1511064.1666666667,2472246.9375,2763345.4375,24197908.333333332,3904704.3333333335,1037693.35425,1129443.6666666667,1615579.3854166667,2266714.0625,1033852.8177166666,817722.7083333334,2916946.8333333335,9583121.166666666,3705034.125,60137200.666666664,1166836.5,5129462.166666667,5824086.208333333,768316.6666666666,5337489.625,1580177.4375,2511226.125,25645782.833333332,3596771.5,1177455.2239666667,950390.75,2215098.5,1999590.8333333333,1013083.6875,503592.1666666667,3679560.5,4095573.9166666665,818005.6406333334,4158601.8333333335,6222866,6058749.833333333,4299741.875,1629177.8333333333,539425,4069407.0416666665,15132965.75,907419.6666666666,3902736.1875,3108785.3333333335,3231941.8333333335,870186.1666666666,5218800,4909156,23684237,1617296.1979166667,1333165.7188333333,2061277.8333333333,2960458.5,3278843.0130166668,3513177.1666666665,97149064.66666667,970112.3333333334,1650031.5208333333,677572.5,1102899.6666666667,1236740.2916666667,503592.1666666667,3500630.4479166665,675367.9817716667,1680868.8958333333,906947.1666666666,16099505.833333334,1961648.2083333333,1939275.2239666667,1202915.5,15684395.333333334,5783322.666666667,2756425.5833333335,23321380.458333332,9583121.166666666,655533.8333333334,1691511.0520833333,830712.5,7909685.333333333,5358016.166666667,3189203.5104166665,2181182.9583333335,517141.8958333333,845554.1666666666,5789479.666666667,2950307.2916666665,644366.6666666666,28585752.166666668,2072823.9166666667,554425.6093833334,1823692.8333333333,1187761.6666666667,692882.3854166666,10551135.5,661432.3229166666,1578589.3333333333,1502224.3333333333,1167077.8333333333,2292734,15647992,419575,17489307,1310981.28125,1052286.5,3798503.7916666665,1002218.6666666666,993252.8229166666,1053995.2083333333,3262631.625,4033170,16960687.333333332,7864708.198,4759501.416666667,3861038.3541666665,1435798.6666666667,1423987.5,2332729.8541666665,2074909.6666666667,20962165.833333332,8181637.625,2618217.2083333335,4141119.5104166665,1405776.90625,6733530.583333333,1122300.8125,3821568.6666666665,14549497,6017985.333333333,673198.3333333334,58596987.333333336,2941880.8854166665,1886014.3333333333,906808.6666666666,1526346.25,1004145.33855,11025541.083333334,1017454.3333333334,3379288.0729166665,720876.3333333334,7153126.916666667,661480.1666666666,1128723,3862769.4375,1792958.6666666667,2715234.8333333335,15247331.916666666,1696881.6458333333,759783.1666666666,766215.375,918173.8333333334,951244.6666666666,1768706.21875,4395517.166666667,1516773.1666666667,2513564.7395833335,1381503.8333333333,134644600,2134185,1176440,1238600.3333333333,1657105,1218956.71875,886856.4166666666,3896946.2916666665,1127022.1666666667,2057672.8333333333,1867683.8333333333,8299313.833333333,688018.3333333334,13550545,1316094.9791666667,2427295.84375,1377102.59375,1514904.6666666667,903690.5663999999,1307865.03125,5937430.5,1043069.5104166666,5983043.0105,1292603.4895833333,631873,4243554.166666667,856874,2919454.5833333335,4537776.833333333,806430.6666666666,1124459.6666666667,1594594,6456270.833333333,873326.8333333334,689475.4687666666,1056803.6145833333,845982.6666666666,2802757.1666666665,6823013.708333333,7863850.5,61859582.666666664,2298678.3333333335,3869448.2083333335,10674662.833333334,1540744,775751.9479166666,693264.1354166666,2261129.3958333335,1550440.1666666667,2659560.1666666665,15839510.166666666,13146680,23569722.416666668,2650448.3541666665,11772329.083333334,2022913.5,3748261,2370727.0625833333,1494886.4166666667,1816147.8020833333,8335033,4560769.166666667,1135536.5781333332,1925839.8333333333,9025545.083333334,1389423.6875,2340689.8333333335,2255498.3333333335,2686896.5,1914314.1614666667,1917861.5,8204833.166666667,1469035.0105,689760.7396,10310831.8125,749638.5,1258825.6666666667,2057193.6145833333,6017481.125,8136012.75,2043409.1666666667,23465451.833333332,766295.1666666666,989162.4166666666,1282087.9895833333,1380711.375,1816147.8020833333,1984102.4583333333,3632577.5,6709560.333333333,5136740.166666667,15744827.666666666,2992644,1288011.46355,1262833.4166666667,3513177.1666666665,9505405.125,574383.3333333334,906808.6666666666,952088.9739666666,1344146.5,21472946.333333332,1273087.5,23441530.666666668,1072008.59375,2726567.5625,7134780.666666667,1688093.8958333333,1913541.9635500002,952908.0455666668,2098817.8333333335,1445516.4479166667,1266472.8333333333,29073651.833333332,895052.5,3573408,1062286.5,3062647,1047960,956140.1666666666,2098817.8333333335,1324454.6666666667,2229952,2914707.8333333335,1602561.8333333333,14322292.083333334,3493756.5,1634842.5,559602.5937666666,23966939,1968093.90625,1123450,1770759.3333333333,2393557.5,839886.9271,6040783.5,1381770.5,9999883.166666666,2193285.8333333335,3270638.63737,935254.5,1738541.375,9001145.583333334,12955508.166666666,759911.5,3267120.2395833335,1650715.125,1444910.8333333333,1412940.0208333333,993458.3645833334,2755185.7448,964440.65625,1405776.90625,3867378.5833333335,849845.1666666666,889880.1666666666,1761995.6666666667,14173095.25,2503614.2916666665,15851653.666666666,4859120.25,5417462.958333333,15588036,4018994.25,7797028.166666667,6099262.958333333,887398.27605,13874947.666666666,1049824.375,1398293.6666666667,8863578.375,767187.2916666666,989545.1145833334,2303841,1937647.3333333333,5577539.333333333,27044550.833333332,749638.5,656038.64585,7139509.875,1522801.8854166667,2607862,6118689,2810489.0208333335,4852395.083333333,15281063,936796.6666666666,692778.1666666666,1068925.6041666667,2904285.1666666665,11181544.833333334,1456927.3333333333,5520765.166666667,7488967,5012046.875,1111588.8333333333,2133919.8333333335,593633.5,2618646.6666666665,1277933.3958333333,1596671.8333333333,1456927.3333333333,1529416.8333333333,14761678.958333334,818988.8333333334,3105507.1979166665,2432973.8333333335,1033395.5,1509913.3645833333,2476005.0520833335,15717199,1011602.4895833334,1784421.0520833333,854796.6458333334,910323.53125,716111.9583333334,2023911.1666666667,3700937.25,1362053.5,4029626,3889244,1158493.8959166666,743512.6666666666,2244151.2708333335,8105364.5,1018839.1666666666,871721.03125,1713429.4375,754286.5,824559,2690353.6666666665,2982635.5,7398662.5,1353837.5208333333,5626613.333333333,2748721.3541666665,2432929.3541666665,7153126.916666667,7499960.625,1053483.6458333333,24103152.833333332,1242124.7291666667,1998520.5,1585591.8333333333,2165563.1666666665,1147371.5364666667,6329653.520833333,1426301.9063333336,969310.25,710359.1666666666,2153377.2291666665,1272129.8855,863022.9323,4946110.083333333,1276500.6459166666,753025,1381566.9583333333,987734.6666666666,6449300.354166667,1255135.3802166667,53274490,8613908.645833334,1461436.8333333333,1075822.9584166666,989545.1145833334,7609903.833333333,497025.8333333333,1012329.7708333334,2002255,3201427.380216667,886186.8541666666,2936943.5208333335,1326002.4166666667,2660892.6666666665,566546.5,49733952,2164782.2708333335,632729.6666666666,7437551.833333333,14376580.833333334,1231015.6666666667,911844.46355,5248000.75,12296755.166666666,721722.3333333334,517141.8958333333,2022913.5,1150827.2813333333,612049.1666666666,1285366,1226820.8255166665,987731.4114666666,3255124.5,3369617,15776898.666666666,24375704.333333332,9157532.75,4791754.3125,4593178.166666667,769139.8333333334,7412880.166666667,2109753.75,5977939.166666667,4094593.3906333335,2253630.5,3346128.6666666665,1228107.6666666667,3267120.2395833335,1265201.3333333333,1350721.3333333333,6463855.5,83498465,4300678.604166667,6515250.791666667,1039225.3333333334,1538197.5,11831520.5,14679464.333333334,1334299.1093833332,10812190.083333334,8751245.5,900454.8541666666,47918004,801902.3020833334,2385038.8958333335,41470005.166666664,4031199,1532021.323,791279.7916666666,1397671.59375,609779.52085,3258184.6666666665,1446965.8958333333,869768.0208333334,817021,786559.46875,1843654.6666666667,1125171.3333333333,2810489.0208333335,891597.4895833334,994556.8333333334,10297735.333333334,2709316.1666666665,24001324.333333332,645511.6666666666,1465273.4271,4317999.333333333,3235550.6041666665,2530000.90625,684844.1302166666,897756.5520833334,9340256.166666666,1045208,1213101.5,5757922.833333333,2191648.2083333335,1775409.59375,1496572.2343833335,1680583.8333333333,4277210.916666667,3616884.9583333335,1028926.5768233333,716090.21355,8015215,2724062.8333333335,2263798,5164957.5,8150238.666666667,1025020.8333333334,675489.8333333334,5845500.791666667,1760711.2916666667,1179983.71355,2625664.3958333335,871657.1041666666,1147371.5364666667,2140190.5,2349994.1979166665,6228051.166666667,1353583.84375,1632612.1145833333,3596284.3333333335,1562874.6666666667,2551757.1666666665,28906297.25,12247914.583333334,1928380,912878.8333333334,1130642.3959166666,1889194.7083333333,1002510.1666666666,830595.2604166666,824163.3333333334,726097.5,798715.7395833334,4525007.25,1258098.1875,934148.3333333334,1213407.2188333333,798715.7395833334,1125261.1666666667,871739.5729166666,638872.4531333334,1662031.9791666667,1275331.1666666667,15669549.333333334,882758.21875,4641579.65625,768316.6666666666,683909.3333333334,5463037.833333333,3663498.1666666665,1749435.6666666667,1822173.7708333333,120514924.66666667,2893682.8333333335,3687821.1666666665,5670123.833333333,5294960.53125,2817314.3333333335,2361205.3125,782663.6666666666,7010976.5,6974758.458333333,2587835.6666666665,901382.40625,1477516.5,4816197.333333333,1296247.6666666667,3840939.9583333335,409174.5,1634865.5,3379442.5208333335,2210335.9895833335,9019585,509185,6549937.5,2568114.5,9111704.479166666,56774071.333333336,3680765.7916666665,920783.52605,1296626.8959166666,821162.8646,1009218.9218833334,1028926.5768233333,749185.3802166666,2704965.0625,5291185,2745877.5,1014201.5,1002840.09375,1420381.625,4921916.333333333,1867683.8333333333,868836.375,1082929,12230130.666666666,2169872.5,15516882.5,631795.8125,3363943.3333333335,1450927.6666666667,2657056.3333333335,1420900.78125,4433227,2625664.3958333335,1527540.8021666668,599450.5156333334,1538197.5,25654224,9341842.416666666,30927640,732985.3333333334,3149261.03125,1970661.5,831465,5012105.666666667,810351.7552166666,1791531.0520833333,9924123.5,772754.2968833334,4454379.333333333,999996.3333333334,5113980.135416667,837041.0208333334,1698098.3724,2211772.8333333335,664832.2083333334,1123915.1458333333,1697322.2916666667,4379297.5,8591707.583333334,13208628.083333334,3335358.4375,2223553.3333333335,646899.9521483333,1341070.4375,5223672.166666667,2797006.5416666665,832303.1979166666,797148.3854166666,2536587.8333333335,1284478.8333333333,550246.4817833334,1757708.8333333333,4268758.635416667,537557.2031333334,51127094.5,2290657.8333333335,1789241.6666666667,3411003.0833333335,944012.9166666666,1045598.3541666666,1026008.7656333334,1710737.1145833333,4063462.5416666665,4308732,1485022.125,3772231.5,7947846,2221668,2588933.8333333335,1329581.1666666667,1772347.8333333333,3357646.1666666665,1212709.2708333333,879449.8333333334,1028926.5768233333,22385589.5,25486488.833333332,974228.2708333334,20230319.5,43874551.166666664,16727576.333333334,16472167.583333334,1157982.5,1448492,1736497,2756280.8958333335,900725.9323,3324130.4166666665,1381983.3333333333,1644102.5833333333,6779958.166666667,1497889.0625,20364760.833333332,1535203.5,5187749.916666667,2242206,1068411.9166666667,1079101.8333333333,3709678,725143.1666666666,1230447.8541666667,11629841.75,5030148.875,1609405.2708333333,20572021.166666668,3203884.5,6013494.583333333,1727434.28125,3500630.4479166665,2326547.6875,1865534,631817.8333333334,776688.5,580477.1927166666,1095306.6666666667,2372987.3333333335,3265974.3333333335,3891819.3333333335,1515869.7604999999,3424418.3333333335,1258396.1666666667,1763005.6666666667,3464536.5,9334010.083333334,938256.8333333334,7005205.166666667,4288503.5,6516984.333333333,1815295.1458333333,1104151.4270833333,1158811.8333333333,8065381,46338218,5586629.166666667,824559,631795.8125,1772876.5,20886505.166666668,711718.90105,4366515.708333333,15956774.666666666,22411429,1866645.8855,1935260.8958333333,1747535.2291666667,3693407.6666666665,1053995.2083333333,899024.4791666666,699122.3333333334,1510328.2968833335,2567092.9583333335,2244151.2708333335,2377832.5,1438669.7395833333,1636507.3333333333,869460.7083333334,632891.6666666666,1203210.5,3861340.1666666665,1921747.8333333333,639875.5,1799150.8333333333,520610.18751666666,9318309.666666666,818005.6406333334,1453229,12067905.666666666,785544.5729166666,1021923.3645833334,46072308,744512.34375,797387.9479166666,1324652.5729166667,1857175.6666666667,3095203.25,4039060.2916666665,18870675.583333332,2687161,1100644,1707857.1666666667,5586629.166666667,7318689.5,1134255,1281953.3333333333,2133731.8333333335,4083254.0416666665,2896736.0416666665,843458.8333333334,1650715.125,886218.6666666666,2904212.5,2260504,757937.1666666666,1678288.8020833333,45542432.666666664,3075561.4166666665,14032203.166666666,2284134.6666666665,19710153.583333332,3633007.4791666665,18685731.333333332,6522686,12431302.166666666,8813854,1053483.6458333333,1081385.33855,701830.27605,1810656.1666666667,1507723.5,1081899.6666666667,9505405.125,5253099,1597651.6666666667,920783.52605,1031474.6666666666,1260125.7291666667,7969663.333333333,6154058.041666667,3511794.2291666665,503592.1666666667,6909317.770833333,845768.3333333334,5004577.5,412794.30859999993,3267120.2395833335,1964258.8333333333,2681149.411466667,4744391.333333333,2625664.3958333335,778589.8698,4803589.166666667,2504417.6666666665,4041478,949058.8333333334,1699010.6666666667,1076852.5,9697718.416666666,722351.5,3457105.625,1010156.7708333334,910932.9895833334,3274571.6666666665,1017985.6666666666,23014430.333333332,10453952.666666666,1164987.96875,878908.3906333334,1429986.5104166667,685561.6354166666,3979882.3020833335,718235,1495647.3229166667,12538007.833333334,11081404.5,2697112.1666666665,7749456,755815.5416666666,2587835.6666666665,10447921,1367102.6666666667,1608933.8333333333,2137187.3333333335,919230.4088549999,6954764.375,7747167.604166667,1058388.9791666667,1457957.3333333333,1888994.3645833333,1035852,17204491.583333332,5290313.0625,4141119.5104166665,1736234,875001.6770833334,1697322.2916666667,38705974.333333336,3473591.875,1370401.3333333333,2628395.8333333335,4370567.5,766325,2997163.7083333335,2875226.5,2540536.2291666665,1165119.1041666667,1433035.6666666667,1138769,7993984.5,1175648.5,1425270.0625,813351.8333333334,11855743.666666666,3835046.5416666665,947226.698,6974758.458333333,13952017.833333334,4937253.583333333,4755115.666666667,5315915.541666667,4290566,2704049.6666666665,1491089.6666666667,1439456.6666666667,2261129.3958333335,19058411.625,2349080.6041666665,2511889.8333333335,10878259.5,7109364.333333333,5942520,1539303.3333333333,13208628.083333334,2004296.8333333333,1138603.8333333333,1711904.3333333333,14509243.833333334,9756229.5,2455070.1875,1109934.0104166667,7764715.958333333,4915471.531333334,1377261.8645833333,1059761.8333333333,9504779.833333334,2304296.2708333335,1380797.6666666667,2212821.3333333335,55959282,779278.5,1067068,19710153.583333332,3233888.8125,1535203.5,1203598.1302166667,2193014.4375,19306721.25,1139806.6666666667,2483553.5,4108501.7291666665,2073125.158866667,1045598.3541666666,5379502.166666667,2141849.6666666665,3441234,1162080.8333333333,2277669.3958333335,15036182.416666666,11731127.333333334,836249.8333333334,2008532.6666666667,25465784,1151200.8854166667,721281.8333333334,2395826.125,1196497.5,18071386.666666668,1794958.6145833333,6800920.833333333,5291648.458333333,1597651.6666666667,3310850.3333333335,17465121.833333332,7524189.833333333,669464.8333333334,1088233.0520833333,1088233.0520833333,3875209.125,1625221.3333333333,4954675.458333333,1700507.1666666667,1357927.8333333333,2383541.6041666665,6013494.583333333,509804,3196690.5,14378132.666666666,6589688.354166667,1088233.0520833333,4911668.166666667,1237262.3333333333,1133609.1666666667,1396773.9479166667,1544174.3333333333,15022197.25,1599779.5,634159.7229816667,689760.7396,799275.1666666666,744636.5,2719787.7291666665,1221988.3646666666,623064.4401,3546835.5416666665,3546063.8333333335,2438326,3603865.2708333335,5439931.666666667,1305947.6666666667,2650752,7153126.916666667,2884297.1666666665,1423987.5,739416.625,7483856.666666667,1680583.8333333333,6019343.916666667,4445969.333333333,2014697.9583333333,706958.3333333334,3404334.1666666665,964008.3854166666,1089761.03125,2127375.7813333333,3151821.8333333335,4447692.333333333,1453229,3914681.8125,864741.0520833334,931841,1597651.6666666667,1143430.5,1826546.6718833335,3904441.375,1595133.0989666667,4937253.583333333,1025020.8333333334,3043465.6666666665,644781.8333333334,714815.3333333334,1013720.9791666666,1354389.59375,6156444.5,11473130.666666666,1935827.6614666667,833138.1666666666,1933197.6289066665,9573811.25,869450.6666666666,1298783.5,952960.1666666666,2983177.8333333335,1201285.0416666667,1395979.5729166667,7679898.416666667,2253235.4531333335,631817.8333333334,3406692.2083333335,3574473.0625,2089691.6041666667,3016375.2291666665,4028771.1666666665,3423612.8333333335,1062175.6666666667,2043409.1666666667,1998384.6666666667,4580050.979166667,1744196.3333333333,408428,2936149.1666666665,769790.9948,1405776.90625,4537776.833333333,1605584.1666666667,3165953.8333333335,3191289.28125,3180350.77605,945123.3958333334,4005720.0833333335,3488421.5,1579872.6666666667,9029463.833333334,1099666.1145833333,3405713.8333333335,1043069.5104166666,3404294.09375,809885.5,1384070.8333333333,2877999.3333333335,4048794.3333333335,989775.5,3381474.3333333335,5768136.666666667,682250.6666666666,6529195.583333333,814292.6979166666,1530974,1577611.1666666667,807939.2083333334,20775811,14920597.333333334,805556.0208333334,1494858.8333333333,1072008.59375,889880.1666666666,1005794.1458333334,3072653.5,874845.1666666666,1484714.6666666667,1071772.3333333333,1180490.5,1675123.6666666667,1303254.5416666667,2987555.71875,18471788.166666668,1918665,882579.3645833334,438461.3333333333,718235,6920566.833333333,1286664.1666666667,3869448.2083333335,3496886.8333333335,4230811.166666667,3847268.1875,17564542.166666668,2745877.5,10613860.020833334,1397296.5678,1535568.6666666667,1869407.3333333333,758146.8333333334,1451316.8333333333,2098817.8333333335,15472293,2346842.7916666665,1388233,761935,3203566.5,1009763.1666666666,2341121.3333333335,1012712,4162124.5833333335,1631335.6744833335,4161709.3333333335,2490725.25,3298051.3333333335,763622.6666666666,3940445,1867683.8333333333,1210814.6458333333,2871206.40885,1837226.6666666667,906947.1666666666,727313.5678,3789374.9583333335,1719278.6666666667,1191981.4166666667,719304.0729166666,979228.8020833334,1674147,2552408.6666666665,20704467.75,2824077.3333333335,1031210,1181289.8333333333,8902050,1990466.6666666667,1483401.1666666667,2011568.5520833333,1297844.1666666667,1081899.6666666667,1598633.6041666667,3044071.8333333335,754400.5573,20461188.75,6737471.3125,1911889.4583333333,678586.3333333334,970011.5,2093176.7291666667,8587007.5,719321.5573,1122300.8125,2769803.1666666665,1559823.7395833333,1439420.59375,4023919.4583333335,3085167.3333333335,2139115.3541666665,3306827,1294768.3333333333,2061845.8125,2531187.1666666665,814374,1148345.5,11415748.916666666,3527768.3333333335,835140.7916666666,1983120.6666666667,2315252.1666666665,1439669.7343833335,2877999.3333333335,1257917.6770833333,1177455.2239666667,2144445.1041666665,4656375.833333333,1094796.3333333333,4310559.5,2008532.6666666667,1937454.79175,1530974,1494501,929482.09375,4273546.333333333,1867683.8333333333,1036776.79175,1556881.3333333333,528120,1763005.6666666667,3040185.9791666665,1338270.6666666667,4183108.5,6740676.458333333,2752713.8541666665,1850435.25,3518116.6458333335,1452440.5,11141688.666666666,11643064.833333334,2284426.5,3070350.1666666665,8936724.833333334,3266040.8333333335,71843240.33333333,1011760.5833333334,1079309.2916666667,8599926.333333334,5143900.75,846193.0052166666,1425949.46875,2594929.71875,1485433.2916666667,2101886.0729166665,42532858,1222931.3333333333,3783591.3333333335,2492183.9375,2470007.6666666665,901382.40625,738161,1439456.6666666667,3608018.625,1659095.3125,3096230.5,1070879.8698,849350.6666666666,2231934.4896666664,2625664.3958333335,5082617.5,7096187,12893876.416666666,671459.8333333334,1821659.0833333333,1060297.4895833333,1298598,1477936.8333333333,5120451.5,11123922.666666666,853725.6666666666,20716640.833333332,2017456.4688333336,4815658.083333333,798715.7395833334,2248196.1666666665,7371151.666666667,2609972.8958333335,2924876.2916666665,1742005.5],"xaxis":"x","y":[7506587,1972078.625,7768294,706505.6875,2274873.5,2547582,4571388,288191,6252842,1126157.25,1153276,4103800.75,1963054,3292049,724259,819803.6875,1589550,967432.8125,984178,968026.3125,1688745,678663.3125,19236770,1359948,342493,497620,2682044,4420650.5,866343,1250669,1770027.5,14084606,1981115.25,1986094.875,32141070,5415438,932083.5,293217,3543985.5,2443065.25,992794.6875,885865.875,552729,3580091,2993610,284867168,1222509,5672838.5,0,965786,1429146.375,1648774.375,726124.625,30648154,1836671,876425,2192111.5,1426823,1748132,1791071.375,646377,1487660,1882845.75,893053.1875,4247457,1796889,6426022,2493240.25,650008,385719,1851260.5,75073888,804002,25453690,1997488,2712983.75,913899,9546165,5122039,25307744,1313366,761504.875,1302636,234144,5968153.5,2966309,58761304,1160989,2217620,142873,2051501,1069490.125,348061,545426.375,1113457.75,2665558.25,628023,4759256,0,2406996.75,501194,6508595,14829099,2491238.25,4135650.5,1023418,454296,1682454.875,813038,11651200,6797528,1145535.75,1348497.125,493000,766628,8204897,3960898.75,609101,17463744,3348280.5,863728.8125,1692323,2112095,1878742.875,6711975,675611.375,2196718,1199742,748775,1545356,19645206,122156,8393893,866825.375,1357116,2496807.5,615894,845044.125,526752.8125,2991674.75,3093944,25471308,7221482.5,3894584.5,2502081,630578,1569569,1435730,2512443,8971926,19248732,1678495.25,586589.1875,1832808.75,1132489.625,811907.625,702385,20361158,5647146,2323265,41655852,0,962971,1200811,1107290.625,1761137.5,14029480,248607,1037555.125,659450,3528086.75,820705,2433777,4549892,1358652,2558573,4287439.5,1239315.375,786979,730765,2781314,567072,967082,3228330,1243282,7606084.5,2103293,114648520,1977299,798970,4700552,1813864,1482554.375,759060.8125,4946798.5,678351,3289053,3326999,6935723,1386331,2740128,888031.875,1809101.5,581289,975431,4475650,1441445.625,8287440.5,659652.625,1538750.625,1306087.75,1058986,1060846,131810,3040978,1593187,791564,1085183,818317,3022275,520738,1224685.125,2114860.25,1088016,4520838,3626431.5,4448582.5,9212950,2392466,6689184,13586070,2074152,567608.625,594848.625,3009463.75,2446842,2161828,7794869,15206762,10084950,3906967,25631512,1022114.5,1182575,1218112,1249939.25,1788148,6102883,3093378,1493131.75,832168,7791957,839362,338923,2092251,1929828,1061458.25,451738,5820965,2284507.5,3740373.75,5417598,1632498.625,992139,1232822.875,3590731,9851937,2069880,27139746,514555,948676.5,665200.1875,468973.5938,13140574,227509.5,4185406,11509942,3185509,20630168,2928099,1391916,111969.7031,4842910,2278160,2094410,1138520,2470566,851651.1875,19397324,1244824,35383012,777587.125,898115.375,3471845,2190460,17686444,696605,2396523,651293.3125,1187261,5661689.5,517029,4673827,1255834,1970956,644415,1290284,2656729,2041912,4700396,29195468,1403869,42709624,1986384,1240875,509485.6875,6856153.5,369291.5,1464856,5420552,1140620,1424157.25,3795938,498745,4028943,873648,1234573.875,936336,2503690,2756214.75,6884914,766381,1384558.625,1260416,1214929,5448975,832986.875,659761.875,209644.9063,1269847.75,2526375,3467070,600318,2338852,11765682,1232210,13901935,7539963.5,4425832.5,24906132,3579008.75,11353672,7475577.5,608491,11289052,698367.6875,1525624,28731502,1656420.25,557082.1875,934357,1020039,5288304,9049598,899266.125,588969.8125,16016644,387810,6056481,3437441,1441665.375,8800292,8386493,497991,738990,2159932,2436403,5747645,1150582,1912690,8787353,3024478.5,863773,2274920.5,1187315,3158151,847298.8125,7613116,456397,1026921,5974993.5,666385,2559461.75,1170616,1967129,3928695.25,5464762,20781390,629027.3125,6129780,964236,929950.125,1815371.25,1777652,1242811.75,562479,8964061,7577723,1537570.25,403800,855946,10416934,1736775,684739.6875,786324,827042,892086,1735816,2404644,3568616,1046995.188,276107,243518.2031,2212058.5,6858533.5,318364.8125,639407.375,10126597,2227908.75,849504,8381744,4397572,1676426.5,3761647,1228837.25,646505.1875,717766,1195351.25,934621.1875,1152896.25,4103027.5,3392129,841891,1692199.25,980356,4374646,2332996.5,35891484,2845946,2674034,627854.125,2181833.5,5424718,298063,3560614.25,978463,2130673.75,1466382.25,3706100.25,33268410,5040382,508051,55813996,1075019.375,2100722,6227419,11356990,653139,204499.0938,5722325.5,10564729,548293,619545.125,1162120.375,901015.1875,1074937,1353337,807185.5,576405.125,1662723,4329966,7133222,27282662,8435093,8058913.5,3567977,609218,6915293,509124,3869754,363808.1875,2204235,4307238.5,1123416,2799595,1997115,1943772,3467698,448385312,2417401.25,5249190,664088,415975,16917964,13609040,3537841.5,13458948,9157150,2204161.25,47952868,847097.8125,4571746.5,65047284,3833162,1211947.375,1458617.125,3014332,705948.3125,2022990.625,1567040.75,352136,2168281,1160087.875,588965,628609,3481727.5,1677002.25,619804,13943008,1053290.75,1990444,565458,3494929.5,3155391,4089677.5,1998725.25,712057,935396.6875,8201974,782202,848497,4368113,1165598.875,2420605.5,592739.3125,2184912,3600897.25,4486477,980603.375,958798.3125,1886382,2503948,4751090,1651120,6335315.5,956481,397624,3063969.75,1841154.25,1295685.75,2586116,727703.5,1636649.125,1855556,719471.1875,2267094,1787119,1331122.125,971308,902694,5024837,17186624,55073120,1819329,512166,772379.3125,1049436.25,887403,416566.1875,673834,609257,759405.6875,3155496,1070085.375,2006626,2804534,747384.5,1057256,1054581.375,507428.9063,2664566.5,2065156,9697866,847514.5,6522551.5,720005,573624,12783255,0,2136086,15480681,253832464,2339234,4212149,3021669,3181045.5,2601369,1656025.625,646994,1565475,7802205.5,1187723,1356874.75,2298648,1136698,1586388.75,1018111,9085108,1734533,1703504.875,1868497.125,11047668,433443,5459657,1841968,9444484,38400928,151754.2969,536642.875,3717064,2159170.25,1625308.625,682632.3125,785144.8125,2553763.5,4750287,9617898,1025205,874707,618876.1875,3162976,1520326,2114053,1109924,7546235,3542542,10444602,462465.5,4023479,1761048,869835,1189942.75,2211671,1735538.25,789630.125,1962351.25,827937,28450436,6074249,9861998,967697,2107743.75,696054,1152890,3791402,965651.125,2090895.75,12480984,552015.625,2904225,466985,1268459,862744,313528.0938,1769998,1547116,2023858.75,863703,4656777,11061916,5233553.5,1168017.75,2300803,727775.8125,1443367.75,4608963,2342494.25,706879.5,574575.375,855670,666740.3125,1981360,1193790,8773058,520348.9063,17666646,1607590,1227772,14332778,410433.0938,926261.1875,2796425,1234813.25,2701833.75,4120963,4742869,37276920,18622292,2157144,1776411.75,666967,1080203,3031811,433871,908834,1076644.75,12448381,15843743,709979.6875,13239050,143423024,12196872,10992344,1291170,835545,1050982,1513700.625,1829122.375,2549649.75,1844825,3989399,1857329,1495686,32381836,1109851,6897247.5,1863784,675772.875,1018068,2378077,972608,1181428.75,5653656,7771933.5,1827424.125,6419090,1218114,16701055,771512.6875,162960,1213444.125,2777052,1185469,490599,541542.6875,1103762,811537,2932696,1848656,775007.6875,2172841,1788999,837792,784762,2737534.5,1695067,3060149,4178233.5,2924362,1271753.25,683274,712149,4020660,59757440,3298570,760090,505601,858897,15660628,323739.4063,5702077,9387211,37022180,1712279.875,1742351.25,1001964,1612797,638507.125,530700.125,2327611,1261634.5,1072487.875,1162561.125,6454983,1063960.125,151376,2534873.5,645756,1200381,2855545,2219744,688145,1362993,456197.5938,9523142,502898.6875,611997,13171370,1009476,739210.5,58041572,1459199.75,607579.6875,839011.3125,862826,1128179.125,2306723.5,8437401,2147062,646691,1619177.125,2236167,4058492,966764,1795462.75,2165371,4322873,2167888.25,1251906,669217.6875,471113.3125,664539,1983509,792015,794724.625,53079164,3629002.5,11924933,1375366,14641502,14469228,11813861,8878186,9273278,1985171,599591.625,489557.0938,3646053.75,695606,518107,699217,2499140,3187950.75,1543282,634056,1117984,687953.6875,7564604,4230514,1859725.375,569115,2564004,435985,1972491,116486.6016,1904641.75,1743198,347437.6875,2147013,5071974,713821.875,1869575,13659926,10721568,968343,6960396,886599,22055140,872555,31803568,757167.3125,1274049.75,4054582,601590,7969653,8259071,2427919.75,588856.125,1642471.5,668915.8125,2244317,656345,317794.4063,18558162,666392,3682374,7654264,623107.6875,3039889,18011944,1483856,807358,5762597,779503.625,10077532,5284343.5,522791.0938,1696446,811703.6875,1419453,53166156,2075891,3441643.25,3220827,1308348.5,127374,25210246,2828098.5,1437220,698177,1786986,571384,2152312.5,1910465,1204956.5,552007.5,1947717,456747,5903142,1259456,2436063,563454,15036617,2050600.75,1256456.625,2334260,18829136,4988493.5,2085349,5587347,2759071.5,2137686,841242,1587454.75,1591124.875,7996408.5,2655387.5,2303386,15324777,2584082,3850963,1037696,9765777,3314418,1929698,1987073,17750994,6914839.5,3841879.25,557863.6875,1951856.25,1555420.25,620104,604591,27119380,7202141.5,752441,2314835,39523852,1084654,769148,4545148,2773647.5,938908,335245,1400053.875,9722237,3035962.25,1817550,10005787,1581036.75,1773864.5,1242764,1992158.25,28229320,696709,849745.6875,6219841.5,5950438.5,731315,2033074,24882924,442375.8125,721723,1929851.5,842923,14776743,938096.1875,11224871,8387933,1645327,3371455,17871658,2290681,737570,492421.9063,2014475.875,10040121,8611054,6298131.5,1140137,1965529,1363296.875,6994740.5,632517,13253979,10614234,4345698,448600.5,4498367,652425,948157,938833.625,2759674,8233527.5,952879,830739.8125,786253.6875,919739,724153,5669431,1427974.625,521341.9063,3936546.75,5214621.5,1415355,8839812,7456330,2527198,1472548,2222097.5,2003759,1280245,1083467.125,2407904.5,12037792,10053489,3983784,1541897.875,2798967,6057734,507625.5,1038815.188,738983,360045,7753080,573395,1152368.25,680395.875,1929011,1612392,1297041,1354000.625,5381105,1100499.75,6136327.5,1357701,1724560,601897,591840,1148202.25,1618916.375,41092076,3548309,3641620.75,458746,1489363.875,4527521,756196,1314357,808829,2971013.5,647321.5,564988.8125,9436408,469499.8125,674961,405062,2375104,1361794.25,3518459.5,1971236,13320624,671079,662012,230365,5792926,1562708,529806,2964206,829440.625,161311.0938,2539565,2156407,3970754,3460275.5,2235765.5,1084553.75,3897738,1398253,1453543,3780078,790039.8125,1655612,1128208.125,7240274,488181,819314,7011508,4045980,1076693,4742636,5231373,151798,8751704,768755.125,1221806,577290,882759.1875,6310254,84980760,759508.6875,1417852,981009.125,639878,844076.8125,7231928,1256360,1054995,1415494,3577120,763693,1177583.375,1063480.5,59107620,1657201,885843.1875,731092,722951,18868768,437619,3091387.5,1878442.25,3118304,8765978,15725581,1529309,465533.3125,3275779.25,1287159.25,3986851,484875,1261963,1644969,7545909.5,1633399.375,3294645,687049,2775717,1098965,1688739,1641546,686019.1875,2852102.75,4079837,2077039.375,1136405,1372475,3000864,11566123,4519115,2295176.5,2185530,991283,574650.375,4039667,585559,776996.875,635516.375,730211.1875,1067793,1437569,8651835,19061780,1372834,1136676,2634166,2009324,964015,1237252.75,507277,814403,1594733.625,7008252,1302192.875,25377650,9097980,1808401.375,1000599,929628,2520310,5917000,658297.5,705262.125,1189402,1034827.313,522880.0938,6361236.5,1118392,1082920.125,2330029,393493,1697909.625,6015128,666909,524053,9686734,2471623,2225265.5,1785906,1245181,669787.8125,4863803,1243200.75,419978.6875,2531163,2805926,2166711,3660395,614768,2213115.5,10975751,427697,784329.5,12187231,4099807,843267,1074180,474359,801264,1955013.5,1365778,1615569,2892428,5826745,2890379,1247362,915417,8050977,18661606,4341706,1039782,8070420,3132453,37797084,679574.875,1022614.313,19018762,4841347.5,585077.125,550324.1875,6881405.5,4644346,1248599,32552428,788027,3529643,3996541.5,3226884,676499.875,559805,123767.2031,1835804.875,1451145.875,1040593,1567746.75,558709,2853189.5,2568377.75,4309177,1188444,9343428,657385.8125,2979413.5,1646088.5,1696009,1524620,2438430,23612824,818484,4222682,2001936.5,3739029.75,2245568.25,5928881,6268627,3027867.5,2277238.25,2017423],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle KNeighborsRegressor(n_jobs=-1) vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predkNN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["n_neighbors = np.linspace(0, 100, dtype=int)\n","param_gridkNN = {'kneighborsregressor__n_neighbors': n_neighbors}\n","\n","\n","GridkNN, \\\n","BestParametreskNN, \\\n","ScoreskNN, \\\n","SiteEnergyUse_predkNN, \\\n","figkNN = reg_modelGrid(model=KNeighborsRegressor(n_jobs=-1),\n","                         scaler=scaler,\n","                         X_train=BEBM_train,\n","                         X_test=BEBM_test,\n","                         y_train=SiteEnergyUse_train,\n","                         y_test=SiteEnergyUse_test,\n","                         y_test_name='SiteEnergyUse_test',\n","                         y_pred_name='SiteEnergyUse_predkNN',\n","                         score=score,\n","                         param_grid=param_gridkNN)\n","\n","print(BestParametreskNN)\n","ScoreskNN\n","figkNN.show()\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[null,12670752.924794305,12028919.277126389,11927193.509757694,12214388.175422987,12266748.442968782,12215731.273375046,12251488.85422829,12334126.32398735,12387910.695127357,12500746.429789875,12519269.639219092,12593778.691266183,12648094.805240512,12654672.534420472,12689843.15070828,12741386.67049728,12741884.625119843,12778270.861465823,12839656.560868556,12875960.210254263,12920383.684533138,12961354.149374472,13002960.117198046,13033514.11322536,13081099.501198906,13109226.988239111,13127903.764068022,13149354.98300733,13183839.13584398,13228895.052343654,13253623.143952874,13271942.392539056,13299346.680326786,13323473.517904663,13356205.739575792,13387827.586840559,13411317.390316986,13435094.503347686,13463052.951644477,13484325.684958633,13513410.495128304,13534448.13427251,13554032.145389304,13573121.55958576,13592920.320128571,13613599.58257607,13631612.097914755,13654059.932627928,13681315.427780235]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[null,19889386.355650358,19029134.946300656,18785902.695602875,19345685.526639633,19615573.16424026,19412155.743224688,19547840.248087954,19713054.92442566,19822573.38413097,19874658.138689086,19951838.413071305,20098184.889739946,20169319.377501436,20120471.77705489,20184788.741355978,20243412.11761419,20271157.879811775,20329140.868828937,20398838.375950925,20444476.14052625,20503258.157499015,20553162.296544854,20602918.435425956,20651119.400144335,20721266.16187732,20758641.20526599,20757440.79298833,20772914.414492466,20829257.655319583,20878602.868154123,20897313.60327248,20909152.870025724,20928912.22150282,20965336.914628636,20992864.445284594,21027259.881115958,21034585.124865722,21057172.770903107,21096702.20405107,21130162.526724722,21164214.83446019,21192135.03013741,21223712.834075667,21251483.14753407,21280139.944793265,21304521.710027304,21324895.9770547,21353721.05067721,21388766.846835114]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[null,5452119.493938252,5028703.607952121,5068484.323912513,5083090.824206341,4917923.7216973035,5019306.803525403,4955137.460368626,4955197.72354904,4953248.006123745,5126834.720890664,5086700.865366882,5089372.492792418,5126870.232979589,5188873.291786051,5194897.560060584,5239361.223380369,5212611.37042791,5227400.8541027075,5280474.745786187,5307444.279982275,5337509.211567259,5369546.00220409,5403001.798970137,5415908.826306384,5440932.840520496,5459812.771212233,5498366.735147716,5525795.551522197,5538420.616368375,5579187.236533182,5609932.684633271,5634731.915052389,5669781.139150755,5681610.12118069,5719547.033866989,5748395.292565158,5788049.655768248,5813016.235792264,5829403.699237881,5838488.843192543,5862606.155796416,5876761.238407611,5884351.45670294,5894759.97163745,5905700.695463878,5922677.455124836,5938328.218774811,5954398.814578645,5973864.008725355]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[null,8212255.451538865,8335490.588576783,8817646.776998049,9242222.636989657,9051347.042231224,9242665.448649777,9044795.147749191,9232192.26357944,9360288.818320366,9544119.021065837,9662904.530210223,9808954.115798166,9800767.775283463,9911928.095161347,9961842.203922577,10013730.127145842,10047657.673467418,10099221.349354407,10157238.50111909,10174588.662261752,10258465.35546634,10294520.851162005,10354695.345841277,10382904.938879164,10444931.615014397,10499222.92728923,10536013.717849994,10562927.311668111,10608428.161848903,10657964.031644564,10665995.102548415,10706568.312048329,10752354.998925336,10789814.581182163,10830928.138282241,10865687.639604932,10897505.57005296,10918892.627708927,10950199.256586324,10975852.739482818,11006232.76395418,11035261.869682338,11057181.337065827,11074140.943669055,11100675.41229954,11128583.251410699,11138433.676622909,11160823.858773675,11191178.217956685],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[null,26988542.80133783,25894801.079496503,25419191.137981236,26210973.60786103,26706290.60298159,26314409.64288354,26536912.458849646,26777474.201655336,26946834.436819732,26904182.759443454,27035387.244916923,27232985.543843687,27320242.90235472,27209319.188499566,27315763.7735054,27368063.37161173,27424219.168406826,27500503.119738538,27571031.895206016,27630926.866117895,27700122.99489612,27752067.6468107,27791960.50707519,27849911.74120888,27927057.118318055,27955884.86578082,27925461.66072631,27925541.897901207,27998937.56353989,28046759.585013017,28054438.716911495,28052815.59562581,28060947.479136348,28102378.973056633,28121988.47167734,28154228.428652693,28141577.186537076,28159190.10075113,28205638.484643716,28246565.681479547,28283758.358859923,28312638.495059438,28348875.836464923,28382316.01173142,28419765.750212114,28444196.7829405,28463310.46806616,28497257.84096646,28536862.722581256],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[null,10295144.179495681,9726935.42652831,10206265.930619905,10241958.69291713,10114175.070899356,10126760.957539681,10308702.715799913,10304973.983046565,10236449.082032619,10508473.034498736,10669180.851530222,10692708.827141203,10752193.42857087,10786070.362633236,10747487.964991907,10840676.648752103,10868019.377439229,10881859.07542694,10943928.170217903,10957474.335466621,10979190.123073671,11036864.057207761,11098536.546555486,11140586.097096795,11229816.20145543,11283537.94183718,11328603.465154769,11383698.645962046,11400301.836180732,11459556.740321891,11511727.23694441,11541279.908947585,11567459.639416039,11598944.390643587,11645044.80285139,11692009.119556384,11728198.83033441,11762475.77546887,11791255.629374279,11811476.163723322,11844724.365507182,11868227.080410048,11901226.431181878,11915511.200755363,11912763.268709406,11930772.080023238,11961421.339169994,11978388.105468033,12012589.519386597],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[null,9933838.685309654,9285265.649892524,8828014.732385041,9217950.836112125,9375151.540571686,9449055.84493671,9461205.649694832,9485490.744460473,9517716.249630276,9645505.834566986,9317570.39077651,9401871.227069983,9497295.511560645,9472453.068014512,9470203.10346881,9534764.187098078,9410980.726331148,9448717.693410309,9546234.223390456,9592349.264758002,9624180.15910946,9675923.095475191,9762451.831231993,9799605.459700245,9829580.282144018,9879229.830891894,9922198.184341485,9952183.3252594,9992390.590768423,10038364.28865385,10080970.689652536,10105393.801989146,10145587.517190212,10165275.48314132,10192223.761525208,10221475.68421031,10261590.193401136,10295420.76737559,10325912.688214248,10350438.90058873,10374739.775658246,10403846.825714266,10423078.80007789,10453897.799931094,10483646.639435636,10512742.265305204,10538173.726283768,10565141.64633168,10584838.765030436],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[null,7923983.50628949,6902103.641137818,6364848.970804239,6158835.10323499,6086777.95816006,5945764.472865516,5905828.299047867,5870500.427194945,5878264.888833789,5901451.499374355,5911305.178661582,5832373.74247788,5869974.408432857,5893591.957793698,5953918.707652712,5949699.017878646,5958546.179954594,5961053.069398928,5979850.014409316,6024461.922667034,6039959.790120093,6047395.096216698,6007156.355286284,5994562.329241707,5974112.289062631,5928259.375396437,5927241.79226756,5922423.73424589,5919137.52688194,5941830.6160849435,5954983.97370752,5953654.344084402,5970383.766966003,5960954.16149961,5990843.523542785,6005737.062178485,6027715.171259344,6039493.2454339145,6042258.699403809,6037294.939518754,6057597.211661975,6052266.400496457,6039798.322155991,6039741.8418418635,6047750.5299861645,6051703.533200704,6056721.279430954,6068688.2115997905,6081107.913946199],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle kNN en fonction de l'hyperparamètre n le nombre de voisins"},"xaxis":{"title":{"text":"n neighbors"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE kNN pour tout les paramètres de GridSearchCV\n","fig1 = go.Figure([\n","    go.Scatter(name='RMSE moyenne',\n","               x=n_neighbors,\n","               y=GridkNN.ScoresMean,\n","               mode='lines',\n","               marker=dict(color='red', size=2),\n","               showlegend=True),\n","    go.Scatter(name='SDup RMSE',\n","               x=n_neighbors,\n","               y=GridkNN.ScoresMean + GridkNN.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               showlegend=False),\n","    go.Scatter(name='SDdown RMSE',\n","               x=n_neighbors,\n","               y=GridkNN.ScoresMean - GridkNN.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               fillcolor='rgba(68, 68, 68, .3)',\n","               fill='tonexty',\n","               showlegend=False)\n","])\n","\n","fig2 = px.line(GridkNN,\n","               x=n_neighbors,\n","               y=[\n","                   'ScoresSplit0', 'ScoresSplit1', 'ScoresSplit2',\n","                   'ScoresSplit3', 'ScoresSplit4'\n","               ])\n","\n","fig3 = go.Figure(data=fig1.data + fig2.data)\n","fig3.update_xaxes(type='log', title='n neighbors')\n","fig3.update_yaxes(title='RMSE')\n","fig3.update_layout(\n","    title=\n","    \"RMSE du modèle kNN en fonction de l'hyperparamètre n le nombre de voisins\"\n",")\n","fig3.show()\n","if write_data is True:\n","    fig3.write_image('./Figures/graphRMSEkNN.pdf')\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                             paramètre RandomForestRegressor()\n","0  randomforestregressor__n_estimators                     100\n","1  randomforestregressor__max_features                    log2\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predRF=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[6530883.84,5048315.8725,4108595.8975,777847.641875,4040773.7525,2413992.830938,5847370.618,1559219.0896979994,4576889.96125,1297251.91875,2195678.1725,2359284.7875,1432863.73,2494064.95,1097754.35625,1101788.684375,851591.259375,1545541.7875,718158.554375,1148413.844689,1559154.086875,838767.2025,12562009.575,1942081.0275,1023204.391875,1617880.59625,2787202.4693799997,764814.915006,1242939.678125,9649209.6575,9226777.396719001,1944054.60875,2385185.514375,2099863.3339050002,34116737.03,5465961.5325,889003.1246880001,1236497.445,3956341.230625,2181528.3625,893800.7893799997,965186.896875,2547727.6025,6731100.395,5610015.285,172502043.78,1094633.2871875,8655933.555,4716530.9875,749220.7575,4110264.1675,1717141.280625,1851820.65878,19681844.08,2521937.76625,1166996.0003229994,1845952.9264063002,1935233.25625,2213644.3875,1619260.6093760002,595893.179064,2402890.87875,3891605.935,1079777.2506389995,4084596.19875,9329121.98375,4762030.695,3960997.6175,770060.63375,493078.321875,3237488.2075,12292264.5575,1359503.905,6365512.93,4703536.71125,3952840.83125,1078468.51375,7342811.29501,4976047.1575,28908192.25,1178518.211265,1548256.198765,1953890.155625,1890734.6368859997,2947576.927653,3520695.3425,64147101.6,972602.1025,1785953.250943,495872.1970290001,1360456.015,1516071.98125,595542.76,3802715.258438,1020422.6664073003,2298519.418125,1209188.674375,7276914.6,2099464.0825,1685983.704688,1392615.6628179997,8713503.06,6185923.1025,4871547.595,10146235.065,4487880.546340002,724775.3040630001,3540736.9425,625190.76563,7641749.725,6813188.16,3996053.49875,3239978.265,609652.0275,674176.543126,7353443.5625,4491809.98375,754897.02375,18738748.045,2420453.49875,472058.6862830005,2308663.53,1321242.44125,1218548.1962579996,11224604.16,733293.866875,1827950.095,1302172.98,875309.725625,2772900.85375,13494495.81,210822.22125400006,14023004.96,1306394.826875,1132770.68375,3844849.18625,764484.44,717555.1493829999,868683.711876,3284326.34,8621815.415,11775042.925,4026717.233765,5249911.63375,1971561.7670320002,1318468.5125,1419730.79,1830093.40125,2084074.95,13335568.595,8522099.54,1741655.862188,3660721.952188,1790853.1503130002,3940999.80881,852201.67625,736278.9759380001,10039007.825,7331036.395,1157115.321875,54375787.76,5123653.0575,2174894.7513392856,1394590.21125,1804813.155625,927968.848125,11425388.20625,794666.3253070003,3431239.4682424003,824081.56375,1972223.0675,728094.596876,1269954.08875,3688564.040625,3514934.38,3077425.56125,7189041.71,1398180.9653739994,687692.605625,775806.8770137,1395646.0356299998,945227.483125,1397311.4331399999,4487015.7175,1316219.92,5213553.55375,1726012.7375,136216678.72,2322521.68375,1329206.97875,2109366.59,1728313.55875,1946444.6275,698244.3825,3360337.4125,918244.7840630001,1715978.97,3467653.28375,7893375.43,955674.2825,7663774.53,933287.745,4865687.586875,1300291.1418849998,1234116.647505,2316666.027968,3020289.0825,6009142.9825,981247.260625,3309119.2725258335,1623332.05375,1031053.1406259999,3065432.9717974,1115747.7710953006,2236060.890313,2636114.96625,964026.75625,1041158.2546880001,1688009.18375,6260827.075,709110.6453130001,833287.0768779999,1292612.1681399997,2007297.548125,4072427.9925,6253109.215625,8813471.64375,31533703.03,2789712.91375,4982723.994844,8592531.9475,1520195.795625,700570.102502,743791.369376,2709726.89625,2871346.835,2481375.1375,12365053.82,24749910.26,27781365.67,2368945.25625,13422528.195,861840.923125,2230323.24,1276717.2769099998,1311584.30125,2504877.84375,6925492.5925,4545759.7225,1201599.83875,2406069.675,11179127.9825,936541.5665630001,2144415.210938,1750947.84125,3035767.495,1692040.826566,1753535.734375,5581086.26,2880103.69875,1401139.701875,6669286.328125,1009082.599375,1353411.456875,1574018.778438,8401316.8925,10880386.63,947211.041875,25647963.33,715647.0075050001,825169.8170943331,1588795.606255,941732.69875,3962116.195625,1736556.43125,2971538.169063,12359334.145,3527393.415625,10088783.905,3401989.965,999992.7631279998,778470.251876,3958084.715,5427371.105,518086.290625,1394590.21125,1049119.0721889997,1042678.699375,21341898.08,1435094.4625,29650427.93,1470099.446563,2618517.6426713,5284360.54,1997180.08938,3301094.9862510003,1324150.6595239996,2724865.1075,1562640.668787999,1859170.254375,11054040.245,937047.5293750002,2653883.5875,1217567.11625,2907945.09625,1260202.2678683335,1190996.58,2039902.85,1349989.3075,3656047.03375,3010738.6925,2216253.43125,15480702.56,2801752.37,1891969.995625,606794.00844,13916778.2425,1972101.02625,1008775.55625,2795027.4575,2677155.45625,1201735.3737589996,6911166.065,1430972.1275,11177239.005,2914326.3850049996,1947013.849961,1664436.3425,2123374.7625,6709837.85875,13805422.21,1127270.30875,2359785.613125,1141157.895702,1668102.21125,2554849.185625,797205.7565619999,3287622.722192,1117488.8696919999,1535139.380628,3650690.0575,2977407.54501,1345390.27125,2474199.42,10357503.8975,2564821.9135938003,12376623.3725,6606791.3225,8197945.7675,19377468.57,3935952.655,9509430.145,8280392.535,560117.4287710002,12815121.935,888515.949375,2286710.470625,14028394.3175,1101836.0378130001,715573.943438,1048962.003125,2110339.045,5800822.2,19144074.675,969008.8303906,706598.1934470001,8839429.4075,982317.30875,2918698.265,10583353.3725,2261541.450939,6892109.88,20069889.8,1077652.213125,954813.68375,1002200.31125,3582075.0975,9942469.205,2676080.369691,4103090.4,8943464.37,4643267.01,754885.20625,1742547.43,772086.615625,2509334.54,1406039.33438,3562722.465,3004279.625638001,1795877.29125,7184060.29,862745.429375,2492104.65125,1327949.50125,1300264.569375,3051381.27875,3565332.9925,23624742.05,1267113.355625,1538696.52125,1018898.099375,840195.371875,1190736.7184380002,1759666.00875,3161071.45625,1893829.5772499999,6500672.25,9857472.48,1623223.153128,797771.1481260001,1301578.028125,12303041.27,1376475.320833333,1035317.785625,1216224.93375,881170.609688,768002.2345833335,2389451.02125,2670116.34875,7574194.7575,1292735.835,2385304.778125,2138622.31375,2984001.6075,4201003.366714002,6589062.7750200005,1361328.21875,18157674.63,1711959.5675,1524446.445625,3348339.198125,3355886.14,943865.9309659988,5923933.98625,1700337.375945,1186164.8225,1113167.8540639998,2144211.070625,1121801.4763799997,1309540.65125,13788252.2,2720699.19125,988413.77375,1433228.2082422,944307.46,5748812.505,1848256.2362800003,61118481.3625,5571812.6625,2583702.12,1548259.29125,927755.4190630001,4710589.945,449408.235,3890729.7637650003,1819462.4215416664,2218523.9414078007,903721.2425,2841064.8625,13451756.086875,4485438.9975,409215.25358433335,53975279.76,1999557.42,809298.0075,7928119.56,10841630.7,1408263.81375,1059491.4093799996,4855723.27375,13050353.46,813775.106875,665645.391877,1030419.4716408001,1242049.8389894,768057.9500000002,2273293.785625,960721.371717,1024266.059692,1839696.670625,3601314.90125,9981949.25,28225484.81,11049837.1575,6415991.595,7664276.63,813099.956876,7843176.5225,1187023.7275,9039529.095,2268268.099375,2176256.90375,4912777.8006299995,1103858.6375,2508655.69375,1429657.3725,1469080.33875,4768956.91125,48785262.71,5554479.45625,5752849.6425,971336.88,776721.3246909999,16032709.945,15469272.05,1671958.961875,13878235.095,10293801.145,1182421.40438,43695374.78,693604.595625,2308920.6146899997,57959546.66,4781716.69125,1143134.9019149998,894903.180625,1483404.713125,1078447.4293809999,3415415.315,1572486.81125,1049105.02751,994722.79125,1305144.80875,853064.760001,1689154.21875,3233525.99,1735782.468125,782533.5825,10998886.1625,2310502.095,10957163.24,906015.166875,2305841.9246970005,3376496.6825,2863184.215,3360946.04125,611671.4297971667,819656.8282790002,9425186.76,946278.140625,862314.22,4605710.0275,3738708.44,1782831.005,742193.299381,1924148.080938,3558923.6275,4047993.02,1137546.5414844,870517.982501,5987739.8675,3808796.9925,3953673.39,5005456.45,6498618.595,1216347.638125,745768.510625,5264609.635,2479922.298441,769169.2393809999,3339327.875,1099397.7062549999,1137883.4915729996,2742493.626563,3503072.588125,6196916.455,2138197.253443,2098666.505625,3711679.981875,1742543.51375,2162370.234375,18927621.09,14129305.79,2161273.24625,1433484.014375,1259691.8804689,1273255.085625,1003933.400625,732881.5390630001,966891.5358333333,795000.719375,867246.62,4254944.0725,1580371.3025,1598663.3225,1417288.65375,954870.56375,1454331.030625,1261507.350625,915732.6662579998,1282224.42625,3230760.2375,10390825.65,966573.5500299999,3806747.190467,726476.687813,830215.057813,5623848.065,3892440.1325,1784886.23375,8390168.207813,141919082.41,2678862.1475,3768812.056875,5257959.3,2466130.74875,2368526.3875,3725583.9368799995,776764.085625,5588184.32625,5300924.88875,1249326.54,1320754.375625,1231268.336563,3780254.60625,1698249.525,2482969.35063,3934762.805,1805669.211875,2274083.16125,1802671.66875,8636855.59,545592.86875,6118627.93,2080272.345,4976410.418125,43962720.6,3051394.6275,600955.2665750002,1288105.800041,1575026.473438,1258940.9153209992,974568.9278126002,780741.876563,3336038.84875,5308020.8425,3069937.0225,1142978.05875,1121563.536875,964509.775625,3696220.1615,1898844.9125,1126636.5275,1465934.920625,7836524.39,2906775.4575,9821733.265,846634.952501,4284447.09125,1240273.555,3546910.255,1340650.453125,3665381.1,3873812.0275,1137596.84001,540593.8434570002,1093133.803282,25479281.55,7808370.7375,23831786.72,936629.360313,3270265.366876,1340828.673375,1293814.381875,4510098.8775,960892.4265749996,1359801.501563,32110825.095,529271.9528400003,4655513.5875,789391.8925,4392287.2125,949247.0745117,1806488.5840549998,1848219.19,928979.336564,1340921.228125,1322948.655,4866466.13375,8799864.9575,11737364.545,2316501.903125,1880324.1525,1005237.85125,1817835.1275,11042516.2025,1704526.0681449997,1208391.014063,1151993.1590626002,4279651.414688,847540.6165630001,837756.5282839999,1971565.96375,5010508.3725,588008.607814,55546088.52,2215492.8825,1227958.7,8696746.305,916806.201875,1273915.5275,1243243.2084419995,2906358.03375,1937518.91375,3747017.78,2453062.68813,16639101.06625,13177543.41,2707268.76,2318646.34,1522993.5775,3765204.2975,3230120.97125,1346951.6387510002,926345.898125,1022943.5221102008,15854020.87,18997201.28,1171076.03,22657390.28,96259837.18,12051178.67,32230474.89,738723.105625,1081134.6585000001,1248208.7409709995,2594412.61125,1055573.169063,3341883.85125,1628754.485938,2282348.128125,4163511.735,1417745.94375,26063109.04,1812244.62875,10402925.705,2046305.31375,934691.0134429997,909014.8803130001,2712710.33875,670296.08875,1012138.0065679998,12379397.055,3466268.165625,2257025.356875,13524701.2875,2669031.99125,5395788.48,1676919.129375,3825935.260938,1028122.6495833334,1543485.39875,767043.64625,840446.2075,508112.6200050001,1732479.27125,1554190.399375,3335494.830625,4302523.45875,2360544.052505,3006082.1375,1501913.149375,1310005.45875,1470996.446875,8768004.78125,1649747.76125,4752833.53125,4473006.565,3966946.9625,2886552.018125,1431705.79125,843427.461875,10239247.19,86086839.4,3329586.004375,748540.5970833334,729815.6050089998,1321407.9846879998,17123571.02,895444.9286766665,6125900.58125,31745918.255,47873973.36,3040354.521349999,2637497.1675,1993077.7865720005,2343133.21,932796.6018770001,935310.14469,1422000.275625,1853713.8143779999,1311002.9575,2961235.182813,3480868.325,1550259.419376,1265509.1725,1174248.696875,738019.35875,1851215.3825,2848289.96375,2399471.62125,1096398.1708333334,1850620.845,582153.7728265001,10355099.025,1001108.7023449999,1081878.81875,11630557.855,1303980.09625,744246.85875,49625588.6,993907.858125,877017.77625,940419.393751,791603.2003130001,1634243.575625,3280251.920625,22338023.98,2646193.34375,632388.09,1777666.353125,2400963.75625,8598861.4925,1276271.344375,1523833.630625,2788100.2,3376664.8150049997,3162346.9875,1361943.39875,840661.08375,1006993.1887539999,943377.21875,3401244.865,1076882.133125,1557385.30125,54437856.72,3627830.832515,13653098.88,2911791.3956299997,12413815.235,5459773.68628,22375732.28,6714470.49,9929607.155,5138957.465,1361328.21875,571299.1515680001,2235321.971875,1878980.761875,752541.8825,980636.4575,3383172.45875,3378464.93,1710908.48,988333.3446900002,1074703.08375,1360877.635,7587928.67,4286415.52,3452216.682188,742715.63375,2072937.896875,1029823.3606309998,2179690.8275,514411.19867000007,1899522.681875,2098316.065,1411459.0956339997,2281847.52125,3466254.7075,781524.2518839999,5942836.538125,7377699.96375,4687509.66125,909896.5,1597883.223125,828582.3434380001,14114719.96,651637.383125,3207163.175,836286.6725100001,1267460.8014844,4008133.63,782033.984375,13415578.01,8264014.52,1445058.33375,942310.2350039999,1335427.06625,698588.2071880001,5808212.713125,887339.966875,804987.780626,15671766.74,5582269.48,2274639.17875,9125207.725,758491.6703130001,1507996.2875,17397891.565,1928668.065,1420287.549375,3129823.31625,1341268.5120313,6391045.76314,5062363.26625,1037667.57625,1764321.88,1746595.1875,1460022.8125,26529771.4625,3258861.9575,4262725.780625,2112103.17625,884163.6422655999,1364487.783126,32481327.84,3132156.6275,2437347.525625,2896551.501875,3852070.4325,850500.6975,2562206.7881499995,2257857.1,2181950.07875,1111417.6996880001,1912790.2875,1297549.995625,7972489.89,985687.93875,2244385.3825,806209.415,14531325.01,1701826.808438,1002270.7637899999,2601636.3375,16680293.65,5545363.809375,5569739.69125,5591113.4375,5135645.9175,2823086.8425,3298164.165626,1474843.728439,2471088.3975,25443446.305,3407752.215,4912366.6025,10449746.94625,8140352.785,6579409.1575,1521603.45625,12245531.15,2985783.021875,1876386.4125,2003449.4125,17217263.775,18081441.54,4158522.9556299997,859691.4616420002,9875058.5075,3846580.54815,1617254.057188,1234049.935625,19059686.05,4983323.195,1325949.45625,1837451.678125,45952703.8,1071453.455,1127087.20625,12319503.26,3257489.5675,1812472.9,875991.791584,2150386.33375,15276042.02,1736496.895625,2374482.694063,5342496.743282,1026291.9087609996,1396918.559375,3925059.9450200005,2015583.484688,5135135.79125,839705.394375,1341397.05375,7668513.8075,8581180.415,1569006.53625,2959371.76,24371638.36,783411.085,765025.175625,2498947.115939,1152066.68,25304400.78,1312404.7953125,8978589.4975,6439335.48125,1634764.35125,2540910.37,15797621.07,7651275.01,1058298.8025,699534.58875,2062416.823125,4915016.7525,3253380.6225,4827993.215,1444421.3225,1737345.5525,1889211.301566,5912400.535,924674.510938,4582197.95875,19497667.09,5494393.815,1150498.094063,6920139.1225,1134728.199375,1016096.338125,2918070.6398750003,1956567.82,21357071.29,1456103.753125,803022.9569336,1618826.535313,883613.11875,565040.382188,4012523.738775,1389915.197009999,375382.3746749999,5263661.2675,5698647.032749999,2178333.555,6780000.131251,5057836.3475,2109292.476875,1718347.44,895160.215782,2296861.79875,1322739.4175,820698.123125,6320199.545,2800506.08375,7116701.045,4246887.0025,2458853.23125,848461.9779688001,4145037.89375,857684.788125,1589948.605625,1403143.2440883333,2610596.55375,9842913.305,1196720.9875,2296386.07313,1005031.248125,860089.87625,1704676.10625,2993446.045,2017942.6225029998,3760852.115625,1653764.3643780001,5224274.19,947072.259375,2168452.089375,676699.529376,803690.68,1229757.98125,2055070.9526172,17868530.775,6588006.305,1861288.1100049997,631912.70125,2256156.5137506,10388250.5525,1071487.555,1642671.4525,1196632.51625,2313056.48375,780129.64125,868192.916871,5676311.42,2238609.96,685933.30625,1535749.08375,2841065.005,1980074.7375,2504137.8475,5300923.2925,8231026.37,793101.25375,1229386.029375,1983879.529375,5434244.06625,2538715.45875,455725.370313,3047049.37875,696302.2581270001,1420282.7431269998,3591440.52625,2197591.44,4199913.3264999995,2263744.03125,2604676.680008,781995.2843760002,7988480.60625,1804303.1725,1474595.110625,7611755.3325,1022455.831875,2713789.5075,1267915.305,4339834.339375,615494.59625,1749782.805,5870271.363125,7408399.325,2448037.47625,3988877.8475,7210223.25,476295.2459490002,12444388.6425,1064227.13125,2259696.035,1329697.3375,782813.9370309999,20702339.905,12646054.11,779361.9265630001,1584927.360625,1406794.930938,1345390.27125,1160208.2496939995,4266721.10375,1088739.26625,884974.155625,1543481.1025,1647692.45,1341736.355625,1789015.20875,2739497.88751,32071483.045,1627883.64,1502250.69125,393200.332813,1181358.24,8283248.78,1256014.23875,3082078.24625,3489212.9543799995,2820680.62625,6236568.01375,19248816.64,3037721.095,3934603.31,2161763.684242,1715262.779375,2528232.995,546852.18375,1871793.6575,1932600.8625,10602207.36,2188244.57125,1256222.339063,946349.08375,4750784.04625,957805.35625,2294077.361875,1276009.35,2995103.00375,1093683.5367207997,4465086.41,2508874.941875,4010412.03125,1085232.503125,4878945.185,4874143.64375,2274304.715234,2534805.371405,1740746.37375,1103350.799375,966955.8521949998,3888735.4425,1765663.955,1215546.1514063,730946.5184400001,998230.690625,1826092.3775,2042026.32875,11602147.415,3545587.61875,723304.595625,1332799.669375,4589955.95,2311832.66375,998853.8912599998,3120493.7275,1334211.36,1103738.55625,2022629.341875,4756018.4525,910693.4856289999,16874479.925,5721238.558754999,2558202.46875,805643.86875,969070.010625,2368556.67875,6920648.82625,857196.5851396999,849194.86625,3498025.19,1273920.871255,1666030.75375,4866741.80875,3271817.8655000003,1669939.0562589997,4495105.1675,1045123.3625,2161252.70125,3702766.055,830315.3025,1080044.82,10256018.275,3560000.305,1084580.496875,1511986.175,1778108.97875,872647.1600029999,3049887.105,1830561.65376,913980.0946899999,2664585.91875,5591107.35,1990993.908125,4905432.00188,2535992.825,1339686.2154829989,4661126.495,1450326.865625,709500.346721,6530988.41,2531754.7025,733375.069063,2630647.345,491173.72281300003,1281287.03375,2127198.12125,2034316.69625,1647186.53375,5254275.59,3573012.463125,2851419.0894534006,2441458.24875,1623551.84875,8853507.95,15030241.6,3306479.36125,5891722.81375,9310527.295,3258344.4725,38284556.26,1253244.1475,1523090.104375,14044188.795,5587861.07875,797391.2578199997,1264677.1525,1378027.155,1655034.577734,1417348.95125,40518759.28,940378.186875,4383814.6825,3306591.64625,2780026.2725,1290997.733125,751995.884375,1556832.679375,3410180.025,1697094.7856299998,1660717.0225,1289750.828125,1030260.995625,2769835.27251,4644452.89,8504280.8875,3993659.235,7385609.6475,523000.4909480001,2215778.650626,1308672.341876,1807611.205,1675995.6425,1867955.97125,14075890.33,810348.2483333333,12930645.97,4799942.8626000015,3650647.76125,1147502.32375,4120988.695625,10023731.895,3401516.1925,2194214.2375,1851502.4225],"xaxis":"x","y":[7506587,1972078.625,7768294,706505.6875,2274873.5,2547582,4571388,288191,6252842,1126157.25,1153276,4103800.75,1963054,3292049,724259,819803.6875,1589550,967432.8125,984178,968026.3125,1688745,678663.3125,19236770,1359948,342493,497620,2682044,4420650.5,866343,1250669,1770027.5,14084606,1981115.25,1986094.875,32141070,5415438,932083.5,293217,3543985.5,2443065.25,992794.6875,885865.875,552729,3580091,2993610,284867168,1222509,5672838.5,0,965786,1429146.375,1648774.375,726124.625,30648154,1836671,876425,2192111.5,1426823,1748132,1791071.375,646377,1487660,1882845.75,893053.1875,4247457,1796889,6426022,2493240.25,650008,385719,1851260.5,75073888,804002,25453690,1997488,2712983.75,913899,9546165,5122039,25307744,1313366,761504.875,1302636,234144,5968153.5,2966309,58761304,1160989,2217620,142873,2051501,1069490.125,348061,545426.375,1113457.75,2665558.25,628023,4759256,0,2406996.75,501194,6508595,14829099,2491238.25,4135650.5,1023418,454296,1682454.875,813038,11651200,6797528,1145535.75,1348497.125,493000,766628,8204897,3960898.75,609101,17463744,3348280.5,863728.8125,1692323,2112095,1878742.875,6711975,675611.375,2196718,1199742,748775,1545356,19645206,122156,8393893,866825.375,1357116,2496807.5,615894,845044.125,526752.8125,2991674.75,3093944,25471308,7221482.5,3894584.5,2502081,630578,1569569,1435730,2512443,8971926,19248732,1678495.25,586589.1875,1832808.75,1132489.625,811907.625,702385,20361158,5647146,2323265,41655852,0,962971,1200811,1107290.625,1761137.5,14029480,248607,1037555.125,659450,3528086.75,820705,2433777,4549892,1358652,2558573,4287439.5,1239315.375,786979,730765,2781314,567072,967082,3228330,1243282,7606084.5,2103293,114648520,1977299,798970,4700552,1813864,1482554.375,759060.8125,4946798.5,678351,3289053,3326999,6935723,1386331,2740128,888031.875,1809101.5,581289,975431,4475650,1441445.625,8287440.5,659652.625,1538750.625,1306087.75,1058986,1060846,131810,3040978,1593187,791564,1085183,818317,3022275,520738,1224685.125,2114860.25,1088016,4520838,3626431.5,4448582.5,9212950,2392466,6689184,13586070,2074152,567608.625,594848.625,3009463.75,2446842,2161828,7794869,15206762,10084950,3906967,25631512,1022114.5,1182575,1218112,1249939.25,1788148,6102883,3093378,1493131.75,832168,7791957,839362,338923,2092251,1929828,1061458.25,451738,5820965,2284507.5,3740373.75,5417598,1632498.625,992139,1232822.875,3590731,9851937,2069880,27139746,514555,948676.5,665200.1875,468973.5938,13140574,227509.5,4185406,11509942,3185509,20630168,2928099,1391916,111969.7031,4842910,2278160,2094410,1138520,2470566,851651.1875,19397324,1244824,35383012,777587.125,898115.375,3471845,2190460,17686444,696605,2396523,651293.3125,1187261,5661689.5,517029,4673827,1255834,1970956,644415,1290284,2656729,2041912,4700396,29195468,1403869,42709624,1986384,1240875,509485.6875,6856153.5,369291.5,1464856,5420552,1140620,1424157.25,3795938,498745,4028943,873648,1234573.875,936336,2503690,2756214.75,6884914,766381,1384558.625,1260416,1214929,5448975,832986.875,659761.875,209644.9063,1269847.75,2526375,3467070,600318,2338852,11765682,1232210,13901935,7539963.5,4425832.5,24906132,3579008.75,11353672,7475577.5,608491,11289052,698367.6875,1525624,28731502,1656420.25,557082.1875,934357,1020039,5288304,9049598,899266.125,588969.8125,16016644,387810,6056481,3437441,1441665.375,8800292,8386493,497991,738990,2159932,2436403,5747645,1150582,1912690,8787353,3024478.5,863773,2274920.5,1187315,3158151,847298.8125,7613116,456397,1026921,5974993.5,666385,2559461.75,1170616,1967129,3928695.25,5464762,20781390,629027.3125,6129780,964236,929950.125,1815371.25,1777652,1242811.75,562479,8964061,7577723,1537570.25,403800,855946,10416934,1736775,684739.6875,786324,827042,892086,1735816,2404644,3568616,1046995.188,276107,243518.2031,2212058.5,6858533.5,318364.8125,639407.375,10126597,2227908.75,849504,8381744,4397572,1676426.5,3761647,1228837.25,646505.1875,717766,1195351.25,934621.1875,1152896.25,4103027.5,3392129,841891,1692199.25,980356,4374646,2332996.5,35891484,2845946,2674034,627854.125,2181833.5,5424718,298063,3560614.25,978463,2130673.75,1466382.25,3706100.25,33268410,5040382,508051,55813996,1075019.375,2100722,6227419,11356990,653139,204499.0938,5722325.5,10564729,548293,619545.125,1162120.375,901015.1875,1074937,1353337,807185.5,576405.125,1662723,4329966,7133222,27282662,8435093,8058913.5,3567977,609218,6915293,509124,3869754,363808.1875,2204235,4307238.5,1123416,2799595,1997115,1943772,3467698,448385312,2417401.25,5249190,664088,415975,16917964,13609040,3537841.5,13458948,9157150,2204161.25,47952868,847097.8125,4571746.5,65047284,3833162,1211947.375,1458617.125,3014332,705948.3125,2022990.625,1567040.75,352136,2168281,1160087.875,588965,628609,3481727.5,1677002.25,619804,13943008,1053290.75,1990444,565458,3494929.5,3155391,4089677.5,1998725.25,712057,935396.6875,8201974,782202,848497,4368113,1165598.875,2420605.5,592739.3125,2184912,3600897.25,4486477,980603.375,958798.3125,1886382,2503948,4751090,1651120,6335315.5,956481,397624,3063969.75,1841154.25,1295685.75,2586116,727703.5,1636649.125,1855556,719471.1875,2267094,1787119,1331122.125,971308,902694,5024837,17186624,55073120,1819329,512166,772379.3125,1049436.25,887403,416566.1875,673834,609257,759405.6875,3155496,1070085.375,2006626,2804534,747384.5,1057256,1054581.375,507428.9063,2664566.5,2065156,9697866,847514.5,6522551.5,720005,573624,12783255,0,2136086,15480681,253832464,2339234,4212149,3021669,3181045.5,2601369,1656025.625,646994,1565475,7802205.5,1187723,1356874.75,2298648,1136698,1586388.75,1018111,9085108,1734533,1703504.875,1868497.125,11047668,433443,5459657,1841968,9444484,38400928,151754.2969,536642.875,3717064,2159170.25,1625308.625,682632.3125,785144.8125,2553763.5,4750287,9617898,1025205,874707,618876.1875,3162976,1520326,2114053,1109924,7546235,3542542,10444602,462465.5,4023479,1761048,869835,1189942.75,2211671,1735538.25,789630.125,1962351.25,827937,28450436,6074249,9861998,967697,2107743.75,696054,1152890,3791402,965651.125,2090895.75,12480984,552015.625,2904225,466985,1268459,862744,313528.0938,1769998,1547116,2023858.75,863703,4656777,11061916,5233553.5,1168017.75,2300803,727775.8125,1443367.75,4608963,2342494.25,706879.5,574575.375,855670,666740.3125,1981360,1193790,8773058,520348.9063,17666646,1607590,1227772,14332778,410433.0938,926261.1875,2796425,1234813.25,2701833.75,4120963,4742869,37276920,18622292,2157144,1776411.75,666967,1080203,3031811,433871,908834,1076644.75,12448381,15843743,709979.6875,13239050,143423024,12196872,10992344,1291170,835545,1050982,1513700.625,1829122.375,2549649.75,1844825,3989399,1857329,1495686,32381836,1109851,6897247.5,1863784,675772.875,1018068,2378077,972608,1181428.75,5653656,7771933.5,1827424.125,6419090,1218114,16701055,771512.6875,162960,1213444.125,2777052,1185469,490599,541542.6875,1103762,811537,2932696,1848656,775007.6875,2172841,1788999,837792,784762,2737534.5,1695067,3060149,4178233.5,2924362,1271753.25,683274,712149,4020660,59757440,3298570,760090,505601,858897,15660628,323739.4063,5702077,9387211,37022180,1712279.875,1742351.25,1001964,1612797,638507.125,530700.125,2327611,1261634.5,1072487.875,1162561.125,6454983,1063960.125,151376,2534873.5,645756,1200381,2855545,2219744,688145,1362993,456197.5938,9523142,502898.6875,611997,13171370,1009476,739210.5,58041572,1459199.75,607579.6875,839011.3125,862826,1128179.125,2306723.5,8437401,2147062,646691,1619177.125,2236167,4058492,966764,1795462.75,2165371,4322873,2167888.25,1251906,669217.6875,471113.3125,664539,1983509,792015,794724.625,53079164,3629002.5,11924933,1375366,14641502,14469228,11813861,8878186,9273278,1985171,599591.625,489557.0938,3646053.75,695606,518107,699217,2499140,3187950.75,1543282,634056,1117984,687953.6875,7564604,4230514,1859725.375,569115,2564004,435985,1972491,116486.6016,1904641.75,1743198,347437.6875,2147013,5071974,713821.875,1869575,13659926,10721568,968343,6960396,886599,22055140,872555,31803568,757167.3125,1274049.75,4054582,601590,7969653,8259071,2427919.75,588856.125,1642471.5,668915.8125,2244317,656345,317794.4063,18558162,666392,3682374,7654264,623107.6875,3039889,18011944,1483856,807358,5762597,779503.625,10077532,5284343.5,522791.0938,1696446,811703.6875,1419453,53166156,2075891,3441643.25,3220827,1308348.5,127374,25210246,2828098.5,1437220,698177,1786986,571384,2152312.5,1910465,1204956.5,552007.5,1947717,456747,5903142,1259456,2436063,563454,15036617,2050600.75,1256456.625,2334260,18829136,4988493.5,2085349,5587347,2759071.5,2137686,841242,1587454.75,1591124.875,7996408.5,2655387.5,2303386,15324777,2584082,3850963,1037696,9765777,3314418,1929698,1987073,17750994,6914839.5,3841879.25,557863.6875,1951856.25,1555420.25,620104,604591,27119380,7202141.5,752441,2314835,39523852,1084654,769148,4545148,2773647.5,938908,335245,1400053.875,9722237,3035962.25,1817550,10005787,1581036.75,1773864.5,1242764,1992158.25,28229320,696709,849745.6875,6219841.5,5950438.5,731315,2033074,24882924,442375.8125,721723,1929851.5,842923,14776743,938096.1875,11224871,8387933,1645327,3371455,17871658,2290681,737570,492421.9063,2014475.875,10040121,8611054,6298131.5,1140137,1965529,1363296.875,6994740.5,632517,13253979,10614234,4345698,448600.5,4498367,652425,948157,938833.625,2759674,8233527.5,952879,830739.8125,786253.6875,919739,724153,5669431,1427974.625,521341.9063,3936546.75,5214621.5,1415355,8839812,7456330,2527198,1472548,2222097.5,2003759,1280245,1083467.125,2407904.5,12037792,10053489,3983784,1541897.875,2798967,6057734,507625.5,1038815.188,738983,360045,7753080,573395,1152368.25,680395.875,1929011,1612392,1297041,1354000.625,5381105,1100499.75,6136327.5,1357701,1724560,601897,591840,1148202.25,1618916.375,41092076,3548309,3641620.75,458746,1489363.875,4527521,756196,1314357,808829,2971013.5,647321.5,564988.8125,9436408,469499.8125,674961,405062,2375104,1361794.25,3518459.5,1971236,13320624,671079,662012,230365,5792926,1562708,529806,2964206,829440.625,161311.0938,2539565,2156407,3970754,3460275.5,2235765.5,1084553.75,3897738,1398253,1453543,3780078,790039.8125,1655612,1128208.125,7240274,488181,819314,7011508,4045980,1076693,4742636,5231373,151798,8751704,768755.125,1221806,577290,882759.1875,6310254,84980760,759508.6875,1417852,981009.125,639878,844076.8125,7231928,1256360,1054995,1415494,3577120,763693,1177583.375,1063480.5,59107620,1657201,885843.1875,731092,722951,18868768,437619,3091387.5,1878442.25,3118304,8765978,15725581,1529309,465533.3125,3275779.25,1287159.25,3986851,484875,1261963,1644969,7545909.5,1633399.375,3294645,687049,2775717,1098965,1688739,1641546,686019.1875,2852102.75,4079837,2077039.375,1136405,1372475,3000864,11566123,4519115,2295176.5,2185530,991283,574650.375,4039667,585559,776996.875,635516.375,730211.1875,1067793,1437569,8651835,19061780,1372834,1136676,2634166,2009324,964015,1237252.75,507277,814403,1594733.625,7008252,1302192.875,25377650,9097980,1808401.375,1000599,929628,2520310,5917000,658297.5,705262.125,1189402,1034827.313,522880.0938,6361236.5,1118392,1082920.125,2330029,393493,1697909.625,6015128,666909,524053,9686734,2471623,2225265.5,1785906,1245181,669787.8125,4863803,1243200.75,419978.6875,2531163,2805926,2166711,3660395,614768,2213115.5,10975751,427697,784329.5,12187231,4099807,843267,1074180,474359,801264,1955013.5,1365778,1615569,2892428,5826745,2890379,1247362,915417,8050977,18661606,4341706,1039782,8070420,3132453,37797084,679574.875,1022614.313,19018762,4841347.5,585077.125,550324.1875,6881405.5,4644346,1248599,32552428,788027,3529643,3996541.5,3226884,676499.875,559805,123767.2031,1835804.875,1451145.875,1040593,1567746.75,558709,2853189.5,2568377.75,4309177,1188444,9343428,657385.8125,2979413.5,1646088.5,1696009,1524620,2438430,23612824,818484,4222682,2001936.5,3739029.75,2245568.25,5928881,6268627,3027867.5,2277238.25,2017423],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle RandomForestRegressor() vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predRF"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["n_estimatorsRF = np.logspace(0, 3, 10, dtype=int)\n","param_gridRF = {\n","    'randomforestregressor__n_estimators': n_estimatorsRF,\n","    'randomforestregressor__max_features': ['auto', 'sqrt', 'log2'],\n","}\n","\n","GridRF, \\\n","BestParametresRF, \\\n","ScoresRF, \\\n","SiteEnergyUse_predRF, \\\n","figRF = reg_modelGrid(model=RandomForestRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBM_train,\n","                         X_test=BEBM_test,\n","                         y_train=SiteEnergyUse_train.ravel(),\n","                         y_test=SiteEnergyUse_test,\n","                         y_test_name='SiteEnergyUse_test',\n","                         y_pred_name='SiteEnergyUse_predRF',\n","                         score=score,\n","                         param_grid=param_gridRF)\n","\n","print(BestParametresRF)\n","ScoresRF\n","figRF.show()\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[13947717.215645004,11596206.567360543,12332685.066931788,11309464.984372765,10917264.945528619,10727107.73752583,10585204.616538875,10815739.242771195,10799598.124658538,10703050.886672135]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[20485815.37548103,18020324.85125068,19979201.0254938,18435218.610863656,18156238.82363715,17857019.912645884,17734659.674727686,18014371.079217784,17973716.364342254,17866112.44841038]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[3607884.8675581524,3405985.8857008554,1819656.508299171,1964517.988921428,3515936.8467401695,3375416.181286511,3069614.109995421,2687999.2070708983,2324702.7434724253,1927210.011586735]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[12018061.556434328,7747834.463758271,8478162.733228957,8021900.583880068,7584746.109450044,6671100.289178609,6898308.260108698,7276117.76608403,7080625.152909294,6993024.857031923],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[26031006.74918356,23794075.6159517,27252275.59254191,25217303.946342148,25114588.131100796,24652195.826121427,24572772.932633694,24918650.91210561,24792306.98107387,24695943.409566067],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[11751539.55598568,9489654.251635857,9221713.681853242,8660610.443697471,9101667.66002989,9297488.213969193,8684285.237407817,8927862.990920344,8952342.591063239,8897214.982208356],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[13678465.47343556,11542612.996403052,10961642.86888696,9653729.27598778,7990093.550532129,8262783.040543734,8313650.11111297,8278724.301545891,8683744.766083464,8438219.112107579],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[6259512.743185884,5406855.509053837,5749630.458147857,4993780.671956352,4795229.276530244,4751971.31781618,4457006.541431194,4677340.243200103,4488971.132162822,4490852.072446761],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle RF pour le paramètre max_features=log2 en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"n_estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE ElasticNet pour tout le meilleur paramètre max features\n","for i in BestParametresRF['RandomForestRegressor()'][\n","        BestParametresRF['paramètre'] ==\n","        'randomforestregressor__max_features']:\n","    fig1 = go.Figure([\n","        go.Scatter(\n","            name='RMSE moyenne',\n","            x=n_estimatorsRF,\n","            y=GridRF.ScoresMean.where(\n","                GridRF.randomforestregressor__max_features == i).dropna(),\n","            mode='lines',\n","            marker=dict(color='red', size=2),\n","            showlegend=True),\n","        go.Scatter(\n","            name='SDup RMSE',\n","            x=n_estimatorsRF,\n","            y=GridRF.ScoresMean.where(\n","                GridRF.randomforestregressor__max_features == i).dropna() +\n","            GridRF.ScoresSD.where(\n","                GridRF.randomforestregressor__max_features == i).dropna(),\n","            mode='lines',\n","            marker=dict(color=\"#444\"),\n","            line=dict(width=1),\n","            showlegend=False),\n","        go.Scatter(\n","            name='SDdown RMSE',\n","            x=n_estimatorsRF,\n","            y=GridEN.ScoresMean.where(\n","                GridRF.randomforestregressor__max_features == i).dropna() -\n","            GridRF.ScoresSD.where(\n","                GridRF.randomforestregressor__max_features == i).dropna(),\n","            mode='lines',\n","            marker=dict(color=\"#444\"),\n","            line=dict(width=1),\n","            fillcolor='rgba(68, 68, 68, .3)',\n","            fill='tonexty',\n","            showlegend=False)\n","    ])\n","\n","    fig2 = px.line(\n","        GridRF.where(GridRF.randomforestregressor__max_features == i).dropna(),\n","        x=n_estimatorsRF,\n","        y=[\n","            'ScoresSplit0', 'ScoresSplit1', 'ScoresSplit2', 'ScoresSplit3',\n","            'ScoresSplit4'\n","        ])\n","\n","    fig3 = go.Figure(data=fig1.data + fig2.data)\n","    fig3.update_xaxes(type='log', title='n_estimators')\n","    fig3.update_yaxes(title='RMSE')\n","    fig3.update_layout(\n","        title=\n","        \"RMSE du modèle RF pour le paramètre max_features={} en fonction de l'hyperparamètre alpha\"\n","        .format(i))\n","    fig3.show()\n","    if write_data is True:\n","        fig3.write_image('./Figures/graphRMSERF{}.pdf'.format(i))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 1.2 Consommation énergétique au log"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["SiteEnergyUse_train_log = np.log2(1+SiteEnergyUse_train)\n","SiteEnergyUse_test_log = np.log2(1+SiteEnergyUse_test)\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["r2 : 0.2985810392343702\n","rmse : 15549884.692595655\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_pred_logLR=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[21.06109619140625,20.63525390625,20.9930419921875,20.57513427734375,20.74005126953125,20.60626220703125,21.00543212890625,20.60479736328125,21.13787841796875,20.58734130859375,20.6981201171875,20.7398681640625,20.6075439453125,20.74127197265625,20.65179443359375,20.5728759765625,20.56622314453125,20.77215576171875,20.5545654296875,20.48541259765625,20.73126220703125,20.675048828125,21.80987548828125,20.873779296875,20.54052734375,20.70526123046875,20.95068359375,20.4891357421875,20.57421875,21.26361083984375,20.77227783203125,20.62115478515625,20.90106201171875,20.70843505859375,25.62762451171875,21.364013671875,20.51324462890625,20.5250244140625,20.41363525390625,20.75335693359375,20.43035888671875,20.52813720703125,20.71868896484375,20.9417724609375,21.26531982421875,23.52838134765625,20.6661376953125,21.16461181640625,21.0692138671875,20.583251953125,20.80419921875,20.5885009765625,20.673828125,22.5478515625,20.74658203125,20.5057373046875,20.3924560546875,20.6849365234375,20.7015380859375,20.46771240234375,20.5877685546875,20.8359375,20.829833984375,20.48492431640625,20.65252685546875,20.88677978515625,21.13580322265625,20.9957275390625,20.731201171875,20.5936279296875,20.67645263671875,21.96673583984375,20.561279296875,21.0382080078125,21.2203369140625,21.08319091796875,20.7564697265625,21.19366455078125,21.21942138671875,24.28302001953125,20.6954345703125,20.71636962890625,20.89581298828125,20.49639892578125,20.6842041015625,21.44964599609375,26.2254638671875,20.55755615234375,20.5347900390625,20.563232421875,20.64141845703125,20.703125,20.59088134765625,20.459228515625,20.62945556640625,20.785400390625,20.5245361328125,21.38372802734375,20.72955322265625,20.7564697265625,20.52374267578125,22.03826904296875,21.61676025390625,20.74560546875,19.37060546875,20.92938232421875,20.6036376953125,20.71173095703125,20.650146484375,21.26434326171875,21.49267578125,20.59271240234375,20.74932861328125,20.53521728515625,20.662109375,21.6287841796875,21.00885009765625,20.63177490234375,23.22662353515625,20.74688720703125,20.5384521484375,20.88153076171875,20.67730712890625,20.53515625,21.7225341796875,20.54693603515625,20.6505126953125,20.69415283203125,20.743896484375,20.6285400390625,21.97271728515625,20.5203857421875,23.239013671875,20.65997314453125,20.56298828125,21.04681396484375,20.614013671875,20.60546875,20.4669189453125,20.9771728515625,21.23480224609375,22.058837890625,20.86627197265625,21.01275634765625,20.5626220703125,20.57159423828125,20.72833251953125,20.90960693359375,20.8016357421875,23.48114013671875,21.2650146484375,20.70831298828125,20.61480712890625,20.51104736328125,20.590576171875,20.52117919921875,20.5106201171875,22.94122314453125,21.77398681640625,20.492431640625,27.46673583984375,20.95977783203125,20.74932861328125,20.6607666015625,20.5865478515625,20.435791015625,20.9427490234375,20.64404296875,20.59588623046875,20.64520263671875,20.17425537109375,20.52886962890625,20.61016845703125,20.93414306640625,20.5947265625,21.0067138671875,21.32275390625,20.63739013671875,20.6646728515625,20.5706787109375,20.7314453125,20.6112060546875,20.67181396484375,20.8916015625,20.7305908203125,20.7352294921875,20.83203125,31.1026611328125,20.90447998046875,20.57464599609375,20.712158203125,20.55322265625,20.67584228515625,20.51654052734375,21.04473876953125,20.75872802734375,20.6329345703125,20.72735595703125,21.46630859375,20.53204345703125,21.40863037109375,20.4619140625,20.96466064453125,20.4661865234375,20.77716064453125,20.5595703125,20.63519287109375,21.45550537109375,20.6041259765625,20.6444091796875,20.6842041015625,20.5362548828125,20.705322265625,20.683837890625,20.513427734375,20.8065185546875,20.59283447265625,20.73907470703125,20.48931884765625,20.76043701171875,20.63739013671875,20.50640869140625,20.61859130859375,20.50506591796875,21.07806396484375,20.80810546875,21.28570556640625,23.52825927734375,20.892333984375,19.71026611328125,21.38153076171875,20.6234130859375,20.51806640625,20.5042724609375,20.84674072265625,20.71685791015625,20.863525390625,22.2308349609375,22.19061279296875,21.29681396484375,20.7388916015625,21.19921875,20.6220703125,20.61578369140625,20.48919677734375,20.64154052734375,20.4874267578125,20.85394287109375,21.23883056640625,20.5001220703125,20.6239013671875,21.63104248046875,20.5164794921875,20.56915283203125,20.6551513671875,20.92303466796875,20.59417724609375,20.5333251953125,21.69659423828125,20.6495361328125,20.5306396484375,20.80657958984375,20.54132080078125,20.66900634765625,20.48297119140625,21.57843017578125,21.66290283203125,20.58685302734375,22.9637451171875,20.61004638671875,20.56024169921875,20.71136474609375,20.50909423828125,20.48809814453125,20.4749755859375,20.4964599609375,22.15704345703125,20.579345703125,23.0074462890625,20.9365234375,20.6446533203125,20.449951171875,21.44580078125,20.79132080078125,20.63055419921875,20.6607666015625,20.44219970703125,20.53204345703125,23.625244140625,20.5374755859375,22.89434814453125,20.50286865234375,20.6695556640625,21.15960693359375,20.77581787109375,20.5400390625,20.55206298828125,20.7723388671875,20.465087890625,20.7982177734375,21.77166748046875,20.56060791015625,20.7965087890625,20.6385498046875,21.07098388671875,20.61224365234375,20.601318359375,20.77978515625,20.56842041015625,20.5914306640625,20.68621826171875,20.75299072265625,22.86688232421875,20.68304443359375,20.65576171875,20.5694580078125,22.1728515625,20.4776611328125,20.6033935546875,20.5802001953125,20.567626953125,20.52337646484375,21.73577880859375,20.5989990234375,21.5198974609375,20.7254638671875,20.599853515625,20.76708984375,20.75787353515625,20.61590576171875,22.1494140625,20.613525390625,20.6575927734375,20.6904296875,20.82147216796875,20.474609375,20.58612060546875,20.43353271484375,20.47637939453125,20.51214599609375,20.91741943359375,20.65667724609375,20.5758056640625,20.779052734375,21.54693603515625,20.5184326171875,21.75335693359375,21.4073486328125,21.2587890625,23.06683349609375,20.8465576171875,22.5364990234375,20.66485595703125,20.4979248046875,21.99945068359375,20.58258056640625,20.6041259765625,21.56561279296875,20.55010986328125,20.52337646484375,20.658935546875,20.58941650390625,21.61053466796875,22.23779296875,20.542236328125,20.4700927734375,21.5018310546875,20.40716552734375,20.8099365234375,21.98077392578125,20.52227783203125,21.2799072265625,22.70281982421875,20.5235595703125,20.63525390625,20.55596923828125,20.93109130859375,21.9530029296875,20.5634765625,20.99359130859375,21.83795166015625,20.97210693359375,20.69805908203125,20.729736328125,20.59619140625,20.57354736328125,20.62841796875,20.720703125,20.56396484375,20.7337646484375,21.671630859375,20.64959716796875,20.51287841796875,20.6640625,20.67449951171875,20.64886474609375,20.947265625,23.2296142578125,20.5478515625,20.4693603515625,20.52978515625,20.51806640625,20.58428955078125,20.797607421875,20.8564453125,20.76727294921875,21.267333984375,21.26141357421875,20.652587890625,20.5040283203125,19.93023681640625,20.9010009765625,20.5816650390625,20.5372314453125,20.566162109375,20.527587890625,20.582763671875,20.99609375,20.8326416015625,21.51678466796875,20.6295166015625,20.70947265625,20.4664306640625,20.783203125,20.268310546875,20.87640380859375,20.5196533203125,22.3035888671875,20.7073974609375,20.74261474609375,20.71661376953125,20.7701416015625,20.46722412109375,20.986328125,20.6466064453125,20.55810546875,20.6141357421875,20.77764892578125,20.72833251953125,20.4169921875,20.0198974609375,20.780517578125,20.64776611328125,20.697998046875,20.61669921875,21.1737060546875,20.46044921875,22.19366455078125,21.18048095703125,20.8577880859375,20.48577880859375,20.52728271484375,21.501953125,20.53314208984375,20.60321044921875,20.6888427734375,20.5574951171875,20.5228271484375,20.998779296875,20.6737060546875,20.93231201171875,20.56103515625,31.531982421875,20.523193359375,20.61907958984375,21.2950439453125,21.7080078125,20.64227294921875,20.468994140625,20.97943115234375,22.8934326171875,20.6190185546875,20.532470703125,20.620361328125,20.68536376953125,20.52783203125,20.8604736328125,20.590087890625,20.47381591796875,20.64398193359375,20.75054931640625,22.09466552734375,25.100341796875,22.958740234375,21.00592041015625,21.4561767578125,20.57720947265625,21.4891357421875,20.46075439453125,21.7010498046875,20.50762939453125,20.7242431640625,20.8709716796875,20.56787109375,20.6583251953125,20.560302734375,20.52490234375,21.10601806640625,22.9486083984375,21.16796875,21.3935546875,20.67669677734375,20.58294677734375,22.86126708984375,21.714599609375,20.49603271484375,23.08056640625,21.5614013671875,20.5643310546875,28.56939697265625,20.53778076171875,20.72491455078125,24.76495361328125,20.9031982421875,20.7193603515625,20.5107421875,20.76080322265625,20.5579833984375,21.0196533203125,20.6414794921875,20.59326171875,20.69305419921875,20.53466796875,20.7279052734375,20.54144287109375,20.5234375,20.61566162109375,20.62677001953125,20.9974365234375,20.6444091796875,21.3695068359375,20.65631103515625,20.506591796875,21.14215087890625,20.90191650390625,20.51739501953125,20.54296875,20.59002685546875,22.6209716796875,20.69366455078125,20.62591552734375,21.09942626953125,20.89422607421875,20.7440185546875,20.68182373046875,20.66644287109375,20.97174072265625,20.944091796875,20.55206298828125,20.54583740234375,21.53167724609375,21.13970947265625,20.825927734375,21.3216552734375,20.48876953125,20.67071533203125,20.531494140625,21.13189697265625,20.5482177734375,20.4495849609375,20.86273193359375,20.59161376953125,20.46826171875,20.56549072265625,20.39678955078125,21.25872802734375,20.49237060546875,20.48870849609375,20.63775634765625,20.6605224609375,20.6513671875,22.170654296875,23.05322265625,20.84808349609375,20.60528564453125,20.6326904296875,20.61517333984375,20.670654296875,20.54974365234375,20.675537109375,20.64007568359375,20.57244873046875,20.82012939453125,20.67523193359375,20.65728759765625,20.6239013671875,20.5718994140625,20.6424560546875,20.6195068359375,20.55731201171875,20.62030029296875,20.5997314453125,22.70556640625,20.593994140625,20.64471435546875,20.58331298828125,20.592529296875,21.1959228515625,20.6961669921875,20.78564453125,20.598876953125,23.950927734375,20.4952392578125,21.02862548828125,21.241943359375,20.65203857421875,20.71221923828125,20.776123046875,20.6099853515625,20.9542236328125,20.78125,20.53564453125,20.52349853515625,20.56072998046875,20.94329833984375,20.650146484375,20.6650390625,20.52197265625,20.7191162109375,20.9351806640625,20.63134765625,22.4361572265625,20.5986328125,21.841064453125,20.76458740234375,21.10406494140625,26.30255126953125,20.68572998046875,20.49884033203125,20.49139404296875,20.496337890625,20.4796142578125,20.5521240234375,20.56427001953125,20.5892333984375,21.32391357421875,20.52972412109375,20.5126953125,20.5689697265625,20.64794921875,21.09539794921875,20.72943115234375,20.61712646484375,20.529541015625,22.055908203125,20.9505615234375,21.3677978515625,20.5028076171875,20.98663330078125,20.51812744140625,20.4881591796875,20.63824462890625,20.87890625,20.85107421875,20.721435546875,20.57489013671875,20.58416748046875,23.4283447265625,21.55841064453125,22.987060546875,20.61370849609375,20.4736328125,20.61700439453125,20.50421142578125,21.0765380859375,20.46868896484375,20.68389892578125,23.88067626953125,20.50537109375,21.49658203125,20.57379150390625,20.55401611328125,20.6051025390625,20.493408203125,20.64215087890625,20.59600830078125,20.65673828125,20.44451904296875,21.1068115234375,21.63653564453125,22.69140625,20.67718505859375,20.63018798828125,20.53948974609375,20.74169921875,21.51458740234375,20.69317626953125,20.505615234375,20.583984375,20.53082275390625,20.557373046875,20.44281005859375,20.7530517578125,20.97186279296875,20.53631591796875,24.7080078125,21.07666015625,20.6068115234375,21.22515869140625,20.474853515625,20.649658203125,20.4697265625,20.772216796875,20.56695556640625,21.12823486328125,20.69921875,21.13555908203125,21.1728515625,20.97113037109375,20.71734619140625,20.61260986328125,20.80914306640625,21.01141357421875,20.4449462890625,20.64404296875,20.5521240234375,22.77789306640625,22.31414794921875,20.54412841796875,24.164306640625,22.4014892578125,22.46405029296875,23.81817626953125,20.5052490234375,20.77105712890625,20.63916015625,20.95947265625,20.56390380859375,20.97955322265625,20.5904541015625,20.58984375,20.82269287109375,20.719970703125,23.73052978515625,20.6329345703125,21.3779296875,20.74908447265625,20.43280029296875,20.56890869140625,20.73785400390625,20.59393310546875,20.56365966796875,22.74609375,20.910888671875,20.68487548828125,21.4365234375,20.7989501953125,21.28179931640625,20.46044921875,20.459228515625,20.59716796875,20.74932861328125,20.60736083984375,20.5885009765625,20.53839111328125,20.83660888671875,20.709228515625,20.67584228515625,20.9971923828125,20.705078125,20.87060546875,20.70166015625,20.62847900390625,20.64593505859375,21.9888916015625,20.78033447265625,21.1556396484375,20.9400634765625,20.98944091796875,20.809326171875,20.43377685546875,20.585693359375,21.5526123046875,29.7674560546875,20.797607421875,20.582763671875,20.49884033203125,20.54913330078125,23.7135009765625,20.50537109375,21.1446533203125,22.0731201171875,22.552490234375,20.94134521484375,20.80328369140625,20.46429443359375,20.69769287109375,20.4678955078125,20.5374755859375,20.6561279296875,20.4705810546875,20.72784423828125,19.88861083984375,20.6134033203125,20.4559326171875,20.58111572265625,20.5281982421875,20.60260009765625,20.67926025390625,20.70867919921875,20.94171142578125,20.56207275390625,20.74261474609375,20.543701171875,22.0504150390625,20.48388671875,20.6126708984375,21.69403076171875,20.50848388671875,20.51385498046875,30.71539306640625,20.54150390625,20.50994873046875,20.62646484375,20.7291259765625,20.52001953125,20.9833984375,22.23406982421875,20.79046630859375,20.62115478515625,20.5675048828125,20.81048583984375,21.532958984375,20.66143798828125,20.70489501953125,20.80914306640625,20.821533203125,21.06390380859375,20.6397705078125,20.675048828125,20.49603271484375,20.5078125,20.85992431640625,20.76556396484375,20.4688720703125,30.21612548828125,20.80560302734375,23.3104248046875,20.78375244140625,21.8681640625,20.903564453125,22.52880859375,21.75689697265625,21.7628173828125,21.07379150390625,20.5196533203125,20.5743408203125,20.5086669921875,20.82635498046875,20.7056884765625,20.638671875,20.7891845703125,21.13409423828125,20.7967529296875,20.49786376953125,20.676025390625,20.63232421875,21.44622802734375,21.227294921875,20.87847900390625,20.58984375,20.5604248046875,20.62750244140625,20.9031982421875,20.4637451171875,20.660400390625,20.687255859375,20.431640625,20.5521240234375,20.853515625,20.56292724609375,21.01934814453125,20.74285888671875,20.60345458984375,20.66552734375,20.71954345703125,20.685302734375,21.62176513671875,20.53948974609375,20.63031005859375,20.606201171875,20.54791259765625,20.96148681640625,20.57904052734375,21.8865966796875,21.634765625,20.57574462890625,20.474365234375,20.5355224609375,20.5784912109375,20.7867431640625,20.521484375,20.4390869140625,21.46612548828125,21.22918701171875,21.0155029296875,21.15240478515625,20.5740966796875,20.53515625,23.23382568359375,20.79949951171875,20.75335693359375,20.68133544921875,20.63818359375,20.96063232421875,21.193115234375,20.54669189453125,20.6915283203125,20.7958984375,20.82666015625,21.454833984375,20.56292724609375,20.6185302734375,20.80914306640625,20.54473876953125,20.44598388671875,24.70587158203125,20.8961181640625,20.796630859375,20.49017333984375,20.88323974609375,20.6314697265625,20.81500244140625,20.67364501953125,20.68292236328125,20.6541748046875,20.74090576171875,20.520263671875,21.17303466796875,20.5838623046875,20.27557373046875,20.57952880859375,21.764892578125,20.55712890625,20.61859130859375,20.7720947265625,22.16015625,21.26922607421875,20.67962646484375,20.9195556640625,21.23309326171875,20.68060302734375,20.5623779296875,20.2806396484375,20.8524169921875,21.789794921875,20.89013671875,21.065185546875,21.15692138671875,21.268798828125,21.2093505859375,20.75323486328125,23.3184814453125,20.64349365234375,20.73248291015625,21.0003662109375,23.135009765625,21.7664794921875,20.93096923828125,20.42950439453125,20.88092041015625,20.8863525390625,20.61968994140625,20.491455078125,21.49658203125,21.0753173828125,20.523681640625,20.72406005859375,26.24005126953125,20.56201171875,20.74298095703125,21.926025390625,20.811767578125,20.63470458984375,20.4549560546875,20.77105712890625,22.16851806640625,20.6005859375,20.527099609375,20.451171875,20.46783447265625,20.65545654296875,20.6466064453125,20.47607421875,20.99700927734375,20.5870361328125,20.3912353515625,21.459716796875,21.70025634765625,20.59765625,20.78521728515625,23.031005859375,20.51318359375,20.610595703125,20.57891845703125,20.56927490234375,24.5426025390625,20.5762939453125,21.90460205078125,21.241455078125,20.8172607421875,20.83380126953125,23.5977783203125,21.27362060546875,20.6884765625,20.5,20.5,21.0999755859375,20.634765625,21.08111572265625,20.58203125,20.63995361328125,20.72467041015625,21.2606201171875,20.62103271484375,21.1575927734375,22.4559326171875,20.59686279296875,20.5,21.50439453125,20.587158203125,20.71722412109375,20.64453125,20.71881103515625,22.10687255859375,20.61737060546875,20.53558349609375,20.5313720703125,20.60150146484375,20.6064453125,20.81805419921875,20.62060546875,20.508544921875,21.27069091796875,20.9112548828125,20.671630859375,20.49420166015625,21.14752197265625,20.7142333984375,20.6776123046875,20.03173828125,20.8436279296875,20.72857666015625,20.5458984375,21.93353271484375,20.6712646484375,21.31793212890625,21.2283935546875,20.73291015625,20.6082763671875,20.99395751953125,20.5328369140625,20.61639404296875,20.56085205078125,20.56353759765625,21.33746337890625,20.6121826171875,20.821044921875,20.59393310546875,20.49945068359375,20.799072265625,20.865478515625,20.474365234375,20.7664794921875,20.497314453125,21.22601318359375,20.67449951171875,20.81048583984375,20.6563720703125,20.677490234375,20.57330322265625,20.5477294921875,21.49285888671875,21.6888427734375,20.7606201171875,20.63812255859375,20.56573486328125,21.96197509765625,20.6580810546875,20.71295166015625,20.7220458984375,20.9007568359375,20.55596923828125,20.5557861328125,21.2296142578125,20.50732421875,20.60687255859375,20.5455322265625,21.0211181640625,20.49249267578125,20.74462890625,21.003662109375,20.68853759765625,20.63739013671875,20.5860595703125,20.5306396484375,21.00933837890625,20.763916015625,20.49786376953125,20.9517822265625,20.5533447265625,20.5115966796875,20.802978515625,20.5487060546875,20.98468017578125,20.72637939453125,20.55084228515625,20.4818115234375,21.037841796875,20.8802490234375,20.548828125,21.48089599609375,20.5457763671875,20.69366455078125,20.6048583984375,20.64569091796875,20.56817626953125,20.6522216796875,20.88348388671875,21.094482421875,20.760986328125,20.9947509765625,21.1644287109375,20.55462646484375,21.20672607421875,20.51849365234375,20.58258056640625,20.6390380859375,20.58892822265625,22.0499267578125,22.02142333984375,20.52764892578125,20.585693359375,20.50543212890625,20.5758056640625,20.4364013671875,20.969970703125,20.632568359375,20.65435791015625,20.54852294921875,20.56732177734375,20.67474365234375,20.695556640625,20.56475830078125,22.5653076171875,20.630615234375,20.62225341796875,20.49578857421875,20.52252197265625,21.34039306640625,20.5015869140625,19.79248046875,20.9508056640625,20.6029052734375,21.0439453125,22.5103759765625,20.532470703125,21.17816162109375,20.52764892578125,20.51068115234375,20.65252685546875,20.5599365234375,20.5272216796875,20.780029296875,21.8447265625,20.6951904296875,20.5020751953125,20.6494140625,21.28778076171875,20.5400390625,20.566650390625,20.53839111328125,20.831787109375,20.44830322265625,21.1710205078125,20.625244140625,20.54437255859375,20.55889892578125,21.30963134765625,20.728271484375,20.44140625,20.68414306640625,20.6865234375,20.5235595703125,20.5533447265625,20.7392578125,20.8011474609375,20.56182861328125,20.58465576171875,20.54571533203125,20.52093505859375,20.6097412109375,22.0390625,20.595703125,20.5556640625,20.524658203125,21.26763916015625,20.74737548828125,20.66070556640625,20.67535400390625,20.61376953125,20.634765625,20.52880859375,20.6888427734375,20.4742431640625,20.902587890625,20.75433349609375,20.79241943359375,20.55999755859375,20.6875,20.773193359375,21.13836669921875,20.56951904296875,20.520751953125,20.95086669921875,20.57177734375,20.62432861328125,20.92803955078125,21.0159912109375,20.5242919921875,20.93963623046875,20.56744384765625,20.82586669921875,20.5369873046875,20.577392578125,20.5689697265625,21.7080078125,20.888427734375,20.513671875,20.529052734375,20.71240234375,20.6566162109375,20.95855712890625,20.7337646484375,20.50653076171875,20.82562255859375,21.44500732421875,20.71209716796875,21.02227783203125,20.7843017578125,20.49945068359375,20.5858154296875,20.5,20.57159423828125,20.8406982421875,20.73004150390625,20.5146484375,20.7152099609375,20.56097412109375,20.628173828125,20.706298828125,20.7816162109375,20.7474365234375,21.58648681640625,20.78204345703125,20.56744384765625,20.64739990234375,20.8018798828125,21.5904541015625,21.9320068359375,20.9232177734375,20.86663818359375,21.74407958984375,21.04681396484375,24.35797119140625,20.6488037109375,20.504150390625,21.716064453125,21.5030517578125,20.58343505859375,20.45770263671875,20.66363525390625,20.53857421875,20.6541748046875,25.9658203125,20.6827392578125,21.08209228515625,20.8768310546875,20.90924072265625,20.52178955078125,20.6077880859375,20.32305908203125,20.9586181640625,20.6942138671875,20.76177978515625,20.50531005859375,20.61431884765625,20.86126708984375,20.88397216796875,21.3876953125,20.73126220703125,21.4012451171875,20.46502685546875,20.79681396484375,20.50726318359375,20.6036376953125,20.75433349609375,20.61639404296875,20.8851318359375,20.56500244140625,21.980712890625,20.961181640625,20.961181640625,20.5714111328125,20.98822021484375,21.61309814453125,20.455810546875,20.79241943359375,20.77008056640625],"xaxis":"x","y":[22.839725872000276,20.91128637269999,22.88916655727953,19.430343691015025,21.11735552640613,21.280697719341468,22.124201158883572,18.136670762341843,22.576080863006137,20.10297814136792,20.137307637847517,21.968529606994224,20.90466916348112,21.65055481705341,19.466148173643653,19.644920713511322,20.60018787473792,19.883803436319816,19.908561207609715,19.88468822756935,20.68752092168076,19.37233862557095,24.197363319180216,20.375121118721335,18.385719188798834,18.92468784406244,21.354902012656122,22.075827573908075,19.724580465382388,20.25426974158722,20.75534115928083,23.747615973251953,20.91788210799568,20.92150383722997,24.937914667295647,22.368646863289158,19.83010122567355,18.16161414446258,21.756941677613227,21.220261555831726,19.921137323148557,19.756730386301125,19.076215392899233,21.77156523118196,21.51345533419679,28.08571411944019,20.221414835639845,22.43563961615873,0,19.8813455186455,20.446723265477033,20.65296343235214,19.469859640957857,24.8692968913913,20.808662576707462,19.741272758302454,21.063890409149668,20.444375957405665,20.737383520261705,20.772392205332533,19.30201857006515,20.50461437981694,20.844484149195353,19.768388190282472,22.01816825105002,20.77707066358558,22.615494713832,21.249591056381075,19.3101001682171,18.557194427644223,20.820077266859332,26.161807884063222,19.616841359021425,24.601371538637977,20.929756127701882,21.371449507411626,19.801676786988352,23.186489993216338,22.288287088869534,24.593075628702742,20.324838680602316,19.538495641550785,20.313003679890702,17.83704270507587,22.508853453023672,21.500237946648834,25.808363100543477,20.146924115160264,21.08058139373211,17.12438387523814,20.968249130032163,20.028493079904162,18.408984789492116,19.05702758537138,20.086616681313576,21.346006819966092,19.260460167342536,22.182304931495295,0,21.19880336254275,18.935012496616167,22.63391493571178,23.82192770536634,21.24843214899758,21.979683188147945,19.96496549198579,18.7932762528682,20.682137237753555,19.632965031850407,23.473975339042966,22.69657897049395,20.12759231185353,20.362922084924325,18.91123104739614,19.548169047796804,22.968053968168725,21.917396756553043,19.216324315987517,24.057859634036678,21.674989393981168,19.72022056098374,20.690574372087426,21.01024397957751,20.841336970061143,22.67830612646918,19.365836228253468,21.066918904158555,20.194293964879012,19.514174668391416,20.55950872954696,24.227674033652324,16.89837701160671,23.000908813982363,19.725383525541226,20.372113673311205,21.25165374313537,19.232324890468607,19.688668857104407,19.006769325349232,21.51252238785962,21.56101612004373,24.60236976749392,22.783863807855585,21.893038360020444,21.254697640624393,19.266317599579637,20.581937941283943,20.45335403901458,21.260660009666424,23.096986451321428,24.198260151329528,20.67873788317677,19.161993412283312,20.805625607806174,20.111067676706508,19.630957845101975,19.421904563712328,24.27931634919893,22.429090755701058,21.147722912842873,25.31201588491192,0,19.877134324376346,20.195578868662224,20.07860380095908,20.74807693982553,23.74195830377693,17.92351319630841,19.98475794826,19.330905939549268,21.75045501313216,19.64650597499701,21.21476614620658,22.11740118712243,20.373745608135653,21.286908529421844,22.031685219396664,20.241113097705927,19.58596744655146,19.47904998638186,21.40733571681439,19.113174941454258,19.88328018890598,21.622357074698563,20.24572329389297,22.858722725717435,21.004219094475406,26.772642502942784,20.915100345640315,19.607783613490604,22.164399063013658,20.790635654144218,20.499654561276497,19.533857847503334,22.238063995762765,19.371674563247215,21.64924126399572,21.665790435544952,22.72561505846151,20.402841366046307,21.38581238341487,19.76025356060854,20.786842719711426,19.14889856436821,19.895681776607635,22.093666112296297,20.459085985918723,22.982495350783758,19.331349158501606,20.55332894959407,20.31682150206961,20.014253448408297,20.016785168783276,17.008111246780107,21.536104422950213,20.603485087703003,19.59434829773337,20.049508251055094,19.642302060721956,21.52720398550173,18.99020093378567,20.22398061738597,21.01213158467734,20.053269667846333,22.108159109401438,21.79011956511657,22.084914601393155,23.135231909261865,21.190067594656295,22.67339901515593,23.695624962968736,20.984090887872558,19.114539527933296,19.182165482881256,21.52107548780855,21.22249010417508,21.043820980181792,22.894093531705956,23.85820974949918,23.265700738521485,21.89761800850832,24.611415306054898,19.96312680068985,20.173501472804098,20.216216542451658,20.253427701740367,20.770035525423065,22.54105973810428,21.560752172206048,20.50991100623185,19.66651702047476,22.893554470389834,19.678935343554116,18.37060227548128,20.996625195887564,20.88004158687887,20.017617555780475,18.785129945834015,22.472827160217914,21.123452379734417,21.834751391009725,22.36922218137253,20.638651229695757,19.920184186837908,20.233535275947663,21.775846548503456,23.23197611805285,20.98111639712373,24.69390393605354,18.972968571042593,19.85555820528158,19.343431218526263,18.83915024282446,23.64752507012282,17.79557360413656,21.996936490386865,23.456377353127298,21.603092935704957,24.298252303878165,21.481533394476365,20.408641755181446,16.772761777957808,22.20744305955703,21.11943827672747,20.998113148879906,20.11872947152901,21.236410750491928,19.699904832330084,24.209354374596852,20.24751150890443,25.07655356851684,19.568646661314627,19.776542871248843,21.727271525181536,21.062803097791257,24.076140757673382,19.409983373621806,21.192511957229144,19.312950102671177,20.179206907329647,22.432801454859,18.979888467927786,22.15617321227827,20.260215495083163,20.910464871041206,19.297632789142515,20.29925833471969,21.341220185389716,20.961489967791394,22.164351182655523,24.799241151178187,20.420977915907716,25.348057895064095,20.921713841616427,20.242927524338644,18.95868492479523,22.708968189549438,18.49440443743341,20.482328404305942,22.37000861087754,20.121388068881203,20.44167803431194,21.856025378041313,18.927945745238855,21.9419703223203,19.736694248820182,20.23558290561487,19.836668342627448,21.25562508835354,21.394257392515996,22.71500741252458,19.547704151356815,20.400995752923983,20.265469687416747,20.212441762527977,22.377553705906216,19.66793597026211,19.3315880738609,17.677595133772794,20.276225239062065,21.268637940423815,21.725285950225775,19.195369804190715,21.15736975857522,23.488081735909503,20.232817889146876,23.728782472563765,22.84612630027242,22.077517747249683,24.569997706071334,21.7711290432985,23.43665576011153,22.83375379636568,19.2148787700197,23.42842113265138,19.4136293492673,20.540968959330602,24.77613012919926,20.659638185399235,19.087533251252736,19.83361590003151,19.96019429668775,22.334373955189864,23.109422435163253,19.778390202711652,19.167836614935887,23.933068642871486,18.56499419954315,22.530048594882032,21.71290394044351,20.45930590947763,23.069120127461606,22.999636382856856,18.925763040720064,19.49519726866692,21.042555130756018,21.216321947424902,22.454539777644875,20.13393362882646,20.867172390851817,23.066997383640125,21.52825545142037,19.72029436584214,21.117385332870825,20.179272523647345,21.590649178257912,19.692513024244924,22.86005581934068,18.799932943985397,19.96989517504386,22.510505954413144,19.34599856610255,21.28740957905986,20.158837704444057,20.907660872216642,21.905619198419114,22.38172749868176,24.308788888311135,19.262765428559003,22.547404100685924,19.87902826477287,19.826795369611226,20.791833978792408,20.76154230516995,20.245177516562332,19.101442274030862,23.095721196617728,22.853333163501407,20.55222183482644,18.623284957897813,19.707161942553423,23.312427516610697,20.72798026434645,19.38519821479218,19.584766197834426,19.657602815047426,19.766824889008415,20.72718342775812,21.19739249261565,21.766933642509713,19.997824758905686,18.07487316385907,17.89367601727741,21.076958761045283,22.70946891023392,18.280325895793162,19.286378115164315,23.27164625070411,21.087259361308853,19.696262913257698,22.998819199975294,22.068276096052415,20.676958661968467,21.84293342285994,20.22886359790674,19.302304652102425,19.45315606988007,20.189004387901257,19.834023760961447,20.136832510309578,21.96825774467718,21.693760030133728,19.68327564708964,20.69046887217181,19.902947681155606,22.060735176692912,21.15375333067992,25.097138279698054,21.440477364164977,21.3505869180748,19.260072175979914,21.05711024152434,22.371116977331784,18.18526261240324,21.763695119906945,19.900159246109634,21.022878950161832,20.48383078099113,21.821470865389173,24.987649624596244,22.265101932257092,18.954616641321604,25.734123629818527,20.035932572983462,21.002454511024606,22.570203152455143,23.437077312046757,19.317032739803228,17.641741979338622,22.448170385491473,23.33275256138136,19.064590160696714,19.240852168960235,20.1483293247843,19.78119349991646,20.035822020141666,20.368090770815378,19.622542520355413,19.136726140380034,20.665117281017753,22.04592459909886,22.766122645362053,24.701481133697825,23.00797271520254,22.942154096547412,21.766675289013797,19.216601410973656,22.721359157406464,18.9576603836655,21.88381079939571,18.472822451162664,21.07184726592027,22.03833211483471,20.099462108962847,21.41678722119122,20.92948670168764,20.890428315584586,21.725547245838506,28.740163782092626,21.205026123435218,22.323663662536706,19.34101707633175,18.666140767995948,24.012052706607076,23.69806207100269,21.754438391935153,23.682062419502355,23.126467381893125,21.07179899498228,25.515113797875166,19.69217074150189,22.12431429427697,25.95498550595432,21.870103917395213,20.208896815397623,20.476170795773445,21.523407372759994,19.429205075454394,20.94805891660743,20.579612186732355,18.425777297525496,21.048120970639673,20.14580390451336,19.1678248265788,19.26180569731426,21.731372277489818,20.677454054144345,19.241454867705745,23.733038602815835,20.006473671385464,20.924659576737604,19.109062896280463,21.73683233585413,21.589387814254906,21.96355600303241,20.930649460210123,19.44163523375263,19.835220336552222,22.967539915594646,19.57718354371762,19.694551733623157,22.05857907784579,20.152641197123305,21.206937139516242,19.177040653531833,21.059144404213697,21.77992540342766,22.0971519060863,19.9033116727081,19.870869348950354,20.847191191309296,21.255773747330974,22.179827408990512,20.655014419256375,22.594985264708335,19.867378293238115,18.6010489412016,21.546971094057223,20.8121798521563,20.30528553843669,21.30235611574127,19.472993206549905,20.642314511859237,20.823420888524133,19.456579392491523,21.112413416140527,20.769205079804532,20.344212591822245,19.88957080391307,19.783879090807634,22.2606456541005,24.0347829295736,25.71484503270827,20.79497581994581,18.966254774482525,19.558951865783115,20.00118447409592,19.759231530600534,18.668189678924886,19.362035840585754,19.21669376414341,19.534513178135196,21.58943582111394,20.029295821796193,20.93634103691578,21.419330156983328,19.511493048750843,20.011894681671833,20.008240359915625,18.95284902418798,21.345469949872594,20.977820033541025,23.20923603756077,19.692880228017533,22.637005220660473,19.457649403400342,19.12974837607436,23.60775201354553,0,21.026538976608304,23.88396569488923,27.919301359795483,21.157605371717533,22.006125382801496,21.526914680071894,21.60107003042797,21.310840181677595,20.659294437479453,19.303395037499914,20.578169960300666,22.895450751816803,20.179768194947595,20.3718571876006,21.13235475498899,20.116418845808077,20.597315830475623,19.957464846805212,23.115072392844226,20.726116689654802,20.700075492290427,20.833447685328323,23.397238664878486,18.725486086935668,22.38037915102657,20.812817350665142,23.171040699108495,25.19463787743612,17.21138734793915,19.0336054859068,21.825732486030095,21.042046241460838,20.6322831501737,19.380751294097546,19.582601081862446,21.284194059773114,22.179583552994327,23.19729034551032,19.967482396438246,19.738441963085798,19.239293618022067,21.59285163447237,20.535950228874793,21.01158079776531,20.08203076321153,22.84732578883419,21.756353934255824,23.316254320731808,18.81898933836999,21.940012430656058,20.748003621067692,19.730383893455734,20.182461946627072,21.076706013187607,20.7269525620563,19.59081933226322,20.904152603600195,19.659163210227153,24.761947476774708,22.534274856548336,23.233448675874918,19.884197354704103,21.007268735063114,19.408841782080135,20.136824689258354,21.854300382519792,19.881144027962787,20.995690391681727,23.573228460474112,19.074352191519743,21.469722294142937,18.833019773765052,20.27464659506303,19.718574682047347,18.258239797953646,20.755317114550273,20.561150873374917,20.948677886535172,19.720177445507503,22.150900676319612,23.39909808655993,22.31935969126546,20.155632003058283,21.13370665837429,19.473136561032486,20.46100849412487,22.136011068345166,21.159614691783496,19.431106818709004,19.13213914887522,19.706696670705693,19.346767596320518,20.918060329643115,20.18711885177595,23.064648540231374,18.98912255599455,24.074524917146316,20.61646897501603,20.227612418267608,23.77281502683423,18.64679105166152,19.821061094559884,21.41515272305351,20.235862606471247,21.36550800836747,21.97455042987335,22.17732889464112,25.151779365365787,24.150527389957542,21.040691724844343,20.760535400558187,19.34725801943171,20.04287236504247,21.531748865354032,18.726909959036774,19.7936588697483,20.03811220566055,23.56945490190484,23.91740995998136,19.437420256199317,23.65829637497728,27.09570141066156,23.540007985307764,23.389995853863947,20.300248650151303,19.67235973078438,20.003307902740296,20.529649424214696,20.802720957968873,21.281868210231526,20.815053320149143,21.927740352835695,20.82479873735278,20.51237686567707,24.94868149530031,20.081935873735674,22.717589515695337,20.82980401444312,19.3661810523187,19.957403913237158,21.18136460499971,19.8915004159983,20.17210241586846,22.4307529287208,22.889842311610867,20.801380865239704,22.613937582771225,20.216218911187326,23.993435990731076,19.557332226454086,17.31416721288844,20.21067743703825,21.405123280549176,20.1770277231615,18.904187708332326,19.046718202293015,20.073998999202523,19.6302991245126,21.48379659194355,20.81804614111716,19.563852956879067,21.051151840896342,20.770721956713995,19.67623430456214,19.581897497422524,21.384446242838667,20.692911719555987,21.545170940853957,21.994462034082755,21.47969097288377,20.278388484789648,19.382106816183008,19.441821622178967,21.939001270486997,25.832615032960224,21.65340972782181,19.535812626400283,18.947642644803835,19.712127286089668,23.90063882300631,18.304477915148002,22.443056343683086,23.16226531069458,25.14188655199703,20.70748794380435,20.732604891283092,19.934400683437797,20.62113432421718,19.28434545274011,19.017540081468233,21.15041915835723,20.26686372980413,20.03253125195289,20.148876382348814,22.62198208884591,20.021014007950576,17.20778649543665,21.273482891420798,19.30063185166596,20.195062159996276,21.445335194484834,21.0819625214105,19.392355160192587,20.378347780631422,18.799302474087973,23.18300636577445,18.93991113136844,19.223167412657357,23.650902186871754,19.945176609895544,19.495627675541318,25.79058328300537,20.476746945456053,19.21271648764287,19.678332456797207,19.71871179715288,20.105565995835796,21.137414277628565,23.008367409641956,21.03393309328942,19.30271923785845,20.626830273876646,21.092597149125172,21.952512694511015,19.882805718288704,20.77592509493985,21.046183463307532,22.0435593555408,21.04785962557455,20.25569596256737,19.352118206703615,18.845717636328267,19.34199651526983,20.91962424027046,19.595170049801663,19.600097337632842,25.661642339745672,21.791142017552335,23.50747794569296,20.391385205003306,23.803560322980168,23.78648471341281,23.49397732898427,23.081833665589276,23.144648130459657,20.920832580620292,19.193623112656937,18.901120542977083,21.79790479470573,19.407912925215978,18.98289333455679,19.415382799115825,21.253000869012993,21.604198363469845,20.557571210143905,19.274253014992237,20.09246940100189,19.391954018620176,22.850833320712763,22.012401869246364,20.826658939867247,19.118363214083377,21.289967644645948,18.733922283625127,20.9115880183164,16.829816883719186,20.861088989018167,20.73330584327736,18.406398882361927,21.03390016791011,22.274116203708253,19.445206608830574,20.834279688605037,23.70344643795525,23.354012710201598,19.88516012413886,22.730738164708086,19.75792383693999,24.394611647917593,19.734888199833236,24.922685338201692,19.530254509942655,20.280991315768514,21.951122115143434,19.19842345841963,22.926085660752005,22.977548256996624,21.21128990053558,19.167558108393887,20.647437784958075,19.35146728073557,21.097845676874776,19.32409702234251,18.277738745265133,24.145550575002787,19.346013720701006,21.812205122435703,22.86783241905905,19.249124305809787,21.53558768926747,24.102450641699225,20.500920634779252,19.622850798933502,22.45828794986169,19.57219805726336,23.264639171673256,22.333293091972617,18.995877797816178,20.694084927736416,19.630595419370202,20.436904665975636,25.664004852328446,20.985299957568404,21.714666548374087,21.619000188699776,20.319316549429495,16.9587226208279,24.58750691743097,21.431401448181294,20.454850489915017,19.41323537187323,20.769097708402093,19.12410363757553,21.037456801472633,20.865493152029874,20.200550831447014,19.074330956681983,20.89335338152254,18.80103888605608,22.493051859930933,20.264370436179377,21.216120605388507,19.103940869449712,23.841976780210096,20.967615900689093,20.26093058536308,21.15453445100085,24.16646354231811,22.25017305320405,20.99185811169326,22.413732248617176,21.395751936133603,21.027619198526104,19.682163069515127,20.59828494694422,20.60161654204836,22.930920924409694,21.340491521133263,21.13532539299306,23.86936283890287,21.30122097904847,21.87678820576361,19.984953818018486,23.219303551688306,21.660324565067476,20.87994439864477,20.922214169866013,24.081396558763124,22.721264543402345,21.873381120752498,19.089555706982512,20.89641611390129,20.568873923302558,19.242152996635603,19.205602363695707,24.69282091354044,22.779994713980038,19.521220854266264,21.142478555307388,25.236220259743373,20.048804801912784,19.552903579436016,22.115896159241377,21.403353538154413,19.840625811644227,18.35486059310022,20.417051943879315,23.212857021000243,21.53372289651904,20.79356441567801,23.25433145453618,20.592440384261195,20.758465193484422,20.24512208583746,20.925901548056537,24.750691092265395,19.41019874513054,19.696673307271265,22.56844661795243,22.50456479926641,19.4801354006198,20.95523200641152,24.568652749155945,18.75491624208908,19.461087704959443,20.880059154822476,19.68504303470752,23.816825076283962,19.839377869097518,23.420195657862312,22.999884079036303,20.649943786599653,21.684940338873297,24.09117022808059,21.12734576220247,19.49242240707679,18.90953834843785,20.94197381362215,23.259273464150183,23.03775857233843,22.58649267850949,20.120777025175872,20.906486953193497,20.378669388865216,22.737839310980224,19.27074701059353,23.65992231174119,23.339497059809283,22.05115682063906,18.775074918844375,22.10097025846529,19.31545475085148,19.854767962698624,19.840511525069328,21.39606694375647,22.97307940319244,19.86193501563865,19.664038907092028,19.58463718752616,19.810866559819527,19.465937010710118,22.434772773304594,20.445539922646542,18.99187307244663,21.90849954742701,22.314131385426723,20.432733544527707,23.07558442006914,22.830034474576443,21.269107840654502,20.48988421076927,21.083491338486773,20.934278289661282,20.287989620862316,20.04722528013383,21.199347342918216,23.521067577919112,23.2611930742384,21.92570835834572,20.556276718935056,21.41646356230674,22.53034703709958,18.95340785965337,19.986508970321974,19.495183602853512,18.457821713581545,22.886338306822665,19.129172314836154,20.136171638349733,19.376016989288264,20.879430687458026,20.620771994361597,20.30679376611553,20.368798039683195,22.35947129540103,20.069728697762105,22.54894428694912,20.37273542865621,20.71779772905426,19.199159497364995,19.174850117652987,20.13094661361353,20.62659792573699,25.292356917614896,21.758700626292807,21.796149648767223,18.807339197843834,20.506265808043914,22.110290220817404,19.52840260028443,20.325926854481473,19.625476983112655,21.50252421557022,19.304125125188563,19.107865328534132,23.169806519640005,18.840768128990423,19.36444675592656,18.627786784209636,21.17955986366495,20.377078375453138,21.746512886013605,20.910669810022966,23.66715843905666,19.356125235979412,19.33650002206052,17.813568277801206,22.465861054219694,20.57561772108256,19.015107379674305,21.499214768467855,19.66178092422977,17.299495077910098,21.27615053752397,21.040198736231254,21.92098191704396,21.7224558930939,21.092338092530998,20.048671453648097,21.894206057556605,20.415195026786527,20.471143313052202,21.849984955018474,19.59156765743604,20.658934051201378,20.105603080001295,22.787613064091445,18.897059576214055,19.64405870233027,22.74129354031224,21.948058114314257,20.038176858751854,22.177258018525677,22.318758483797644,17.211802761238985,23.061132678317314,19.552166474373674,20.22058498039463,19.1389392082589,19.751662039949778,22.589266875609983,26.340632927208585,19.534708840769273,20.435276534084654,19.903908501005596,19.287439594230733,19.687016476370815,22.7859490833811,20.26081963393427,20.008806098312462,20.432875222497227,21.770367489208184,19.54263516359492,20.167399003287947,20.02036350615788,25.816840819488693,20.66031803587651,19.756693437676223,19.47969541290554,19.46354033785,24.169496968680054,18.739319147762597,21.559823540829672,20.841106100432793,21.572330613609903,23.06348379203432,23.90661007609995,20.544449448312626,18.828527980887348,21.643407148870626,20.295760247970698,21.926818619899166,18.887256320586236,20.26723932452398,20.649629842496918,22.847263558158666,20.639447033358472,21.651692029555726,19.3900555692985,21.40442957359937,20.067715321953298,20.687515795874024,20.646624626390846,19.38789150561276,21.443595032483266,21.960080436904104,20.986097829919892,20.116046923130224,20.38834949087981,21.51694698715695,23.463402138610206,22.107609158967936,21.13017429963753,21.059552411062068,19.918938919026616,19.132327452939787,21.94580529932221,19.159457479949786,19.567551127469855,19.27757204338643,19.477956222501692,20.02620191687006,20.455200776646638,23.044574887856943,24.184179585129318,20.388726808803085,20.116390923202314,21.32891538129463,20.93827950221911,19.878697565839243,20.238709984208988,18.95241706841831,19.63538512176151,20.604884938737765,22.740623426919786,20.3125128271046,24.597055201180662,23.117114991068007,20.78628428680642,19.932433926767427,19.826295549099655,21.26517033895163,22.496434708234126,19.328382384869762,19.427802084161115,20.18180618902739,19.980960001271505,18.996123380968683,22.600876020595507,20.092995805378262,20.046496736817677,21.151917099556776,18.585982112110514,20.695329089308867,22.520164246985647,19.3471325562148,18.999355953219133,23.20757904373024,21.23702785682451,21.08554669425927,20.768225524289374,20.2479251962018,19.3533467533098,22.213653662019038,20.24562900900173,18.67996002744327,21.27136955536682,21.42004604510025,21.047075972047644,21.8035683042629,19.22968489276112,21.077647966937757,23.387816453129698,18.706232935340964,19.58110217755027,23.538867158230946,21.96712491711981,19.68563168309507,20.03480567791853,18.855622835428132,19.611919934246885,20.89874787711282,20.381292625866948,20.623611830893093,21.463850115483186,22.47425899129698,21.46282774650559,20.250449939937386,19.804071134581005,22.940732615911422,24.1535698900103,22.049830938319623,19.98785104241217,22.944212504137,21.578861893090256,25.17177163891695,19.37427511207424,19.96383210404445,24.180920079085045,22.206977518330888,19.158269753875864,19.069924834450198,22.714272038785044,22.147044335773373,20.251879940231216,24.956261862764006,19.58788736648328,21.751091249893005,21.93032100053686,21.621710733177533,19.367732274650958,19.09456742423413,16.917281198342668,20.807982080155547,20.46876211568297,19.988975863285916,20.580262018382033,19.091740116853863,21.44414464440018,21.29242653675922,22.038981262669978,20.180643707721647,23.155520679582406,19.32638298997934,21.506597416475046,20.650611348398325,20.693713245656816,20.540019222244897,21.217521718500812,24.49306731779338,19.642596451766774,22.00972851632279,20.932965503614184,21.834232905374517,21.098649782686852,22.49932865299591,22.579718287600105,21.529871119919466,21.118854440357843,20.944082894927185],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données de consommation prédites par le modèle de régression linéaire vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_pred_logLR"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle régression linéaire\n","pipeLR = make_pipeline(scaler, LinearRegression())\n","\n","pipeLR.fit(BEBM_train, SiteEnergyUse_train_log)\n","\n","SiteEnergyUse_pred_logLR = pipeLR.predict(BEBM_test)\n","\n","LRr2_log = metrics.r2_score(SiteEnergyUse_test_log, SiteEnergyUse_pred_logLR)\n","print(\"r2 :\", LRr2)\n","LRrmse_log = metrics.mean_squared_error(SiteEnergyUse_test_log,\n","                                    SiteEnergyUse_pred_logLR,\n","                                    squared=False)\n","print(\"rmse :\", LRrmse)\n","\n","fig = px.scatter(\n","        x=SiteEnergyUse_pred_logLR.squeeze(),\n","        y=SiteEnergyUse_test_log.squeeze(),\n","        labels={\n","            'x': f'{SiteEnergyUse_pred_logLR=}'.partition('=')[0],\n","            'y': f'{SiteEnergyUse_test_log=}'.partition('=')[0]\n","        },\n","        title=\n","        'Visualisation des données de consommation prédites par le modèle de régression linéaire vs les données test')\n","fig.show()\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre      Ridge()\n","0  ridge__alpha  7996.554526\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_pred_logRidge=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[20.98655683545223,20.742685832402046,20.937624749789247,20.709666210342174,20.807899452328797,20.69527268576585,20.946971535611254,20.700043942992984,21.042722323382257,20.73749203127253,20.75440057873999,20.77773880739686,20.693483327261035,20.780895405489716,20.727724273445684,20.702548246486536,20.671850788681137,20.81928791696075,20.66489818932428,20.6495521528808,20.79197443082061,20.777989542251415,21.66210201797985,20.91216552663402,20.65382829030884,20.778416940849706,20.917187737607875,20.65191283156314,20.677148447499594,21.0624468416013,20.95971136925867,20.69180864526254,20.964535981304476,20.790792216628038,24.36757242046088,21.28557396867825,20.659322320659605,20.642749145815532,20.67800701068557,20.822627439116662,20.615740627421566,20.67834871479546,20.76515000749341,20.88369568279494,21.110625671928997,22.399041034656555,20.736581441243384,21.071685000041875,21.017805680202862,20.682896052907633,20.831029888323677,20.716020955282264,20.775281327135104,21.92774024624259,20.788358481225956,20.66045259110607,20.662613559309534,20.746181348915197,20.756366783699953,20.642640684427324,20.68830771946776,20.858273578314197,20.865845988874057,20.649231642981373,20.704998643700183,20.828990167433084,21.035755780304385,20.985674404235407,20.7830522353786,20.70325216972369,20.77068022043883,21.832195920452932,20.663776681949386,20.99823298373726,21.18778862700538,20.99222707678702,20.837411441925884,21.05380205937939,21.09262436083692,23.325182681488528,20.788542424224,20.796213413373,20.92252966457903,20.614299145638654,20.77210758388092,21.34079752788906,24.210389379291158,20.664842736275777,20.688415892069717,20.67148096956641,20.73815952960698,20.77722385559365,20.689340131163036,20.631458772511518,20.740573048617115,20.839324399917572,20.643568891701136,21.190743909278986,20.80331059366153,20.826652283996562,20.63746276342691,21.523038251451343,21.398079412450635,20.803582689740384,20.55690828095266,20.885810848517878,20.697691879503942,20.79706626428706,20.726208015075308,21.116936797233404,21.250609801494726,20.71663425846622,20.79307471161811,20.68437366472084,20.747538212735133,21.55181744519086,20.958887436086716,20.723497958370096,22.2723972968782,20.82043007739129,20.686977807653708,20.874844441298364,20.743490895036857,20.682690785645306,21.41179392071662,20.69213062065223,20.72025047693077,20.753839666597283,20.811692510643255,20.70702370795499,21.86384145565998,20.641621177696912,22.549981257547284,20.766257740735778,20.667274680791973,21.054395567211806,20.720910298842927,20.742571746292942,20.638808907597998,21.029318282288223,21.13721837048921,21.566998121460614,20.884079583710104,20.979608478601005,20.684081346675665,20.675524325220753,20.77721221482929,20.972937994635,20.82302972131794,23.265312017218175,21.236129281818382,20.79471842325864,20.71414272438127,20.663611462711025,20.695551193333355,20.67375063046903,20.63365467741922,22.495189633819848,21.57644229155449,20.623960269058987,25.301700227822394,20.94108982631623,20.801006200778193,20.733142123476426,20.71303985114226,20.616471852887226,21.036397243677474,20.718354832847947,20.718267036647372,20.727928275059615,20.587491490842588,20.646838709040512,20.69511110899059,20.976072199925255,20.68417733936506,20.977114550148407,21.18414711726863,20.740259566252,20.739971410157633,20.704422489604994,20.81650762961164,20.70240837857883,20.76344114728234,20.84572632451617,20.77865450937672,20.77801782774248,20.87458128057108,26.844957337610854,20.922396781417024,20.674162561803755,20.765199277071297,20.643744446386762,20.7722950774211,20.671559452214275,20.980446417031466,20.814277270905816,20.709769614881832,20.772469324320333,21.238031486386085,20.648724786525616,21.32867340610222,20.632974690548846,20.938445642734198,20.635948992929485,20.802444573261543,20.68635995600062,20.731582125599388,21.220300380615168,20.72789829269819,20.754950541121723,20.7720072041168,20.64925296071003,20.78064542817499,20.74221608101677,20.66043914904958,20.81985243803207,20.688899449870025,20.818614120489293,20.61979262426889,20.777964009196847,20.7198013031244,20.665010572057664,20.734499219050388,20.62547270432819,21.074976020705734,20.808246006389584,21.269183722039404,22.427532074899133,20.841649695751702,20.49675958541517,21.383322799056174,20.70672195153447,20.672446287385984,20.663750105241636,20.848374996876828,20.78308460844983,20.881485857824387,22.121548311646947,21.677333774262927,21.313107402781686,20.81176094061432,21.084412032313203,20.740955969455282,20.69449078756446,20.64235434079949,20.74217316119206,20.64884027090366,20.85329474994081,21.17528560052789,20.661854782278663,20.708049916360945,21.364221388800328,20.67104786910557,20.667876370568337,20.729293025898656,20.901881280592118,20.72115778414852,20.643054880569032,21.53953920640015,20.75601070906931,20.676348293707555,20.859559126209934,20.684455749425016,20.74870598377425,20.640884011655032,21.233823316057027,21.364042642561685,20.68333985566401,22.026227147254858,20.701596552990264,20.69921731780515,20.785671134561632,20.66710621370778,20.648826406878534,20.637632579349518,20.615592941878873,21.71359994095704,20.688269405754934,22.509992032095685,20.90463651227165,20.752953537720906,20.622997110269516,21.32901667378837,20.844480236537645,20.71649685899304,20.733135959824516,20.620214256637396,20.67381302426177,22.853363365600774,20.651945006375275,22.348565186436705,20.653543528931642,20.77255082933804,21.011243692179093,20.869415229111645,20.696607456439,20.68480519395524,20.80465279314199,20.634907335756342,20.84203247997456,21.80786027708719,20.666391570970756,20.782705273839056,20.71733853979249,20.99523809069731,20.7390281846851,20.69406459017237,20.809411132418138,20.67382315729301,20.686149634810434,20.744973292959127,20.780824571857483,22.52061864697095,20.742896142264748,20.726013029220667,20.70810370476375,21.553354782263558,20.638088414126837,20.691035342900765,20.672170414630923,20.667890157258334,20.673686037520547,21.52868301197235,20.68299603749066,21.33087465034926,20.777517295780235,20.721046562778472,20.825555554545254,20.81716307862709,20.713825519758363,21.838895002821587,20.70146713611881,20.761052576333416,20.780093428502248,20.88767129522075,20.7137286215753,20.71097531447977,20.608628451592864,20.643684356260476,20.66432028268092,20.92777007544064,20.72872368315385,20.67791582216265,20.806560463227495,21.308554426012247,20.670385296343724,21.32573912179176,21.205989644962493,21.147331528106474,22.439835037800503,20.963426473265265,22.03729094897708,20.760342311338267,20.660175187132616,21.51400794831343,20.71436907675111,20.676078899878522,21.414994183143488,20.694078334656457,20.675192925016464,20.73218059377814,20.67358172331187,21.604476374165333,21.92069639425452,20.68355174000932,20.640540893785303,21.34938113704083,20.599151991014207,20.826253331086676,21.70988537756669,20.69526764516325,21.142339205079743,21.89907100898094,20.643775374040192,20.738685646921247,20.697813507715193,20.888282963335264,21.658300061997494,20.668621054897898,20.93591708536301,21.673301253950022,20.953061120971114,20.760351525205014,20.8079610293918,20.690583379630077,20.66317152036983,20.744475414697096,20.762516943002584,20.66886953993134,20.792051376516575,21.382113774636743,20.728577919416807,20.65852666125687,20.714457560635978,20.739465685492583,20.73670984109062,20.935247055845633,22.43049818571526,20.69079929165792,20.637782679373338,20.67937496283883,20.672061789845113,20.717228991419624,20.82244107274163,20.98221927292735,20.808496119540937,21.208959681948397,21.13084514697633,20.76571237162923,20.625655063643986,20.506153383370098,20.901207006808452,20.681761940955806,20.684148711244756,20.701980156226483,20.645773371187598,20.68260019761585,20.976352759415082,20.83366501672094,21.20620715157294,20.745458517177077,20.757488607985366,20.641678426561946,20.842104327035873,20.649794427495028,20.825680629290453,20.67294935572046,21.79976076211559,20.797387577839064,20.787977013075082,20.767966756779835,20.796582253309797,20.63276738224699,21.095412937177247,20.7542232500148,20.699268129566452,20.6993182096295,20.82853312095179,20.800862850332948,20.59736359001888,20.893917182878553,20.86208679358898,20.725006102952445,20.791297171426788,20.725460382094376,21.145967552290326,20.621941768720102,22.729195553989893,21.196909868943237,20.883595882393987,20.638946592759602,20.679667338401277,21.419271187388624,20.649784934654498,20.730801081548808,20.75067771316637,20.69171696929408,20.674897069724683,21.02760940857577,20.76387582101361,20.88348924932744,20.669483946346887,28.80365485226168,20.67329454004624,20.707150003363072,21.128670838602904,21.383395053081223,20.733898957487607,20.639523891219802,20.959029095569008,22.42629287690443,20.70673703868496,20.684274831308766,20.74089702490698,20.805532731657042,20.64643190801431,20.88921714671124,20.71471734230167,20.640934117084722,20.717822656388247,20.77633470124645,21.557215459711813,24.108004572693787,22.636309980788862,20.98555429229794,21.361169459138917,20.684791710020313,21.36972151371716,20.63006652575092,21.587972823890098,20.657921797618844,20.750247223046077,20.9138155996154,20.673533465653144,20.761521013878735,20.67158845754661,20.638788226503006,21.005828236329645,22.401442222587548,21.05105066877335,21.212127665236643,20.74523210688332,20.67870480924481,22.57552268848229,21.38746002115467,20.65595002356557,22.6347210019047,21.32314993392141,20.696260459033,26.405564269474432,20.684826712955086,20.79960909960283,22.94203019326799,20.96839293970746,20.79601713999454,20.664903911374456,20.831318148675322,20.694351743187305,20.96197120224819,20.73718828107981,20.721309348804144,20.76434933590404,20.684997185890634,20.800723975106305,20.643913059201797,20.683302939312743,20.724650993143047,20.725339523392655,21.04136371011967,20.74852807528542,21.212353471555907,20.751924078022316,20.662583527359,21.051408275241656,20.91212984835108,20.658141088080967,20.687926534433853,20.718936342817983,22.223959066972668,20.755725724263684,20.71111631336851,20.98541288347366,20.928670336555168,20.82089541329195,20.79540373870882,20.731013805860684,20.963146335771476,20.946704774477222,20.69329865250828,20.691024245134006,21.4272068086,21.088645596391817,20.85405217649647,21.130065373345584,20.851589110950947,20.74114852108698,20.64844125853766,21.071761857305923,20.671112433436328,20.617833824584864,20.88118689656076,20.72003963651025,20.633821760060812,20.683757331714713,20.58719269450826,21.07488148127729,20.651870783732853,20.648065825980503,20.714025601695802,20.730484069633444,20.723289565532333,21.983408473076004,22.65840850496315,20.843217442279997,20.698983164579527,20.747634286302052,20.73660751303129,20.739244138869413,20.687666006333135,20.74044904894939,20.73027680433239,20.707706169034125,20.85295876953859,20.772385218734474,20.73451614331011,20.739988636256836,20.707579814169925,20.718006317584454,20.73269822585865,20.692828392762053,20.735769296008655,20.672182193290084,22.297002916374858,20.721241548633113,20.746546481014384,20.682926871167194,20.696079101128458,21.07890438587768,20.732630899068585,20.812927495833996,20.76086663293236,23.160857743385627,20.65319545867462,20.98820118899412,21.04770311162281,20.748354405061356,20.771907289294468,20.8170585033646,20.701115788141117,21.041446818113332,20.832870982172523,20.649590078148588,20.674551595578134,20.666491513162264,20.911710050392543,20.75528531934806,20.763585857087996,20.641489701459278,20.768061893513902,20.94792819992566,20.7367441233305,21.989807412073255,20.694395343248395,21.461641530555287,20.795758683070233,21.0940719774495,24.741895739574403,20.77272042885915,20.660158728760923,20.642901350269856,20.652587791084926,20.64428649426784,20.693449661980125,20.69922756390731,20.714415343777866,21.17546186827703,20.64795754220149,20.634972980605294,20.6986572888481,20.75739030839643,21.009267525849516,20.773652745487457,20.7319798924256,20.65058857844943,21.669963361405582,20.96113164113097,21.189616999468214,20.65956314464963,20.98872302640315,20.638431507651102,20.610999366610372,20.748947163978034,20.86716150864604,20.88266607861454,20.79314784499661,20.71153039255085,20.679444447474264,22.524565466610614,21.3082511831673,22.190705360101934,20.707391793282103,20.640454622477236,20.70179214708184,20.629087411352348,20.988886650182955,20.634368396053734,20.781355606094486,22.756227852738693,20.664517479904696,21.426692590834772,20.673601188963797,20.689412986649344,20.727909624709078,20.654532383625785,20.70566720631189,20.71943341777585,20.754932027128152,20.62304745789255,21.062787489323828,21.471172420311177,22.601399421477684,20.759773253173996,20.710077777658732,20.682680633021118,20.80071338588435,21.350508940359152,20.797817893354956,20.658810215507195,20.709330453164213,20.64628091836117,20.715943832400733,20.621294910508453,20.78094173265624,20.931708486062977,20.685369094504647,22.99564010659172,21.06961396552641,20.696676504279864,21.101284793076058,20.643474792095464,20.74401560705431,20.63964121558762,20.816411688874332,20.69252642386022,21.04092700498382,20.786884441497055,21.07228243340635,20.99783943577423,20.961118480812207,20.782756221053177,20.702693683030617,20.84346427688483,20.9521028929238,20.62065328677744,20.722528314883778,20.69368696257874,22.258191189728883,21.716832767491915,20.688552640535956,23.44841057209252,22.181730867039334,22.260518139271156,22.7512613862965,20.62280029102706,20.80586701274997,20.741727902015775,21.011085090661172,20.702519455950085,20.94183956853497,20.69338235851191,20.71019755998396,20.83288033536706,20.797239396528102,22.588850401437387,20.70407899843026,21.134958020879527,20.78581979436196,20.613831538446743,20.6642887871018,20.781132317264095,20.68979317939728,20.68923616848306,22.606365887919875,20.91980075732511,20.772486009805586,21.13992448732172,20.816458208121926,21.149787958596818,20.630328528028453,20.63145569068556,20.723257062808372,20.80278435979715,20.699842994021267,20.68553827397942,20.68663264314663,20.875705230950807,20.789385755807032,20.744686477274694,20.983786225188016,20.83325241134281,20.864323127121807,20.753882509358203,20.710928341803896,20.717921044535593,21.961563301583485,20.845109312077827,21.043321722482613,20.935643066113524,20.946356549998043,20.917925918420238,20.616046560905293,20.684089684066254,21.409629422586644,27.495166396343233,20.817697362889316,20.682575543008205,20.658305394565467,20.659840932749102,23.35082008131195,20.664665407550583,21.05966110503528,21.757455517822294,21.96493953674107,20.905038240909896,20.839623514753125,20.629354232467858,20.79199321773001,20.638602425258945,20.681901791020877,20.726982556980836,20.630643339748428,20.812685409126317,20.480351258551625,20.699577996626456,20.623548246295133,20.68003411506933,20.678709288432323,20.697097087094424,20.74462808881464,20.75332087150355,20.944727448006855,20.669977038499855,20.78312520795513,20.688931225557766,21.709008727826692,20.648861823866646,20.702890919891804,21.351189724955937,20.666150847661402,20.669722683944467,27.45798002337767,20.68698090929837,20.662747971711745,20.746686906967934,20.8044368528383,20.670152781687133,20.98090566681181,21.770678542244887,20.78647022395048,20.70801907828268,20.701755485092278,20.81979264770226,21.242365131613635,20.737653435255666,20.794511921100877,20.836023507380993,20.88858358000118,21.017368001460987,20.714685186087323,20.778085768936407,20.65480177603588,20.631240620871775,20.857290400290093,20.822674833840107,20.635581651708563,27.37531369667944,20.861546284067142,22.83237031767742,20.826866924430124,21.443080542479017,20.874663886590923,21.76978603041649,21.441301693479755,21.398947264649625,20.938906170908766,20.67294935572046,20.721303907031675,20.660520524496572,20.863363980942733,20.779556468944744,20.71891470378378,20.840958179540678,21.09756073399192,20.81521117422089,20.65976530428046,20.745863491349176,20.73817362041758,21.223907537485605,21.12469257056185,20.908659613231034,20.68915213977972,20.694509849746417,20.70692114854621,20.858891046394522,20.63696597567628,20.762729089653508,20.741710589772694,20.60802576910514,20.659788561526554,20.87119840524898,20.702211273354482,20.943412109176393,20.82949806396241,20.6914265867914,20.73832265308985,20.769754215834347,20.772889908316955,21.373016801166674,20.653590989710224,20.740006826516932,20.72892762256751,20.691757930781595,20.90952224650224,20.680479901358087,21.49661125840894,21.390031231012752,20.70795581675527,20.642291095464167,20.679002101535556,20.71392835582069,20.91846583406754,20.642477925312694,20.619035932082664,21.20251399310356,21.076337615628873,20.994869248693334,21.131059224924314,20.7092717366198,20.649119280066696,22.6369451323937,20.825541369834713,20.829909511145985,20.74350558683372,20.737961088216395,20.88801791107558,21.199975458713375,20.68654121770975,20.74801960275214,20.8892016294366,20.8676296716993,21.26683499312797,20.695918244208336,20.715522220196625,20.83418751832016,20.684495987005196,20.623295015553932,23.766348131234746,20.91499588703409,20.85911027535135,20.61359491803506,20.87157774542236,20.710812765055973,20.860814344538124,20.75315247614032,20.775290632069087,20.76642827806545,20.785058543713404,20.63700095573357,20.977231585398236,20.6811375810987,20.583974562803686,20.68015939145866,21.41414630022144,20.684121481340934,20.736820178840965,20.829894424358447,21.782630878469124,21.170495401834838,20.71237071292787,21.033707618792256,21.192990963451845,20.740207774449004,20.667906071276093,20.594686655078096,20.84765683870645,21.38064617932859,20.93365251826459,21.007512202596242,21.16176323717125,21.213813474631344,21.07244807941198,20.776733975081374,22.90629736076166,20.746095493122052,20.805604845418433,21.000744065423497,22.61538813390696,21.44924934160873,20.983756559960955,20.613334554063965,20.845680282767397,20.893587362019215,20.722044671298534,20.621506119500456,21.348823751154985,21.087544147188463,20.63709018897814,20.745477561992036,24.73692927313221,20.665766999098437,20.806251698815323,21.484760200058762,20.85743976493495,20.701237427485413,20.628688210977025,20.8116613353696,21.63961531984971,20.715791696167948,20.643941812460525,20.64407011152633,20.636662615648813,20.746904183036968,20.70051765977555,20.62853407377501,20.9544417628165,20.685571077837487,20.589586003246616,21.265892610209068,21.549041605777063,20.691821020876365,20.808277060103723,22.26476889622087,20.664554877336816,20.701984863060726,20.708032882222877,20.672093214523926,23.60851448608319,20.708387272389118,21.523385097393337,21.171147574861934,20.819131282376073,20.823675944307006,23.05150123433343,21.108176430293547,20.767630130298613,20.66093639814376,20.661001116488837,21.013931589928454,20.710992481538344,21.030789313862194,20.674174745831973,20.73155038659366,20.804807053733647,21.13746373659856,20.713984441074263,21.159140548421778,21.886556928809796,20.700032177257988,20.661115144049212,21.36365438624628,20.681368454446304,20.772515492253557,20.761748940272135,20.767264120280558,21.627315708430103,20.704724339340565,20.67996466353759,20.676342485735482,20.6959907115762,20.70430646808838,20.86868724286768,20.738052909223384,20.66652066677613,21.082461752442185,20.915526040373354,20.737626059362835,20.655206528271805,21.061314828493565,20.76648748032093,20.74263606862545,20.498537666447053,20.814575837445613,20.777390960734742,20.691548155546535,21.57191164494705,20.735869003996733,21.15850733859032,21.15093401873252,20.806503365352,20.698511665026956,20.942603675645778,20.680571807537227,20.722643068735046,20.696825680639645,20.66288826406104,21.158756292732985,20.703149793272114,20.90255489708269,20.723611452974612,20.626261376950644,20.81644828830968,20.922892384862433,20.63023511291017,20.84023729453269,20.671911392482066,21.11541440589064,20.74178664528316,20.839337218487657,20.744206713725465,20.764939949353707,20.701047161889598,20.683253907633144,21.273281179237138,21.503915090183543,20.828878220842853,20.737617398784923,20.699841388831686,21.825823988305505,20.733659850418338,20.767248162239202,20.80802907540238,20.900739578720046,20.697872062408358,20.691375512254155,21.06155484130822,20.65812363716505,20.699784439328102,20.68481861529288,20.979776178189482,20.65213474285068,20.809905552944514,20.96209322073015,20.745120500821283,20.73266542720847,20.682587890130733,20.64259904579171,21.002554937632432,20.808306360764053,20.625333747337873,20.919962353476286,20.696124667091276,20.664012100085312,20.81517422623078,20.658658994268574,20.932726093573056,20.833563145926398,20.69046647427337,20.6478972123424,21.114902481750935,20.87774748171156,20.65770177296262,21.206576787506663,20.68956581819134,20.74932174938312,20.72816949338232,20.765102933092027,20.681534592517018,20.7409187985671,20.90223896686809,21.11986894819313,20.82492023078116,20.940609532555715,21.023290823753733,20.665268008439007,21.125878680534136,20.66919609536156,20.66812295899131,20.712751784384245,20.7180025495533,21.569230528456234,21.837162386895127,20.677978895680738,20.666304142098515,20.654977664384685,20.67791582216265,20.614385198532965,20.932027702094235,20.70833225226111,20.717867125414656,20.656889631802045,20.67292018228789,20.762589517954197,20.776946228300385,20.680880081449583,21.92014548481195,20.71230899983221,20.736088132399868,20.624464672418267,20.643041899462652,21.154754425039012,20.625619753219635,20.483444012160394,20.925428853657312,20.69107834045837,21.002012016169882,21.831470900677587,20.647251693537328,21.161362728627108,20.64963352713949,20.668067743406066,20.72169628133176,20.6685224166486,20.63560980619062,20.809441950677698,21.571992436272485,20.782868842095024,20.628058081483026,20.732012471458567,21.19795742989404,20.652490646044363,20.670855378716038,20.652287377330815,20.868953466706767,20.62417154014362,21.05065745852289,20.738875776572357,20.653032292422697,20.668183415793433,21.24381011762821,20.7728415069379,20.693169760622464,20.774361951753786,20.749192273055556,20.64348876422628,20.696451340642618,20.950246961384227,20.840961259310454,20.694936919567073,20.716292505668672,20.684087027527937,20.634320698910052,20.688064129657832,21.87037614059865,20.701573922881195,20.66574877328815,20.644336266364196,21.096961763264794,20.78032960428224,20.728448351905765,20.761972352756686,20.703803140374795,20.718424816357516,20.674805051831704,20.745565873913783,20.642389989358932,21.083696297968437,20.87323122677171,20.838250975837806,20.66903708158326,20.780945186571333,20.830976999217395,21.149823128736635,20.7022807790615,20.673762957772855,20.933636802997363,20.705487274164472,20.7248133780889,20.934223418992616,20.953064305526375,20.706850450867535,20.912573577842153,20.67362283860587,20.86987797485617,20.64874268743537,20.679074588722127,20.674069703369497,21.39323920219251,20.86390797897873,20.66616884610968,20.635200598546778,20.762182248645086,20.78556867166778,20.917096314793273,20.830915063212196,20.66079467378719,20.824942608711524,21.286036405994338,20.75370231421384,20.973432379740874,20.808461638742287,20.647063991043748,20.67131323344402,20.626868496663988,20.70586985548833,20.85431622153781,20.77439238371691,20.6622156692993,20.774195988807197,20.669320609571216,20.71085437798095,20.789411558599724,20.852364180645164,20.787222005353257,21.461278863954146,20.81269432447748,20.722249496491933,20.745850024888703,20.855700076514562,21.310147328903753,21.59311389224721,20.934519535232955,20.98439244225234,21.52139303799095,21.030581733910626,23.086961819831618,20.759659551363868,20.662841275708622,21.378326682616517,21.339953258498436,20.725310337890196,20.630654415936988,20.764787749392152,20.679210872922866,20.758901461816087,24.492501503697962,20.748853252381686,21.007695715337107,20.916184092681863,20.871138873742975,20.67438548661598,20.711013343398644,20.613839146547743,20.955187123182327,20.790292326880007,20.782283000694814,20.64795216326136,20.704314723483503,20.88350486136049,20.90760711139996,21.24951374069021,20.766938819198877,21.291466694693934,20.637411276506747,20.85400655136379,20.658270317824677,20.68822037407534,20.810701611130362,20.699476316188612,20.86824018736807,20.67084073644126,21.489726666500957,20.906387264478415,20.956807019779873,20.707434968349993,20.957500122523964,21.313217649609495,20.627893036431406,20.821256614994173,20.801312113624334],"xaxis":"x","y":[22.839725872000276,20.91128637269999,22.88916655727953,19.430343691015025,21.11735552640613,21.280697719341468,22.124201158883572,18.136670762341843,22.576080863006137,20.10297814136792,20.137307637847517,21.968529606994224,20.90466916348112,21.65055481705341,19.466148173643653,19.644920713511322,20.60018787473792,19.883803436319816,19.908561207609715,19.88468822756935,20.68752092168076,19.37233862557095,24.197363319180216,20.375121118721335,18.385719188798834,18.92468784406244,21.354902012656122,22.075827573908075,19.724580465382388,20.25426974158722,20.75534115928083,23.747615973251953,20.91788210799568,20.92150383722997,24.937914667295647,22.368646863289158,19.83010122567355,18.16161414446258,21.756941677613227,21.220261555831726,19.921137323148557,19.756730386301125,19.076215392899233,21.77156523118196,21.51345533419679,28.08571411944019,20.221414835639845,22.43563961615873,0,19.8813455186455,20.446723265477033,20.65296343235214,19.469859640957857,24.8692968913913,20.808662576707462,19.741272758302454,21.063890409149668,20.444375957405665,20.737383520261705,20.772392205332533,19.30201857006515,20.50461437981694,20.844484149195353,19.768388190282472,22.01816825105002,20.77707066358558,22.615494713832,21.249591056381075,19.3101001682171,18.557194427644223,20.820077266859332,26.161807884063222,19.616841359021425,24.601371538637977,20.929756127701882,21.371449507411626,19.801676786988352,23.186489993216338,22.288287088869534,24.593075628702742,20.324838680602316,19.538495641550785,20.313003679890702,17.83704270507587,22.508853453023672,21.500237946648834,25.808363100543477,20.146924115160264,21.08058139373211,17.12438387523814,20.968249130032163,20.028493079904162,18.408984789492116,19.05702758537138,20.086616681313576,21.346006819966092,19.260460167342536,22.182304931495295,0,21.19880336254275,18.935012496616167,22.63391493571178,23.82192770536634,21.24843214899758,21.979683188147945,19.96496549198579,18.7932762528682,20.682137237753555,19.632965031850407,23.473975339042966,22.69657897049395,20.12759231185353,20.362922084924325,18.91123104739614,19.548169047796804,22.968053968168725,21.917396756553043,19.216324315987517,24.057859634036678,21.674989393981168,19.72022056098374,20.690574372087426,21.01024397957751,20.841336970061143,22.67830612646918,19.365836228253468,21.066918904158555,20.194293964879012,19.514174668391416,20.55950872954696,24.227674033652324,16.89837701160671,23.000908813982363,19.725383525541226,20.372113673311205,21.25165374313537,19.232324890468607,19.688668857104407,19.006769325349232,21.51252238785962,21.56101612004373,24.60236976749392,22.783863807855585,21.893038360020444,21.254697640624393,19.266317599579637,20.581937941283943,20.45335403901458,21.260660009666424,23.096986451321428,24.198260151329528,20.67873788317677,19.161993412283312,20.805625607806174,20.111067676706508,19.630957845101975,19.421904563712328,24.27931634919893,22.429090755701058,21.147722912842873,25.31201588491192,0,19.877134324376346,20.195578868662224,20.07860380095908,20.74807693982553,23.74195830377693,17.92351319630841,19.98475794826,19.330905939549268,21.75045501313216,19.64650597499701,21.21476614620658,22.11740118712243,20.373745608135653,21.286908529421844,22.031685219396664,20.241113097705927,19.58596744655146,19.47904998638186,21.40733571681439,19.113174941454258,19.88328018890598,21.622357074698563,20.24572329389297,22.858722725717435,21.004219094475406,26.772642502942784,20.915100345640315,19.607783613490604,22.164399063013658,20.790635654144218,20.499654561276497,19.533857847503334,22.238063995762765,19.371674563247215,21.64924126399572,21.665790435544952,22.72561505846151,20.402841366046307,21.38581238341487,19.76025356060854,20.786842719711426,19.14889856436821,19.895681776607635,22.093666112296297,20.459085985918723,22.982495350783758,19.331349158501606,20.55332894959407,20.31682150206961,20.014253448408297,20.016785168783276,17.008111246780107,21.536104422950213,20.603485087703003,19.59434829773337,20.049508251055094,19.642302060721956,21.52720398550173,18.99020093378567,20.22398061738597,21.01213158467734,20.053269667846333,22.108159109401438,21.79011956511657,22.084914601393155,23.135231909261865,21.190067594656295,22.67339901515593,23.695624962968736,20.984090887872558,19.114539527933296,19.182165482881256,21.52107548780855,21.22249010417508,21.043820980181792,22.894093531705956,23.85820974949918,23.265700738521485,21.89761800850832,24.611415306054898,19.96312680068985,20.173501472804098,20.216216542451658,20.253427701740367,20.770035525423065,22.54105973810428,21.560752172206048,20.50991100623185,19.66651702047476,22.893554470389834,19.678935343554116,18.37060227548128,20.996625195887564,20.88004158687887,20.017617555780475,18.785129945834015,22.472827160217914,21.123452379734417,21.834751391009725,22.36922218137253,20.638651229695757,19.920184186837908,20.233535275947663,21.775846548503456,23.23197611805285,20.98111639712373,24.69390393605354,18.972968571042593,19.85555820528158,19.343431218526263,18.83915024282446,23.64752507012282,17.79557360413656,21.996936490386865,23.456377353127298,21.603092935704957,24.298252303878165,21.481533394476365,20.408641755181446,16.772761777957808,22.20744305955703,21.11943827672747,20.998113148879906,20.11872947152901,21.236410750491928,19.699904832330084,24.209354374596852,20.24751150890443,25.07655356851684,19.568646661314627,19.776542871248843,21.727271525181536,21.062803097791257,24.076140757673382,19.409983373621806,21.192511957229144,19.312950102671177,20.179206907329647,22.432801454859,18.979888467927786,22.15617321227827,20.260215495083163,20.910464871041206,19.297632789142515,20.29925833471969,21.341220185389716,20.961489967791394,22.164351182655523,24.799241151178187,20.420977915907716,25.348057895064095,20.921713841616427,20.242927524338644,18.95868492479523,22.708968189549438,18.49440443743341,20.482328404305942,22.37000861087754,20.121388068881203,20.44167803431194,21.856025378041313,18.927945745238855,21.9419703223203,19.736694248820182,20.23558290561487,19.836668342627448,21.25562508835354,21.394257392515996,22.71500741252458,19.547704151356815,20.400995752923983,20.265469687416747,20.212441762527977,22.377553705906216,19.66793597026211,19.3315880738609,17.677595133772794,20.276225239062065,21.268637940423815,21.725285950225775,19.195369804190715,21.15736975857522,23.488081735909503,20.232817889146876,23.728782472563765,22.84612630027242,22.077517747249683,24.569997706071334,21.7711290432985,23.43665576011153,22.83375379636568,19.2148787700197,23.42842113265138,19.4136293492673,20.540968959330602,24.77613012919926,20.659638185399235,19.087533251252736,19.83361590003151,19.96019429668775,22.334373955189864,23.109422435163253,19.778390202711652,19.167836614935887,23.933068642871486,18.56499419954315,22.530048594882032,21.71290394044351,20.45930590947763,23.069120127461606,22.999636382856856,18.925763040720064,19.49519726866692,21.042555130756018,21.216321947424902,22.454539777644875,20.13393362882646,20.867172390851817,23.066997383640125,21.52825545142037,19.72029436584214,21.117385332870825,20.179272523647345,21.590649178257912,19.692513024244924,22.86005581934068,18.799932943985397,19.96989517504386,22.510505954413144,19.34599856610255,21.28740957905986,20.158837704444057,20.907660872216642,21.905619198419114,22.38172749868176,24.308788888311135,19.262765428559003,22.547404100685924,19.87902826477287,19.826795369611226,20.791833978792408,20.76154230516995,20.245177516562332,19.101442274030862,23.095721196617728,22.853333163501407,20.55222183482644,18.623284957897813,19.707161942553423,23.312427516610697,20.72798026434645,19.38519821479218,19.584766197834426,19.657602815047426,19.766824889008415,20.72718342775812,21.19739249261565,21.766933642509713,19.997824758905686,18.07487316385907,17.89367601727741,21.076958761045283,22.70946891023392,18.280325895793162,19.286378115164315,23.27164625070411,21.087259361308853,19.696262913257698,22.998819199975294,22.068276096052415,20.676958661968467,21.84293342285994,20.22886359790674,19.302304652102425,19.45315606988007,20.189004387901257,19.834023760961447,20.136832510309578,21.96825774467718,21.693760030133728,19.68327564708964,20.69046887217181,19.902947681155606,22.060735176692912,21.15375333067992,25.097138279698054,21.440477364164977,21.3505869180748,19.260072175979914,21.05711024152434,22.371116977331784,18.18526261240324,21.763695119906945,19.900159246109634,21.022878950161832,20.48383078099113,21.821470865389173,24.987649624596244,22.265101932257092,18.954616641321604,25.734123629818527,20.035932572983462,21.002454511024606,22.570203152455143,23.437077312046757,19.317032739803228,17.641741979338622,22.448170385491473,23.33275256138136,19.064590160696714,19.240852168960235,20.1483293247843,19.78119349991646,20.035822020141666,20.368090770815378,19.622542520355413,19.136726140380034,20.665117281017753,22.04592459909886,22.766122645362053,24.701481133697825,23.00797271520254,22.942154096547412,21.766675289013797,19.216601410973656,22.721359157406464,18.9576603836655,21.88381079939571,18.472822451162664,21.07184726592027,22.03833211483471,20.099462108962847,21.41678722119122,20.92948670168764,20.890428315584586,21.725547245838506,28.740163782092626,21.205026123435218,22.323663662536706,19.34101707633175,18.666140767995948,24.012052706607076,23.69806207100269,21.754438391935153,23.682062419502355,23.126467381893125,21.07179899498228,25.515113797875166,19.69217074150189,22.12431429427697,25.95498550595432,21.870103917395213,20.208896815397623,20.476170795773445,21.523407372759994,19.429205075454394,20.94805891660743,20.579612186732355,18.425777297525496,21.048120970639673,20.14580390451336,19.1678248265788,19.26180569731426,21.731372277489818,20.677454054144345,19.241454867705745,23.733038602815835,20.006473671385464,20.924659576737604,19.109062896280463,21.73683233585413,21.589387814254906,21.96355600303241,20.930649460210123,19.44163523375263,19.835220336552222,22.967539915594646,19.57718354371762,19.694551733623157,22.05857907784579,20.152641197123305,21.206937139516242,19.177040653531833,21.059144404213697,21.77992540342766,22.0971519060863,19.9033116727081,19.870869348950354,20.847191191309296,21.255773747330974,22.179827408990512,20.655014419256375,22.594985264708335,19.867378293238115,18.6010489412016,21.546971094057223,20.8121798521563,20.30528553843669,21.30235611574127,19.472993206549905,20.642314511859237,20.823420888524133,19.456579392491523,21.112413416140527,20.769205079804532,20.344212591822245,19.88957080391307,19.783879090807634,22.2606456541005,24.0347829295736,25.71484503270827,20.79497581994581,18.966254774482525,19.558951865783115,20.00118447409592,19.759231530600534,18.668189678924886,19.362035840585754,19.21669376414341,19.534513178135196,21.58943582111394,20.029295821796193,20.93634103691578,21.419330156983328,19.511493048750843,20.011894681671833,20.008240359915625,18.95284902418798,21.345469949872594,20.977820033541025,23.20923603756077,19.692880228017533,22.637005220660473,19.457649403400342,19.12974837607436,23.60775201354553,0,21.026538976608304,23.88396569488923,27.919301359795483,21.157605371717533,22.006125382801496,21.526914680071894,21.60107003042797,21.310840181677595,20.659294437479453,19.303395037499914,20.578169960300666,22.895450751816803,20.179768194947595,20.3718571876006,21.13235475498899,20.116418845808077,20.597315830475623,19.957464846805212,23.115072392844226,20.726116689654802,20.700075492290427,20.833447685328323,23.397238664878486,18.725486086935668,22.38037915102657,20.812817350665142,23.171040699108495,25.19463787743612,17.21138734793915,19.0336054859068,21.825732486030095,21.042046241460838,20.6322831501737,19.380751294097546,19.582601081862446,21.284194059773114,22.179583552994327,23.19729034551032,19.967482396438246,19.738441963085798,19.239293618022067,21.59285163447237,20.535950228874793,21.01158079776531,20.08203076321153,22.84732578883419,21.756353934255824,23.316254320731808,18.81898933836999,21.940012430656058,20.748003621067692,19.730383893455734,20.182461946627072,21.076706013187607,20.7269525620563,19.59081933226322,20.904152603600195,19.659163210227153,24.761947476774708,22.534274856548336,23.233448675874918,19.884197354704103,21.007268735063114,19.408841782080135,20.136824689258354,21.854300382519792,19.881144027962787,20.995690391681727,23.573228460474112,19.074352191519743,21.469722294142937,18.833019773765052,20.27464659506303,19.718574682047347,18.258239797953646,20.755317114550273,20.561150873374917,20.948677886535172,19.720177445507503,22.150900676319612,23.39909808655993,22.31935969126546,20.155632003058283,21.13370665837429,19.473136561032486,20.46100849412487,22.136011068345166,21.159614691783496,19.431106818709004,19.13213914887522,19.706696670705693,19.346767596320518,20.918060329643115,20.18711885177595,23.064648540231374,18.98912255599455,24.074524917146316,20.61646897501603,20.227612418267608,23.77281502683423,18.64679105166152,19.821061094559884,21.41515272305351,20.235862606471247,21.36550800836747,21.97455042987335,22.17732889464112,25.151779365365787,24.150527389957542,21.040691724844343,20.760535400558187,19.34725801943171,20.04287236504247,21.531748865354032,18.726909959036774,19.7936588697483,20.03811220566055,23.56945490190484,23.91740995998136,19.437420256199317,23.65829637497728,27.09570141066156,23.540007985307764,23.389995853863947,20.300248650151303,19.67235973078438,20.003307902740296,20.529649424214696,20.802720957968873,21.281868210231526,20.815053320149143,21.927740352835695,20.82479873735278,20.51237686567707,24.94868149530031,20.081935873735674,22.717589515695337,20.82980401444312,19.3661810523187,19.957403913237158,21.18136460499971,19.8915004159983,20.17210241586846,22.4307529287208,22.889842311610867,20.801380865239704,22.613937582771225,20.216218911187326,23.993435990731076,19.557332226454086,17.31416721288844,20.21067743703825,21.405123280549176,20.1770277231615,18.904187708332326,19.046718202293015,20.073998999202523,19.6302991245126,21.48379659194355,20.81804614111716,19.563852956879067,21.051151840896342,20.770721956713995,19.67623430456214,19.581897497422524,21.384446242838667,20.692911719555987,21.545170940853957,21.994462034082755,21.47969097288377,20.278388484789648,19.382106816183008,19.441821622178967,21.939001270486997,25.832615032960224,21.65340972782181,19.535812626400283,18.947642644803835,19.712127286089668,23.90063882300631,18.304477915148002,22.443056343683086,23.16226531069458,25.14188655199703,20.70748794380435,20.732604891283092,19.934400683437797,20.62113432421718,19.28434545274011,19.017540081468233,21.15041915835723,20.26686372980413,20.03253125195289,20.148876382348814,22.62198208884591,20.021014007950576,17.20778649543665,21.273482891420798,19.30063185166596,20.195062159996276,21.445335194484834,21.0819625214105,19.392355160192587,20.378347780631422,18.799302474087973,23.18300636577445,18.93991113136844,19.223167412657357,23.650902186871754,19.945176609895544,19.495627675541318,25.79058328300537,20.476746945456053,19.21271648764287,19.678332456797207,19.71871179715288,20.105565995835796,21.137414277628565,23.008367409641956,21.03393309328942,19.30271923785845,20.626830273876646,21.092597149125172,21.952512694511015,19.882805718288704,20.77592509493985,21.046183463307532,22.0435593555408,21.04785962557455,20.25569596256737,19.352118206703615,18.845717636328267,19.34199651526983,20.91962424027046,19.595170049801663,19.600097337632842,25.661642339745672,21.791142017552335,23.50747794569296,20.391385205003306,23.803560322980168,23.78648471341281,23.49397732898427,23.081833665589276,23.144648130459657,20.920832580620292,19.193623112656937,18.901120542977083,21.79790479470573,19.407912925215978,18.98289333455679,19.415382799115825,21.253000869012993,21.604198363469845,20.557571210143905,19.274253014992237,20.09246940100189,19.391954018620176,22.850833320712763,22.012401869246364,20.826658939867247,19.118363214083377,21.289967644645948,18.733922283625127,20.9115880183164,16.829816883719186,20.861088989018167,20.73330584327736,18.406398882361927,21.03390016791011,22.274116203708253,19.445206608830574,20.834279688605037,23.70344643795525,23.354012710201598,19.88516012413886,22.730738164708086,19.75792383693999,24.394611647917593,19.734888199833236,24.922685338201692,19.530254509942655,20.280991315768514,21.951122115143434,19.19842345841963,22.926085660752005,22.977548256996624,21.21128990053558,19.167558108393887,20.647437784958075,19.35146728073557,21.097845676874776,19.32409702234251,18.277738745265133,24.145550575002787,19.346013720701006,21.812205122435703,22.86783241905905,19.249124305809787,21.53558768926747,24.102450641699225,20.500920634779252,19.622850798933502,22.45828794986169,19.57219805726336,23.264639171673256,22.333293091972617,18.995877797816178,20.694084927736416,19.630595419370202,20.436904665975636,25.664004852328446,20.985299957568404,21.714666548374087,21.619000188699776,20.319316549429495,16.9587226208279,24.58750691743097,21.431401448181294,20.454850489915017,19.41323537187323,20.769097708402093,19.12410363757553,21.037456801472633,20.865493152029874,20.200550831447014,19.074330956681983,20.89335338152254,18.80103888605608,22.493051859930933,20.264370436179377,21.216120605388507,19.103940869449712,23.841976780210096,20.967615900689093,20.26093058536308,21.15453445100085,24.16646354231811,22.25017305320405,20.99185811169326,22.413732248617176,21.395751936133603,21.027619198526104,19.682163069515127,20.59828494694422,20.60161654204836,22.930920924409694,21.340491521133263,21.13532539299306,23.86936283890287,21.30122097904847,21.87678820576361,19.984953818018486,23.219303551688306,21.660324565067476,20.87994439864477,20.922214169866013,24.081396558763124,22.721264543402345,21.873381120752498,19.089555706982512,20.89641611390129,20.568873923302558,19.242152996635603,19.205602363695707,24.69282091354044,22.779994713980038,19.521220854266264,21.142478555307388,25.236220259743373,20.048804801912784,19.552903579436016,22.115896159241377,21.403353538154413,19.840625811644227,18.35486059310022,20.417051943879315,23.212857021000243,21.53372289651904,20.79356441567801,23.25433145453618,20.592440384261195,20.758465193484422,20.24512208583746,20.925901548056537,24.750691092265395,19.41019874513054,19.696673307271265,22.56844661795243,22.50456479926641,19.4801354006198,20.95523200641152,24.568652749155945,18.75491624208908,19.461087704959443,20.880059154822476,19.68504303470752,23.816825076283962,19.839377869097518,23.420195657862312,22.999884079036303,20.649943786599653,21.684940338873297,24.09117022808059,21.12734576220247,19.49242240707679,18.90953834843785,20.94197381362215,23.259273464150183,23.03775857233843,22.58649267850949,20.120777025175872,20.906486953193497,20.378669388865216,22.737839310980224,19.27074701059353,23.65992231174119,23.339497059809283,22.05115682063906,18.775074918844375,22.10097025846529,19.31545475085148,19.854767962698624,19.840511525069328,21.39606694375647,22.97307940319244,19.86193501563865,19.664038907092028,19.58463718752616,19.810866559819527,19.465937010710118,22.434772773304594,20.445539922646542,18.99187307244663,21.90849954742701,22.314131385426723,20.432733544527707,23.07558442006914,22.830034474576443,21.269107840654502,20.48988421076927,21.083491338486773,20.934278289661282,20.287989620862316,20.04722528013383,21.199347342918216,23.521067577919112,23.2611930742384,21.92570835834572,20.556276718935056,21.41646356230674,22.53034703709958,18.95340785965337,19.986508970321974,19.495183602853512,18.457821713581545,22.886338306822665,19.129172314836154,20.136171638349733,19.376016989288264,20.879430687458026,20.620771994361597,20.30679376611553,20.368798039683195,22.35947129540103,20.069728697762105,22.54894428694912,20.37273542865621,20.71779772905426,19.199159497364995,19.174850117652987,20.13094661361353,20.62659792573699,25.292356917614896,21.758700626292807,21.796149648767223,18.807339197843834,20.506265808043914,22.110290220817404,19.52840260028443,20.325926854481473,19.625476983112655,21.50252421557022,19.304125125188563,19.107865328534132,23.169806519640005,18.840768128990423,19.36444675592656,18.627786784209636,21.17955986366495,20.377078375453138,21.746512886013605,20.910669810022966,23.66715843905666,19.356125235979412,19.33650002206052,17.813568277801206,22.465861054219694,20.57561772108256,19.015107379674305,21.499214768467855,19.66178092422977,17.299495077910098,21.27615053752397,21.040198736231254,21.92098191704396,21.7224558930939,21.092338092530998,20.048671453648097,21.894206057556605,20.415195026786527,20.471143313052202,21.849984955018474,19.59156765743604,20.658934051201378,20.105603080001295,22.787613064091445,18.897059576214055,19.64405870233027,22.74129354031224,21.948058114314257,20.038176858751854,22.177258018525677,22.318758483797644,17.211802761238985,23.061132678317314,19.552166474373674,20.22058498039463,19.1389392082589,19.751662039949778,22.589266875609983,26.340632927208585,19.534708840769273,20.435276534084654,19.903908501005596,19.287439594230733,19.687016476370815,22.7859490833811,20.26081963393427,20.008806098312462,20.432875222497227,21.770367489208184,19.54263516359492,20.167399003287947,20.02036350615788,25.816840819488693,20.66031803587651,19.756693437676223,19.47969541290554,19.46354033785,24.169496968680054,18.739319147762597,21.559823540829672,20.841106100432793,21.572330613609903,23.06348379203432,23.90661007609995,20.544449448312626,18.828527980887348,21.643407148870626,20.295760247970698,21.926818619899166,18.887256320586236,20.26723932452398,20.649629842496918,22.847263558158666,20.639447033358472,21.651692029555726,19.3900555692985,21.40442957359937,20.067715321953298,20.687515795874024,20.646624626390846,19.38789150561276,21.443595032483266,21.960080436904104,20.986097829919892,20.116046923130224,20.38834949087981,21.51694698715695,23.463402138610206,22.107609158967936,21.13017429963753,21.059552411062068,19.918938919026616,19.132327452939787,21.94580529932221,19.159457479949786,19.567551127469855,19.27757204338643,19.477956222501692,20.02620191687006,20.455200776646638,23.044574887856943,24.184179585129318,20.388726808803085,20.116390923202314,21.32891538129463,20.93827950221911,19.878697565839243,20.238709984208988,18.95241706841831,19.63538512176151,20.604884938737765,22.740623426919786,20.3125128271046,24.597055201180662,23.117114991068007,20.78628428680642,19.932433926767427,19.826295549099655,21.26517033895163,22.496434708234126,19.328382384869762,19.427802084161115,20.18180618902739,19.980960001271505,18.996123380968683,22.600876020595507,20.092995805378262,20.046496736817677,21.151917099556776,18.585982112110514,20.695329089308867,22.520164246985647,19.3471325562148,18.999355953219133,23.20757904373024,21.23702785682451,21.08554669425927,20.768225524289374,20.2479251962018,19.3533467533098,22.213653662019038,20.24562900900173,18.67996002744327,21.27136955536682,21.42004604510025,21.047075972047644,21.8035683042629,19.22968489276112,21.077647966937757,23.387816453129698,18.706232935340964,19.58110217755027,23.538867158230946,21.96712491711981,19.68563168309507,20.03480567791853,18.855622835428132,19.611919934246885,20.89874787711282,20.381292625866948,20.623611830893093,21.463850115483186,22.47425899129698,21.46282774650559,20.250449939937386,19.804071134581005,22.940732615911422,24.1535698900103,22.049830938319623,19.98785104241217,22.944212504137,21.578861893090256,25.17177163891695,19.37427511207424,19.96383210404445,24.180920079085045,22.206977518330888,19.158269753875864,19.069924834450198,22.714272038785044,22.147044335773373,20.251879940231216,24.956261862764006,19.58788736648328,21.751091249893005,21.93032100053686,21.621710733177533,19.367732274650958,19.09456742423413,16.917281198342668,20.807982080155547,20.46876211568297,19.988975863285916,20.580262018382033,19.091740116853863,21.44414464440018,21.29242653675922,22.038981262669978,20.180643707721647,23.155520679582406,19.32638298997934,21.506597416475046,20.650611348398325,20.693713245656816,20.540019222244897,21.217521718500812,24.49306731779338,19.642596451766774,22.00972851632279,20.932965503614184,21.834232905374517,21.098649782686852,22.49932865299591,22.579718287600105,21.529871119919466,21.118854440357843,20.944082894927185],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Ridge() vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_pred_logRidge"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["alphasridge = np.logspace(-3, 5, 1000)\n","param_gridRidge = {'ridge__alpha': alphasridge}\n","\n","GridRidge, \\\n","BestParametresRidge, \\\n","ScoresRidge, \\\n","SiteEnergyUse_pred_logRidge, \\\n","figRidge = reg_modelGrid(model=Ridge(),\n","                            scaler=scaler,\n","                            X_train=BEBM_train,\n","                            X_test=BEBM_test,\n","                            y_train=SiteEnergyUse_train_log,\n","                            y_test=SiteEnergyUse_test_log,\n","                            y_test_name='SiteEnergyUse_test_log',\n","                            y_pred_name='SiteEnergyUse_pred_logRidge',\n","                            score=score,\n","                            param_grid=param_gridRidge)\n","\n","print(BestParametresRidge)\n","ScoresRidge\n","figRidge.show()"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[1.9033728857749792,1.9033728850135503,1.903372884237951,1.9033728834479173,1.9033728826431813,1.9033728818234692,1.903372880988502,1.9033728801379959,1.903372879271662,1.9033728783892054,1.9033728774903262,1.9033728765747189,1.9033728756420718,1.903372874692068,1.9033728737243856,1.9033728727386934,1.9033728717346574,1.9033728707119366,1.9033728696701828,1.903372868609042,1.903372867528153,1.9033728664271483,1.9033728653056543,1.903372864163289,1.9033728629996645,1.9033728618143844,1.9033728606070466,1.9033728593772399,1.9033728581245462,1.9033728568485402,1.9033728555487872,1.9033728542248465,1.9033728528762666,1.9033728515025896,1.9033728501033482,1.903372848678067,1.9033728472262612,1.9033728457474375,1.9033728442410927,1.9033728427067143,1.9033728411437814,1.903372839551762,1.9033728379301156,1.9033728362782898,1.9033728345957237,1.903372832881845,1.9033728311360707,1.9033728293578078,1.9033728275464512,1.903372825701385,1.9033728238219823,1.9033728219076043,1.903372819957599,1.9033728179713045,1.9033728159480447,1.9033728138871322,1.9033728117878663,1.9033728096495328,1.903372807471405,1.9033728052527423,1.90337280299279,1.90337280069078,1.90337279834593,1.9033727959574425,1.9033727935245046,1.9033727910462908,1.9033727885219567,1.9033727859506449,1.9033727833314813,1.903372780663575,1.9033727779460192,1.9033727751778897,1.9033727723582452,1.9033727694861278,1.9033727665605604,1.903372763580548,1.9033727605450779,1.903372757453118,1.9033727543036167,1.9033727510955039,1.9033727478276883,1.9033727444990585,1.9033727411084835,1.9033727376548104,1.903372734136865,1.9033727305534505,1.9033727269033491,1.90337272318532,1.903372719398099,1.903372715540398,1.9033727116109056,1.9033727076082863,1.9033727035311785,1.9033726993781965,1.9033726951479282,1.903372690838935,1.9033726864497527,1.903372681978888,1.9033726774248219,1.903372672786005,1.9033726680608605,1.9033726632477825,1.903372658345134,1.9033726533512483,1.9033726482644275,1.903372643082942,1.9033726378050304,1.9033726324288982,1.9033726269527178,1.903372621374627,1.903372615692729,1.9033726099050932,1.903372604009751,1.9033725980046978,1.9033725918878925,1.9033725856572556,1.9033725793106684,1.9033725728459736,1.9033725662609733,1.9033725595534274,1.9033725527210574,1.9033725457615394,1.9033725386725073,1.903372531451551,1.9033725240962158,1.9033725166040008,1.903372508972359,1.9033725011986953,1.9033724932803675,1.9033724852146836,1.9033724769989007,1.9033724686302258,1.903372460105814,1.903372451422768,1.9033724425781344,1.9033724335689073,1.9033724243920225,1.903372415044362,1.9033724055227463,1.9033723958239392,1.9033723859446425,1.9033723758814989,1.9033723656310861,1.9033723551899204,1.9033723445544506,1.9033723337210624,1.9033723226860726,1.9033723114457288,1.9033722999962106,1.9033722883336253,1.903372276454008,1.9033722643533202,1.9033722520274483,1.9033722394722024,1.903372226683313,1.9033722136564333,1.9033722003871347,1.9033721868709061,1.9033721731031534,1.9033721590791945,1.9033721447942635,1.9033721302435036,1.9033721154219687,1.9033721003246196,1.903372084946324,1.9033720692818554,1.9033720533258873,1.9033720370729963,1.9033720205176572,1.9033720036542419,1.9033719864770187,1.9033719689801476,1.903371951157681,1.9033719330035606,1.9033719145116152,1.9033718956755585,1.9033718764889875,1.903371856945381,1.903371837038095,1.903371816760362,1.9033717961052896,1.9033717750658572,1.9033717536349122,1.90337173180517,1.9033717095692109,1.9033716869194754,1.9033716638482658,1.9033716403477392,1.9033716164099075,1.9033715920266345,1.9033715671896314,1.903371541890457,1.903371516120511,1.903371489871035,1.9033714631331062,1.9033714358976372,1.9033714081553696,1.9033713798968754,1.9033713511125494,1.9033713217926074,1.9033712919270855,1.9033712615058307,1.9033712305185055,1.9033711989545767,1.9033711668033177,1.9033711340537998,1.903371100694893,1.9033710667152597,1.9033710321033515,1.9033709968474049,1.9033709609354375,1.903370924355245,1.9033708870943955,1.9033708491402248,1.903370810479835,1.9033707711000876,1.9033707309875996,1.9033706901287393,1.9033706485096207,1.9033706061161013,1.9033705629337738,1.9033705189479637,1.903370474143724,1.9033704285058293,1.9033703820187704,1.9033703346667514,1.9033702864336814,1.9033702373031702,1.903370187258524,1.9033701362827369,1.9033700843584878,1.903370031468134,1.9033699775937039,1.9033699227168916,1.9033698668190517,1.9033698098811915,1.903369751883965,1.903369692807668,1.9033696326322278,1.9033695713371999,1.903369508901759,1.9033694453046937,1.9033693805243974,1.9033693145388615,1.9033692473256694,1.9033691788619869,1.9033691091245557,1.9033690380896853,1.9033689657332442,1.9033688920306528,1.9033688169568752,1.903368740486409,1.903368662593278,1.9033685832510248,1.903368502432698,1.9033684201108472,1.9033683362575105,1.9033682508442062,1.9033681638419253,1.903368075221118,1.9033679849516854,1.90336789300297,1.9033677993437443,1.9033677039422006,1.9033676067659395,1.903367507781962,1.9033674069566526,1.9033673042557737,1.9033671996444508,1.9033670930871622,1.9033669845477246,1.9033668739892842,1.9033667613743028,1.9033666466645431,1.903366529821059,1.9033664108041812,1.9033662895735024,1.9033661660878658,1.9033660403053503,1.9033659121832556,1.9033657816780891,1.9033656487455501,1.9033655133405158,1.903365375417024,1.9033652349282622,1.9033650918265455,1.9033649460633053,1.903364797589072,1.9033646463534553,1.9033644923051323,1.903364335391825,1.9033641755602873,1.9033640127562825,1.903363846924569,1.9033636780088798,1.903363505951902,1.903363330695262,1.9033631521795011,1.9033629703440578,1.9033627851272463,1.9033625964662382,1.9033624042970374,1.9033622085544621,1.9033620091721204,1.90336180608239,1.9033615992163946,1.9033613885039782,1.903361173873686,1.9033609552527377,1.9033607325670032,1.9033605057409784,1.9033602746977583,1.9033600393590135,1.9033597996449623,1.9033595554743428,1.9033593067643895,1.9033590534308005,1.9033587953877134,1.9033585325476736,1.9033582648216067,1.903357992118788,1.9033577143468114,1.9033574314115604,1.9033571432171736,1.9033568496660156,1.903356550658642,1.903356246093767,1.9033559358682297,1.9033556198769592,1.9033552980129396,1.903354970167173,1.9033546362286464,1.9033542960842902,1.9033539496189427,1.9033535967153123,1.903353237253937,1.9033528711131442,1.9033524981690122,1.903352118295326,1.9033517313635362,1.9033513372427184,1.9033509357995264,1.9033505268981485,1.9033501104002633,1.903349686164993,1.9033492540488564,1.9033488139057213,1.9033483655867556,1.903347908940378,1.903347443812208,1.9033469700450136,1.9033464874786605,1.903345995950056,1.903345495293101,1.9033449853386266,1.9033444659143441,1.9033439368447866,1.9033433979512488,1.9033428490517312,1.9033422899608765,1.90334172048991,1.9033411404465777,1.903340549635081,1.9033399478560136,1.9033393349062948,1.9033387105791033,1.903338074663808,1.9033374269459,1.903336767206919,1.9033360952243854,1.9033354107717237,1.9033347136181908,1.9033340035287978,1.9033332802642327,1.9033325435807853,1.9033317932302634,1.9033310289599128,1.9033302505123355,1.903329457625404,1.9033286500321764,1.903327827460809,1.9033269896344667,1.9033261362712337,1.903325267084022,1.9033243817804757,1.9033234800628782,1.903322561628054,1.9033216261672692,1.9033206733661339,1.9033197029044977,1.9033187144563477,1.9033177076897008,1.9033166822664982,1.903315637842495,1.9033145740671482,1.9033134905835059,1.9033123870280904,1.9033112630307802,1.9033101182146932,1.903308952196064,1.903307764584121,1.9033065549809607,1.9033053229814194,1.903304068172946,1.903302790135467,1.9033014884412545,1.9033001626547876,1.9032988123326142,1.9032974370232125,1.9032960362668425,1.9032946095954042,1.903293156532286,1.9032916765922152,1.9032901692811042,1.9032886340958943,1.9032870705243954,1.9032854780451292,1.9032838561271572,1.9032822042299204,1.9032805218030675,1.9032788082862815,1.903277063109104,1.9032752856907595,1.903273475439971,1.9032716317547762,1.9032697540223427,1.9032678416187743,1.9032658939089195,1.9032639102461744,1.9032618899722813,1.903259832417128,1.9032577368985393,1.903255602722068,1.903253429180781,1.9032512155550438,1.9032489611122998,1.9032466651068447,1.9032443267796026,1.9032419453578922,1.9032395200551935,1.90323705007091,1.9032345345901256,1.9032319727833573,1.9032293638063091,1.9032267067996151,1.9032240008885828,1.9032212451829316,1.9032184387765274,1.9032155807471103,1.9032126701560248,1.9032097060479365,1.9032066874505549,1.9032036133743406,1.9032004828122193,1.903197294739282,1.9031940481124892,1.9031907418703606,1.9031873749326713,1.9031839462001348,1.9031804545540845,1.9031768988561513,1.903173277947936,1.9031695906506745,1.9031658357649015,1.9031620120701072,1.9031581183243884,1.9031541532641,1.9031501156034902,1.903146004034344,1.903141817225611,1.903137553823034,1.9031332124487683,1.9031287917009998,1.9031242901535532,1.9031197063554999,1.9031150388307547,1.9031102860776712,1.9031054465686297,1.9031005187496202,1.903095501039821,1.9030903918311666,1.9030851894879164,1.9030798923462118,1.9030744987136328,1.9030690068687435,1.9030634150606347,1.903057721508461,1.9030519244009674,1.9030460218960172,1.9030400121201048,1.903033893167872,1.9030276631016079,1.9030213199507517,1.9030148617113825,1.9030082863457047,1.9030015917815313,1.9029947759117523,1.902987836593804,1.9029807716491267,1.9029735788626212,1.9029662559820906,1.9029588007176854,1.9029512107413311,1.9029434836861605,1.9029356171459266,1.9029276086744205,1.9029194557848732,1.9029111559493574,1.9029027065981765,1.902894105119249,1.9028853488574875,1.9028764351141692,1.9028673611462945,1.9028581241659488,1.9028487213396457,1.9028391497876704,1.9028294065834122,1.9028194887526915,1.902809393273077,1.9027991170731968,1.9027886570320427,1.9027780099782632,1.9027671726894533,1.9027561418914334,1.9027449142575215,1.902733486407795,1.90272185490835,1.902710016270546,1.9026979669502455,1.9026857033470468,1.9026732218035043,1.902660518604344,1.902647589975669,1.9026344320841564,1.9026210410362432,1.9026074128773083,1.9025935435908417,1.902579429097603,1.9025650652547779,1.9025504478551158,1.9025355726260647,1.9025204352288938,1.902505031257808,1.9024893562390486,1.9024734056299883,1.9024571748182146,1.9024406591206007,1.9024238537823663,1.9024067539761322,1.902389354800956,1.9023716512813635,1.902353638366364,1.9023353109284582,1.9023166637626274,1.902297691585321,1.9022783890334196,1.9022587506631965,1.902238770949257,1.9022184442834724,1.9021977649738946,1.9021767272436616,1.9021553252298848,1.9021335529825265,1.9021114044632579,1.9020888735443076,1.9020659540072902,1.902042639542024,1.9020189237453269,1.9019948001198035,1.9019702620726087,1.9019453029142,1.901919915857071,1.9018940940144653,1.9018678303990761,1.9018411179217256,1.9018139493900263,1.901786317507026,1.9017582148698282,1.9017296339682002,1.901700567183158,1.9016710067855296,1.901640944934502,1.9016103736761458,1.9015792849419195,1.9015476705471535,1.9015155221895106,1.9014828314474297,1.9014495897785448,1.9014157885180825,1.901381418877241,1.9013464719415434,1.9013109386691718,1.9012748098892793,1.901238076300282,1.9012007284681247,1.9011627568245302,1.901124151665226,1.901084903148148,1.9010450012916285,1.901004435972555,1.9009631969245224,1.900921273735951,1.9008786558481987,1.9008353325536458,1.9007912929937683,1.9007465261571899,1.9007010208777242,1.9006547658323953,1.9006077495394489,1.9005599603563514,1.9005113864777736,1.9004620159335694,1.9004118365867417,1.9003608361314046,1.9003090020907396,1.9002563218149444,1.900202782479188,1.900148371081558,1.9000930744410163,1.9000368791953561,1.8999797717991687,1.8999217385218188,1.8998627654454314,1.8998028384628949,1.8997419432758833,1.8996800653928978,1.8996171901273335,1.8995533025955715,1.8994883877151083,1.8994224302027114,1.8993554145726192,1.899287325134783,1.89921814599315,1.8991478610440027,1.8990764539743483,1.8990039082603722,1.8989302071659473,1.898855333741217,1.8987792708212516,1.898702001024778,1.8986235067529922,1.8985437701884664,1.8984627732941388,1.89838049781241,1.8982969252643358,1.8982120369489377,1.8981258139426185,1.898038237098706,1.8979492870471162,1.8978589441941558,1.8977671887224545,1.8976740005910486,1.8975793595356083,1.8974832450688262,1.8973856364809631,1.897286512840567,1.897185852995363,1.8970836355733258,1.8969798389839414,1.8968744414196572,1.8967674208575374,1.896658755061124,1.8965484215825092,1.8964363977646272,1.8963226607437733,1.896207187452357,1.8960899546218868,1.8959709387862123,1.8958501162850048,1.8957274632675076,1.895602955696544,1.8954765693528,1.8953482798393815,1.8952180625866606,1.8950858928574086,1.8949517457522251,1.8948155962152775,1.8946774190403386,1.8945371888771525,1.8943948802381148,1.8942504675052867,1.8941039249377376,1.8939552266792394,1.8938043467663,1.8936512591365549,1.8934959376375171,1.8933383560356976,1.8931784880260962,1.8930163072420716,1.8928517872656005,1.8926849016379255,1.8925156238706033,1.8923439274569607,1.8921697858839557,1.8919931726444648,1.8918140612499879,1.89163242524379,1.8914482382144757,1.8912614738100146,1.8910721057522095,1.8908801078516337,1.8906854540230202,1.8904881183011344,1.8902880748571185,1.8900852980153258,1.8898797622706431,1.8896714423063194,1.8894603130122962,1.889246349504057,1.8890295271419955,1.8888098215513147,1.888587208642464,1.8883616646321209,1.8881331660647187,1.8879016898345475,1.8876672132084078,1.8874297138488507,1.8871891698379948,1.886945559701939,1.8866988624357688,1.8864490575291746,1.8861961249926804,1.8859400453844977,1.8856807998380063,1.885418370089876,1.8851527385088294,1.8848838881250605,1.8846118026603116,1.8843364665586144,1.8840578650177107,1.883775984021146,1.8834908103710521,1.8832023317216258,1.8829105366132985,1.882615414507616,1.8823169558228194,1.8820151519701462,1.8817099953908378,1.8814014795938767,1.881089599194437,1.880774349953062,1.8804557288155679,1.8801337339536695,1.8798083648063328,1.8794796221218495,1.879147508000629,1.878812025938713,1.8784731808719974,1.8781309792211602,1.8777854289372897,1.8774365395482029,1.877084322205438,1.876728789731923,1.8763699566702918,1.8760078393318445,1.8756424558461249,1.8752738262111062,1.87490197234396,1.8745269181323798,1.874148689486447,1.8737673143910005,1.8733828229584837,1.872995247482245,1.8726046224902426,1.872210984799129,1.871814373568672,1.8714148303564673,1.871012399172899,1.8706071265363042,1.8701990615282802,1.8697882558490952,1.8693747638731317,1.8689586427043081,1.8685399522314186,1.8681187551833147,1.867695117183872,1.867269106806652,1.8668407956292001,1.866410258286883,1.8659775725261938,1.8655428192574335,1.8651060826066714,1.864667449966906,1.864227012048311,1.8637848629274765,1.863341100095538,1.8628958245050864,1.8624491406157417,1.8620011564382835,1.861551983577216,1.861101737271649,1.8606505364343646,1.860198503688953,1.859745765404877,1.8592924517303373,1.8588386966228054,1.8583846378770787,1.8579304171507278,1.857476179986786,1.8570220758335367,1.85656825806126,1.8561148839757835,1.855662114828693,1.8552101158240504,1.8547590561214722,1.8543091088354124,1.8538604510305063,1.8534132637128131,1.8529677318168223,1.8525240441880555,1.852082393561134,1.851642976533149,1.8512059935321994,1.8507716487809518,1.8503401502550818,1.8499117096364572,1.8494865422609368,1.8490648670606475,1.848646906500619,1.848232886509657,1.8478230364053387,1.8474175888130202,1.8470167795787589,1.84662084767605,1.8462300351062968,1.8458445867929263,1.8454647504690864,1.8450907765588618,1.84472291805195,1.8443614303717677,1.8440065712369418,1.8436586005161768,1.8433177800764855,1.8429843736247857,1.8426586465428834,1.8423408657158693,1.842031299353979,1.8417302168079648,1.8414378883780653,1.841154585116649,1.8408805786246467,1.840616140841885,1.840361543831463,1.8401170595583214,1.8398829596621784,1.8396595152250164,1.839446996533327,1.8392456728353381,1.8390558120934586,1.8388776807322067,1.8387115433818935,1.8385576626183535,1.8384162986990447,1.838287709295831,1.8381721492248133,1.8380698701735554,1.8379811204260985,1.8379061445861526,1.8378451832988838,1.8377984729717152,1.837766245494602,1.837748727960212,1.8377461423845034,1.837758705428163,1.8377866281194095,1.837830115578655,1.8378893667455372,1.8379645741088466,1.8380559234398581,1.8381635935296163,1.8382877559306912,1.8384285747039446,1.8385862061708458,1.8387607986718666,1.8389524923314897,1.839161418830357,1.8393877011850777,1.8396314535362133,1.8398927809449426,1.840171779198894,1.8404685346276324,1.8407831239282628,1.8411156140015947,1.8414660617993053,1.841834514182505,1.8422210077920966,1.8426255689312938,1.8430482134606436,1.8434889467058533,1.8439477633787333,1.8444246475114927,1.8449195724046252,1.8454325005885892,1.8459633837994383,1.8465121629685413,1.8470787682264924,1.8476631189212747,1.848265123650713,1.8488846803092112,1.8495216761487339,1.850175987853967,1.8508474816315448,1.8515360133132035,1.8522414284726856,1.8529635625561884,1.8537022410261124,1.8544572795178378,1.8552284840092266,1.8560156510025076,1.856818567718197,1.8576370123006452,1.8584707540348133,1.8593195535738225,1.8601831631768213,1.8610613269566874,1.861953781137051,1.8628602543181327,1.8637804677508427,1.8647141356186008,1.8656609653263108,1.8666206577959117,1.8675929077679332,1.8685774041084566,1.8695738301208966,1.8705818638620049,1.871601178461501,1.8726314424447374,1.8736723200578038,1.8747234715944845,1.8757845537244904,1.8768552198223882,1.8779351202966654,1.8790239029183788,1.8801212131488392,1.8812266944658105,1.8823399886877106,1.8834607362953015,1.8845885767504114,1.8857231488111978,1.8868640908435226,1.8880110411280093,1.8891636381623627,1.890321520958579,1.8914843293346653,1.892651704200529,1.893823287837704,1.8949987241726176,1.896177659043106,1.8973597404579177,1.8985446188489667,1.8997319473161114,1.9009213818642603,1.90211258163263,1.9033052091159894,1.9044989303777584,1.905693415254841,1.9068883375540924,1.908083375240339,1.909278210615892,1.9104725304915084,1.9116660263487701,1.9128583944938753,1.9140493362028437,1.9152385578581579,1.9164257710768768,1.917610692830269,1.9187930455550315,1.919972557256169,1.9211489616016213,1.922321998008739,1.923491411722718,1.9246569538871134,1.9258183816065615,1.926975458001852,1.9281279522574963,1.929275639661943,1.9304183016406107,1.9315557257818976,1.932687705856349,1.9338140418291547,1.9349345398661726,1.9360490123336493,1.9371572777918462,1.9382591609827542,1.9393544928121025,1.9404431103258528,1.9415248566813907,1.9425995811136048,1.943667138896067,1.9447273912975114,1.9457802055338178,1.946825454715701,1.9478630177923126,1.9488927794909503,1.949914630253081,1.950928466166871,1.9519341888964248,1.9529317056079172,1.9539209288928228,1.9549017766884211]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[2.1863307890717723,2.1863307871231883,2.18633078513834,2.1863307831165537,2.1863307810571415,2.186330778959404,2.1863307768226274,2.1863307746460845,2.1863307724290366,2.186330770170729,2.186330767870394,2.186330765527249,2.1863307631404982,2.186330760709329,2.1863307582329177,2.186330755710418,2.186330753140975,2.186330750523714,2.1863307478577463,2.1863307451421647,2.186330742376045,2.1863307395584473,2.186330736688415,2.18633073376497,2.1863307307871205,2.186330727753852,2.1863307246641352,2.1863307215169177,2.18633071831113,2.186330715045683,2.1863307117194655,2.1863307083313472,2.186330704880175,2.186330701364777,2.1863306977839563,2.1863306941364966,2.1863306904211575,2.186330686636676,2.186330682781765,2.1863306788551125,2.186330674855386,2.1863306707812242,2.186330666631242,2.186330662404028,2.1863306580981456,2.1863306537121305,2.186330649244492,2.18633064469371,2.186330640058237,2.1863306353364984,2.1863306305268875,2.1863306256277704,2.18633062063748,2.1863306155543203,2.1863306103765625,2.1863306051024467,2.1863305997301796,2.1863305942579343,2.18633058868385,2.1863305830060322,2.18633057722255,2.1863305713314367,2.1863305653306906,2.1863305592182702,2.186330552992097,2.186330546650056,2.186330540189988,2.1863305336096994,2.1863305269069513,2.1863305200794647,2.1863305131249193,2.1863305060409495,2.1863304988251464,2.1863304914750588,2.186330483988186,2.1863304763619817,2.1863304685938543,2.186330460681163,2.1863304526212164,2.1863304444112748,2.186330436048546,2.186330427530187,2.1863304188533026,2.186330410014942,2.1863304010121,2.1863303918417154,2.1863303825006706,2.1863303729857897,2.186330363293838,2.18633035342152,2.186330343365478,2.186330333122295,2.1863303226884874,2.1863303120605075,2.186330301234742,2.18633029020751,2.1863302789750634,2.1863302675335814,2.1863302558791755,2.1863302440078827,2.186330231915666,2.1863302195984162,2.1863302070519435,2.1863301942719833,2.18633018125419,2.1863301679941385,2.186330154487319,2.1863301407291407,2.186330126714925,2.186330112439908,2.1863300978992353,2.186330083087964,2.186330068001059,2.186330052633389,2.1863300369797307,2.186330021034762,2.1863300047930605,2.1863299882491063,2.1863299713972735,2.186329954231831,2.1863299367469464,2.1863299189366727,2.186329900794955,2.1863298823156256,2.1863298634924018,2.1863298443188834,2.1863298247885528,2.1863298048947692,2.18632978463077,2.1863297639896655,2.1863297429644377,2.186329721547937,2.1863296997328847,2.1863296775118637,2.1863296548773183,2.186329631821554,2.1863296083367314,2.1863295844148674,2.1863295600478283,2.18632953522733,2.1863295099449345,2.1863294841920466,2.1863294579599097,2.186329431239608,2.1863294040220542,2.186329376297998,2.1863293480580124,2.186329319292496,2.1863292899916713,2.1863292601455755,2.1863292297440626,2.186329198776797,2.18632916723325,2.1863291351027,2.1863291023742213,2.186329069036688,2.186329035078767,2.1863290004889144,2.186328965255371,2.1863289293661565,2.1863288928090725,2.1863288555716904,2.186328817641351,2.1863287790051595,2.186328739649981,2.1863286995624382,2.1863286587289004,2.1863286171354885,2.1863285747680616,2.186328531612217,2.186328487653285,2.18632844287632,2.186328397266101,2.1863283508071234,2.1863283034835934,2.186328255279424,2.1863282061782274,2.186328156163314,2.1863281052176804,2.1863280533240084,2.1863280004646573,2.186327946621659,2.1863278917767106,2.1863278359111664,2.186327779006039,2.186327721041982,2.186327661999294,2.186327601857903,2.186327540597367,2.1863274781968602,2.1863274146351723,2.1863273498906985,2.186327283941429,2.1863272167649477,2.18632714833842,2.1863270786385876,2.186327007641757,2.186326935323797,2.1863268616601257,2.186326786625704,2.186326710195029,2.1863266323421193,2.186326553040515,2.18632647226326,2.1863263899829,2.1863263061714666,2.1863262208004746,2.1863261338409066,2.1863260452632063,2.1863259550372676,2.1863258631324234,2.1863257695174387,2.186325674160495,2.1863255770291823,2.1863254780904886,2.1863253773107885,2.1863252746558297,2.1863251700907234,2.186325063579931,2.1863249550872554,2.1863248445758243,2.186324732008079,2.1863246173457638,2.1863245005499117,2.186324381580829,2.186324260398086,2.1863241369605007,2.1863240112261226,2.1863238831522254,2.186323752695285,2.1863236198109677,2.186323484454119,2.18632334657874,2.1863232061379794,2.1863230630841146,2.1863229173685337,2.1863227689417215,2.186322617753245,2.1863224637517296,2.1863223068848465,2.1863221470992946,2.1863219843407817,2.1863218185540054,2.186321649682636,2.1863214776692956,2.18632130245554,2.186321123981838,2.1863209421875527,2.1863207570109187,2.186320568389022,2.186320376257781,2.186320180551921,2.186319981204953,2.186319778149156,2.186319571315545,2.186319360633857,2.186319146032521,2.1863189274386334,2.18631870477794,2.1863184779748046,2.186318246952183,2.1863180116316028,2.18631777193313,2.186317527775347,2.1863172790753205,2.1863170257485804,2.186316767709082,2.186316504869185,2.1863162371396174,2.186315964429453,2.186315686646071,2.1863154036951324,2.186315115480545,2.1863148219044297,2.1863145228670904,2.1863142182669795,2.1863139080006593,2.1863135919627745,2.18631327004601,2.186312942141058,2.1863126081365802,2.1863122679191695,2.186311921373314,2.186311568381353,2.186311208823444,2.186310842577517,2.1863104695192344,2.1863100895219505,2.1863097024566667,2.18630930819199,2.1863089065940864,2.1863084975266385,2.1863080808507953,2.186307656425129,2.186307224105588,2.186306783745441,2.186306335195239,2.186305878302756,2.186305412912938,2.1863049388678553,2.186304456006648,2.186303964165467,2.1863034631774263,2.186302952872539,2.186302433077667,2.1863019036164584,2.1863013643092857,2.1863008149731935,2.186300255421829,2.1862996854653836,2.1862991049105265,2.1862985135603403,2.186297911214256,2.186297297667985,2.186296672713447,2.1862960361387067,2.1862953877278946,2.1862947272611417,2.1862940545144998,2.186293369259869,2.1862926712649218,2.1862919602930226,2.1862912361031506,2.1862904984498153,2.1862897470829785,2.1862889817479676,2.18628820218539,2.186287408131047,2.1862865993158445,2.1862857754657044,2.186284936301468,2.1862840815388096,2.1862832108881345,2.1862823240544853,2.1862814207374424,2.1862805006310237,2.1862795634235805,2.1862786087976973,2.1862776364300793,2.1862766459914478,2.1862756371464322,2.1862746095534544,2.1862735628646144,2.1862724967255764,2.186271410775449,2.1862703046466647,2.186269177964858,2.1862680303487387,2.1862668614099667,2.1862656707530213,2.186264457975069,2.186263222665831,2.1862619644074415,2.186260682774319,2.1862593773330117,2.186258047642062,2.1862566932518566,2.1862553137044767,2.186253908533548,2.1862524772640834,2.186251019412325,2.1862495344855857,2.186248021982084,2.1862464813907776,2.1862449121911944,2.186243313853261,2.1862416858371247,2.1862400275929788,2.1862383385608726,2.186236618170537,2.1862348658411874,2.1862330809813364,2.1862312629885965,2.18622941124948,2.186227525139201,2.186225604021465,2.1862236472482612,2.1862216541596493,2.1862196240835408,2.186217556335479,2.186215450218415,2.1862133050224752,2.186211120024732,2.186208894488966,2.186206627665421,2.186204318790563,2.1862019670868276,2.186199571762365,2.1861971320107823,2.1861946470108804,2.186192115926385,2.1861895379056726,2.186186912081496,2.1861842375706964,2.1861815134739224,2.186178738875332,2.1861759128422977,2.1861730344251034,2.1861701026566336,2.1861671165520637,2.1861640751085396,2.1861609773048514,2.1861578221011024,2.1861546084383794,2.1861513352384025,2.1861480014031835,2.186144605814669,2.1861411473343804,2.1861376248030515,2.1861340370402504,2.1861303828440066,2.1861266609904204,2.1861228702332745,2.1861190093036327,2.186115076909439,2.186111071735097,2.1861069924410623,2.1861028376634035,2.1860986060133762,2.1860942960769796,2.1860899064145087,2.1860854355600954,2.186080882021249,2.186076244278379,2.1860715207843198,2.1860667099638413,2.186061810213153,2.1860568198993997,2.186051737360149,2.186046560902869,2.1860412888044025,2.1860359193104215,2.1860304506348864,2.186024880959485,2.186019208433069,2.1860134311710766,2.186007547254951,2.1860015547315443,2.1859954516125124,2.185989235873705,2.185982905454539,2.1859764582573664,2.1859698921468276,2.185963204949199,2.1859563944517304,2.185949458401964,2.1859423945070513,2.1859352004330566,2.1859278738042427,2.1859204122023614,2.1859128131659094,2.1859050741893977,2.1858971927225883,2.185889166169732,2.1858809918887894,2.185872667190641,2.1858641893382815,2.185855555546008,2.1858467629785925,2.1858378087504353,2.18582868992472,2.185819403512541,2.185809946472025,2.185800315707439,2.185790508068282,2.1857805203483642,2.1857703492848786,2.1857599915574406,2.185749443787139,2.185738702535551,2.1857277643037514,2.1857166255313114,2.185705282595272,2.185693731809113,2.1856819694217013,2.1856699916162223,2.1856577945091007,2.185645374148904,2.185632726515227,2.185619847517568,2.1856067329941795,2.185593378710908,2.1855797803600185,2.1855659335590003,2.1855518338493556,2.1855374766953726,2.1855228574828818,2.185507971517992,2.1854928140258183,2.1854773801491745,2.185461664947272,2.185445663394378,2.185429370378474,2.185412780699882,2.1853958890698792,2.185378690109301,2.18536117834711,2.1853433482189586,2.185325194065728,2.1853067101320516,2.1852878905648105,2.1852687294116246,2.185249220619306,2.1852293580323154,2.1852091353911693,2.18518854633086,2.1851675843792284,2.185146242955336,2.185124515367801,2.185102394813125,2.1850798743739963,2.1850569470175714,2.1850336055937323,2.1850098428333298,2.1849856513464014,2.1849610236203665,2.184935952018203,2.184910428776602,2.184884446004099,2.1848579956791814,2.184831069648382,2.184803659624337,2.184775757183834,2.1847473537658297,2.1847184406694478,2.1846890090519513,2.1846590499266942,2.1846285541610504,2.184597512474311,2.184565915435572,2.1845337534615803,2.184501016814571,2.1844676956000733,2.1844337797646864,2.18439925909384,2.184364123209524,2.184328361567993,2.1842919634574427,2.1842549179956685,2.184217214127685,2.1841788406233276,2.184139786074826,2.184100038894346,2.1840595873115034,2.184018419370859,2.1839765229293686,2.1839338856538197,2.1838904950182263,2.1838463383012026,2.1838014025833,2.183755674744315,2.183709141460568,2.183661789202147,2.183613604230114,2.1835645725936943,2.1835146801274083,2.1834639124481923,2.1834122549524637,2.183359692813168,2.1833062109767742,2.183251794160245,2.183196426847959,2.1831400932886007,2.183082777492008,2.18302446322598,2.1829651340130414,2.18290477312717,2.1828433635904707,2.1827808881698187,2.182717329373445,2.182652669447482,2.182586890372465,2.1825199738597756,2.1824519013480455,2.1823826539995035,2.1823122126962735,2.1822405580366215,2.182167670331141,2.1820935295988946,2.1820181155634897,2.1819414076491013,2.181863384976434,2.181784026358626,2.181703310297088,2.1816212149772873,2.1815377182644555,2.181452797699243,2.1813664304933016,2.1812785935248002,2.1811892633338723,2.1810984161179903,2.1810060277272747,2.1809120736597225,2.180816529056375,2.180719368696391,2.180620566992066,2.180520097983759,2.180417935334748,2.1803140523260067,2.180208421850899,2.1801010164097967,2.1799918081046123,2.1798807686332546,2.1797678692839972,2.1796530809297705,2.179536374022362,2.179417718586545,2.1792970842141144,2.179174440057842,2.179049754825355,2.1789229967729224,2.1787941336991654,2.1786631329386856,2.178529961355612,2.1783945853370703,2.1782569707865687,2.178117083117316,2.1779748872454556,2.1778303475832317,2.1776834280320836,2.1775340919756694,2.177382302272826,2.177228021250463,2.1770712106963996,2.176911831852142,2.17674984540561,2.176585211483814,2.1764178896454824,2.17624783887366,2.176075017568257,2.1758993835385794,2.175720893995829,2.1755395055455855,2.1753551741802757,2.1751678552716416,2.174977503563202,2.174784073162728,2.1745875175347305,2.174387789492978,2.17418484119304,2.173978624124872,2.1737690891054617,2.173556186271516,2.1733398650722373,2.1731200742621652,2.1728967618941204,2.172669875312242,2.1724393611451434,2.1722051652991903,2.1719672329519204,2.171725508545609,2.1714799357810026,2.1712304576112245,2.170977016235878,2.1707195530953456,2.170458008865317,2.170192323451547,2.169922435984864,2.169648284816448,2.169369807513384,2.169086940854524,2.1687996208266584,2.168507782621021,2.1682113606301487,2.1679102884451087,2.1676044988531156,2.1672939238355498,2.166978494566411,2.1666581414112103,2.1663327939263355,2.1660023808588944,2.1656668301470763,2.1653260689210305,2.164980023504309,2.164628619415871,2.1642717813726864,2.1639094332929645,2.1635414983000074,2.163167898726749,2.1627885561209688,2.1624033912512264,2.1620123241135305,2.1616152739387813,2.1612121592009936,2.160802897626342,2.1603874062030473,2.159965601192136,2.1595373981391,2.159102711886478,2.158661456587398,2.158213545720101,2.1577588921034834,2.157297407913682,2.156829004701734,2.15635359341235,2.1558710844038176,2.155381387469087,2.1548844118580495,2.154380066301071,2.1538682590337794,2.153348897823178,2.1528218899950833,2.1522871424629613,2.1517445617581634,2.151194054061626,2.150635525237052,2.1500688808656285,2.1494940262823112,2.148910866613712,2.148319306817637,2.1477192517243155,2.1471106060793534,2.146493274588463,2.1458671619639986,2.1452321729733597,2.144588212489281,2.143935185542076,2.143272997373857,2.1426015534947975,2.141920759741458,2.141230522337241,2.140530747955008,2.1398213437819056,2.1391022175864474,2.138373277787897,2.1376344335279955,2.136885594745077,2.1361266722506236,2.135357577808289,2.134578224215457,2.133788525387353,2.132988396443765,2.1321777537984183,2.1313565152510314,2.130524600082104,2.1296819291504656,2.128828424993633,2.127964011930994,2.127088616169869,2.126202165914468,2.125304591477777,2.1243958253963995,2.123475802548378,2.1225444602740127,2.1216017384996997,2.120647579864796,2.1196819298515366,2.118704736917992,2.1177159526340863,2.1167155318206583,2.1157034326915833,2.114679616998913,2.1136440501810507,2.112596701513916,2.111537544265088,2.11046655585088,2.1093837179963217,2.1082890168979938,2.1071824433896578,2.1060639931106357,2.10493366667686,2.103791469854514,2.102637413736201,2.101471514919519,2.100293795687966,2.0991042841940484,2.097903014644483,2.0966900274873517,2.0954653696010777,2.094229094485069,2.0929812624518633,2.0917219408206096,2.09045120411169,2.0891691342422973,2.0878758207227484,2.086571360853324,2.085255859921382,2.083929431398525,2.082592197137525,2.081244287568768,2.0798858418959023,2.0785170082903988,2.077137944084705,2.0757488159636615,2.0743498001538283,2.0729410826103636,2.0715228592010764,2.070095335887253,2.0686587289008527,2.067213264917649,2.0657591812258613,2.064296725889834,2.062826157908272,2.0613477473665505,2.0598617755825797,2.0583685352457093,2.05686833054811,2.0553614773080953,2.0538483030847803,2.0523291472834955,2.0508043612513385,2.0492743083622376,2.0477393640908734,2.0461999160747975,2.044656364164074,2.0431091204577365,2.0415586093263673,2.0400052674200495,2.0384495436609775,2.0368918992199503,2.0353328074759993,2.033772753958363,2.0322122362700243,2.03065176399202,2.0290918585677193,2.0275330531662545,2.0259758925243117,2.0244209327654574,2.0228687411962016,2.021319896077994,2.0197749863743617,2.0182346114724012,2.0166993808778773,2.0151699138831662,2.013646839207349,2.012130794607752,2.010622426462304,2.009122389322096,2.007631345433601,2.006149964230053,2.0046789217915677,2.0032189002736387,2.0017705873037537,2.000334675345943,1.9989118610331882,1.997502844467728,1.9961083284894197,1.9947290179124426,1.9933656187307838,1.9920188372930907,1.9906893794476408,1.989377949658361,1.9880852500930017,1.986811979684774,1.9855588331689589,1.9843265000962036,1.983115663824445,1.9819270004916107,1.98076117797149,1.9796188548153761,1.9785006791823287,1.9774072877611077,1.9763393046870772,1.9752973404575591,1.9742819908493525,1.973293835842287,1.9723334385528872,1.9714013441823452,1.9704980789831492,1.9696241492488054,1.9687800403311866,1.9679662156900424,1.9671831159792634,1.966431158174422,1.9657107347460636,1.9650222128831123,1.9643659337705939,1.9637422119257029,1.9631513345959806,1.9625935612231316,1.9620691229756524,1.9615782223531146,1.9611210328645556,1.9606976987829987,1.960308334977689,1.9599530268251517,1.9596318301996913,1.959344771543459,1.9590918480156938,1.9588730277202362,1.9586882500099208,1.9585374258659507,1.9584204383498778,1.958337143125379,1.9582873690465727,1.9582709188092426,1.95828756966098,1.958337074165945,1.958419161019662,1.9585335359090816,1.958679882412925,1.958857862937221,1.9590671196808866,1.9593072756261385,1.9595779355485727,1.9598786870417972,1.9602091015516132,1.9605687354148895,1.960957130898447,1.9613738172334878,1.9618183116413497,1.96229012034662,1.962788739573933,1.963313656525077,1.9638643503333408,1.9644402929923506,1.9650409502569697,1.965665782514153,1.9663142456219556,1.9669857917152298,1.9676798699768099,1.968395927373311,1.9691334093549102,1.9698917605187622,1.9706704252359377,1.9714688482419895,1.9722864751914821,1.9731227531769822,1.973977131213202,1.9748490606871232,1.975737995775068,1.9766433938278067,1.9775647157248695,1.9785014261993317,1.9794529941343908,1.9804188928331175,1.981398600262782,1.9823915992751926,1.9833973778044893,1.9844154290438314,1.9854452516024115,1.9864863496442053,1.9875382330098426,1.9886004173229392,1.9896724240822032,1.990753780740574,1.9918440207725903,1.9929426837311601,1.9940493152948098,1.995163467306452,1.9962846978046556,1.9974125710483115,1.9985466575355582,1.999686534017738,2.0008317835091165,2.0019819952930136,2.0031367649249603,2.0042956942334067,2.00545839131847,2.0066244705491534,2.007793552559401,2.0089652642433227,2.010139238749858,2.0113151154771045,2.0124925400665097,2.0136711643970613,2.0148506465795917,2.0160306509512695,2.017210848070319,2.018390914710988,2.0195705338587464,2.020749394705679,2.021927192646025,2.0231036292717803,2.0242784123682744,2.0254512559096236,2.026621880053937,2.027790011138159,2.028955381672411,2.030117730333701,2.0312768019588594,2.0324323475365587,2.0335841241982755,2.0347318952080657,2.0358754299509974,2.037014503920125,2.038148898701867,2.039278401959674,2.040402807415856,2.0415219148314807,2.042635529984236,2.0437434646441512,2.0448455365471205,2.0459415693661382,2.047031392680188,2.048114841940733,2.049191758435769,2.0502619892513936,2.0513253872308885,2.0523818109312777,2.05343112457738,2.0544731980133424,2.0555079066516835,2.0565351314198566,2.057554758704377,2.05856668029254,2.059570793311789,2.0605670001667735,2.061555208474174,2.0625353309953467]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[1.6204149824781862,1.6204149829039123,1.6204149833375616,1.620414983779281,1.6204149842292208,1.6204149846875342,1.6204149851543765,1.6204149856299073,1.6204149861142874,1.620414986607682,1.6204149871102589,1.6204149876221887,1.6204149881436456,1.6204149886748067,1.6204149892158535,1.6204149897669686,1.62041499032834,1.620414990900159,1.6204149914826194,1.6204149920759197,1.620414992680261,1.6204149932958494,1.6204149939228936,1.620414994561608,1.6204149952122084,1.6204149958749166,1.620414996549958,1.6204149972375619,1.620414997937962,1.6204149986513974,1.6204149993781092,1.620415000118346,1.620415000872358,1.6204150016404024,1.6204150024227402,1.6204150032196374,1.620415004031365,1.620415004858199,1.6204150057004205,1.6204150065583158,1.6204150074321766,1.6204150083223,1.6204150092289893,1.6204150101525516,1.6204150110933015,1.620415012051559,1.6204150130276498,1.620415014021906,1.6204150150346652,1.6204150160662718,1.620415017117077,1.620415018187438,1.620415019277718,1.6204150203882888,1.620415021519527,1.6204150226718177,1.620415023845553,1.6204150250411313,1.6204150262589598,1.6204150274994524,1.6204150287630301,1.620415030050123,1.6204150313611696,1.620415032696615,1.6204150340569121,1.6204150354425255,1.620415036853925,1.6204150382915903,1.6204150397560113,1.6204150412476852,1.6204150427671191,1.62041504431483,1.6204150458913438,1.6204150474971966,1.620415049132935,1.6204150507991142,1.6204150524963012,1.6204150542250733,1.620415055986017,1.6204150577797332,1.6204150596068305,1.6204150614679298,1.6204150633636643,1.620415065294679,1.6204150672616298,1.6204150692651855,1.6204150713060277,1.6204150733848501,1.6204150755023599,1.6204150776592763,1.6204150798563328,1.6204150820942773,1.6204150843738696,1.6204150866958855,1.6204150890611144,1.6204150914703603,1.620415093924442,1.6204150964241948,1.6204150989704682,1.6204151015641273,1.6204151042060546,1.6204151068971488,1.6204151096383244,1.620415112430513,1.6204151152746646,1.6204151181717457,1.620415121122742,1.6204151241286557,1.6204151271905105,1.620415130309346,1.620415133486223,1.6204151367222224,1.620415140018443,1.6204151433760066,1.6204151467960546,1.6204151502797495,1.6204151538282763,1.6204151574428411,1.6204151611246733,1.6204151648750236,1.6204151686951684,1.620415172586406,1.6204151765500596,1.6204151805874765,1.62041518470003,1.620415188889118,1.620415193156165,1.6204151975026213,1.6204152019299651,1.6204152064397017,1.6204152110333638,1.6204152157125145,1.6204152204787432,1.620415225333672,1.6204152302789505,1.6204152353162604,1.6204152404473136,1.6204152456738565,1.6204152509976644,1.6204152564205483,1.6204152619443506,1.6204152675709511,1.6204152733022625,1.6204152791402326,1.620415285086847,1.6204152911441272,1.6204152973141328,1.6204153035989615,1.62041531000075,1.6204153165216748,1.6204153231639533,1.6204153299298434,1.6204153368216465,1.6204153438417048,1.620415350992405,1.6204153582761787,1.620415365695502,1.6204153732528979,1.620415380950936,1.6204153887922326,1.6204153967794546,1.6204154049153165,1.6204154132025863,1.6204154216440794,1.6204154302426672,1.6204154390012728,1.6204154479228743,1.620415457010504,1.6204154662672525,1.6204154756962668,1.6204154853007524,1.620415495083975,1.6204155050492608,1.6204155151999975,1.6204155255396369,1.6204155360716932,1.6204155467997474,1.6204155577274482,1.6204155688585093,1.6204155801967155,1.620415591745922,1.620415603510055,1.620415615493114,1.6204156276991735,1.6204156401323826,1.6204156527969686,1.6204156656972377,1.6204156788375752,1.6204156922224484,1.6204157058564088,1.6204157197440903,1.6204157338902156,1.620415748299593,1.620415762977122,1.6204157779277923,1.620415793156687,1.6204158086689824,1.6204158244699542,1.6204158405649733,1.6204158569595108,1.620415873659142,1.6204158906695423,1.6204159079964962,1.6204159256458937,1.6204159436237355,1.6204159619361327,1.6204159805893115,1.620415999589613,1.6204160189434966,1.6204160386575421,1.6204160587384515,1.6204160791930515,1.620416100028296,1.6204161212512673,1.6204161428691815,1.6204161648893867,1.6204161873193694,1.6204162101667552,1.6204162334393102,1.620416257144947,1.6204162812917235,1.6204163058878485,1.6204163309416841,1.6204163564617469,1.6204163824567115,1.6204164089354167,1.6204164359068625,1.6204164633802178,1.6204164913648225,1.6204165198701888,1.6204165489060076,1.6204165784821494,1.6204166086086675,1.6204166392958037,1.6204166705539889,1.6204167023938496,1.6204167348262084,1.6204167678620909,1.620416801512726,1.6204168357895532,1.6204168707042235,1.6204169062686058,1.6204169424947894,1.620416979395087,1.6204170169820433,1.6204170552684338,1.6204170942672735,1.6204171339918179,1.6204171744555698,1.6204172156722838,1.6204172576559692,1.6204173004208973,1.6204173439816032,1.6204173883528936,1.620417433549851,1.6204174795878372,1.6204175264825003,1.6204175742497793,1.6204176229059104,1.6204176724674315,1.6204177229511876,1.6204177743743373,1.6204178267543585,1.6204178801090545,1.6204179344565588,1.6204179898153437,1.6204180462042233,1.6204181036423628,1.6204181621492841,1.620418221744871,1.620418282449378,1.6204183442834361,1.6204184072680605,1.6204184714246563,1.6204185367750272,1.6204186033413832,1.6204186711463453,1.620418740212957,1.6204188105646904,1.620418882225453,1.620418955219598,1.6204190295719307,1.6204191053077175,1.6204191824526952,1.6204192610330799,1.620419341075574,1.6204194226073763,1.6204195056561932,1.6204195902502438,1.6204196764182743,1.6204197641895635,1.620419853593936,1.6204199446617698,1.6204200374240088,1.6204201319121718,1.6204202281583628,1.6204203261952845,1.6204204260562467,1.6204205277751775,1.6204206313866374,1.6204207369258286,1.6204208444286075,1.620420953931498,1.6204210654717017,1.620421179087113,1.620421294816331,1.6204214126986707,1.6204215327741784,1.6204216550836463,1.6204217796686229,1.6204219065714305,1.620422035835176,1.6204221675037709,1.6204223016219395,1.6204224382352381,1.6204225773900722,1.6204227191337064,1.6204228635142852,1.6204230105808477,1.6204231603833446,1.6204233129726542,1.6204234684006003,1.6204236267199703,1.620423787984532,1.6204239522490527,1.6204241195693165,1.620424290002144,1.6204244636054126,1.6204246404380738,1.6204248205601748,1.6204250040328783,1.6204251909184835,1.6204253812804457,1.6204255751834002,1.6204257726931823,1.6204259738768503,1.6204261788027077,1.620426387540327,1.6204266001605727,1.6204268167356246,1.6204270373390046,1.6204272620455984,1.6204274909316827,1.6204277240749503,1.620427961554537,1.620428203451048,1.6204284498465844,1.6204287008247724,1.620428956470789,1.6204292168713947,1.6204294821149583,1.6204297522914903,1.6204300274926706,1.620430307811883,1.6204305933442413,1.6204308841866264,1.6204311804377167,1.620431482198021,1.6204317895699143,1.6204321026576696,1.6204324215674948,1.6204327464075696,1.6204330772880777,1.6204334143212495,1.6204337576213952,1.6204341073049457,1.620434463490491,1.6204348262988213,1.6204351958529655,1.6204355722782338,1.6204359557022603,1.6204363462550453,1.620436744068999,1.6204371492789853,1.6204375620223694,1.6204379824390616,1.6204384106715641,1.6204388468650217,1.6204392911672674,1.6204397437288736,1.6204402047032032,1.6204406742464579,1.6204411525177354,1.620441639679078,1.6204421358955305,1.6204426413351933,1.6204431561692805,1.6204436805721736,1.6204442147214853,1.620444758798115,1.6204453129863103,1.6204458774737287,1.6204464524515005,1.6204470381142932,1.620447634660374,1.6204482422916797,1.6204488612138828,1.6204494916364574,1.620450133772753,1.6204507878400642,1.6204514540597021,1.6204521326570702,1.6204528238617362,1.6204535279075125,1.6204542450325317,1.6204549754793254,1.620455719494906,1.620456477330848,1.6204572492433735,1.6204580354934348,1.6204588363468018,1.6204596520741514,1.620460482951156,1.6204613292585754,1.62046219128235,1.6204630693136939,1.620463963649196,1.6204648745909112,1.6204658024464649,1.6204667475291554,1.6204677101580542,1.6204686906581127,1.6204696893602701,1.6204707066015627,1.6204717427252326,1.620472798080844,1.6204738730243955,1.6204749679184396,1.6204760831321998,1.6204772190416934,1.6204783760298536,1.6204795544866568,1.6204807548092492,1.620481977402077,1.620483222677019,1.6204844910535228,1.6204857829587382,1.620487098827661,1.6204884391032717,1.6204898042366818,1.6204911946872809,1.6204926109228848,1.620494053419887,1.6204955226634192,1.6204970191474999,1.6204985433752015,1.620500095858812,1.6205016771199983,1.6205032876899779,1.6205049281096884,1.6205065989299638,1.620508300711712,1.6205100340260932,1.6205117994547065,1.6205135975897744,1.620515429034337,1.6205172944024397,1.6205191943193344,1.620521129421677,1.6205231003577334,1.6205251077875824,1.6205271523833311,1.6205292348293239,1.6205313558223642,1.6205335160719323,1.6205357163004126,1.6205379572433218,1.6205402396495396,1.6205425642815487,1.6205449319156713,1.6205473433423163,1.6205497993662255,1.6205523008067275,1.6205548484979932,1.6205574432892984,1.6205600860452871,1.6205627776462415,1.6205655189883554,1.6205683109840132,1.6205711545620736,1.6205740506681536,1.6205770002649247,1.620580004332405,1.6205830638682652,1.6205861798881314,1.620589353425897,1.6205925855340404,1.6205958772839424,1.6205992297662162,1.620602644091035,1.620606121388472,1.6206096628088376,1.6206132695230293,1.6206169427228831,1.6206206836215302,1.6206244934537617,1.6206283734763947,1.6206323249686492,1.6206363492325253,1.6206404475931906,1.6206446213993704,1.6206488720237462,1.6206532008633558,1.6206576093400058,1.620662098900684,1.6206666710179811,1.6206713271905178,1.620676068943379,1.6206808978285523,1.6206858154253732,1.6206908233409787,1.6206959232107667,1.620701116698857,1.6207064054985676,1.62071179133289,1.6207172759549744,1.6207228611486215,1.6207285487287808,1.6207343405420553,1.620740238467212,1.6207462444157035,1.6207523603321894,1.6207585881950726,1.620764930017037,1.6207713878455952,1.6207779637636388,1.6207846598900058,1.6207914783800417,1.62079842142618,1.6208054912585217,1.6208126901454285,1.6208200203941165,1.620827484351265,1.6208350844036263,1.6208428229786462,1.6208507025450924,1.6208587256136904,1.6208668947377631,1.6208752125138872,1.6208836815825465,1.6208923046288015,1.6209010843829614,1.6209100236212701,1.6209191251665938,1.620928391889118,1.6209378267070604,1.6209474325873816,1.6209572125465064,1.6209671696510617,1.6209773070186122,1.6209876278184119,1.6209981352721599,1.6210088326547694,1.6210197232951404,1.6210308105769478,1.6210420979394307,1.6210535888782007,1.6210652869460502,1.621077195753777,1.6210893189710152,1.6211016603270783,1.6211142236118108,1.6211270126764523,1.621140031434508,1.6211532838626352,1.6211667740015392,1.6211805059568782,1.621194483900183,1.621208712069788,1.6212231947717721,1.621237936380918,1.6212529413416772,1.621268214169155,1.6212837594501068,1.621299581843948,1.6213156860837792,1.6213320769774306,1.6213487594085152,1.621365738337506,1.6213830188028262,1.6214006059219577,1.6214185048925698,1.6214367209936658,1.6214552595867509,1.6214741261170196,1.6214933261145659,1.6215128651956168,1.6215327490637883,1.6215529835113647,1.6215735744206101,1.6215945277650965,1.621615849611069,1.621637546118836,1.6216596235441894,1.6216820882398584,1.6217049466569944,1.6217282053466928,1.6217518709615482,1.62177595025725,1.6218004500942114,1.6218253774392482,1.6218507393672899,1.6218765430631428,1.6219027958232943,1.6219295050577662,1.6219566782920176,1.6219843231689033,1.6220124474506763,1.6220410590210559,1.6220701658873475,1.6220997761826248,1.6221298981679735,1.6221605402347978,1.6221917109071973,1.622223418844409,1.6222556728433202,1.6222884818410603,1.6223218549176608,1.6223558012988009,1.6223903303586287,1.6224254516226682,1.6224611747708118,1.6224975096403997,1.6225344662293903,1.622572054699625,1.6226102853801856,1.6226491687708533,1.6226887155456606,1.6227289365565567,1.6227698428371655,1.6228114456066591,1.622853756273737,1.622896786440715,1.6229405479077295,1.6229850526770553,1.6230303129575423,1.6230763411691667,1.6231231499477035,1.6231707521495253,1.6232191608565159,1.623268389381112,1.6233184512714711,1.6233693603167616,1.6234211305525825,1.6234737762665064,1.623527312003755,1.623581752572995,1.6236371130522684,1.623693408795042,1.6237506554363912,1.6238088688993002,1.6238680654010949,1.6239282614599921,1.6239894739017742,1.6240517198665805,1.624115016815817,1.6241793825391788,1.6242448351617877,1.6243113931514352,1.6243790753259304,1.6244479008605504,1.6245178892955898,1.6245890605439977,1.6246614348991057,1.624735033042438,1.6248098760515983,1.6248859854082238,1.624963383006014,1.6250420911587993,1.6251221326086798,1.6252035305341932,1.6252863085585236,1.6253704907577324,1.625456101669012,1.6255431662989464,1.6256317101317639,1.6257217591375905,1.6258133397806698,1.6259064790275557,1.626001204355261,1.626097543759347,1.626195525761945,1.6262951794196976,1.6263965343316062,1.6264996206467677,1.626604469071987,1.626711110879259,1.6268195779130923,1.626929902597665,1.627042117943803,1.6271562575557497,1.627272355637723,1.6273904470002394,1.6275105670661771,1.6276327518765794,1.627757038096158,1.627883463018493,1.628012064570902,1.6281428813189582,1.6282759524706396,1.6284113178800899,1.6285490180509572,1.6286890941393075,1.6288315879560735,1.6289765419690256,1.6291239993042346,1.6292740037470104,1.629426599742281,1.629581832394402,1.629739747466354,1.6299003913783139,1.6300638112055745,1.6302300546757786,1.630399170165439,1.6305712066957354,1.6307462139275344,1.6309242421556256,1.6311053423021324,1.6312895659090803,1.6314769651300798,1.631667592721108,1.6318615020303529,1.6320587469870897,1.6322593820895652,1.6324634623918546,1.6326710434896632,1.6328821815050438,1.6330969330699996,1.6333153553089397,1.6335375058199642,1.6337634426549388,1.6339932242983393,1.6342269096448332,1.6344645579755657,1.634706228933128,1.634951982495173,1.6352018789466594,1.6354559788506817,1.6357143430178844,1.6359770324744058,1.6362441084283472,1.6365156322347363,1.636791665358961,1.637072269338653,1.6373575057439993,1.637647436136462,1.6379421220258896,1.6382416248260028,1.6385460058082344,1.6388553260539211,1.6391696464048224,1.6394890274119631,1.6398135292827951,1.6401432118266674,1.640478134398601,1.6408183558413696,1.6411639344258904,1.6415149277899261,1.6418713928751019,1.6422333858622582,1.6426009621051412,1.6429741760624532,1.6433530812282893,1.6437377300609661,1.6441281739102922,1.6445244629432956,1.6449266460684515,1.6453347708584467,1.6457488834715308,1.6461690285714914,1.6465952492463272,1.6470275869256537,1.6474660812969262,1.64791077022054,1.648361689643881,1.6488188735144205,1.649282353691922,1.6497521598598754,1.650228319436241,1.6507108574836198,1.6511997966189644,1.6516951569229443,1.6521969558491072,1.6527052081329607,1.6532199257011244,1.6537411175807124,1.6542687898090962,1.6548029453442306,1.6553435839757142,1.6558907022367835,1.6564442933174366,1.6570043469788953,1.6575708494696337,1.6581437834432033,1.658723127878095,1.6593088579999016,1.6599009452060476,1.6604993569933604,1.6611040568887916,1.661715004383578,1.6623321548711816,1.6629554595893294,1.6635848655665124,1.664220315573303,1.6648617480788706,1.6655090972130884,1.6661622927346453,1.6668212600055767,1.6674859199726673,1.668156189156161,1.6688319796462685,1.6695131991079348,1.6701997507943744,1.6708915335698833,1.671588441942444,1.6722903661066597,1.6729971919975617,1.6737088013558377,1.6744250718050364,1.6751458769413197,1.6758710864363158,1.6766005661536392,1.6773341782796405,1.6780717814689337,1.6788132310052446,1.6795583789781006,1.6803070744758692,1.6810591637956276,1.6818144906702994,1.6825728965134823,1.6833342206823156,1.6840983007587147,1.684864972849217,1.6856340719036285,1.6864054320525785,1.6871788869640105,1.6879542702185382,1.688731415703487,1.6895101580253469,1.6902903329402075,1.6910717778016526,1.6918543320254091,1.6926378375699245,1.6934221394318687,1.694207086155398,1.6949925303538291,1.6957783292422093,1.6965643451790655,1.6973504462154272,1.6981365066490373,1.6989224075814582,1.6997080374755993,1.7004932927110121,1.7012780781341028,1.702062307600274,1.7028459045048239,1.7036288022993098,1.70441094498996,1.7051922876146184,1.705972796694625,1.7067524506580174,1.7075312402303817,1.7083091687897434,1.709086252681904,1.7098625214927554,1.7106380182741976,1.7114127997204804,1.7121869362919904,1.7129605122837357,1.7137336258361011,1.71450638888573,1.7152789270547746,1.716051379477136,1.7168238985607345,1.7175966496852904,1.7183698108355625,1.719143572170464,1.7199181355289674,1.7206937138741913,1.721470530677552,1.722248819245344,1.723028821990575,1.7238107896533117,1.7245949804732315,1.7253816593184372,1.7261710967749506,1.7269635682016076,1.7277593527553423,1.7285587323920446,1.729361990848385,1.7301694126100604,1.7309812818720294,1.7317978814962918,1.732619491972738,1.7334463903885098,1.7342788494111876,1.7351171362909361,1.7359615118865366,1.7368122297199753,1.73766953506398,1.7385336640665843,1.7394048429164695,1.740283287052474,1.741169200420294,1.742062774779036,1.7429641890598742,1.743873608778706,1.7447911855043001,1.7457170563830597,1.7466513437211642,1.7475941546244804,1.7485455806963157,1.7495056977927348,1.7504745658348804,1.751452228677437,1.7524387140321125,1.7534340334447833,1.7544381823247033,1.7554511400239996,1.7564728699654983,1.7575033198167553,1.7585424217080596,1.7595900924920438,1.7606462340424616,1.761710733589619,1.7627834640898845,1.763864284626693,1.764953040840415,1.7660495653844797,1.7671536784051494,1.768265188042365,1.7693838909491255,1.7705095728269151,1.7716420089747391,1.7727809648494177,1.7739261966348472,1.7750774518180128,1.7762344697696626,1.777396982327586,1.7785647143805932,1.779737384451363,1.7809147052764138,1.7820963843816,1.7832821246515924,1.7844716248919419,1.7856645803823945,1.786860683420275,1.7880596238528053,1.789261089597365,1.7904647671487801,1.791670342072822,1.7928774994851977,1.7940859245154024,1.7952953027548744,1.7965053206890074,1.797715666112621,1.7989260285285928,1.8001360995294087,1.8013455731614654,1.802554146272029,1.803761518838794,1.8049673942820716,1.8061714797596622,1.8073734864445354,1.808573129785479,1.8097701297509143,1.810964211056126,1.8121551033741794,1.813342541530832,1.8145262656837768,1.8157060214865763,1.8168815602376684,1.8180526390148475,1.8192190207956385,1.8203804745639953,1.8215367754037612,1.822687704579354,1.8238330496041215,1.8249726042968424,1.8261061688268287,1.8272335497481091,1.8283545600231474,1.8294690190365717,1.8305767525993701,1.831677592944017,1.8327713787109727,1.8338579549270129,1.8349371729758157,1.8360088905612453,1.8370729716637448,1.8381292864902554,1.8391777114180592,1.8402181289329416,1.8412504275620438,1.842274501801785,1.843290252041202,1.8442975844810607,1.8452964110490606,1.8462866493114714,1.8472682223814954]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.9214567265139932,1.9214567264342362,1.9214567263529947,1.9214567262702416,1.9214567261859483,1.9214567261000866,1.9214567260126267,1.9214567259235393,1.9214567258327944,1.9214567257403605,1.9214567256462067,1.9214567255503006,1.92145672545261,1.9214567253531012,1.9214567252517407,1.9214567251484944,1.921456725043326,1.9214567249362013,1.9214567248270826,1.9214567247159335,1.921456724602716,1.9214567244873917,1.9214567243699214,1.921456724250265,1.921456724128382,1.9214567240042308,1.9214567238777693,1.9214567237489546,1.921456723617743,1.9214567234840894,1.9214567233479485,1.9214567232092747,1.9214567230680204,1.921456722924137,1.9214567227775765,1.9214567226282888,1.921456722476223,1.9214567223213272,1.92145672216355,1.921456722002836,1.9214567218391312,1.9214567216723804,1.9214567215025267,1.9214567213295124,1.9214567211532783,1.921456720973765,1.9214567207909115,1.921456720604655,1.9214567204149329,1.9214567202216803,1.9214567200248318,1.9214567198243204,1.9214567196200778,1.9214567194120347,1.9214567192001202,1.9214567189842626,1.9214567187643883,1.9214567185404225,1.9214567183122895,1.9214567180799116,1.9214567178432096,1.9214567176021031,1.9214567173565102,1.9214567171063475,1.9214567168515297,1.9214567165919705,1.921456716327582,1.9214567160582736,1.9214567157839542,1.9214567155045306,1.9214567152199076,1.9214567149299886,1.9214567146346753,1.9214567143338672,1.9214567140274619,1.9214567137153553,1.9214567133974416,1.9214567130736124,1.9214567127437578,1.9214567124077662,1.9214567120655228,1.9214567117169112,1.9214567113618137,1.9214567110001088,1.9214567106316744,1.9214567102563846,1.9214567098741122,1.9214567094847275,1.9214567090880976,1.9214567086840884,1.9214567082725622,1.9214567078533793,1.9214567074263975,1.9214567069914714,1.9214567065484531,1.921456706097193,1.9214567056375367,1.9214567051693285,1.9214567046924096,1.9214567042066175,1.9214567037117873,1.921456703207751,1.9214567026943374,1.921456702171372,1.9214567016386774,1.9214567010960721,1.9214567005433723,1.9214566999803904,1.9214566994069346,1.9214566988228101,1.921456698227819,1.921456697621759,1.921456697004424,1.9214566963756052,1.9214566957350878,1.921456695082655,1.921456694418085,1.9214566937411524,1.921456693051627,1.921456692349275,1.9214566916338573,1.9214566909051312,1.9214566901628498,1.92145668940676,1.921456688636606,1.9214566878521262,1.9214566870530536,1.9214566862391174,1.9214566854100412,1.9214566845655434,1.9214566837053373,1.921456682829131,1.9214566819366268,1.9214566810275222,1.921456680101508,1.9214566791582703,1.9214566781974887,1.9214566772188368,1.9214566762219831,1.921456675206589,1.9214566741723094,1.921456673118794,1.921456672045685,1.9214566709526182,1.9214566698392224,1.92145666870512,1.9214566675499265,1.9214566663732495,1.9214566651746898,1.9214566639538408,1.9214566627102883,1.9214566614436104,1.9214566601533773,1.9214566588391517,1.9214566575004872,1.9214566561369304,1.9214566547480183,1.9214566533332795,1.9214566518922351,1.921456650424396,1.921456648929264,1.9214566474063326,1.921456645855085,1.9214566442749954,1.9214566426655284,1.9214566410261378,1.921456639356268,1.921456637655353,1.9214566359228158,1.92145663415807,1.9214566323605167,1.9214566305295468,1.92145662866454,1.921456626764864,1.9214566248298754,1.9214566228589183,1.9214566208513248,1.921456618806415,1.921456616723496,1.9214566146018621,1.9214566124407948,1.9214566102395623,1.9214566079974187,1.9214566057136047,1.9214566033873475,1.9214566010178584,1.9214565986043364,1.9214565961459633,1.9214565936419075,1.921456591091321,1.9214565884933408,1.9214565858470878,1.921456583151666,1.9214565804061636,1.9214565776096515,1.9214565747611836,1.9214565718597965,1.9214565689045084,1.9214565658943203,1.9214565628282136,1.921456559705152,1.9214565565240795,1.9214565532839198,1.9214565499835783,1.9214565466219395,1.921456543197867,1.9214565397102037,1.921456536157771,1.9214565325393689,1.9214565288537744,1.9214565250997435,1.921456521276008,1.9214565173812763,1.9214565134142334,1.9214565093735405,1.9214565052578325,1.9214565010657216,1.9214564967957921,1.921456492446604,1.921456488016689,1.9214564835045536,1.9214564789086754,1.921456474227505,1.9214564694594636,1.921456464602944,1.9214564596563093,1.9214564546178923,1.921456449485995,1.9214564442588886,1.9214564389348128,1.9214564335119741,1.9214564279885464,1.9214564223626704,1.9214564166324524,1.9214564107959637,1.9214564048512408,1.9214563987962836,1.9214563926290558,1.9214563863474834,1.9214563799494546,1.9214563734328185,1.921456366795386,1.9214563600349257,1.9214563531491675,1.921456346135799,1.9214563389924648,1.9214563317167679,1.921456324306266,1.9214563167584728,1.9214563090708567,1.9214563012408399,1.9214562932657966,1.9214562851430548,1.921456276869892,1.9214562684435368,1.921456259861168,1.9214562511199114,1.9214562422168417,1.9214562331489797,1.9214562239132926,1.9214562145066916,1.9214562049260326,1.9214561951681133,1.9214561852296743,1.921456175107396,1.9214561647978998,1.9214561542977442,1.9214561436034263,1.92145613271138,1.9214561216179735,1.9214561103195102,1.9214560988122262,1.9214560870922894,1.9214560751557987,1.9214560629987822,1.921456050617197,1.9214560380069257,1.9214560251637782,1.9214560120834887,1.9214559987617132,1.9214559851940307,1.921455971375941,1.9214559573028613,1.9214559429701281,1.9214559283729937,1.9214559135066245,1.9214558983661012,1.921455882946416,1.9214558672424709,1.9214558512490774,1.921455834960954,1.921455818372725,1.9214558014789183,1.9214557842739646,1.9214557667521952,1.9214557489078403,1.9214557307350282,1.921455712227782,1.9214556933800186,1.9214556741855482,1.9214556546380697,1.921455634731171,1.9214556144583277,1.9214555938128979,1.9214555727881244,1.9214555513771303,1.9214555295729168,1.9214555073683626,1.9214554847562215,1.921455461729119,1.9214554382795526,1.9214554143998872,1.921455390082355,1.9214553653190525,1.9214553401019372,1.9214553144228277,1.9214552882733988,1.9214552616451825,1.9214552345295624,1.9214552069177724,1.9214551788008962,1.9214551501698622,1.921455121015443,1.9214550913282515,1.9214550610987395,1.921455030317195,1.921454998973739,1.9214549670583234,1.9214549345607297,1.921454901470563,1.921454867777253,1.921454833470049,1.921454798538018,1.921454762970042,1.9214547267548159,1.9214546898808424,1.921454652336432,1.9214546141096982,1.9214545751885561,1.9214545355607175,1.9214544952136907,1.9214544541347753,1.92145441231106,1.9214543697294197,1.9214543263765134,1.9214542822387786,1.921454237302431,1.9214541915534609,1.921454144977628,1.9214540975604617,1.921454049287255,1.921454000143063,1.9214539501126995,1.9214538991807335,1.9214538473314868,1.92145379454903,1.92145374081718,1.9214536861194962,1.921453630439278,1.9214535737595615,1.9214535160631163,1.9214534573324416,1.921453397549764,1.9214533366970352,1.9214532747559265,1.9214532117078276,1.9214531475338426,1.9214530822147877,1.9214530157311878,1.921452948063273,1.9214528791909764,1.921452809093931,1.9214527377514667,1.9214526651426072,1.9214525912460674,1.921452516040251,1.9214524395032475,1.9214523616128298,1.921452282346451,1.9214522016812423,1.921452119594012,1.92145203606124,1.9214519510590786,1.9214518645633496,1.9214517765495411,1.921451686992807,1.921451595867964,1.9214515031494919,1.9214514088115295,1.9214513128278754,1.9214512151719856,1.9214511158169727,1.9214510147356054,1.9214509119003076,1.9214508072831564,1.9214507008558857,1.9214505925898817,1.9214504824561853,1.9214503704254926,1.9214502564681546,1.9214501405541786,1.9214500226532303,1.921449902734632,1.9214497807673694,1.9214496567200883,1.9214495305611021,1.9214494022583906,1.9214492717796046,1.921449139092071,1.9214490041627934,1.9214488669584602,1.9214487274454468,1.9214485855898213,1.9214484413573516,1.9214482947135105,1.9214481456234829,1.9214479940521731,1.9214478399642132,1.921447683323971,1.9214475240955602,1.9214473622428496,1.9214471977294734,1.921447030518844,1.9214468605741624,1.9214466878584322,1.9214465123344724,1.9214463339649337,1.9214461527123117,1.9214459685389647,1.921445781407131,1.9214455912789472,1.9214453981164672,1.9214452018816832,1.921445002536547,1.9214448000429931,1.9214445943629632,1.921444385458431,1.9214441732914285,1.9214439578240756,1.9214437390186074,1.921443516837408,1.9214432912430401,1.9214430621982816,1.9214428296661603,1.921442593609992,1.9214423539934185,1.9214421107804516,1.921441863935513,1.9214416134234826,1.921441359209744,1.9214411012602337,1.9214408395414952,1.9214405740207294,1.921440304665854,1.9214400314455622,1.9214397543293824,1.921439473287744,1.9214391882920439,1.921438899314716,1.9214386063293059,1.9214383093105432,1.9214380082344247,1.921437703078294,1.921437393820928,1.9214370804426264,1.9214367629253046,1.9214364412525913,1.9214361154099269,1.9214357853846704,1.9214354511662066,1.9214351127460607,1.9214347701180141,1.9214344232782279,1.9214340722253684,1.9214337169607403,1.9214333574884217,1.921432993815408,1.921432625951757,1.9214322539107427,1.9214318777090138,1.9214314973667588,1.921431112907874,1.9214307243601427,1.9214303317554158,1.9214299351298039,1.9214295345238714,1.9214291299828414,1.9214287215568056,1.9214283093009428,1.9214278932757431,1.9214274735472439,1.9214270501872694,1.9214266232736814,1.9214261928906367,1.9214257591288553,1.921425322085896,1.9214248818664412,1.921424438582593,1.9214239923541758,1.9214235433090532,1.9214230915834498,1.9214226373222894,1.9214221806795377,1.9214217218185625,1.9214212609125,1.9214207981446343,1.9214203337087896,1.9214198678097356,1.9214194006635998,1.9214189324982998,1.9214184635539824,1.9214179940834806,1.9214175243527807,1.921417054641505,1.9214165852434075,1.9214161164668861,1.9214156486355056,1.9214151820885406,1.9214147171815283,1.921414254286842,1.9214137937942766,1.9214133361116505,1.9214128816654283,1.9214124309013516,1.9214119842850972,1.9214115423029419,1.9214111054624536,1.9214106742931951,1.9214102493474459,1.9214098312009444,1.921409420453648,1.921409017730509,1.9214086236822743,1.9214082389862988,1.9214078643473833,1.9214075004986257,1.921407148202299,1.921406808250743,1.9214064814672775,1.9214061687071402,1.9214058708584358,1.9214055888431139,1.921405323617962,1.921405076175622,1.9214048475456245,1.9214046387954447,1.9214044510315809,1.9214042854006488,1.9214041430905007,1.921404025331363,1.9214039333969928,1.9214038686058585,1.9214038323223357,1.9214038259579265,1.9214038509724953,1.9214039088755284,1.921404001227408,1.9214041296407083,1.9214042957815078,1.9214045013707226,1.921404748185454,1.9214050380603562,1.9214053728890204,1.921405754625374,1.9214061852850965,1.9214066669470515,1.9214072017547332,1.9214077919177237,1.9214084397131692,1.9214091474872648,1.9214099176567536,1.9214107527104345,1.9214116552106832,1.9214126277949801,1.9214136731774485,1.9214147941504,1.921415993585887,1.9214172744372588,1.9214186397407262,1.9214200926169271,1.9214216362724938,1.921423274001625,1.921425009187656,1.9214268453046273,1.9214287859188544,1.9214308346904896,1.921432995375085,1.9214352718251464,1.9214376679916783,1.921440187925727,1.9214428357799098,1.9214456158099333,1.9214485323761021,1.9214515899448137,1.9214547930900383,1.9214581464947833,1.9214616549525414,1.921465323368721,1.9214691567620565,1.9214731602659987,1.9214773391300843,1.9214816987212822,1.9214862445253178,1.921490982147971,1.92149591731635,1.9215010558801389,1.921506403812819,1.9215119672128598,1.921517752304886,1.9215237654408115,1.9215300131009458,1.9215365018950723,1.9215432385634925,1.9215502299780445,1.921557483143087,1.9215650051964566,1.921572803410392,1.9215808851924299,1.92158925808627,1.9215979297726113,1.9216069080699616,1.9216162009354132,1.9216258164654005,1.9216357628964211,1.9216460486057412,1.9216566821120713,1.92166767207622,1.9216790273017292,1.921690756735488,1.9217028694683282,1.9217153747356055,1.921728281917767,1.921741600540905,1.921755340277302,1.92176951094597,1.9217841225131835,1.9217991850930103,1.9218147089478426,1.9218307044889333,1.9218471822769367,1.9218641530224567,1.921881627586611,1.9218996169816074,1.9219181323713386,1.9219371850720004,1.9219567865527323,1.9219769484362874,1.921997682499733,1.922019000675186,1.9220409150505875,1.9220634378705144,1.9220865815370418,1.9221103586106472,1.9221347818111696,1.9221598640188216,1.9221856182752568,1.9222120577847017,1.9222391959151472,1.9222670461996083,1.9222956223374503,1.9223249381957908,1.9223550078109726,1.922385845390112,1.9224174653127297,1.9224498821324598,1.9224831105788436,1.9225171655592035,1.92255206216061,1.92258781565193,1.922624441485968,1.9226619553016937,1.922700372926564,1.922739710378932,1.9227799838705535,1.922821209809176,1.922863404801231,1.922906585654607,1.9229507693815222,1.9229959732014807,1.923042214544323,1.923089511053361,1.923137880588604,1.9231873412300646,1.9232379112811528,1.9232896092721499,1.9233424539637591,1.923396464350738,1.9234516596655982,1.9235080593823837,1.9235656832205115,1.9236245511486803,1.9236846833888388,1.9237461004202128,1.9238088229833876,1.923872872084438,1.9239382689991076,1.9240050352770264,1.924073192745971,1.924142763516153,1.9242137699845385,1.9242862348391943,1.9243601810636464,1.9244356319412603,1.9245126110596265,1.9245911423149518,1.9246712499164498,1.9247529583907266,1.9248362925861553,1.9249212776772329,1.92500793916892,1.925096302900952,1.9251863950521173,1.9252782421445027,1.925371871047694,1.9254673089829324,1.925564583527216,1.9256637226173485,1.9257647545539207,1.9258677080052307,1.9259726120111282,1.926079495986786,1.9261883897263867,1.9262993234067265,1.926412327590728,1.9265274332308575,1.926644671672445,1.9267640746569004,1.9268856743248222,1.9270095032189953,1.927135594287276,1.927263980885356,1.9273946967794044,1.9275277761485874,1.9276632535874578,1.9278011641082113,1.927941543142811,1.9280844265449748,1.9282298505920206,1.9283778519865715,1.9285284678581172,1.9286817357644264,1.9288376936928133,1.9289963800612535,1.9291578337193465,1.9293220939491262,1.9294892004657196,1.9296591934178435,1.9298321133881515,1.9300080013934169,1.9301868988845647,1.9303688477465362,1.9305538902980015,1.9307420692909074,1.930933427909867,1.9311280097713872,1.9313258589229376,1.9315270198418562,1.9317315374340929,1.931939457032796,1.9321508243967302,1.9323656857085403,1.9325840875728437,1.9328060770141684,1.933031701474721,1.9332610088119944,1.9334940472962088,1.9337308656075853,1.9339715128334543,1.9342160384651907,1.934464492394982,1.9347169249124174,1.9349733867009042,1.9352339288339013,1.9354986027709706,1.935767460353638,1.936040553801064,1.9363179357055134,1.9365996590276198,1.9368857770914423,1.9371763435792968,1.9374714125263655,1.9377710383150646,1.9380752756691682,1.9383841796476713,1.938697805638385,1.9390162093512462,1.9393394468113312,1.939667574351557,1.9400006486050527,1.940338726497186,1.9406818652372255,1.9410301223096194,1.9413835554648666,1.9417422227099659,1.942106182298412,1.9424754927197188,1.9428502126884442,1.9432304011326886,1.9436161171820387,1.944007420154934,1.9444043695454152,1.9448070250092355,1.9452154463492952,1.9456296935003727,1.9460498265131119,1.94647590553724,1.9469079908039755,1.9473461426075926,1.9477904212861092,1.9482408872010584,1.9486976007163108,1.9491606221759121,1.9496300118808971,1.9501058300650476,1.9505881368695588,1.9510769923165778,1.951572456281583,1.9520745884645718,1.952583448360022,1.9530990952256013,1.9536215880495933,1.954150985517015,1.954687345974398,1.955230727393218,1.955781187331941,1.9563387828966816,1.9569035707004472,1.957475606820966,1.9580549467570803,1.9586416453837112,1.9592357569053815,1.9598373348083091,1.9604464318110715,1.9610630998138507,1.9616873898462814,1.9623193520139075,1.9629590354432862,1.963606488225756,1.964261757359908,1.9649248886927964,1.9655959268599321,1.9662749152241044,1.9669618958130899,1.9676569092563003,1.9683599947204353,1.96907118984421,1.969790530672224,1.9705180515880614,1.9712537852466918,1.9719977625062677,1.972750012359411,1.9735105618640796,1.974279436074121,1.975056657969614,1.9758422483871112,1.9766362259498886,1.9774386069983254,1.9782494055205295,1.9790686330833294,1.979896298763758,1.9807324090811593,1.981576967930036,1.9824299765137807,1.9832914332794125,1.9841613338534543,1.985039670979083,1.9859264344546892,1.9868216110739694,1.9877251845676929,1.9886371355472654,1.989557441450226,1.9904860764877972,1.9914230115946212,1.9923682143807977,1.9933216490863506,1.994283276538231,1.9952530541099807,1.996230935684155,1.9972168716176162,1.9982108087097972,1.999212690174028,2.000222455612021,2.001240040991595,2.0022653786277234,2.003298397166975,2.0043390215754213,2.005387173130073,2.0064427694138933,2.0075057243144507,2.0085759480262437,2.009653347056747,2.010737824236192,2.0118292787311236,2.012927606061745,2.0140326981230534,2.0151444432097816,2.0162627260451402,2.01738742781335,2.0185184261959512,2.0196555954118667,2.0207988062611957,2.021947926172701,2.023102819254948,2.0242633463510606,2.0254293650970268,2.0266007299835143,2.02777729242112,2.0289589008090023,2.0301454006067985,2.0313366344097847,2.0325324420271644,2.0337326605634223,2.034937124502637,2.0361456657956714,2.037358113950135,2.0385742961230124,2.039794037215865,2.041017159972484,2.0422434850788873,2.0434728312655532,2.0447050154117536,2.04593985265189,2.0471771564836923,2.0484167388781604,2.049658410391123,2.0509019802762882,2.0521472565996444,2.0533940463550904,2.0546421555811563,2.055891389478681,2.057141552529313,2.0583924486146943,2.0596438811361955,2.0608956531350637,2.062147567412838,2.063399426651909,2.0646510335360757,2.0659021908709594,2.0671527017041504,2.068402369444938,2.0696509979834974,2.0708983918093966,2.0721443561292894,2.0733886969836646,2.0746312213625178,2.0758717373198268,2.0771100540866905,2.07834598218302,2.079579333527653,2.0808099215467735,2.0820375612805218,2.083262069487681,2.084483264748328,2.0857009675643416,2.0869150004576666,2.088125188066233,2.0893313572374286,2.0905333371190413,2.0917309592475775,2.092924057633869,2.0941124688459007,2.0952960320887684,2.0964745892817094,2.097647985132134,2.0988160672065908,2.09997868599863,2.1011356949934865,2.1022869507295607,2.103432312856644,2.1045716441908526,2.105704810766251,2.106831681883125,2.1079521301528996,2.1090660315396734,2.110173265398373,2.1112737145095184,2.112367265110596,2.1134538069240585,2.114533233181948,2.115605440647176,2.1166703296314644,2.1177278040099896,2.118777771232751,2.1198201423327077,2.1208548319307106,2.1218817582372926,2.122900843051353,2.12391201175579,2.124915193310142,2.125910320240297,2.126897328625336,2.127876158081563,2.1288467517438194,2.1298090562441248,2.1307630216877436,2.1317086016267432,2.132645753031127,2.1335744362576254,2.134494615016227,2.1354062563345417],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[2.4216611166358675,2.421661113404985,2.4216611101139747,2.4216611067617184,2.421661103347076,2.4216610998688872,2.4216610963259693,2.421661092717116,2.421661089041103,2.4216610852966785,2.4216610814825694,2.4216610775974794,2.421661073640087,2.4216610696090477,2.421661065502991,2.4216610613205187,2.421661057060211,2.4216610527206175,2.421661048300265,2.421661043797649,2.421661039211238,2.4216610345394733,2.4216610297807684,2.4216610249335013,2.421661019996028,2.421661014966666,2.421661009843709,2.421661004625412,2.421660999310002,2.4216609938956717,2.4216609883805806,2.421660982762853,2.4216609770405793,2.4216609712118133,2.4216609652745733,2.4216609592268408,2.42166095306656,2.4216609467916355,2.421660940399934,2.421660933889282,2.4216609272574674,2.4216609205022332,2.4216609136212837,2.4216609066122796,2.4216608994728377,2.42166089220053,2.4216608847928844,2.4216608772473815,2.4216608695614563,2.421660861732496,2.421660853757838,2.4216608456347717,2.4216608373605344,2.4216608289323127,2.421660820347242,2.4216608116024023,2.4216608026948214,2.421660793621469,2.4216607843792612,2.421660774965056,2.421660765375652,2.421660755607789,2.4216607456581456,2.421660735523339,2.4216607251999234,2.4216607146843896,2.42166070397316,2.4216606930625955,2.421660681948985,2.421660670628549,2.4216606590974408,2.421660647351738,2.4216606353874472,2.421660623200503,2.4216606107867578,2.421660598141993,2.4216605852619097,2.4216605721421285,2.4216605587781883,2.4216605451655453,2.4216605312995707,2.421660517175551,2.421660502788684,2.421660488134078,2.4216604732067504,2.4216604580016257,2.4216604425135344,2.42166042673721,2.4216604106672897,2.421660394298309,2.4216603776247023,2.421660360640801,2.4216603433408306,2.4216603257189093,2.421660307769045,2.4216602894851347,2.4216602708609645,2.4216602518901977,2.421660232566388,2.4216602128829647,2.4216601928332344,2.4216601724103812,2.4216601516074605,2.4216601304174006,2.4216601088329965,2.42166008684691,2.4216600644516637,2.4216600416396474,2.421660018403102,2.421659994734129,2.4216599706246793,2.4216599460665567,2.4216599210514134,2.4216598955707416,2.4216598696158793,2.421659843178003,2.421659816248122,2.4216597888170828,2.4216597608755577,2.4216597324140468,2.421659703422874,2.421659673892183,2.4216596438119327,2.421659613171897,2.4216595819616575,2.421659550170604,2.4216595177879285,2.4216594848026203,2.421659451203466,2.4216594169790424,2.421659382117713,2.4216593466076235,2.4216593104367043,2.4216592735926574,2.421659236062955,2.421659197834839,2.4216591588953116,2.4216591192311347,2.4216590788288226,2.42165903767464,2.421658995754595,2.4216589530544366,2.4216589095596457,2.421658865255438,2.421658820126747,2.4216587741582334,2.4216587273342682,2.4216586796389317,2.4216586310560095,2.421658581568984,2.421658531161031,2.4216584798150147,2.4216584275134765,2.4216583742386377,2.421658319972385,2.4216582646962697,2.4216582083915,2.4216581510389337,2.4216580926190736,2.4216580331120574,2.4216579724976564,2.421657910755264,2.4216578478638877,2.4216577838021496,2.4216577185482686,2.4216576520800643,2.421657584374936,2.4216575154098705,2.4216574451614203,2.421657373605704,2.4216573007183975,2.4216572264747205,2.421657150849434,2.4216570738168297,2.421656995350719,2.421656915424429,2.4216568340107876,2.4216567510821183,2.4216566666102306,2.4216565805664074,2.421656492921399,2.421656403645412,2.4216563127080963,2.421656220078538,2.421656125725252,2.4216560296161598,2.421655931718593,2.4216558319992716,2.4216557304242983,2.421655626959143,2.4216555215686357,2.421655414216951,2.421655304867597,2.421655193483402,2.421655080026505,2.421654964458339,2.4216548467396186,2.4216547268303295,2.421654604689712,2.4216544802762487,2.42165435354765,2.4216542244608377,2.4216540929719352,2.4216539590362465,2.421653822608247,2.421653683641563,2.4216535420889596,2.4216533979023223,2.421653251032643,2.42165310143,2.4216529490435432,2.421652793821479,2.421652635711048,2.4216524746585093,2.421652310609123,2.4216521435071323,2.421651973295742,2.421651799917099,2.4216516233122767,2.4216514434212515,2.421651260182883,2.421651073534894,2.421650883413848,2.4216506897551304,2.421650492492924,2.4216502915601854,2.4216500868886284,2.4216498784086924,2.421649666049527,2.4216494497389616,2.421649229403483,2.4216490049682124,2.421648776356879,2.421648543491791,2.421648306293814,2.42164806468234,2.4216478185752637,2.421647567888953,2.4216473125382185,2.4216470524362896,2.4216467874947787,2.4216465176236572,2.4216462427312218,2.421645962724064,2.4216456775070374,2.4216453869832284,2.421645091053918,2.4216447896185564,2.421644482574721,2.421644169818084,2.4216438512423824,2.421643526739371,2.421643196198797,2.421642859508359,2.4216425165536615,2.4216421672181876,2.4216418113832527,2.4216414489279643,2.421641079729186,2.421640703661489,2.421640320597114,2.421639930405928,2.421639532955378,2.4216391281104452,2.421638715733604,2.4216382956847733,2.421637867821264,2.4216374319977385,2.4216369880661563,2.421636535875729,2.42163607527286,2.421635606101102,2.4216351282011006,2.421634641410537,2.421634145564079,2.421633640493321,2.4216331260267276,2.421632601989577,2.4216320682039005,2.4216315244884217,2.421630970658497,2.421630406526052,2.4216298318995197,2.4216292465837705,2.421628650380054,2.421628043085924,2.4216274244951763,2.4216267943977745,2.421626152579781,2.421625498823285,2.421624832906329,2.4216241546028305,2.421623463682511,2.421622759910813,2.4216220430488247,2.421621312853196,2.4216205690760595,2.4216198114649448,2.4216190397626933,2.4216182537073703,2.4216174530321806,2.4216166374653745,2.421615806730156,2.421614960544593,2.4216140986215184,2.4216132206684358,2.4216123263874163,2.421611415475006,2.4216104876221154,2.421609542513921,2.421608579829755,2.4216075992430017,2.4216066004209806,2.4216055830248426,2.4216045467094505,2.4216034911232613,2.421602415908214,2.4216013206996037,2.42160020512596,2.421599068808922,2.4215979113631136,2.421596732396009,2.4215955315078053,2.4215943082912825,2.4215930623316733,2.4215917932065185,2.421590500485527,2.42158918373043,2.4215878424948354,2.421586476324076,2.4215850847550575,2.4215836673161037,2.421582223526798,2.421580752897819,2.4215792549307813,2.4215777291180656,2.4215761749426465,2.4215745918779246,2.421572979387543,2.421571336925212,2.421569663934526,2.4215679598487756,2.421566224090758,2.421564456072583,2.42156265519548,2.421560820849591,2.421558952413773,2.4215570492553855,2.421555110730084,2.421553136181598,2.4215511249415167,2.4215490763290664,2.4215469896508757,2.421544864200756,2.4215426992594544,2.4215404940944194,2.4215382479595564,2.421535960094978,2.4215336297267513,2.4215312560666398,2.4215288383118394,2.421526375644714,2.4215238672325228,2.4215213122271386,2.4215187097647743,2.4215160589656897,2.4215133589339,2.4215106087568805,2.421507807505259,2.421504954232515,2.421502047974658,2.421499087749914,2.4214960725583956,2.4214930013817737,2.4214898731829395,2.421486686905662,2.421483441474237,2.4214801357931344,2.4214767687466323,2.421473339198454,2.4214698459913864,2.4214662879469055,2.4214626638647805,2.4214589725226863,2.4214552126757938,2.421451383056365,2.4214474823733347,2.4214435093118847,2.421439462533014,2.421435340673098,2.4214311423434425,2.4214268661298277,2.4214225105920444,2.4214180742634213,2.4214135556503504,2.421408953231792,2.4214042654587815,2.4213994907539242,2.4213946275108778,2.4213896740938314,2.421384628836974,2.421379490043946,2.421374255987294,2.421368924907909,2.4213634950144494,2.4213579644827665,2.42135233145531,2.421346594040527,2.4213407503122517,2.4213347983090765,2.4213287360337294,2.421322561452418,2.421316272494181,2.4213098670502187,2.4213033429732165,2.421296698076651,2.4212899301340918,2.4212830368784855,2.421276016001429,2.4212688651524292,2.4212615819381584,2.4212541639216787,2.421246608621676,2.4212389135116594,2.4212310760191627,2.4212230935249264,2.421214963362065,2.4212066828152206,2.421198249119702,2.4211896594606137,2.421180910971963,2.4211720007357562,2.4211629257810805,2.421153683083169,2.4211442695624497,2.4211346820835757,2.42112491745445,2.4211149724252206,2.4211048436872655,2.4210945278721656,2.421084021550647,2.4210733212315247,2.4210624233606084,2.4210513243196043,2.4210400204250004,2.42102850792692,2.421016783007967,2.4210048417820573,2.4209926802932102,2.4209802945143495,2.4209676803460565,2.4209548336153266,2.4209417500742885,2.4209284253989143,2.4209148551877013,2.4209010349603406,2.4208869601563534,2.4208726261337183,2.4208580281674705,2.4208431614482717,2.4208280210809776,2.4208126020831586,2.420796899383618,2.420780907820872,2.4207646221416166,2.420748036999162,2.420731146951857,2.4207139464614653,2.4206964298915428,2.420678591505774,2.4206604254662865,2.4206419258319425,2.420623086556602,2.4206039014873615,2.420584364362764,2.420564468810983,2.420544208347982,2.420523576375642,2.4205025661798656,2.4204811709286504,2.420459383670134,2.420437197330612,2.420414604712527,2.420391598492424,2.420368171218887,2.4203443153104294,2.4203200230533706,2.420295286599671,2.420270097964742,2.42024444902522,2.420218331516715,2.4201917370315225,2.420164657016308,2.420137082769749,2.420109005440158,2.420080416023067,2.420051305358773,2.4200216641298553,2.41999148285866,2.41996075190475,2.4199294614623117,2.419897601557543,2.419865162045988,2.419832132609855,2.419798502755276,2.4197642618095556,2.4197293989183626,2.4196939030428943,2.4196577629570033,2.4196209672442834,2.4195835042951224,2.419545362303713,2.4195065292650244,2.419466992971741,2.4194267410111543,2.4193857607620206,2.419344039391377,2.4193015638513184,2.4192583208757297,2.419214296976985,2.4191694784425963,2.419123851331828,2.4190774014722627,2.419030114456334,2.418981975637803,2.4189329701282065,2.4188830827932475,2.4188322982491544,2.4187806008589825,2.418727974728885,2.4186744037043226,2.418619871366243,2.418564361027202,2.418507855727443,2.4184503382309295,2.4183917910213255,2.4183321962979325,2.418271535971573,2.4182097916604275,2.4181469446858195,2.418082976067948,2.4180178665215735,2.4179515964516467,2.4178841459488813,2.417815494785288,2.4177456224096305,2.4176745079428468,2.4176021301734063,2.4175284675526054,2.4174534981898135,2.417377199847653,2.417299549937126,2.417220525512673,2.417140103267173,2.417058259526884,2.416974970246313,2.4168902110030217,2.416803956992372,2.4167161830221935,2.4166268635073904,2.4165359724644726,2.416443483506016,2.416349369835049,2.4162536042393663,2.4161561590857636,2.4160570063141895,2.4159561174318314,2.415853463507099,2.415749015163545,2.415642742573685,2.415534615452734,2.4154246030522635,2.4153126741537476,2.4151987970620334,2.415082939598704,2.4149650690953512,2.414845152386742,2.4147231558038853,2.4145990451669928,2.4144727857783326,2.414344342414972,2.414213679321408,2.4140807602020833,2.4139455482137806,2.4138080059579057,2.4136680954726333,2.4135257782249377,2.4133810151024915,2.4132337664054315,2.413083991837988,2.4129316504999823,2.4127767008781773,2.4126191008374893,2.4124588076120537,2.412295777796136,2.4121299673348977,2.4119613315150037,2.411789824955072,2.411615401595963,2.4114380146909085,2.41125761679547,2.4110741597573306,2.410887594705918,2.4106978720418484,2.410504941426203,2.4103087517696165,2.4101092512211895,2.4099063871572226,2.409700106169759,2.409490354054948,2.4092770758012136,2.4090602155772403,2.4088397167197693,2.4086155217211953,2.408387572216985,2.4081558089728907,2.407920171871981,2.407680599901474,2.40743703113938,2.4071894027409555,2.4069376509249647,2.4066817109597523,2.4064215171491297,2.4061570028180777,2.4058881002982653,2.40561474091339,2.40533685496434,2.4050543717141877,2.40476721937301,2.4044753250825512,2.4041786149007205,2.4038770137859453,2.4035704455813685,2.4032588329989144,2.4029420976032183,2.4026201597954326,2.4022929387969136,2.4019603526328077,2.4016223181155336,2.4012787508281828,2.4009295651078357,2.400574674028827,2.4002139893859407,2.3998474216775785,2.3994748800888965,2.3990962724749307,2.3987115053437256,2.3983204838394783,2.3979231117257216,2.3975192913685555,2.3971089237199465,2.3966919083011193,2.396268143186051,2.3958375249850947,2.3953999488287456,2.394955308351581,2.3945034956763944,2.3940444013985274,2.3935779145704603,2.3931039226866466,2.3926223116686463,2.3921329658505632,2.3916357679648317,2.39113059912836,2.390617338829075,2.3900958649128934,2.3895660535711394,2.389027779328453,2.388480915031218,2.387925331836528,2.3873608992017417,2.3867874848746524,2.3862049548843043,2.3856131735324992,2.385012003386016,2.3844013052696,2.3837809382597395,2.3831507596792787,2.3825106250929093,2.381860388303574,2.3811999013498246,2.3805290145041793,2.379847576272526,2.3791554333945992,2.378452430845597,2.37773841183897,2.3770132178304246,2.376276688523208,2.375528661874701,2.374768974104378,2.3739974597031877,2.3732139514443977,2.3724182803959613,2.371610275934456,2.3707897657606494,2.369956575916745,2.3691105308053717,2.3682514532103585,2.3673791643193747,2.3664934837484717,2.365594229568602,2.3646812183341694,2.3637542651136783,2.362813183522535,2.3618577857580774,2.3608878826368898,2.3599032836344693,2.3589037969273203,2.357889229437526,2.356859386879887,2.3558140738116844,2.354753093685139,2.3536762489026404,2.352583340874819,2.3514741700815343,2.350348536135843,2.3492062378510408,2.348047073310832,2.346870839942717,2.3456773345946655,2.3444663536151538,2.3432376929366407,2.3419911481625677,2.3407265146579386,2.3394435876435784,2.3381421622941323,2.3368220338398853,2.3354829976724782,2.334124849454587,2.332747385233652,2.3313504015597166,2.3299336956074477,2.328497065302421,2.327040309451714,2.3255632278789005,2.3240656215634865,2.322547292784863,2.321008045270831,2.319447684350745,2.3178660171133467,2.3162628525693147,2.3146380018185977,2.31299127822255,2.3113224975809246,2.3096314783137446,2.3079180416480773,2.306182011809748,2.3044232162199854,2.302641485697028,2.300836654662675,2.299008561353802,2.297157048038797,2.2952819612389286,2.2933831519545955,2.2914604758964328,2.289513793721227,2.287542971272577,2.2855478798262574,2.2835283963401793,2.2814844037088933,2.2794157910225183,2.2773224538299957,2.2752042944065547,2.2730612220252455,2.2708931532324117,2.2687000121269336,2.266481730643081,2.264238248836788,2.261969515175156,2.2596754868289777,2.257356129968047,2.2550114200590365,2.2526413421656604,2.2502458912508834,2.2478250724808744,2.2453789015304118,2.242907404889425,2.2404106201703513,2.2378885964159476,2.2353413944072154,2.2327690869710524,2.2301717592872405,2.227549509194371,2.224902447494278,2.222230698254554,2.219534399108683,2.216813701553344,2.2140687712423843,2.2112997882769876,2.2085069474915215,2.2056904587345394,2.202850547144416,2.199987453419058,2.1971014340791384,2.1941927617242905,2.191261725281675,2.1883086302463246,2.185333798912695,2.1823375705967774,2.1793203018481826,2.176282366651571,2.1732241566167927,2.1701460811571076,2.1670485676548465,2.163932061613874,2.1607970267982015,2.1576439453561185,2.154473317929174,2.1512856637453863,2.1480815206960195,2.144861445395295,2.141626013222401,2.138375818345161,2.135111473724753,2.1318336111008502,2.12854288095658,2.1252399524627124,2.1219255134004875,2.1186002700625206,2.1152649471312297,2.1119202875342475,2.1085670522763063,2.1052060202470986,2.101837988004635,2.0984637695336636,2.0950841959787083,2.091700115351352,2.0883123922113787,2.0849219073214593,2.0815295572750667,2.0781362540973625,2.0747429248188274,2.071350511021449,2.0679599683573184,2.06457226603954,2.061188386305392,2.05780932385174,2.054436085242737,2.051069688289911,2.047711161404795,2.0443615429242894,2.041021880409048,2.037693229915185,2.0343766552397105,2.0310732271401424,2.0277840225288033,2.0245101236424072,2.0212526171875633,2.018012593462951,2.0147911454589376,2.011589367935525,2.008408356479555,2.0052492065421967,2.0021130124578024,1.9990008664452998,1.9959138575933542,1.992853070830621,1.9898195858824663,1.9868144762156217,1.983838807972296,1.9808936388953486,1.9779800172461852,1.9750989807171073,1.9722515553399136,1.9694387543925953,1.966661577306045,1.9639210085727226,1.9612180166593034,1.958553552925337,1.9559285505500217,1.9533439234692014,1.950800565324736,1.9482993484284103,1.945841122742561,1.9434267148796096,1.9410569271226934,1.938732536469567,1.936454293701952,1.934222922482475,1.932039118481308,1.9299035485345994,1.927816849836716,1.925779629168289,1.923792462161974,1.9218558926077842,1.9199704317997608,1.9181365579256773,1.9163547155013738,1.9146253148512187,1.9129487316360916,1.9113253064301736,1.9097553443476978,1.9082391147207116,1.9067768508287528,1.9053687496812208,1.904014971853081,1.9027156413744004,1.9014708456740705,1.9002806355779214,1.8991450253612927,1.8980639928559704,1.8970374796112452,1.8960653911087195,1.895147597030313,1.8942839315787996,1.8934741938500448,1.8927181482559823,1.8920155249972297,1.8913660205841096,1.8907692984047186,1.8902249893385605,1.8897326924141504,1.8892919755088788,1.8889023760893329,1.8885634019901698,1.8882745322295518,1.888035217859076,1.887844882846059,1.8877029249859747,1.8876087168427909,1.887561606714904,1.8875609196243397,1.8876059583268514,1.8876960043405402,1.8878303189906034,1.8880081444678232,1.8882287048984043,1.888491207422801,1.8887948432811774,1.8891387889031959,1.8895222069998499,1.8899442476551127,1.890404049415221,1.8909007403734697,1.8914334392484589,1.8920012564538051,1.8926032951573915,1.8932386523283251,1.893906419769838,1.8946056851364563,1.895335532933857,1.896095045499908,1.8968833039654953,1.8976993891938205,1.8985423826969556,1.8994113675285318,1.900305429151541,1.9012236562803169,1.9021651416958612,1.903128983033775,1.904114283544147,1.9051201528228405,1.9061457075137132,1.907190071981387,1.9082523789542765,1.9093317701376582,1.910427396796654,1.9115384203090646,1.9126640126880763,1.9138033570749255,1.914955648201679,1.9161200928243434,1.9172959101265885,1.9184823320944138,1.919678603862154,1.9208839840302547,1.9220977449553094,1.9233191730128767,1.9245475688336529,1.9257822475135935,1.9270225387986237,1.9282677872445932,1.9295173523531721,1.93077060868439,1.9320269459465544,1.9332857690642908,1.9345464982254679,1.9358085689077746,1.9370714318857298,1.9383345532189113,1.9395974142221883,1.9408595114187484,1.942120356476708,1.9433794761300882,1.9446364120849378,1.9458907209113812,1.9471419739223452,1.9483897570397326,1.9496336706487787,1.9508733294413245,1.9521083622487274,1.953338411865109,1.954563134861629,1.955782201392461,1.956995294993119,1.9582021123717748,1.959402363194183,1.960595769862817,1.9617820672907913,1.9629610026711366],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.7521232333365209,1.7521232332882468,1.7521232332390744,1.7521232331889869,1.7521232331379673,1.7521232330859982,1.7521232330330618,1.7521232329791405,1.7521232329242153,1.752123232868268,1.7521232328112797,1.7521232327532306,1.752123232694101,1.7521232326338714,1.752123232572521,1.7521232325100284,1.752123232446373,1.752123232381533,1.7521232323154858,1.7521232322482096,1.7521232321796818,1.7521232321098785,1.7521232320387756,1.7521232319663502,1.7521232318925763,1.7521232318174296,1.7521232317408846,1.7521232316629145,1.7521232315834938,1.752123231502595,1.7521232314201904,1.7521232313362523,1.752123231250752,1.7521232311636605,1.7521232310749482,1.752123230984585,1.75212323089254,1.7521232307987817,1.7521232307032786,1.7521232306059984,1.7521232305069074,1.7521232304059724,1.7521232303031586,1.7521232301984317,1.7521232300917557,1.7521232299830942,1.7521232298724105,1.752123229759667,1.7521232296448248,1.7521232295278457,1.7521232294086893,1.7521232292873155,1.7521232291636828,1.7521232290377489,1.752123228909471,1.7521232287788062,1.7521232286457096,1.7521232285101358,1.7521232283720387,1.7521232282313717,1.7521232280880863,1.7521232279421346,1.7521232277934666,1.7521232276420315,1.752123227487778,1.7521232273306537,1.752123227170605,1.7521232270075775,1.7521232268415161,1.752123226672364,1.7521232265000637,1.7521232263245567,1.752123226145783,1.7521232259636823,1.7521232257781927,1.7521232255892503,1.752123225396792,1.7521232252007513,1.752123225001062,1.7521232247976564,1.7521232245904652,1.7521232243794174,1.752123224164442,1.7521232239454654,1.7521232237224131,1.7521232234952098,1.7521232232637773,1.752123223028038,1.7521232227879107,1.7521232225433143,1.7521232222941656,1.7521232220403797,1.7521232217818703,1.7521232215185494,1.7521232212503282,1.7521232209771143,1.752123220698815,1.7521232204153365,1.7521232201265817,1.7521232198324526,1.752123219532849,1.7521232192276688,1.752123218916809,1.752123218600163,1.7521232182776236,1.7521232179490809,1.7521232176144232,1.7521232172735366,1.7521232169263052,1.752123216572611,1.7521232162123335,1.7521232158453504,1.7521232154715367,1.752123215090765,1.7521232147029064,1.7521232143078285,1.752123213905397,1.752123213495475,1.752123213077923,1.7521232126525992,1.7521232122193584,1.752123211778054,1.7521232113285352,1.7521232108706495,1.752123210404241,1.752123209929151,1.752123209445218,1.7521232089522767,1.7521232084501601,1.752123207938698,1.7521232074177147,1.7521232068870343,1.7521232063464756,1.7521232057958551,1.7521232052349853,1.7521232046636754,1.7521232040817312,1.7521232034889542,1.7521232028851428,1.752123202270092,1.7521232016435924,1.7521232010054306,1.75212320035539,1.7521231996932485,1.7521231990187813,1.7521231983317591,1.752123197631948,1.7521231969191096,1.7521231961930017,1.752123195453377,1.7521231946999838,1.7521231939325657,1.7521231931508616,1.752123192354606,1.752123191543527,1.752123190717349,1.7521231898757905,1.7521231890185658,1.7521231881453827,1.752123187255944,1.752123186349947,1.7521231854270836,1.7521231844870395,1.7521231835294944,1.7521231825541228,1.7521231815605929,1.7521231805485658,1.7521231795176975,1.7521231784676368,1.7521231773980268,1.7521231763085028,1.752123175198694,1.7521231740682226,1.752123172916704,1.752123171743746,1.7521231705489495,1.752123169331907,1.752123168092205,1.7521231668294206,1.7521231655431242,1.7521231642328778,1.7521231628982354,1.7521231615387423,1.7521231601539358,1.752123158743344,1.7521231573064864,1.752123155842874,1.7521231543520086,1.7521231528333816,1.752123151286476,1.752123149710766,1.7521231481057136,1.7521231464707727,1.7521231448053862,1.7521231431089865,1.7521231413809963,1.7521231396208266,1.7521231378278774,1.7521231360015386,1.7521231341411871,1.7521231322461897,1.7521231303159006,1.7521231283496619,1.7521231263468033,1.7521231243066429,1.7521231222284848,1.7521231201116212,1.7521231179553307,1.7521231157588775,1.752123113521514,1.7521231112424769,1.7521231089209897,1.7521231065562608,1.7521231041474838,1.752123101693838,1.7521230991944863,1.7521230966485764,1.752123094055241,1.7521230914135952,1.7521230887227381,1.752123085981752,1.7521230831897023,1.7521230803456367,1.7521230774485845,1.752123074497558,1.7521230714915506,1.7521230684295366,1.752123065310471,1.7521230621332897,1.7521230588969086,1.7521230556002234,1.752123052242109,1.7521230488214192,1.7521230453369865,1.7521230417876215,1.7521230381721122,1.752123034489225,1.7521230307377018,1.752123026916262,1.752123023023601,1.7521230190583885,1.752123015019271,1.7521230109048689,1.7521230067137765,1.7521230024445624,1.7521229980957678,1.7521229936659066,1.7521229891534658,1.7521229845569024,1.752122979874646,1.7521229751050964,1.7521229702466226,1.7521229652975643,1.752122960256229,1.7521229551208928,1.7521229498897997,1.7521229445611608,1.7521229391331525,1.7521229336039195,1.7521229279715689,1.752122922234174,1.7521229163897716,1.7521229104363614,1.752122904371905,1.7521228981943273,1.7521228919015126,1.752122885491307,1.7521228789615142,1.752122872309898,1.7521228655341807,1.75212285863204,1.7521228516011114,1.7521228444389856,1.752122837143207,1.7521228297112759,1.7521228221406433,1.7521228144287135,1.7521228065728418,1.7521227985703338,1.752122790418444,1.752122782114375,1.7521227736552776,1.7521227650382478,1.7521227562603279,1.752122747318504,1.7521227382097049,1.7521227289308021,1.7521227194786082,1.752122709849875,1.7521227000412938,1.7521226900494924,1.752122679871036,1.7521226695024241,1.752122658940091,1.752122648180402,1.7521226372196554,1.7521226260540788,1.752122614679828,1.752122603092987,1.7521225912895653,1.7521225792654957,1.7521225670166365,1.7521225545387646,1.7521225418275799,1.752122528878698,1.7521225156876525,1.7521225022498934,1.752122488560783,1.752122474615595,1.7521224604095151,1.7521224459376366,1.7521224311949595,1.7521224161763889,1.7521224008767333,1.7521223852907022,1.752122369412904,1.752122353237845,1.7521223367599268,1.752122319973444,1.752122302872583,1.7521222854514187,1.7521222677039134,1.7521222496239142,1.7521222312051503,1.752122212441231,1.7521221933256446,1.7521221738517532,1.7521221540127938,1.7521221338018713,1.7521221132119618,1.7521220922359044,1.752122070866402,1.7521220490960168,1.7521220269171696,1.7521220043221342,1.7521219813030364,1.7521219578518514,1.7521219339603988,1.7521219096203429,1.7521218848231845,1.7521218595602635,1.7521218338227509,1.7521218076016485,1.7521217808877834,1.7521217536718072,1.7521217259441892,1.7521216976952152,1.7521216689149826,1.7521216395933976,1.7521216097201706,1.7521215792848124,1.7521215482766297,1.7521215166847228,1.7521214844979796,1.7521214517050714,1.7521214182944493,1.7521213842543402,1.7521213495727408,1.7521213142374141,1.7521212782358833,1.7521212415554293,1.7521212041830831,1.7521211661056226,1.752121127309566,1.7521210877811684,1.7521210475064137,1.7521210064710122,1.7521209646603917,1.7521209220596954,1.7521208786537714,1.7521208344271713,1.7521207893641408,1.7521207434486152,1.7521206966642118,1.7521206489942236,1.7521206004216134,1.7521205509290059,1.7521205004986804,1.7521204491125657,1.752120396752229,1.7521203433988723,1.7521202890333223,1.7521202336360233,1.7521201771870287,1.7521201196659937,1.7521200610521652,1.7521200013243747,1.7521199404610288,1.7521198784401006,1.7521198152391193,1.7521197508351625,1.7521196852048448,1.752119618324309,1.7521195501692155,1.7521194807147324,1.7521194099355235,1.7521193378057391,1.7521192642990044,1.7521191893884072,1.7521191130464875,1.752119035245224,1.7521189559560242,1.7521188751497094,1.7521187927965027,1.7521187088660173,1.7521186233272408,1.7521185361485234,1.7521184472975613,1.7521183567413854,1.7521182644463436,1.752118170378088,1.7521180745015574,1.752117976780963,1.75211787717977,1.752117775660684,1.7521176721856304,1.7521175667157391,1.7521174592113262,1.7521173496318747,1.752117237936016,1.7521171240815119,1.7521170080252317,1.7521168897231345,1.752116769130248,1.7521166462006452,1.7521165208874252,1.7521163931426873,1.752116262917512,1.7521161301619343,1.7521159948249199,1.7521158568543413,1.7521157161969516,1.7521155727983582,1.7521154266029961,1.7521152775541007,1.7521151255936787,1.7521149706624806,1.7521148126999684,1.7521146516442876,1.752114487432235,1.752114319999225,1.75211414927926,1.7521139752048929,1.7521137977071943,1.7521136167157167,1.7521134321584577,1.752113243961822,1.7521130520505839,1.7521128563478463,1.752112656775,1.7521124532516847,1.7521122456957428,1.752112034023176,1.7521118181481028,1.7521115979827082,1.7521113734371991,1.7521111444197546,1.7521109108364745,1.752110672591329,1.7521104295861052,1.7521101817203544,1.7521099288913324,1.7521096709939463,1.7521094079206931,1.7521091395615997,1.7521088658041604,1.7521085865332746,1.752108301631179,1.752108010977381,1.7521077144485908,1.7521074119186493,1.752107103258453,1.7521067883358832,1.7521064670157256,1.752106139159592,1.7521058046258393,1.7521054632694861,1.752105114942126,1.7521047594918402,1.7521043967631056,1.7521040265967043,1.7521036488296255,1.7521032632949678,1.7521028698218395,1.752102468235253,1.7521020583560205,1.7521016400006426,1.7521012129811973,1.7521007771052244,1.7521003321756052,1.7520998779904438,1.7520994143429398,1.7520989410212595,1.752098457808406,1.7520979644820813,1.7520974608145494,1.7520969465724914,1.75209642151686,1.7520958854027278,1.752095337979133,1.7520947789889207,1.7520942081685795,1.7520936252480739,1.752093029950672,1.7520924219927696,1.7520918010837085,1.752091166925591,1.752090519213088,1.7520898576332442,1.7520891818652764,1.7520884915803674,1.7520877864414544,1.7520870661030115,1.7520863302108278,1.7520855784017775,1.752084810303587,1.7520840255345944,1.7520832237035022,1.752082404409125,1.7520815672401318,1.7520807117747785,1.7520798375806375,1.7520789442143179,1.7520780312211806,1.752077098135045,1.7520761444778896,1.7520751697595456,1.7520741734773806,1.7520731551159787,1.7520721141468107,1.7520710500278962,1.7520699622034581,1.7520688501035715,1.7520677131438003,1.7520665507248299,1.7520653622320865,1.7520641470353555,1.7520629044883824,1.7520616339284723,1.7520603346760768,1.7520590060343737,1.7520576472888367,1.7520562577067984,1.7520548365369994,1.7520533830091358,1.7520518963333898,1.7520503756999561,1.7520488202785571,1.7520472292179499,1.7520456016454222,1.7520439366662817,1.7520422333633339,1.75204049079635,1.7520387080015296,1.7520368839909486,1.7520350177520039,1.752033108246843,1.7520311544117906,1.7520291551567604,1.7520271093646635,1.7520250158908048,1.7520228735622727,1.7520206811773187,1.7520184375047319,1.7520161412832018,1.7520137912206784,1.7520113859937203,1.7520089242468386,1.7520064045918349,1.7520038256071284,1.7520011858370845,1.7519984837913318,1.7519957179440755,1.75199288673341,1.751989988560622,1.7519870217894942,1.7519839847456058,1.7519808757156277,1.751977692946622,1.7519744346453343,1.7519710989774917,1.7519676840670975,1.75196418799573,1.7519606088018431,1.7519569444800713,1.7519531929805368,1.751949352208166,1.7519454200220086,1.7519413942345685,1.7519372726111409,1.7519330528691597,1.751928732677559,1.7519243096561459,1.751919781374986,1.7519151453538089,1.7519103990614264,1.7519055399151746,1.751900565280372,1.751895472469803,1.7518902587432241,1.7518849213068977,1.7518794573131509,1.751873863859967,1.7518681379906083,1.7518622766932705,1.7518562769007755,1.7518501354903029,1.7518438492831594,1.751837415044593,1.751830829483652,1.7518240892530919,1.7518171909493314,1.7518101311124619,1.751802906226312,1.7517955127185705,1.7517879469609685,1.751780205269527,1.7517722839048686,1.7517641790726002,1.7517558869237653,1.7517474035553744,1.7517387250110092,1.7517298472815122,1.7517207663057555,1.7517114779715013,1.7517019781163492,1.7516922625287774,1.751682326949283,1.7516721670716175,1.7516617785441293,1.7516511569712108,1.751640297914854,1.7516291968963191,1.751617849397923,1.7516062508649388,1.7515943967076268,1.7515822823033846,1.751569902999031,1.751557254113218,1.751544330938982,1.7515311287464321,1.7515176427855803,1.7515038682893178,1.7514898004765393,1.7514754345554193,1.751460765726842,1.7514457891879904,1.7514305001360964,1.7514148937723528,1.7513989653059945,1.7513827099585482,1.7513661229682551,1.7513491995946697,1.7513319351234364,1.7513143248712473,1.751296364190984,1.751278048477045,1.7512593731708646,1.7512403337666198,1.7512209258171354,1.7512011449399807,1.75118098682377,1.751160447234661,1.751139522023057,1.751118207130516,1.7510964985968662,1.7510743925675314,1.751051885301068,1.751028973176916,1.7510056527033648,1.7509819205257373,1.7509577734347908,1.75093320837534,1.7509082224551025,1.7508828129537668,1.7508569773322844,1.7508307132423913,1.7508040185363536,1.7507768912769455,1.7507493297476544,1.750721332463117,1.7506928981797911,1.7506640259068549,1.750634714917344,1.7506049647595223,1.7505747752684848,1.750544146578,1.750513079132586,1.7504815736998214,1.750449631382898,1.7504172536334024,1.7503844422643404,1.7503511994633951,1.75031752780642,1.7502834302711707,1.750248910251268,1.7502139715703977,1.7501786184967427,1.7501428557576486,1.7501066885545165,1.750070122577931,1.7500331640230102,1.749995819604987,1.7499580965750086,1.7499200027361634,1.7498815464597222,1.7498427367015967,1.749803583019013,1.7497640955873917,1.7497242852174364,1.7496841633724212,1.74964374218568,1.749603034478285,1.7495620537769132,1.7495208143318999,1.749479331135463,1.7494376199401038,1.7493956972771674,1.749353580475564,1.7493112876806383,1.7492688378731829,1.7492262508885845,1.7491835474360962,1.7491407491182274,1.7490978784502385,1.7490549588797344,1.7490120148063437,1.7489690716014734,1.7489261556281275,1.7488832942607768,1.7488405159052693,1.7487978500187655,1.748755327129687,1.7487129788576636,1.7486708379334641,1.7486289382188973,1.748587314726662,1.748546003640139,1.748505042333101,1.7484644693893228,1.7484243246220819,1.7483846490935204,1.7483454851338611,1.748306876360445,1.7482688676965843,1.7482315053902002,1.7481948370322316,1.7481589115747869,1.7481237793490256,1.7480894920827392,1.7480561029176138,1.748023666426151,1.747992238628224,1.7479618770072403,1.7479326405258953,1.747904589641485,1.7478777863207569,1.7478522940542707,1.7478281778702487,1.7478055043478808,1.74778434163007,1.7477647594355776,1.7477468290705556,1.747730623439427,1.7477162170550933,1.747703686048439,1.7476931081771052,1.747684562833505,1.7476781310520466,1.747673895515542,1.7476719405607684,1.7476723521831503,1.7476752180405348,1.7476806274560284,1.7476886714198652,1.7476994425902708,1.7477130352932992,1.7477295455215973,1.7477490709320755,1.7477717108424455,1.7477975662265919,1.7478267397087437,1.7478593355564136,1.7478954596720626,1.7479352195834605,1.7479787244327045,1.7480260849638543,1.7480774135091524,1.7481328239737834,1.7481924318191424,1.7482563540445641,1.7483247091674785,1.7483976172019475,1.7484751996355465,1.748557579404543,1.7486448808673343,1.7487372297761001,1.748834753246625,1.7489375797262507,1.74904583895991,1.7491596619542025,1.749279180939465,1.7494045293297962,1.749535841680986,1.7496732536463133,1.7498169019301635,1.749966924239425,1.7501234592326278,1.7502866464667743,1.7504566263418326,1.7506335400428488,1.7508175294796438,1.7510087372240608,1.7512073064447304,1.7514133808393257,1.751627104564277,1.7518486221619258,1.7520780784850962,1.7523156186190638,1.7525613878009143,1.752815531336279,1.7530781945134424,1.7533495225148261,1.7536296603258477,1.7539187526411748,1.754216943768381,1.754524377529034,1.7548411971572413,1.7551675451956872,1.7555035633892104,1.7558493925759642,1.756205172576219,1.7565710420788743,1.7569471385257467,1.757333597993719,1.7577305550748363,1.7581381427544442,1.7585564922874768,1.7589857330730037,1.759425992527159,1.759877395954582,1.7603400664185045,1.7608141246096356,1.7612996887139885,1.7617968742798242,1.7623057940838667,1.7628265579969764,1.7633592728494625,1.7639040422962224,1.7644609666819087,1.765030142906323,1.7656116642902495,1.7662056204419352,1.7668120971244423,1.7674311761240908,1.768062935120216,1.7687074475564735,1.76936478251392,1.7700350045861015,1.7707181737563853,1.7714143452777686,1.772123569555396,1.7728458920320196,1.773581353076631,1.774329987876498,1.7750918263328217,1.7758668929602446,1.7766552067904207,1.77745678127986,1.7782716242222523,1.7790997376654685,1.7799411178334321,1.7807957550530404,1.7816636336863159,1.7825447320679473,1.78343902244838,1.7843464709426031,1.785267037484765,1.786200675788744,1.787147333314789,1.7881069512423295,1.7890794644490462,1.7900648014962797,1.7910628846208443,1.7920736297332969,1.7930969464227053,1.7941327379679415,1.7951809013555164,1.7962413273039581,1.7973139002947267,1.7983984986096404,1.7994949943747798,1.8006032536108292,1.8017231362897876,1.8028544963979913,1.8039971820053595,1.805151035340777,1.8063158928735081,1.8074915854005387,1.8086779381397138,1.8098747708285499,1.8110818978285812,1.8122991282350893,1.8135262659920652,1.8147631100122399,1.8160094543020158,1.8172650880911214,1.8185297959668143,1.819803358012435,1.821085549950134,1.8223761432875663,1.8236749054683605,1.8249816000261536,1.8262959867419941,1.827617821804897,1.8289468579753447,1.8302828447515238,1.8316255285380785,1.8329746528171746,1.83432995832165,1.8356911832100467,1.8370580632433042,1.8384303319628994,1.839807720870224,1.841189959606984,1.8425767761364107,1.8439678969250752,1.845363047125099,1.846761950756552,1.8481643308898417,1.849569909827882,1.8509784092878587,1.8523895505823806,1.8538030547998352,1.8552186429837563,1.8566360363110168,1.8580549562686668,1.859475124829237,1.8608962646243332,1.8623180991163504,1.8637403527681424,1.865162751210479,1.8665850214071376,1.8680068918174744,1.8694280925563211,1.8708483555510689,1.8722674146957956,1.873685006002302,1.8751008677479284,1.8765147406200209,1.8779263678569382,1.8793354953854733,1.8807418719545879,1.8821452492653528,1.883545382096998,1.884942028428982,1.8863349495589872,1.8877239102167704,1.889108678673786,1.890489026848518,1.891864730407457,1.8932355688616662,1.894601325658887,1.8959617882711375,1.897316748277775,1.898666001443978,1.9000093477946387,1.9013465916836327,1.9026775418584654,1.9040020115202834,1.905319818379252,1.906630784705308,1.9079347373742972,1.9092315079095192,1.9105209325187005,1.911802852126432,1.9130771124021004,1.9143435637833632,1.9156020614952103,1.916852465564666,1.9180946408311932,1.9193284569528588,1.9205537884083301,1.9217705144947788,1.922978519321764,1.9241776918011815,1.925367925633364,1.9265491192894177,1.927721175989898,1.9288840036799093,1.9300375150007418,1.9311816272581384,1.9323162623872994,1.9334413469147398,1.9345568119171,1.9356625929770293,1.9367586301362527,1.9378448678459381,1.9389212549144785,1.9399877444528129],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.8418461438214198,1.841846143226342,1.8418461426201898,1.8418461420027572,1.8418461413738338,1.841846140733206,1.8418461400806563,1.8418461394159622,1.8418461387388982,1.8418461380492346,1.8418461373467354,1.8418461366311627,1.8418461359022733,1.8418461351598192,1.841846134403548,1.8418461336332024,1.8418461328485205,1.8418461320492356,1.8418461312350758,1.8418461304057645,1.8418461295610198,1.8418461287005539,1.8418461278240748,1.8418461269312845,1.8418461260218788,1.8418461250955493,1.8418461241519806,1.8418461231908518,1.8418461222118365,1.8418461212146016,1.841846120198808,1.8418461191641105,1.8418461181101566,1.8418461170365892,1.8418461159430424,1.8418461148291443,1.8418461136945163,1.841846112538773,1.8418461113615212,1.8418461101623607,1.8418461089408835,1.8418461076966743,1.8418461064293106,1.841846105138361,1.8418461038233869,1.8418461024839408,1.8418461011195673,1.8418460997298027,1.8418460983141747,1.8418460968722012,1.841846095403393,1.84184609390725,1.8418460923832631,1.8418460908309153,1.8418460892496777,1.8418460876390137,1.8418460859983745,1.8418460843272033,1.8418460826249312,1.84184608089098,1.8418460791247593,1.8418460773256693,1.8418460754930979,1.8418460736264226,1.841846071725008,1.841846069788208,1.841846067815364,1.8418460658058053,1.8418460637588483,1.8418460616737975,1.8418460595499437,1.841846057386565,1.8418460551829254,1.841846052938276,1.8418460506518535,1.8418460483228807,1.8418460459505654,1.8418460435341013,1.8418460410726667,1.8418460385654245,1.8418460360115223,1.8418460334100921,1.8418460307602489,1.8418460280610922,1.8418460253117042,1.84184602251115,1.841846019658477,1.8418460167527155,1.8418460137928785,1.841846010777958,1.84184600770693,1.8418460045787506,1.841846001392355,1.841845998146661,1.8418459948405643,1.841845991472941,1.8418459880426465,1.8418459845485138,1.841845980989356,1.8418459773639615,1.8418459736710986,1.8418459699095115,1.8418459660779216,1.8418459621750256,1.8418459581994966,1.8418459541499834,1.8418459500251085,1.8418459458234697,1.8418459415436392,1.8418459371841607,1.8418459327435521,1.8418459282203046,1.8418459236128795,1.8418459189197103,1.8418459141392014,1.8418459092697275,1.8418459043096331,1.8418458992572315,1.8418458941108051,1.841845888868604,1.841845883528846,1.8418458780897156,1.8418458725493634,1.8418458669059055,1.8418458611574235,1.841845855301963,1.841845849337533,1.8418458432621052,1.8418458370736148,1.841845830769957,1.8418458243489892,1.8418458178085277,1.8418458111463498,1.841845804360189,1.8418457974477391,1.8418457904066494,1.8418457832345259,1.8418457759289308,1.8418457684873795,1.8418457609073426,1.8418457531862427,1.8418457453214538,1.8418457373103034,1.8418457291500672,1.8418457208379704,1.841845712371187,1.8418457037468392,1.8418456949619944,1.8418456860136656,1.8418456768988103,1.8418456676143298,1.8418456581570681,1.841845648523809,1.8418456387112776,1.841845628716138,1.8418456185349918,1.8418456081643775,1.8418455976007693,1.8418455868405759,1.8418455758801386,1.8418455647157317,1.841845553343559,1.841845541759754,1.8418455299603789,1.8418455179414217,1.8418455056987966,1.8418454932283412,1.8418454805258155,1.8418454675869018,1.8418454544071998,1.8418454409822296,1.8418454273074272,1.8418454133781428,1.841845399189641,1.8418453847370986,1.8418453700156017,1.8418453550201448,1.8418453397456307,1.8418453241868662,1.841845308338562,1.8418452921953292,1.8418452757516808,1.8418452590020256,1.8418452419406697,1.8418452245618127,1.8418452068595466,1.8418451888278526,1.841845170460601,1.8418451517515473,1.8418451326943315,1.8418451132824742,1.8418450935093769,1.8418450733683172,1.841845052852448,1.8418450319547943,1.8418450106682522,1.841844988985585,1.8418449668994217,1.8418449444022538,1.8418449214864332,1.8418448981441704,1.841844874367529,1.841844850148427,1.8418448254786306,1.8418448003497538,1.8418447747532534,1.8418447486804284,1.8418447221224155,1.8418446950701866,1.841844667514545,1.8418446394461236,1.841844610855381,1.8418445817325981,1.8418445520678746,1.8418445218511266,1.8418444910720826,1.8418444597202794,1.8418444277850596,1.8418443952555679,1.8418443621207459,1.8418443283693302,1.8418442939898485,1.8418442589706139,1.8418442232997225,1.8418441869650488,1.841844149954243,1.841844112254723,1.8418440738536752,1.841844034738046,1.8418439948945395,1.841843954309613,1.841843912969471,1.8418438708600613,1.8418438279670712,1.8418437842759212,1.8418437397717602,1.8418436944394618,1.8418436482636176,1.8418436012285322,1.8418435533182191,1.841843504516394,1.84184345480647,1.8418434041715508,1.8418433525944269,1.8418433000575678,1.8418432465431178,1.8418431920328882,1.8418431365083525,1.8418430799506393,1.8418430223405269,1.841842963658435,1.8418429038844204,1.8418428429981673,1.8418427809789835,1.841842717805792,1.8418426534571222,1.841842587911106,1.8418425211454674,1.8418424531375168,1.8418423838641422,1.841842313301802,1.8418422414265168,1.8418421682138613,1.841842093638955,1.8418420176764567,1.8418419403005521,1.8418418614849477,1.8418417812028611,1.8418416994270115,1.8418416161296112,1.8418415312823548,1.8418414448564124,1.8418413568224168,1.841841267150454,1.8418411758100555,1.8418410827701852,1.8418409879992306,1.8418408914649904,1.8418407931346656,1.8418406929748472,1.8418405909515048,1.8418404870299752,1.841840381174951,1.8418402733504686,1.8418401635198955,1.8418400516459186,1.8418399376905303,1.8418398216150176,1.8418397033799467,1.841839582945152,1.8418394602697201,1.8418393353119784,1.8418392080294779,1.8418390783789826,1.8418389463164515,1.8418388117970255,1.8418386747750122,1.8418385352038695,1.8418383930361897,1.8418382482236855,1.8418381007171714,1.8418379504665476,1.8418377974207842,1.8418376415279019,1.8418374827349566,1.84183732098802,1.8418371562321614,1.8418369884114298,1.8418368174688349,1.8418366433463271,1.8418364659847788,1.8418362853239634,1.8418361013025362,1.8418359138580118,1.841835722926745,1.841835528443908,1.841835330343469,1.8418351285581693,1.8418349230195012,1.8418347136576851,1.8418345004016445,1.8418342831789838,1.8418340619159623,1.84183383653747,1.8418336069670025,1.8418333731266348,1.8418331349369945,1.8418328923172358,1.8418326451850116,1.8418323934564471,1.841832137046109,1.8418318758669785,1.8418316098304222,1.8418313388461598,1.841831062822237,1.841830781664992,1.8418304952790243,1.8418302035671639,1.841829906430437,1.8418296037680328,1.8418292954772708,1.8418289814535644,1.841828661590387,1.8418283357792353,1.8418280039095933,1.8418276658688957,1.8418273215424878,1.8418269708135888,1.8418266135632522,1.8418262496703248,1.8418258790114073,1.8418255014608123,1.8418251168905206,1.8418247251701407,1.8418243261668636,1.8418239197454167,1.8418235057680212,1.8418230840943441,1.8418226545814513,1.8418222170837593,1.8418217714529868,1.841821317538105,1.8418208551852866,1.841820384237854,1.841819904536227,1.841819415917869,1.8418189182172333,1.8418184112657061,1.8418178948915525,1.8418173689198551,1.841816833172459,1.8418162874679107,1.8418157316213963,1.8418151654446826,1.8418145887460502,1.8418140013302327,1.841813402998349,1.841812793547839,1.8418121727723935,1.841811540461887,1.8418108964023074,1.8418102403756833,1.841809572160012,1.8418088915291864,1.8418081982529166,1.8418074920966576,1.8418067728215266,1.8418060401842253,1.84180529393696,1.841804533827357,1.8418037595983794,1.8418029709882422,1.8418021677303238,1.8418013495530787,1.841800516179946,1.841799667329259,1.8417988027141494,1.8417979220424545,1.8417970250166187,1.8417961113335943,1.841795180684743,1.8417942327557322,1.84179326722643,1.8417922837708016,1.8417912820567985,1.8417902617462512,1.8417892224947554,1.8417881639515592,1.841787085759447,1.841785987554621,1.8417848689665821,1.8417837296180062,1.8417825691246212,1.84178138709508,1.84178018313083,1.8417789568259844,1.8417777077671866,1.8417764355334754,1.8417751396961455,1.8417738198186084,1.8417724754562474,1.8417711061562718,1.841769711457569,1.841768290890554,1.8417668439770138,1.8417653702299523,1.84176386915343,1.8417623402424024,1.841760782982555,1.8417591968501357,1.8417575813117821,1.8417559358243503,1.8417542598347347,1.8417525527796916,1.8417508140856527,1.8417490431685395,1.8417472394335743,1.841745402275087,1.8417435310763168,1.8417416252092151,1.8417396840342388,1.8417377069001457,1.8417356931437836,1.841733642089874,1.8417315530507956,1.841729425326363,1.8417272582035997,1.8417250509565077,1.8417228028458361,1.8417205131188419,1.8417181810090475,1.8417158057359957,1.841713386504999,1.8417109225068853,1.841708412917738,1.8417058568986324,1.8417032535953686,1.8417006021381979,1.8416979016415451,1.841695151203728,1.8416923499066673,1.8416894968155972,1.8416865909787674,1.8416836314271416,1.8416806171740878,1.8416775472150697,1.8416744205273254,1.8416712360695457,1.841667992781545,1.8416646895839264,1.8416613253777425,1.8416578990441488,1.8416544094440517,1.8416508554177515,1.8416472357845777,1.841643549342521,1.8416397948678538,1.84163597111475,1.841632076814897,1.8416281106770969,1.8416240713868668,1.8416199576060288,1.8416157679722949,1.8416115010988428,1.8416071555738882,1.841602729960246,1.8415982227948864,1.841593632588483,1.841588957824954,1.8415841969609952,1.8415793484256053,1.8415744106196021,1.8415693819151342,1.8415642606551803,1.8415590451530444,1.8415537336918375,1.8415483245239566,1.841542815870552,1.8415372059209842,1.8415314928322764,1.8415256747285536,1.8415197497004758,1.8415137158046593,1.841507571063091,1.8415013134625318,1.8414949409539096,1.841488451451705,1.8414818428333237,1.841475112938462,1.8414682595684597,1.8414612804856438,1.841454173412661,1.8414469360318,1.8414395659843035,1.841432060869667,1.841424418244929,1.8414166356239492,1.8414087104766736,1.8414006402283907,1.8413924222589728,1.8413840539021102,1.841375532444528,1.8413668551251925,1.8413580191345083,1.841349021613499,1.8413398596529755,1.8413305302926972,1.8413210305205088,1.8413113572714783,1.8413015074270098,1.8412914778139486,1.8412812652036739,1.8412708663111736,1.8412602777941092,1.8412494962518633,1.8412385182245783,1.8412273401921753,1.8412159585733623,1.8412043697246268,1.8411925699392149,1.8411805554460956,1.841168322408909,1.841155866924903,1.8411431850238524,1.841130272666965,1.841117125745772,1.8411037400810035,1.8410901114214524,1.8410762354428174,1.8410621077465366,1.8410477238586043,1.8410330792283733,1.8410181692273422,1.8410029891479303,1.8409875342022337,1.8409717995207728,1.8409557801512217,1.8409394710571259,1.840922867116604,1.8409059631210392,1.8408887537737562,1.8408712336886839,1.8408533973890089,1.840835239305814,1.8408167537767075,1.8407979350444392,1.840778777255509,1.840759274458762,1.840739420603976,1.8407192095404423,1.8406986350155343,1.8406776906732731,1.840656370052883,1.840634666587346,1.8406125736019483,1.8405900843128242,1.8405671918255009,1.8405438891334363,1.8405201691165627,1.8404960245398299,1.8404714480517503,1.8404464321829488,1.840420969344722,1.8403950518276004,1.8403686717999235,1.8403418213064258,1.8403144922668344,1.8402866764744858,1.840258365594957,1.8402295511647173,1.8402002245898061,1.84017037714453,1.8401399999701922,1.8401090840738505,1.8400776203271088,1.8400455994649447,1.8400130120845772,1.839979848644376,1.8399460994628198,1.8399117547175003,1.839876804444183,1.8398412385359217,1.8398050467422384,1.8397682186683655,1.8397307437745551,1.839692611375468,1.8396538106396367,1.8396143305890094,1.839574160098587,1.8395332878961477,1.8394917025620685,1.8394493925292552,1.8394063460831724,1.8393625513619896,1.8393179963568487,1.8392726689122487,1.8392265567265682,1.8391796473527162,1.8391319281989293,1.8390833865297125,1.8390340094669375,1.8389837839910956,1.8389326969427235,1.8388807350239922,1.8388278848004829,1.8387741327031426,1.8387194650304337,1.838663867950679,1.8386073275046135,1.838549829608147,1.8384913600553416,1.8384319045216158,1.8383714485671778,1.8383099776406973,1.8382474770832165,1.8381839321323175,1.8381193279265384,1.8380536495100588,1.8379868818376472,1.8379190097798874,1.837850018128684,1.8377798916030537,1.837708614855208,1.8376361724769343,1.8375625490062808,1.837487728934545,1.8374116967135796,1.8373344367634143,1.8372559334801977,1.837176171244468,1.8370951344297537,1.8370128074115066,1.836929174576375,1.8368442203318145,1.8367579291160459,1.8366702854083556,1.8365812737397496,1.8364908787039511,1.836399084968757,1.8363058772877412,1.836211240512319,1.8361151596041572,1.8360176196479485,1.8359186058645314,1.835818103624369,1.8357160984613785,1.8356125760871094,1.8355075224052755,1.8354009235266266,1.8352927657841693,1.8351830357487229,1.8350717202448144,1.834958806366906,1.834844281495946,1.834728133316245,1.8346103498326656,1.8344909193881243,1.834369830681394,1.8342470727852063,1.8341226351646371,1.8339965076957774,1.8338686806846716,1.8337391448865212,1.833607891525141,1.8334749123126555,1.833340199469432,1.8332037457442305,1.8330655444345676,1.8329255894072736,1.8327838751192396,1.832640396638334,1.832495149664479,1.8323481305508738,1.832199336325347,1.8320487647118269,1.8318964141519138,1.8317422838265374,1.8315863736776858,1.8314286844301881,1.8312692176135354,1.8311079755837216,1.8309449615450901,1.830780179572165,1.8306136346314508,1.8304453326031844,1.8302752803030178,1.8301034855036142,1.8299299569561416,1.8297547044116376,1.8295777386422374,1.8293990714622343,1.8292187157489612,1.829036685463472,1.8288529956709978,1.8286676625611675,1.8284807034679695,1.8282921368894287,1.828101982506989,1.8279102612045781,1.8277169950873307,1.8275222074999578,1.8273259230447398,1.8271281675991238,1.8269289683329062,1.8267283537249852,1.82652635357966,1.8263229990424652,1.8261183226155113,1.8259123581723316,1.8257051409721976,1.825496707673905,1.825287096348999,1.8250763464944348,1.8248644990446483,1.8246515963830279,1.8244376823527713,1.8242228022671099,1.8240070029188935,1.823790332589514,1.8235728410571643,1.8233545796044106,1.8231356010250737,1.8229159596304056,1.8226957112545505,1.8224749132592777,1.8222536245379812,1.8220319055189345,1.8218098181677895,1.821587425989316,1.8213647940283713,1.8211419888700897,1.8209190786392944,1.8206961329991107,1.8204732231487921,1.8202504218207378,1.8200278032767057,1.8198054433032154,1.8195834192061304,1.8193618098044262,1.8191406954231266,1.8189201578854197,1.8187002805039385,1.8184811480712073,1.8182628468492532,1.8180454645583768,1.817829090365081,1.8176138148691532,1.8173997300899045,1.8171869294515566,1.8169755077677767,1.8167655612253633,1.8165571873670687,1.8163504850735723,1.8161455545445857,1.8159424972790985,1.8157414160547605,1.8155424149063921,1.815345599103626,1.8151510751276756,1.8149589506472255,1.8147693344934421,1.8145823366340994,1.81439806814682,1.81421664119142,1.814038168981362,1.8138627657543036,1.8136905467417421,1.813521628137749,1.8133561270667864,1.8131941615506024,1.8130358504741984,1.8128813135508601,1.812730671286252,1.8125840449415596,1.81244155649568,1.8123033286064494,1.8121694845709022,1.8120401482845536,1.8119154441996985,1.811795497282717,1.8116804329703837,1.8115703771251683,1.811465455989522,1.8113657961391432,1.8112715244352133,1.8111827679755996,1.8110996540450133,1.8110223100641207,1.8109508635376037,1.8108854420011569,1.8108261729674262,1.810773183870882,1.810726602011622,1.8106865544981054,1.810653168188819,1.8106265696328718,1.8106068850095274,1.8105942400666681,1.8105887600582098,1.8105905696804605,1.8105997930074431,1.8106165534251863,1.8106409735650018,1.8106731752357599,1.8107132793551841,1.8107614058801826,1.810817673736243,1.810882200745913,1.8109551035564007,1.811036497566318,1.8111264968516105,1.8112252140907092,1.8113327604889418,1.8114492457022537,1.8115747777602824,1.8117094629888402,1.8118534059318567,1.812006709272843,1.812169473755937,1.8123417981065975,1.8125237789520134,1.8127155107413027,1.812917085665577,1.813128593577949,1.8133501219135688,1.8135817556097682,1.813823577026412,1.8140756658665356,1.8143380990973748,1.8146109508718744,1.8148942924507847,1.8151881921254365,1.8154927151413107,1.815807923622496,1.8161338764971473,1.816470629424053,1.816818234720418,1.8171767412909727,1.8175461945585194,1.8179266363960211,1.8183181050603527,1.8187206351278122,1.8191342574315084,1.8195589990007317,1.81999488300241,1.82044192868476,1.8209001513232315,1.821369562168848,1.821850168399038,1.8223419730710573,1.8228449750780857,1.823359169108094,1.823884545605561,1.8244210907361242,1.8249687863542372,1.8255276099739095,1.8260975347425965,1.8266785294182986,1.8272705583499333,1.8278735814610327,1.8284875542368093,1.8291124277146433,1.8297481484780225,1.830394658653969,1.8310518959139856,1.8317197934785365,1.8323982801250884,1.8330872801997193,1.8337867136323027,1.834496495955272,1.83521653832596,1.8359467475525058,1.836687026123322,1.8374372722400953,1.8381973798543096,1.838967238707257,1.8397467343735145,1.840535748307843,1.8413341578954816,1.8421418365057824,1.8429586535491524,1.8437844745372436,1.8446191611463467,1.8454625712839248,1.8463145591582362,1.8471749753509783,1.8480436668928937,1.8489204773422658,1.8498052468662405,1.850697812324896,1.8515980073579912,1.8525056624743153,1.8534206051435578,1.8543426598906203,1.855271648392289,1.8562073895761804,1.8571496997218742,1.8580983925641528,1.8590532793982422,1.8600141691869825,1.860980868669817,1.8619531824735167,1.8629309132245389,1.8639138616629225,1.8649018267576232,1.8658946058231796,1.8668919946376192,1.867893787561488,1.8688997776579057,1.8699097568135354,1.8709235158603617,1.8719408446981614,1.8729615324175632,1.8739853674235745,1.8750121375594664,1.876041630230903,1.8770736325301889,1.878107931360529,1.8791443135601729,1.8801825660263278,1.8812224758387208,1.8822638303826862,1.8833064174716592,1.884350025468955,1.8853944434087078,1.8864394611158533,1.88748486932503,1.8885304597982782,1.8895760254414227,1.890621360419008,1.8916662602676841,1.8927105220079106,1.8937539442538764,1.8947963273215167,1.8958374733345118,1.8968771863281735,1.8979152723510946,1.8989515395644747,1.899985798339011,1.9010178613492648,1.902047543665406,1.9030746628422501,1.904099039005497,1.905120494935099,1.9061388561456694,1.9071539509638744,1.908165610602726,1.909173669232725,1.9101779640497907,1.9111783353399254,1.9121746265405728,1.9131666842986184,1.9141543585250074,1.9151375024459403,1.9161159726506287,1.9170896291355874,1.9180583353454508,1.9190219582103083,1.9199803681795529,1.9209334392522477,1.9218810490040215,1.9228230786105052,1.9237594128673297,1.9246899402067126,1.9256145527106592,1.92653314612082,1.9274456198450383,1.9283518769606371,1.9292518242144951,1.9301453720199608,1.931032434450671,1.9319129292313268,1.9327867777255026,1.9336539049205477,1.9345142394096628,1.9353677133712206,1.9362142625454117,1.9370538262083,1.9378863471433665,1.9387117716106343],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.5797772085670954,1.5797772087139412,1.5797772088635205,1.579777209015883,1.579777209171081,1.5797772093291675,1.579777209490196,1.579777209654221,1.5797772098212992,1.5797772099914862,1.5797772101648406,1.5797772103414214,1.579777210521288,1.5797772107045018,1.5797772108911259,1.5797772110812227,1.5797772112748574,1.5797772114720956,1.5797772116730044,1.5797772118776525,1.5797772120861089,1.579777212298445,1.579777212514732,1.579777212735045,1.5797772129594576,1.579777213188047,1.57977721342089,1.5797772136580666,1.579777213899657,1.5797772141457436,1.5797772143964097,1.579777214651741,1.579777214911824,1.5797772151767473,1.579777215446601,1.5797772157214764,1.5797772160014676,1.5797772162866692,1.579777216577179,1.579777216873095,1.5797772171745181,1.5797772174815508,1.5797772177942975,1.5797772181128644,1.5797772184373602,1.5797772187678947,1.5797772191045807,1.5797772194475324,1.5797772197968667,1.5797772201527023,1.5797772205151597,1.579777220884363,1.5797772212604373,1.5797772216435102,1.5797772220337123,1.5797772224311761,1.5797772228360374,1.5797772232484328,1.5797772236685033,1.5797772240963914,1.5797772245322426,1.579777224976205,1.5797772254284304,1.5797772258890712,1.5797772263582848,1.5797772268362307,1.5797772273230715,1.5797772278189723,1.5797772283241025,1.579777228838633,1.5797772293627395,1.5797772298965997,1.579777230440395,1.5797772309943108,1.5797772315585352,1.5797772321332602,1.579777232718681,1.5797772333149969,1.57977723392241,1.5797772345411278,1.5797772351713602,1.5797772358133213,1.57977723646723,1.579777237133308,1.579777237811782,1.5797772385028828,1.5797772392068454,1.5797772399239092,1.579777240654318,1.5797772413983198,1.579777242156168,1.5797772429281203,1.5797772437144388,1.5797772445153915,1.5797772453312502,1.5797772461622925,1.579777247008801,1.5797772478710634,1.5797772487493733,1.5797772496440288,1.5797772505553345,1.5797772514836004,1.5797772524291416,1.57977725339228,1.5797772543733435,1.5797772553726646,1.579777256390584,1.5797772574274476,1.5797772584836078,1.5797772595594242,1.5797772606552622,1.5797772617714945,1.5797772629085007,1.5797772640666674,1.5797772652463884,1.5797772664480647,1.5797772676721056,1.5797772689189267,1.579777270188952,1.5797772714826137,1.5797772728003514,1.5797772741426133,1.5797772755098558,1.579777276902544,1.5797772783211514,1.5797772797661602,1.579777281238062,1.579777282737357,1.5797772842645557,1.5797772858201766,1.5797772874047493,1.5797772890188124,1.5797772906629146,1.5797772923376154,1.5797772940434842,1.579777295781101,1.5797772975510562,1.579777299353953,1.5797773011904028,1.5797773030610314,1.5797773049664743,1.57977730690738,1.5797773088844078,1.5797773108982303,1.5797773129495327,1.5797773150390126,1.5797773171673797,1.5797773193353588,1.5797773215436866,1.5797773237931143,1.5797773260844066,1.579777328418343,1.5797773307957172,1.5797773332173375,1.5797773356840275,1.5797773381966262,1.5797773407559879,1.5797773433629834,1.5797773460184983,1.5797773487234368,1.5797773514787183,1.57977735428528,1.5797773571440763,1.5797773600560792,1.5797773630222793,1.5797773660436856,1.579777369121325,1.579777372256245,1.5797773754495112,1.5797773787022096,1.5797773820154468,1.5797773853903496,1.5797773888280655,1.5797773923297638,1.579777395896636,1.579777399529894,1.5797774032307748,1.5797774070005364,1.579777410840461,1.5797774147518548,1.5797774187360478,1.5797774227943955,1.5797774269282783,1.5797774311391017,1.5797774354282985,1.579777439797327,1.5797774442476735,1.579777448780852,1.5797774533984041,1.5797774581019004,1.5797774628929406,1.5797774677731546,1.5797774727442024,1.5797774778077744,1.5797774829655937,1.5797774882194142,1.579777493571023,1.5797774990222408,1.5797775045749216,1.5797775102309546,1.5797775159922633,1.5797775218608079,1.5797775278385844,1.5797775339276263,1.5797775401300056,1.579777546447831,1.5797775528832527,1.5797775594384595,1.5797775661156819,1.579777572917191,1.5797775798453007,1.5797775869023678,1.5797775940907934,1.579777601413023,1.5797776088715474,1.5797776164689048,1.579777624207679,1.579777632090504,1.5797776401200607,1.5797776482990815,1.5797776566303492,1.5797776651166984,1.5797776737610159,1.579777682566244,1.5797776915353776,1.5797777006714695,1.5797777099776276,1.5797777194570182,1.5797777291128678,1.5797777389484615,1.579777748967146,1.5797777591723308,1.5797777695674884,1.5797777801561563,1.579777790941938,1.5797778019285034,1.5797778131195919,1.5797778245190113,1.5797778361306418,1.5797778479584348,1.5797778600064156,1.5797778722786848,1.5797778847794193,1.5797778975128733,1.5797779104833811,1.5797779236953573,1.5797779371532987,1.579777950861786,1.5797779648254855,1.5797779790491504,1.5797779935376224,1.579778008295833,1.5797780233288066,1.5797780386416604,1.5797780542396076,1.5797780701279578,1.5797780863121205,1.5797781027976052,1.579778119590025,1.5797781366950967,1.5797781541186444,1.5797781718666004,1.5797781899450078,1.5797782083600225,1.5797782271179146,1.5797782462250718,1.5797782656880006,1.579778285513329,1.5797783057078083,1.579778326278316,1.5797783472318576,1.5797783685755693,1.5797783903167213,1.5797784124627177,1.5797784350211022,1.5797784579995584,1.5797784814059137,1.5797785052481415,1.5797785295343634,1.5797785542728533,1.5797785794720387,1.5797786051405054,1.5797786312869981,1.579778657920426,1.5797786850498634,1.5797787126845553,1.5797787408339174,1.5797787695075431,1.5797787987152034,1.5797788284668528,1.5797788587726307,1.5797788896428662,1.5797789210880815,1.579778953118995,1.5797789857465245,1.5797790189817937,1.579779052836132,1.5797790873210817,1.5797791224484,1.5797791582300633,1.5797791946782727,1.5797792318054573,1.579779269624277,1.57977930814763,1.5797793473886539,1.5797793873607333,1.579779428077502,1.5797794695528484,1.5797795118009215,1.5797795548361337,1.5797795986731675,1.5797796433269795,1.5797796888128066,1.5797797351461698,1.5797797823428805,1.5797798304190467,1.5797798793910762,1.579779929275685,1.5797799800899008,1.5797800318510709,1.5797800845768661,1.5797801382852885,1.5797801929946775,1.5797802487237145,1.5797803054914317,1.5797803633172172,1.5797804222208216,1.5797804822223662,1.5797805433423486,1.5797806056016501,1.5797806690215435,1.5797807336236998,1.5797807994301962,1.5797808664635236,1.5797809347465936,1.5797810043027485,1.5797810751557673,1.5797811473298746,1.5797812208497497,1.5797812957405342,1.5797813720278415,1.5797814497377654,1.579781528896889,1.5797816095322943,1.5797816916715717,1.5797817753428283,1.5797818605747,1.5797819473963601,1.579782035837529,1.5797821259284854,1.5797822177000767,1.5797823111837304,1.5797824064114634,1.5797825034158948,1.5797826022302566,1.5797827028884055,1.5797828054248346,1.579782909874686,1.579783016273762,1.5797831246585385,1.5797832350661787,1.579783347534544,1.5797834621022087,1.5797835788084726,1.579783697693377,1.579783818797716,1.5797839421630524,1.5797840678317328,1.5797841958469003,1.5797843262525124,1.5797844590933552,1.5797845944150586,1.5797847322641139,1.5797848726878891,1.5797850157346458,1.5797851614535572,1.5797853098947243,1.5797854611091942,1.579785615148978,1.57978577206707,1.579785931917466,1.5797860947551818,1.579786260636274,1.5797864296178588,1.5797866017581335,1.5797867771163963,1.5797869557530684,1.5797871377297144,1.579787323109065,1.5797875119550395,1.5797877043327686,1.579787900308617,1.5797880999502092,1.5797883033264508,1.5797885105075555,1.5797887215650699,1.579788936571899,1.579789155602332,1.5797893787320698,1.5797896060382524,1.5797898375994852,1.5797900734958699,1.5797903138090312,1.5797905586221486,1.5797908080199832,1.5797910620889122,1.5797913209169574,1.5797915845938195,1.5797918532109083,1.5797921268613786,1.5797924056401613,1.5797926896440009,1.5797929789714884,1.579793273723099,1.5797935740012286,1.5797938799102302,1.5797941915564544,1.5797945090482852,1.5797948324961826,1.579795162012723,1.5797954977126396,1.5797958397128649,1.5797961881325746,1.5797965430932308,1.5797969047186278,1.5797972731349372,1.5797976484707559,1.5797980308571518,1.579798420427715,1.5797988173186055,1.5797992216686059,1.5797996336191709,1.5798000533144818,1.5798004809015007,1.5798009165300242,1.5798013603527405,1.579801812525287,1.579802273206308,1.5798027425575143,1.5798032207437458,1.5798037079330323,1.579804204296657,1.5798047100092214,1.5798052252487131,1.579805750196571,1.5798062850377552,1.579806829960818,1.5798073851579741,1.5798079508251766,1.5798085271621887,1.5798091143726625,1.5798097126642154,1.5798103222485114,1.57981094334134,1.5798115761627014,1.5798122209368906,1.5798128778925824,1.5798135472629218,1.5798142292856128,1.57981492420301,1.5798156322622143,1.5798163537151664,1.5798170888187462,1.5798178378348726,1.5798186010306043,1.5798193786782453,1.579820171055451,1.5798209784453345,1.579821801136581,1.5798226394235577,1.579823493606431,1.5798243639912832,1.5798252508902335,1.5798261546215604,1.5798270755098267,1.5798280138860088,1.5798289700876254,1.5798299444588717,1.5798309373507562,1.579831949121238,1.579832980135369,1.5798340307654404,1.5798351013911272,1.5798361923996413,1.5798373041858849,1.5798384371526066,1.5798395917105625,1.5798407682786797,1.5798419672842217,1.5798431891629612,1.5798444343593516,1.579845703326706,1.5798469965273765,1.5798483144329396,1.5798496575243848,1.5798510262923064,1.5798524212370992,1.5798538428691593,1.5798552917090862,1.579856768287893,1.5798582731472173,1.5798598068395389,1.5798613699283974,1.5798629629886216,1.5798645866065553,1.5798662413802929,1.579867927919918,1.5798696468477458,1.5798713987985724,1.5798731844199252,1.579875004372323,1.5798768593295367,1.579878749978857,1.579880677021368,1.579882641172223,1.5798846431609295,1.5798866837316352,1.5798887636434238,1.5798908836706114,1.5798930446030532,1.5798952472464514,1.579897492422672,1.5798997809700654,1.5799021137437934,1.5799044916161629,1.579906915476962,1.579909386233807,1.579911904812491,1.5799144721573422,1.5799170892315841,1.579919757017706,1.5799224765178363,1.579925248754125,1.5799280747691298,1.5799309556262096,1.579933892409925,1.5799368862264442,1.5799399382039556,1.579943049493086,1.5799462212673279,1.5799494547234703,1.5799527510820361,1.5799561115877299,1.579959537509886,1.5799630301429293,1.5799665908068379,1.5799702208476154,1.579973921637768,1.579977694576788,1.5799815410916453,1.579985462637285,1.579989460697129,1.5799935367835884,1.5799976924385786,1.580001929234043,1.5800062487724815,1.5800106526874875,1.580015142644289,1.5800197203402961,1.580024387505658,1.5800291459038207,1.5800339973320967,1.5800389436222353,1.580043986641003,1.5800491282907683,1.580054370510093,1.5800597152743268,1.5800651645962123,1.5800707205264921,1.5800763851545223,1.5800821606088926,1.5800880490580516,1.5800940527109366,1.5801001738176117,1.580106414669906,1.5801127776020634,1.5801192649913942,1.5801258792589319,1.5801326228700974,1.5801394983353654,1.5801465082109394,1.58015365509943,1.5801609416505387,1.5801683705617475,1.5801759445790133,1.5801836664974687,1.5801915391621262,1.5801995654685914,1.5802077483637782,1.580216090846632,1.5802245959688588,1.5802332668356587,1.5802421066064682,1.5802511184957062,1.5802603057735312,1.5802696717665987,1.5802792198588322,1.5802889534922004,1.5802988761674988,1.5803089914451445,1.5803193029459777,1.580329814352071,1.5803405294075528,1.5803514519194362,1.5803625857584613,1.5803739348599506,1.5803855032246734,1.5803972949197251,1.58040931407942,1.5804215649061981,1.580434051671548,1.5804467787169463,1.580459750454812,1.5804729713694825,1.5804864460182066,1.5805001790321584,1.5805141751174727,1.5805284390563026,1.5805429757079028,1.5805577900097363,1.5805728869786069,1.580588271711824,1.5806039493883906,1.5806199252702273,1.5806362047034255,1.5806527931195367,1.5806696960368951,1.5806869190619792,1.5807044678908122,1.580722348310401,1.5807405662002192,1.5807591275337343,1.58077803837998,1.5807973049051764,1.5808169333743989,1.5808369301533012,1.5808573017098881,1.5808780546163452,1.5808991955509268,1.5809207312999005,1.5809426687595527,1.5809650149382597,1.5809877769586207,1.5810109620596569,1.5810345775990815,1.5810586310556378,1.5810831300315102,1.5811080822548074,1.5811334955821232,1.5811593780011732,1.5811857376335103,1.5812125827373198,1.5812399217102984,1.5812677630926146,1.5812961155699552,1.5813249879766567,1.581354389298924,1.5813843286781404,1.5814148154142627,1.5814458589693117,1.5814774689709503,1.5815096552161576,1.581542427674993,1.5815757964944577,1.5816097720024471,1.5816443647118021,1.5816795853244514,1.5817154447356538,1.5817519540383325,1.5817891245275082,1.5818269677048258,1.5818654952831779,1.5819047191914237,1.5819446515792017,1.5819853048218375,1.582026691525346,1.5820688245315253,1.5821117169231447,1.5821553820292202,1.5821998334303868,1.5822450849643515,1.5822911507314408,1.5823380451002307,1.5823857827132644,1.5824343784928496,1.58248384764694,1.5825342056750957,1.5825854683745213,1.5826376518461787,1.5826907725009753,1.5827448470660221,1.5827998925909614,1.5828559264543598,1.582912966370168,1.5829710303942386,1.5830301369309052,1.583090304739616,1.5831515529416207,1.583213901026708,1.5832773688599875,1.5833419766887205,1.5834077451491846,1.5834746952735812,1.5835428484969727,1.5836122266642516,1.5836828520371364,1.583754747301189,1.583827935572851,1.5839024404064972,1.583978285801496,1.5840554962092812,1.584134096540424,1.5842141121717035,1.5842955689531704,1.5843784932152036,1.584462911775547,1.584548851946331,1.5846363415410654,1.5847254088816052,1.5848160828050817,1.5849083926707934,1.5850023683670507,1.585098040317974,1.585195439490232,1.58529459739972,1.5853955461181737,1.5854983182797069,1.5856029470872706,1.5857094663190308,1.5858179103346517,1.5859283140814828,1.5860407131006442,1.5861551435329981,1.5862716421250092,1.5863902462344752,1.586510993836132,1.5866339235271198,1.5867590745323055,1.5868864867094525,1.5870162005542339,1.5871482572050761,1.5872826984478332,1.587419566720277,1.587558905116399,1.5877007573905149,1.5878451679611665,1.587992181914808,1.5881418450092732,1.5882942036770138,1.5884493050280994,1.588607196852972,1.5887679276249458,1.5889315465024438,1.589098103330966,1.5892676486447694,1.589440233668268,1.589615910317126,1.5897947311990472,1.5899767496142492,1.5901620195556048,1.5903505957084534,1.5905425334500631,1.5907378888487398,1.590936718662566,1.5911390803377687,1.5913450320066989,1.5915546324854124,1.5917679412708456,1.5919850185375715,1.592205925134123,1.59243072257888,1.5926594730554977,1.5928922394078713,1.5931290851346234,1.5933700743830979,1.5936152719428525,1.593864743238631,1.594118554322806,1.5943767718672732,1.5946394631547862,1.5949066960697147,1.5951785390882094,1.5954550612677587,1.595736332236123,1.5960224221796195,1.5963134018307525,1.5966093424551604,1.5969103158378661,1.597216394268807,1.5975276505276286,1.5978441578677167,1.5981659899994487,1.598493221072642,1.5988259256581752,1.5991641787287618,1.5995080556388497,1.5998576321036215,1.6002129841770774,1.6005741882291649,1.6009413209219396,1.6013144591847268,1.6016936801882553,1.6020790613177431,1.6024706801449038,1.6028686143988484,1.6032729419358582,1.6036837407079985,1.6041010887305502,1.6045250640482371,1.6049557447002116,1.6053932086837905,1.6058375339169066,1.6062887981992562,1.606747079172123,1.607212454276855,1.6076850007119798,1.6081647953889362,1.608651914886414,1.6091464354032783,1.60964843271008,1.6101579820991314,1.610675158333152,1.6112000355924696,1.611732687420789,1.6122731866695204,1.6128216054406772,1.6133780150283565,1.6139424858588096,1.614515087429123,1.615095888244533,1.6156849557543906,1.6162823562868198,1.6168881549820922,1.6175024157247593,1.618125201074593,1.6187565721963697,1.6193965887885666,1.6200453090110118,1.6207027894115669,1.6213690848518978,1.6220442484324167,1.6227283314164653,1.6234213831538278,1.6241234510036586,1.624834580256922,1.625554814058434,1.6262841933286167,1.6270227566850675,1.6277705403640528,1.6285275781420452,1.629293901257423,1.6300695383324482,1.6308545152956597,1.6316488553048003,1.6324525786704167,1.633265702780265,1.6340882420246547,1.6349202077228757,1.6357616080508461,1.6366124479701163,1.637472729158381,1.638342449941632,1.6392216052281008,1.6401101864441254,1.6410081814720894,1.6419155745905643,1.6428323464167995,1.6437584738516913,1.6446939300273624,1.645638684257486,1.6465927019904751,1.647555944765664,1.6485283701725957,1.6495099318135342,1.6505005792693053,1.651500258068571,1.6525089096606396,1.6535264713918958,1.6545528764859445,1.6555880540275427,1.6566319289503966,1.6576844220288878,1.6587454498737884,1.6598149249320164,1.6608927554904835,1.6619788456840574,1.66307309550769,1.664175400832714,1.6652856534273366,1.666403740981334,1.667529547134943,1.668662951511951,1.66980382975696,1.6709520535768105,1.672107490786132,1.6732700053569822,1.6744394574725412,1.6756157035847945,1.676798596476164,1.6779879853250137,1.6791837157749667,1.6803856300079536,1.681593566820914,1.6828073617060644,1.684026846934639,1.6852518516440083,1.6864822019280723,1.6877177209308212,1.688958228942956,1.6902035435014517,1.6914534794919438,1.6927078492538195,1.6939664626878865,1.6952291273664888,1.696495648645945,1.697765829781165,1.6990394720423239,1.7003163748334393,1.7015963358127257,1.702879151014577,1.704164614973037,1.705452520846615,1.7067426605443012,1.708034824852636,1.70932880356369,1.7106243856038028,1.7119213591629439,1.713219511824541,1.714518630695632,1.7158185025371986,1.7171189138945342,1.7184196512275007,1.7197205010405334,1.7210212500122521,1.7223216851245347,1.723621593790918,1.7249207639841855,1.726218984363007,1.7275160443974933,1.7288117344935405,1.730105846115828,1.7313981719093436,1.7326885058193136,1.7339766432094137,1.7352623809781424,1.736545517673239,1.7378258536040319,1.739103190951613,1.7403773338767243,1.7416480886252568,1.7429152636312653,1.7441786696173958,1.7454381196926447,1.7466934294473493,1.7479444170453386,1.7491909033131583,1.7504327118262968,1.7516696689923432,1.7529016041310108,1.754128349550966,1.7553497406234084,1.756565615852348,1.7577758169415356,1.758980188858006,1.7601785798921954,1.761370841714608,1.7625568294289968,1.7637364016220491,1.7649094204095535,1.7660757514790404,1.767235264128892,1.7683878313039223,1.7695333296274305,1.7706716394297377,1.7718026447732242,1.7729262334738867,1.7740422971194385,1.7751507310839845,1.776251434539302,1.7773443104627724,1.7784292656419949,1.7795062106761443,1.780575059974111,1.7816357317494849,1.7826881480124408,1.7837322345585889,1.7847679209548524,1.7857951405224455,1.7868138303170213,1.7878239311060649,1.788825387343612,1.7898181471423678,1.7908021622433123,1.7917773879828778,1.7927437832577784,1.7937013104875885,1.7946499355751502,1.7955896278649062,1.7965203600992494,1.7974421083729797],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Ridge en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Ridge pour tout les paramètres de GridSearchCV\n","fig1 = go.Figure([\n","    go.Scatter(name='RMSE moyenne',\n","               x=alphasridge,\n","               y=GridRidge.ScoresMean,\n","               mode='lines',\n","               marker=dict(color='red', size=2),\n","               showlegend=True),\n","    go.Scatter(name='SDup RMSE',\n","               x=alphasridge,\n","               y=GridRidge.ScoresMean + GridRidge.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               showlegend=False),\n","    go.Scatter(name='SDdown RMSE',\n","               x=alphasridge,\n","               y=GridRidge.ScoresMean - GridRidge.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               fillcolor='rgba(68, 68, 68, .3)',\n","               fill='tonexty',\n","               showlegend=False)\n","])\n","\n","fig2 = px.line(GridRidge,\n","               x=alphasridge,\n","               y=[\n","                   'ScoresSplit0', 'ScoresSplit1', 'ScoresSplit2',\n","                   'ScoresSplit3', 'ScoresSplit4'\n","               ])\n","\n","fig3 = go.Figure(data=fig1.data + fig2.data)\n","fig3.update_xaxes(type='log', title='alpha')\n","fig3.update_yaxes(title='RMSE')\n","fig3.update_layout(\n","    title=\"RMSE du modèle Ridge en fonction de l'hyperparamètre alpha\")\n","fig3.show()\n","if write_data is True:\n","    fig3.write_image('./Figures/graphRMSERidge.pdf')"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre  Lasso()\n","0  lasso__alpha    0.325\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_pred_logLasso=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[20.921824671976154,20.78913971814864,20.83157765064908,20.739689243984607,20.84024627673293,20.74772490481564,20.92249151431101,20.745668712678263,20.917772860249197,20.79104382574202,20.78261226643338,20.84778968916144,20.73957529208634,20.79418803464459,20.749989602710517,20.74949496429625,20.717662782244986,20.8292402795941,20.712192535238003,20.71565994629599,20.821052200259366,20.758585324013374,21.706689117621515,20.94312052914624,20.7151168012031,20.816288394803156,20.866452969559738,20.71751730853329,20.721830935881954,21.050993812296458,21.04217213977817,20.767452722092077,20.987144499022236,20.815152476909308,23.90683281303028,21.32382123273839,20.721736370441672,20.718033792903558,20.78964892825427,20.816932223828392,20.699137678131656,20.726682870394797,20.80270379694166,20.926191724999466,21.045707811917143,21.903331199312326,20.75695834823271,21.040834112515615,20.99376477080483,20.726353103731253,20.875150623215866,20.756323074035464,20.768046729132205,21.50630439915534,20.799590591844325,20.735870289793343,20.75122137818904,20.776145445171135,20.784159260046167,20.7063488875607,20.718976987808347,20.864693295133726,20.864518049909442,20.715407771788577,20.784755772908486,20.904659901674112,20.945810335657992,20.875412392513393,20.75861927058168,20.756773353589434,20.799328526569074,21.86588081463771,20.717299069013137,20.96673116707802,21.193545590746798,20.996267106258895,20.83410657868635,20.985978790240065,20.948671500091013,22.859972401284566,20.77848044937646,20.790128971815086,20.96400227994208,20.706482260660092,20.81334847086039,21.311817865754946,23.505662915381855,20.72378287514058,20.7735303170869,20.717371811659508,20.786937153562917,20.805036387973153,20.719789280692808,20.71305819589189,20.757462685666496,20.814378933778734,20.70704479221097,21.07938766876724,20.82500183122773,20.808464968209943,20.70828626670901,21.42188672017823,21.132391094883964,20.836386055384548,21.068132002943187,20.918512526297732,20.726360366414845,20.777311705943745,20.74879662331006,21.02634127153663,21.039039631770546,20.768439585746776,20.830923071061157,20.719789280692808,20.779218150647836,21.588860165540034,20.956656310555854,20.755244762162157,21.9929455449127,20.80356938810928,20.721838198565546,20.842474580142536,20.762394648671386,20.748451497438953,21.18350414902955,20.725892388723203,20.779421312174726,20.770536975555007,20.83720218055404,20.768604480659594,21.794967263664752,20.705512347127453,22.201175956918096,20.760947046846546,20.71767490601938,20.948343594782635,20.77672098176727,20.779218150647836,20.70720725078786,21.063360314447234,21.113633181336294,21.461616340451382,20.912006908957427,20.974597095067526,20.75700686649238,20.72055309006073,20.777292331066803,20.97654975300786,20.81334116185262,23.286713609679854,21.24231473405408,20.794973643644337,20.783276660851264,20.7383556635443,20.7722440261519,20.723065136115356,20.710878341255682,22.431206102325362,21.553780830780607,20.70115264943609,24.48109688999122,20.956673318583142,20.82080597802226,20.754252321787767,20.755775079432812,20.7012666129154,21.138697280207086,20.74703140175816,20.760452431594366,20.754846268527807,20.775575650936663,20.70961745713757,20.740891933985626,20.99307427971769,20.733375193860784,20.958426399745022,21.054837002455464,20.772227029705657,20.736357607618796,20.73824893958649,20.851955132776247,20.729764722264935,20.779079375412614,20.894606856364813,20.778427116350166,20.816088409130504,20.90870369711602,25.991383417866444,20.940009748527206,20.723094233173903,20.779474622038933,20.735707831216452,20.788965112635264,20.7213411353964,20.956331370239983,20.798485720567694,20.770764937256764,20.796828604288844,21.139861078611123,20.711101407123504,21.374342747992362,20.714234202008193,20.962611543281767,20.716591063750563,20.792919864680794,20.745014017279892,20.782481329669913,21.16196998201663,20.75403409384866,20.76368460668596,20.77524340161302,20.713836530626995,20.81453857780759,20.75752572929335,20.740933177980658,20.857377181533977,20.73107652623551,20.835580035727176,20.699971793810025,20.835442848898712,20.733110872416887,20.716188531278565,20.76143444573931,20.709355595191685,21.094404734034708,20.893714558150393,21.245971329166476,21.90988774808614,20.87565979857836,20.71567448324422,21.37429520995031,20.746240943248658,20.722131605486947,20.71519680653306,20.870567801751456,20.80887808181442,20.869660428043446,21.994063553770758,21.5766613299258,21.392341093558887,20.8200165236155,21.031205376148044,20.752673794780506,20.75398319715829,20.71985719699151,20.774387486302828,20.726733813409343,20.877284419090415,21.133854729913995,20.713705582282486,20.746143953053497,21.254687782241206,20.72093862608649,20.73997539980346,20.75122137818904,20.852112969205443,20.777876743316817,20.723055460257928,21.54325801589221,20.776152707854727,20.747935870071156,20.87995892372193,20.734063824246412,20.736168476738236,20.71615944580106,21.24302954920202,21.241560113002222,20.738336253924224,21.908944831126416,20.729432530846516,20.732528942826974,20.789513084075825,20.71783736459627,20.727509734970617,20.721590908311022,20.70631495257344,21.258087022217513,20.741716373806568,22.431206102325362,20.90081664202884,20.773747351014777,20.70909857117451,21.31060156447035,20.85513183535395,20.729204592306846,20.75424747227801,20.705534181502408,20.730465488006004,22.47894887578619,20.723627702409367,21.94978197679444,20.71657406730432,20.765898396309428,20.98837208121084,20.90870369711602,20.739294032101423,20.740630060458695,20.798882348509252,20.71577149660147,20.868621915777357,21.92314534558763,20.716397060198155,20.84778968916144,20.75345216425875,20.857937149357443,20.756386106081273,20.735140415412683,20.8026261700424,20.71921462536753,20.740467613462847,20.78682892674897,20.801675677710886,22.494528921348447,20.785194641960537,20.7624213325561,20.726825907770564,21.497136516768357,20.723055460257928,20.737274199706185,20.736765012762643,20.729665330476987,20.743956847314745,21.46952056009983,20.755513229067972,21.324738725125684,20.765898396309428,20.7624213325561,20.87482481861753,20.82090882182992,20.79358188066798,21.74406907621597,20.740964676631997,20.78011960683674,20.767959426375516,20.904152590740043,20.791440787366703,20.74663131720313,20.702069206780344,20.71104321300641,20.738913357166467,20.888023600544198,20.75262773601885,20.72243469984682,20.81201725726974,21.245791391333356,20.732051289277905,21.291968365343454,21.136514905297084,21.071929157502627,22.209426327926394,21.007858982191298,21.654233467958253,20.813624904497637,20.712384090873442,21.44233958548766,20.743389419929933,20.750964377333958,21.384291093324748,20.72742483380672,20.724199921398718,20.753495798265526,20.732890242884988,21.559590541060988,22.003252687055184,20.734776702180834,20.70856996302985,21.20988610714284,20.687639490495528,20.82751144094643,21.746634588126113,20.73776402335383,21.103380676199983,21.76948504268076,20.70720725078786,20.79434514967303,20.730363636720043,20.898425833718164,21.404310693284405,20.72675562462221,20.94869587503129,21.723779892486363,20.945160559255648,20.7523925116335,20.80539281535932,20.722909940222056,20.734580308616682,20.755442864852302,20.795022161904004,20.727053869472325,20.814492135975314,21.43469841401608,20.7390272743216,20.740870134353806,20.770483642528713,20.76143444573931,20.799110310211013,20.952117204165535,21.698722017054497,20.736478880105878,20.718033792903558,20.727490313769497,20.721736370441672,20.73400560696723,20.801244048180337,21.02984059705187,20.785068508382654,21.167107996807424,21.215277188346594,20.766203927005222,20.708262030741263,20.736866840886517,20.89721590419564,20.725460793935788,20.73124625907704,20.745275890806823,20.709205248808143,20.726120327262873,20.961653687221514,20.82099367666964,21.176586416008174,20.756216361658698,20.80830984180291,20.714995575040195,20.830988527861845,20.811243783625432,20.924508933532408,20.72243469984682,21.5246041516185,20.784033138049328,20.77412801436869,20.781652051920258,20.81850591290694,20.70832748754195,21.169391019007183,20.77474635002492,20.731508121022923,20.73215795533049,20.821713828868695,20.796610341606602,20.694574289449413,21.324738725125684,20.8979281950108,20.747850968907258,20.779508545445147,20.77140680203379,21.146227045100765,20.716370411056577,23.046281008638722,21.2216163709146,20.906802467963033,20.718831525677697,20.72905520004883,21.448931222707845,20.711935522801877,20.75262773601885,20.76804917704917,20.751900332717245,20.723967144930334,21.018529804823963,20.789578575619647,20.86254183993994,20.715800570497926,27.91804767121132,20.734340257883662,20.733801939138438,21.082624774435903,21.254231916742913,20.781570115429528,20.707769793919784,20.947519845752897,22.2024280492103,20.733477021984655,20.718967288788832,20.751912421748507,20.843305705012884,20.709297389493543,20.919874862284075,20.747884915475563,20.71120809633818,20.756599496091667,20.832422764153122,21.454022008170412,23.444577300269717,22.4612734416948,20.9218537690347,21.297200599046,20.733718548637743,21.293227150511488,20.7142681485765,21.594744482123037,20.73769128070746,20.799956538082732,20.860655392225137,20.71898669840891,20.780488169578344,20.7144427077657,20.709297389493543,20.9804092209314,21.86339583388684,21.033586508601296,21.125904177946662,20.752130649687615,20.746323408076634,22.614295267324977,21.280698139409083,20.720693737424757,22.533299285022238,21.245641044949817,20.74569294864601,25.683274134629713,20.731779705150412,20.804371993555268,22.90911263102361,21.007858982191298,20.792124545080487,20.719347986885875,20.800502061606323,20.73215795533049,20.94543452181384,20.77559986374232,20.748849967917398,20.79040595961316,20.729616823798363,20.824480921857372,20.718831525677697,20.741716373806568,20.77158448124377,20.775429535550867,21.138697280207086,20.793533373989355,21.0638183176893,20.78034030347994,20.726103365559762,20.899955738236354,20.898985940514162,20.73453666302886,20.724195060307917,20.74698290666058,21.96478003086748,20.760386940050545,20.73692259866769,20.956331370239983,20.922789103385863,20.79230154060561,20.808417798446584,20.787476347882702,20.90422337630963,20.879653323539866,20.738445356312734,20.725021901721647,21.445027432949672,21.09400215398278,20.90355950616794,21.19895820606529,21.020286751252172,20.758757516352915,20.71087832967464,20.97909738221769,20.750964377333958,20.710187286115175,20.843725730498,20.747850968907258,20.709297389493543,20.72451515111403,20.68449458341749,21.108531608584077,20.71965594233655,20.727895270996374,20.77286477498196,20.76433203940074,20.76976835142046,22.03140707248034,22.725329977543208,20.847736321392013,20.72737633870914,20.75792823860326,20.749252465646265,20.75905333644815,20.72872692717673,20.742555281089473,20.762780200460178,20.738147099881573,20.88192053883798,20.78864262023636,20.741861812775127,20.763563380523056,20.738047684931537,20.755775079432812,20.762544983473884,20.732630782531892,20.771860903299974,20.758689657959437,22.28959600719894,20.74879662331006,20.803608242092565,20.726377351280043,20.744536476489785,20.956491357737818,20.796171507297686,20.80539281535932,20.83838326168808,22.841238776640722,20.73109594743663,20.982751769417884,21.02237683205263,20.80895482660072,20.777859735289532,20.842998350358485,20.729054269085395,21.03647537909415,20.851594117985517,20.72343372201905,20.72452726330738,20.725080119000832,20.882570338402413,20.795856289163417,20.793746740837662,20.70638283412901,20.773674596787362,20.84571399475563,20.77882479931241,21.62455313283857,20.724391465453113,21.150195175446537,20.815152476909308,21.082628103754622,23.8628563537964,20.824201685279753,20.712546549450334,20.721224758743254,20.74131630083258,20.713836530626995,20.738564169301803,20.735210721723128,20.76669376223391,20.986921996725897,20.720155453422667,20.712182859380576,20.747923723134672,20.7558186902775,20.94823753845394,20.797759710162374,20.761111953340407,20.708962773320245,21.442632754733538,20.920382657320996,21.048811486581197,20.71484521707561,20.942980239192952,20.714636711318104,20.702241364376754,20.770595169672102,20.871331622700424,20.836073204099936,20.794066773738553,20.729204592306846,20.746905349247587,21.94659136691868,21.148306418787303,21.721152218381313,20.761244674994092,20.720136055383634,20.743559164352504,20.707284854525035,20.940189777010602,20.708725147342104,20.772825920998677,22.950960725474708,20.715800570497926,21.46013986739361,20.722114632202793,20.759768673880583,20.74445630049564,20.719578350180424,20.758595069357067,20.74123865077123,20.76217882232507,20.70631495257344,21.04482842327443,21.368868339379045,22.600100454155037,20.80257284859715,20.759373427254264,20.73309149753994,20.824145881174406,21.372346418779586,20.792203692981957,20.717299069013137,20.745241932657475,20.72081256199487,20.77672098176727,20.70506135430101,20.78269467335613,20.96888438415369,20.72057247651872,22.78605622826695,21.072357224034896,20.738030723228427,21.034575773848786,20.71087832967464,20.779171527679058,20.708606334353036,20.811549244834964,20.767370280426192,20.903343132383334,20.800443902232363,21.06715037436235,21.049022532904022,20.90119603529172,20.813149617798224,20.730295743583433,20.832204192494544,20.868785549010184,20.706644719236984,20.745901465984556,20.738750875427485,22.03017056172803,21.473609085434468,20.734711233799104,23.281955112719388,22.12561850235731,22.33228420586646,22.950960725474708,20.710187286115175,20.788203716441174,20.772973854208384,21.02548907809467,20.734066237420247,20.942374504318654,20.7735303170869,20.777602746015493,20.8441088996741,20.820225064116137,22.021797494758896,20.743413667478723,21.131932118575808,20.807332642424594,20.701467867570358,20.721736370441672,20.792010604763266,20.731779705150412,20.737570008220377,22.600100454155037,20.8956543157294,20.794614791503292,21.131932118575808,20.85843963571241,21.097607323252085,20.71409841573497,20.713055771137007,20.750382413000914,20.792203692981957,20.728052845320374,20.719643795400067,20.7215666260191,20.872698735354234,20.8390670517824,20.78964892825427,20.947475382923642,20.85896041862393,20.857464437966488,20.775449528939824,20.74840867411047,20.77717841391167,22.129401840662744,20.891434393302582,20.986665065357084,20.90006493327322,20.956212591994046,20.91050422505314,20.700931996742103,20.72779340812937,21.441408391193136,26.602644183970295,20.831184956169132,20.72610092922384,20.713489779098257,20.71984749797199,22.911534573336944,20.71591695873212,20.992246827926408,21.392341093558887,21.70217873149313,20.90851762878135,20.855134248527786,20.70722907358177,20.865217169441458,20.70704479221097,20.72202247993635,20.752329491168737,20.7105946333538,20.774208008117608,20.726028198158513,20.751112287381577,20.712517475553874,20.735484742186543,20.726966566715635,20.725892388723203,20.763289383221732,20.80895482660072,20.9051293184443,20.716188531278565,20.80446415740276,20.72458544584343,21.570897794429815,20.7151168012031,20.73045092789569,21.2724272541927,20.717085690583787,20.719895981488527,26.66654821598579,20.733474608810823,20.71926796997487,20.744504784012175,20.82633558874349,20.733026040739254,20.923378715724724,21.76948504268076,20.834696001233898,20.734485720014312,20.746240943248658,20.83923756712223,21.1723673309377,20.753716450959512,20.783177199577047,20.861721163039245,20.91098569750536,20.958518499140823,20.74534862187215,20.75861927058168,20.712047049945262,20.709202835634308,20.85193114733265,20.835274741159242,20.718390231870767,26.206000997943338,20.84067057409362,22.79366931608108,20.858296737611834,21.372273822180436,20.91050837569908,21.634949896651182,21.092798791125126,21.222617881563444,20.96826361216154,20.72243469984682,20.76002395793404,20.7375942905123,20.845357132778812,20.799252820481087,20.743049954246874,20.854622636829365,21.04034178203998,20.792813163885075,20.712061598474538,20.751912421748507,20.769450685369222,21.163650383471943,21.03132660231095,20.838085739068447,20.71964137064519,20.762666255960966,20.739187319724657,20.88947848821472,20.70575724737023,20.781438673490907,20.77882479931241,20.701162348455604,20.731440262629445,20.840670539350484,20.73382376193235,20.962611543281767,20.783458494305098,20.756332796217066,20.746694349248937,20.783058409750065,20.815444868700073,21.191804119723436,20.714930095077417,20.78682892674897,20.75484396197824,20.73597938060081,20.914128546314444,20.72445209590613,21.273137637885974,21.298320031162177,20.749977490517168,20.711052912025927,20.750464889409933,20.73140869449184,20.961275491762404,20.70618642898381,20.703284008974716,21.191634502692356,21.064043889379313,20.961202996673066,21.137276145836054,20.739378875360096,20.723045761238414,22.54984045039668,20.792049366098194,20.884520547997873,20.774457815775364,20.7636918809506,20.952548787371903,21.220444836118258,20.73655162275225,20.77034299516469,20.91084843809262,20.847283417782034,21.12824646799767,20.763774368940663,20.785439542203314,20.81296701163813,20.735715082319,20.706635020217465,22.987407261607295,20.866339006080423,20.864802652199867,20.702069206780344,20.863172310951608,20.740693092504504,20.84697736153385,20.796077098430384,20.802955959868033,20.747298090051714,20.78346575698869,20.706387683638766,21.003832318319226,20.727012637058337,20.724253289168146,20.724199921398718,21.24848277660067,20.753464299614187,20.761053759223312,20.845917766813823,21.659189602557927,20.986921996725897,20.790255105392973,21.063931416150737,21.18380379745358,20.78435325201753,20.726193081490287,20.715359276690997,20.873477507606236,21.32607253113528,20.994635593542718,21.031846775070672,21.216566670732746,21.23864204317055,21.021215409208732,20.79339270346324,22.87921553552451,20.834034837749737,20.82560649391069,21.00348670663202,22.39540167792965,21.28810570765915,20.942980239192952,20.698798212448597,20.93065327397679,20.921652583866013,20.76529708026153,20.70131995752274,21.384571798003538,21.079092813228968,20.718545416183023,20.801025854984363,23.8628563537964,20.717454253325393,20.8424536824335,21.390958971696822,20.85595624043176,20.74520313657941,20.710878341255682,20.840122591072014,21.501792092460178,20.76411623621651,20.718972173041724,20.705805754048853,20.70802439318208,20.78214670191557,20.7722440261519,20.724248439658385,20.858565207195277,20.72845779096621,20.680113051351167,21.154055582091633,21.556791427370296,20.733375193860784,20.82500183122773,21.70632238279833,20.720441551336297,20.729738049961266,20.76167209487954,20.72067675255956,23.332375756255125,20.750316956200226,21.23250107822077,21.062951476666502,20.804990282887317,20.853579992231403,23.041659299552506,21.085335661971648,20.799089260819777,20.71298300532855,20.71303392518101,20.973588339132647,20.771742113472992,20.945810335657992,20.72752910984756,20.77875247504101,20.80291714062788,21.087910728491035,20.74214300213897,21.14597949763801,21.63075510545368,20.784755772908486,20.713123641111533,21.241422545973705,20.729299180909216,20.76196301914084,20.789914472895195,20.792733193298247,21.481016804238113,20.7428099150949,20.73142569093808,20.748372325949372,20.725021901721647,20.736396149980386,20.84690461888748,20.762023661174904,20.717376661169265,21.056653155440866,20.913292005881196,20.769717419986954,20.729888419506896,21.03324622078791,20.780488169578344,20.78199639027516,20.705587526109746,20.852035411792446,20.777432966849783,20.725434110051076,21.209768966487186,20.78894089982961,21.117286633849897,21.146227045100765,20.827513877282353,20.738639336703052,20.89865615068853,20.719643795400067,20.76295719180331,20.743559164352504,20.73769128070746,21.153373205943044,20.73065460730552,20.912446182979707,20.7390272743216,20.70506135430101,20.794054638383113,20.91050422505314,20.713455844110996,20.92043100880576,20.762437685367466,20.97092831354414,20.761053759223312,20.849009071381758,20.780381674892546,20.801468331639438,20.750193293701397,20.75723480503205,21.266531690903737,21.511685175585026,20.810470240494862,20.776816180507186,20.755227096411208,21.72415671656365,20.74302569511704,20.76996472182252,20.832210136515627,20.883489308920502,20.730409707062744,20.74140598201997,21.09134419298072,20.737463353748836,20.728006774977676,20.755488981519182,20.893096129845805,20.729325876374975,20.82423074759517,20.934494027799875,20.797922191901357,20.78262439550037,20.737744613733753,20.721590908311022,20.919159866364154,20.8018357115329,20.704331503082436,20.866339006080423,20.729034871046363,20.73867088167857,20.853696403627684,20.71965594233655,20.924247048424434,20.853466895231026,20.747850992069345,20.71435785292598,21.119552315404665,20.821490728257743,20.71977233057074,21.16360913947691,20.7354265364884,20.790250255883215,20.75424747227801,20.80557237964589,20.733012788247414,20.787792429014637,20.856140521802562,21.119552315404665,20.846192229375514,20.90006493327322,21.014360638746897,20.71248350582348,21.215277188346594,20.723094233173903,20.738976389212276,20.74520313657941,20.74624820593225,21.410968002633997,21.86588081463771,20.726391899809318,20.7415805759523,20.71805074302562,20.72243469984682,20.70305365726121,20.875831921431637,20.741905469943994,20.75396135120229,20.71977233057074,20.71850417218799,20.812706455485145,20.80090218090449,20.76875239070721,21.567796194467352,20.74949496429625,20.763565805277935,20.70364772220656,20.706630159126664,21.097607323252085,20.706644719236984,20.790822509615698,20.90936629298899,20.756058798915742,20.97166069374699,21.61423031988611,20.72157635978175,21.134174174728823,20.752559877625373,20.718593888118512,20.780148738638424,20.715044046975684,20.7105946333538,20.80265041759119,21.43181366665278,20.808918432115448,20.706474986395452,20.75983890790189,21.18380379745358,20.7151168012031,20.72851357190947,20.71390442376361,20.865015124659635,20.707992882949696,21.005071391224476,20.774305056217987,20.727895270996374,20.714777323938996,21.15770918455994,20.79738872266589,20.77526524756902,20.81311084488225,20.766880445197504,20.706981748584116,20.729291895063536,21.00281877077981,20.832655591420384,20.744361758217448,20.734808200832173,20.736769850691356,20.70722907358177,20.75157541556346,21.91221133332652,20.739294032101423,20.712861767584602,20.707648556175837,21.024939797959714,20.80782001815527,20.76039647355521,20.791326823887392,20.731168655339864,20.74086282534603,20.737667021577625,20.78854807795817,20.710024815957237,21.1860187356495,20.82455082682024,20.847736321392013,20.715448981040474,20.814998579551233,20.827576897747118,21.180146283495812,20.737759139100937,20.72307483513487,20.922789103385863,20.748035261859105,20.767520580485552,20.893101037260784,20.927285254706753,20.808887497240903,20.851429176748525,20.7190570163004,20.842474580142536,20.72433815558891,20.723346407681316,20.719408605757852,21.22208201073519,20.868557691537827,20.720759194225444,20.722415324969877,20.780296613942905,20.81158627692899,20.898985940514162,20.808606810293988,20.73613943758491,20.871840809643967,21.2279141261546,20.772627102679643,21.03997503446079,20.825055175835068,20.72544625698756,20.74037304802257,20.70553903101217,20.73853991017197,20.84998893025563,20.79834165133333,20.72215100352598,20.79833438864974,20.71567205848934,20.748350479993373,20.814066186723522,20.870746401427812,20.796801920404132,21.22326810253335,20.83793060108037,20.75086981189368,20.796171507297686,20.900371797382572,21.231002799745404,21.4284061135915,20.94517840250001,21.015406408047532,21.392097635558468,21.026852527047794,22.492238911075003,20.75575564665065,20.714474229579125,21.188440949963155,21.078793983000807,20.762780200460178,20.712425334868474,20.783058409750065,20.733375193860784,20.778427127931213,23.781223976177145,20.754979736670418,20.900464971504075,20.922217555512546,20.8615477020206,20.723564635620424,20.755200035723885,20.7377737107923,20.9130833932328,20.796544896386962,20.806602779624974,20.738564192463894,20.731571164649775,20.861038503496015,20.851429176748525,21.17585358416307,20.791326823887392,21.209892620266128,20.705471126294512,20.818353107025384,20.717662782244986,20.75723480503205,20.812537873697185,20.762666255960966,20.895448257888862,20.71714873421064,21.390958971696822,20.92156523478515,20.899235655523565,20.737933721452222,20.966361203055712,21.148306418787303,20.711853092717035,20.85193114733265,20.807887899710835],"xaxis":"x","y":[22.839725872000276,20.91128637269999,22.88916655727953,19.430343691015025,21.11735552640613,21.280697719341468,22.124201158883572,18.136670762341843,22.576080863006137,20.10297814136792,20.137307637847517,21.968529606994224,20.90466916348112,21.65055481705341,19.466148173643653,19.644920713511322,20.60018787473792,19.883803436319816,19.908561207609715,19.88468822756935,20.68752092168076,19.37233862557095,24.197363319180216,20.375121118721335,18.385719188798834,18.92468784406244,21.354902012656122,22.075827573908075,19.724580465382388,20.25426974158722,20.75534115928083,23.747615973251953,20.91788210799568,20.92150383722997,24.937914667295647,22.368646863289158,19.83010122567355,18.16161414446258,21.756941677613227,21.220261555831726,19.921137323148557,19.756730386301125,19.076215392899233,21.77156523118196,21.51345533419679,28.08571411944019,20.221414835639845,22.43563961615873,0,19.8813455186455,20.446723265477033,20.65296343235214,19.469859640957857,24.8692968913913,20.808662576707462,19.741272758302454,21.063890409149668,20.444375957405665,20.737383520261705,20.772392205332533,19.30201857006515,20.50461437981694,20.844484149195353,19.768388190282472,22.01816825105002,20.77707066358558,22.615494713832,21.249591056381075,19.3101001682171,18.557194427644223,20.820077266859332,26.161807884063222,19.616841359021425,24.601371538637977,20.929756127701882,21.371449507411626,19.801676786988352,23.186489993216338,22.288287088869534,24.593075628702742,20.324838680602316,19.538495641550785,20.313003679890702,17.83704270507587,22.508853453023672,21.500237946648834,25.808363100543477,20.146924115160264,21.08058139373211,17.12438387523814,20.968249130032163,20.028493079904162,18.408984789492116,19.05702758537138,20.086616681313576,21.346006819966092,19.260460167342536,22.182304931495295,0,21.19880336254275,18.935012496616167,22.63391493571178,23.82192770536634,21.24843214899758,21.979683188147945,19.96496549198579,18.7932762528682,20.682137237753555,19.632965031850407,23.473975339042966,22.69657897049395,20.12759231185353,20.362922084924325,18.91123104739614,19.548169047796804,22.968053968168725,21.917396756553043,19.216324315987517,24.057859634036678,21.674989393981168,19.72022056098374,20.690574372087426,21.01024397957751,20.841336970061143,22.67830612646918,19.365836228253468,21.066918904158555,20.194293964879012,19.514174668391416,20.55950872954696,24.227674033652324,16.89837701160671,23.000908813982363,19.725383525541226,20.372113673311205,21.25165374313537,19.232324890468607,19.688668857104407,19.006769325349232,21.51252238785962,21.56101612004373,24.60236976749392,22.783863807855585,21.893038360020444,21.254697640624393,19.266317599579637,20.581937941283943,20.45335403901458,21.260660009666424,23.096986451321428,24.198260151329528,20.67873788317677,19.161993412283312,20.805625607806174,20.111067676706508,19.630957845101975,19.421904563712328,24.27931634919893,22.429090755701058,21.147722912842873,25.31201588491192,0,19.877134324376346,20.195578868662224,20.07860380095908,20.74807693982553,23.74195830377693,17.92351319630841,19.98475794826,19.330905939549268,21.75045501313216,19.64650597499701,21.21476614620658,22.11740118712243,20.373745608135653,21.286908529421844,22.031685219396664,20.241113097705927,19.58596744655146,19.47904998638186,21.40733571681439,19.113174941454258,19.88328018890598,21.622357074698563,20.24572329389297,22.858722725717435,21.004219094475406,26.772642502942784,20.915100345640315,19.607783613490604,22.164399063013658,20.790635654144218,20.499654561276497,19.533857847503334,22.238063995762765,19.371674563247215,21.64924126399572,21.665790435544952,22.72561505846151,20.402841366046307,21.38581238341487,19.76025356060854,20.786842719711426,19.14889856436821,19.895681776607635,22.093666112296297,20.459085985918723,22.982495350783758,19.331349158501606,20.55332894959407,20.31682150206961,20.014253448408297,20.016785168783276,17.008111246780107,21.536104422950213,20.603485087703003,19.59434829773337,20.049508251055094,19.642302060721956,21.52720398550173,18.99020093378567,20.22398061738597,21.01213158467734,20.053269667846333,22.108159109401438,21.79011956511657,22.084914601393155,23.135231909261865,21.190067594656295,22.67339901515593,23.695624962968736,20.984090887872558,19.114539527933296,19.182165482881256,21.52107548780855,21.22249010417508,21.043820980181792,22.894093531705956,23.85820974949918,23.265700738521485,21.89761800850832,24.611415306054898,19.96312680068985,20.173501472804098,20.216216542451658,20.253427701740367,20.770035525423065,22.54105973810428,21.560752172206048,20.50991100623185,19.66651702047476,22.893554470389834,19.678935343554116,18.37060227548128,20.996625195887564,20.88004158687887,20.017617555780475,18.785129945834015,22.472827160217914,21.123452379734417,21.834751391009725,22.36922218137253,20.638651229695757,19.920184186837908,20.233535275947663,21.775846548503456,23.23197611805285,20.98111639712373,24.69390393605354,18.972968571042593,19.85555820528158,19.343431218526263,18.83915024282446,23.64752507012282,17.79557360413656,21.996936490386865,23.456377353127298,21.603092935704957,24.298252303878165,21.481533394476365,20.408641755181446,16.772761777957808,22.20744305955703,21.11943827672747,20.998113148879906,20.11872947152901,21.236410750491928,19.699904832330084,24.209354374596852,20.24751150890443,25.07655356851684,19.568646661314627,19.776542871248843,21.727271525181536,21.062803097791257,24.076140757673382,19.409983373621806,21.192511957229144,19.312950102671177,20.179206907329647,22.432801454859,18.979888467927786,22.15617321227827,20.260215495083163,20.910464871041206,19.297632789142515,20.29925833471969,21.341220185389716,20.961489967791394,22.164351182655523,24.799241151178187,20.420977915907716,25.348057895064095,20.921713841616427,20.242927524338644,18.95868492479523,22.708968189549438,18.49440443743341,20.482328404305942,22.37000861087754,20.121388068881203,20.44167803431194,21.856025378041313,18.927945745238855,21.9419703223203,19.736694248820182,20.23558290561487,19.836668342627448,21.25562508835354,21.394257392515996,22.71500741252458,19.547704151356815,20.400995752923983,20.265469687416747,20.212441762527977,22.377553705906216,19.66793597026211,19.3315880738609,17.677595133772794,20.276225239062065,21.268637940423815,21.725285950225775,19.195369804190715,21.15736975857522,23.488081735909503,20.232817889146876,23.728782472563765,22.84612630027242,22.077517747249683,24.569997706071334,21.7711290432985,23.43665576011153,22.83375379636568,19.2148787700197,23.42842113265138,19.4136293492673,20.540968959330602,24.77613012919926,20.659638185399235,19.087533251252736,19.83361590003151,19.96019429668775,22.334373955189864,23.109422435163253,19.778390202711652,19.167836614935887,23.933068642871486,18.56499419954315,22.530048594882032,21.71290394044351,20.45930590947763,23.069120127461606,22.999636382856856,18.925763040720064,19.49519726866692,21.042555130756018,21.216321947424902,22.454539777644875,20.13393362882646,20.867172390851817,23.066997383640125,21.52825545142037,19.72029436584214,21.117385332870825,20.179272523647345,21.590649178257912,19.692513024244924,22.86005581934068,18.799932943985397,19.96989517504386,22.510505954413144,19.34599856610255,21.28740957905986,20.158837704444057,20.907660872216642,21.905619198419114,22.38172749868176,24.308788888311135,19.262765428559003,22.547404100685924,19.87902826477287,19.826795369611226,20.791833978792408,20.76154230516995,20.245177516562332,19.101442274030862,23.095721196617728,22.853333163501407,20.55222183482644,18.623284957897813,19.707161942553423,23.312427516610697,20.72798026434645,19.38519821479218,19.584766197834426,19.657602815047426,19.766824889008415,20.72718342775812,21.19739249261565,21.766933642509713,19.997824758905686,18.07487316385907,17.89367601727741,21.076958761045283,22.70946891023392,18.280325895793162,19.286378115164315,23.27164625070411,21.087259361308853,19.696262913257698,22.998819199975294,22.068276096052415,20.676958661968467,21.84293342285994,20.22886359790674,19.302304652102425,19.45315606988007,20.189004387901257,19.834023760961447,20.136832510309578,21.96825774467718,21.693760030133728,19.68327564708964,20.69046887217181,19.902947681155606,22.060735176692912,21.15375333067992,25.097138279698054,21.440477364164977,21.3505869180748,19.260072175979914,21.05711024152434,22.371116977331784,18.18526261240324,21.763695119906945,19.900159246109634,21.022878950161832,20.48383078099113,21.821470865389173,24.987649624596244,22.265101932257092,18.954616641321604,25.734123629818527,20.035932572983462,21.002454511024606,22.570203152455143,23.437077312046757,19.317032739803228,17.641741979338622,22.448170385491473,23.33275256138136,19.064590160696714,19.240852168960235,20.1483293247843,19.78119349991646,20.035822020141666,20.368090770815378,19.622542520355413,19.136726140380034,20.665117281017753,22.04592459909886,22.766122645362053,24.701481133697825,23.00797271520254,22.942154096547412,21.766675289013797,19.216601410973656,22.721359157406464,18.9576603836655,21.88381079939571,18.472822451162664,21.07184726592027,22.03833211483471,20.099462108962847,21.41678722119122,20.92948670168764,20.890428315584586,21.725547245838506,28.740163782092626,21.205026123435218,22.323663662536706,19.34101707633175,18.666140767995948,24.012052706607076,23.69806207100269,21.754438391935153,23.682062419502355,23.126467381893125,21.07179899498228,25.515113797875166,19.69217074150189,22.12431429427697,25.95498550595432,21.870103917395213,20.208896815397623,20.476170795773445,21.523407372759994,19.429205075454394,20.94805891660743,20.579612186732355,18.425777297525496,21.048120970639673,20.14580390451336,19.1678248265788,19.26180569731426,21.731372277489818,20.677454054144345,19.241454867705745,23.733038602815835,20.006473671385464,20.924659576737604,19.109062896280463,21.73683233585413,21.589387814254906,21.96355600303241,20.930649460210123,19.44163523375263,19.835220336552222,22.967539915594646,19.57718354371762,19.694551733623157,22.05857907784579,20.152641197123305,21.206937139516242,19.177040653531833,21.059144404213697,21.77992540342766,22.0971519060863,19.9033116727081,19.870869348950354,20.847191191309296,21.255773747330974,22.179827408990512,20.655014419256375,22.594985264708335,19.867378293238115,18.6010489412016,21.546971094057223,20.8121798521563,20.30528553843669,21.30235611574127,19.472993206549905,20.642314511859237,20.823420888524133,19.456579392491523,21.112413416140527,20.769205079804532,20.344212591822245,19.88957080391307,19.783879090807634,22.2606456541005,24.0347829295736,25.71484503270827,20.79497581994581,18.966254774482525,19.558951865783115,20.00118447409592,19.759231530600534,18.668189678924886,19.362035840585754,19.21669376414341,19.534513178135196,21.58943582111394,20.029295821796193,20.93634103691578,21.419330156983328,19.511493048750843,20.011894681671833,20.008240359915625,18.95284902418798,21.345469949872594,20.977820033541025,23.20923603756077,19.692880228017533,22.637005220660473,19.457649403400342,19.12974837607436,23.60775201354553,0,21.026538976608304,23.88396569488923,27.919301359795483,21.157605371717533,22.006125382801496,21.526914680071894,21.60107003042797,21.310840181677595,20.659294437479453,19.303395037499914,20.578169960300666,22.895450751816803,20.179768194947595,20.3718571876006,21.13235475498899,20.116418845808077,20.597315830475623,19.957464846805212,23.115072392844226,20.726116689654802,20.700075492290427,20.833447685328323,23.397238664878486,18.725486086935668,22.38037915102657,20.812817350665142,23.171040699108495,25.19463787743612,17.21138734793915,19.0336054859068,21.825732486030095,21.042046241460838,20.6322831501737,19.380751294097546,19.582601081862446,21.284194059773114,22.179583552994327,23.19729034551032,19.967482396438246,19.738441963085798,19.239293618022067,21.59285163447237,20.535950228874793,21.01158079776531,20.08203076321153,22.84732578883419,21.756353934255824,23.316254320731808,18.81898933836999,21.940012430656058,20.748003621067692,19.730383893455734,20.182461946627072,21.076706013187607,20.7269525620563,19.59081933226322,20.904152603600195,19.659163210227153,24.761947476774708,22.534274856548336,23.233448675874918,19.884197354704103,21.007268735063114,19.408841782080135,20.136824689258354,21.854300382519792,19.881144027962787,20.995690391681727,23.573228460474112,19.074352191519743,21.469722294142937,18.833019773765052,20.27464659506303,19.718574682047347,18.258239797953646,20.755317114550273,20.561150873374917,20.948677886535172,19.720177445507503,22.150900676319612,23.39909808655993,22.31935969126546,20.155632003058283,21.13370665837429,19.473136561032486,20.46100849412487,22.136011068345166,21.159614691783496,19.431106818709004,19.13213914887522,19.706696670705693,19.346767596320518,20.918060329643115,20.18711885177595,23.064648540231374,18.98912255599455,24.074524917146316,20.61646897501603,20.227612418267608,23.77281502683423,18.64679105166152,19.821061094559884,21.41515272305351,20.235862606471247,21.36550800836747,21.97455042987335,22.17732889464112,25.151779365365787,24.150527389957542,21.040691724844343,20.760535400558187,19.34725801943171,20.04287236504247,21.531748865354032,18.726909959036774,19.7936588697483,20.03811220566055,23.56945490190484,23.91740995998136,19.437420256199317,23.65829637497728,27.09570141066156,23.540007985307764,23.389995853863947,20.300248650151303,19.67235973078438,20.003307902740296,20.529649424214696,20.802720957968873,21.281868210231526,20.815053320149143,21.927740352835695,20.82479873735278,20.51237686567707,24.94868149530031,20.081935873735674,22.717589515695337,20.82980401444312,19.3661810523187,19.957403913237158,21.18136460499971,19.8915004159983,20.17210241586846,22.4307529287208,22.889842311610867,20.801380865239704,22.613937582771225,20.216218911187326,23.993435990731076,19.557332226454086,17.31416721288844,20.21067743703825,21.405123280549176,20.1770277231615,18.904187708332326,19.046718202293015,20.073998999202523,19.6302991245126,21.48379659194355,20.81804614111716,19.563852956879067,21.051151840896342,20.770721956713995,19.67623430456214,19.581897497422524,21.384446242838667,20.692911719555987,21.545170940853957,21.994462034082755,21.47969097288377,20.278388484789648,19.382106816183008,19.441821622178967,21.939001270486997,25.832615032960224,21.65340972782181,19.535812626400283,18.947642644803835,19.712127286089668,23.90063882300631,18.304477915148002,22.443056343683086,23.16226531069458,25.14188655199703,20.70748794380435,20.732604891283092,19.934400683437797,20.62113432421718,19.28434545274011,19.017540081468233,21.15041915835723,20.26686372980413,20.03253125195289,20.148876382348814,22.62198208884591,20.021014007950576,17.20778649543665,21.273482891420798,19.30063185166596,20.195062159996276,21.445335194484834,21.0819625214105,19.392355160192587,20.378347780631422,18.799302474087973,23.18300636577445,18.93991113136844,19.223167412657357,23.650902186871754,19.945176609895544,19.495627675541318,25.79058328300537,20.476746945456053,19.21271648764287,19.678332456797207,19.71871179715288,20.105565995835796,21.137414277628565,23.008367409641956,21.03393309328942,19.30271923785845,20.626830273876646,21.092597149125172,21.952512694511015,19.882805718288704,20.77592509493985,21.046183463307532,22.0435593555408,21.04785962557455,20.25569596256737,19.352118206703615,18.845717636328267,19.34199651526983,20.91962424027046,19.595170049801663,19.600097337632842,25.661642339745672,21.791142017552335,23.50747794569296,20.391385205003306,23.803560322980168,23.78648471341281,23.49397732898427,23.081833665589276,23.144648130459657,20.920832580620292,19.193623112656937,18.901120542977083,21.79790479470573,19.407912925215978,18.98289333455679,19.415382799115825,21.253000869012993,21.604198363469845,20.557571210143905,19.274253014992237,20.09246940100189,19.391954018620176,22.850833320712763,22.012401869246364,20.826658939867247,19.118363214083377,21.289967644645948,18.733922283625127,20.9115880183164,16.829816883719186,20.861088989018167,20.73330584327736,18.406398882361927,21.03390016791011,22.274116203708253,19.445206608830574,20.834279688605037,23.70344643795525,23.354012710201598,19.88516012413886,22.730738164708086,19.75792383693999,24.394611647917593,19.734888199833236,24.922685338201692,19.530254509942655,20.280991315768514,21.951122115143434,19.19842345841963,22.926085660752005,22.977548256996624,21.21128990053558,19.167558108393887,20.647437784958075,19.35146728073557,21.097845676874776,19.32409702234251,18.277738745265133,24.145550575002787,19.346013720701006,21.812205122435703,22.86783241905905,19.249124305809787,21.53558768926747,24.102450641699225,20.500920634779252,19.622850798933502,22.45828794986169,19.57219805726336,23.264639171673256,22.333293091972617,18.995877797816178,20.694084927736416,19.630595419370202,20.436904665975636,25.664004852328446,20.985299957568404,21.714666548374087,21.619000188699776,20.319316549429495,16.9587226208279,24.58750691743097,21.431401448181294,20.454850489915017,19.41323537187323,20.769097708402093,19.12410363757553,21.037456801472633,20.865493152029874,20.200550831447014,19.074330956681983,20.89335338152254,18.80103888605608,22.493051859930933,20.264370436179377,21.216120605388507,19.103940869449712,23.841976780210096,20.967615900689093,20.26093058536308,21.15453445100085,24.16646354231811,22.25017305320405,20.99185811169326,22.413732248617176,21.395751936133603,21.027619198526104,19.682163069515127,20.59828494694422,20.60161654204836,22.930920924409694,21.340491521133263,21.13532539299306,23.86936283890287,21.30122097904847,21.87678820576361,19.984953818018486,23.219303551688306,21.660324565067476,20.87994439864477,20.922214169866013,24.081396558763124,22.721264543402345,21.873381120752498,19.089555706982512,20.89641611390129,20.568873923302558,19.242152996635603,19.205602363695707,24.69282091354044,22.779994713980038,19.521220854266264,21.142478555307388,25.236220259743373,20.048804801912784,19.552903579436016,22.115896159241377,21.403353538154413,19.840625811644227,18.35486059310022,20.417051943879315,23.212857021000243,21.53372289651904,20.79356441567801,23.25433145453618,20.592440384261195,20.758465193484422,20.24512208583746,20.925901548056537,24.750691092265395,19.41019874513054,19.696673307271265,22.56844661795243,22.50456479926641,19.4801354006198,20.95523200641152,24.568652749155945,18.75491624208908,19.461087704959443,20.880059154822476,19.68504303470752,23.816825076283962,19.839377869097518,23.420195657862312,22.999884079036303,20.649943786599653,21.684940338873297,24.09117022808059,21.12734576220247,19.49242240707679,18.90953834843785,20.94197381362215,23.259273464150183,23.03775857233843,22.58649267850949,20.120777025175872,20.906486953193497,20.378669388865216,22.737839310980224,19.27074701059353,23.65992231174119,23.339497059809283,22.05115682063906,18.775074918844375,22.10097025846529,19.31545475085148,19.854767962698624,19.840511525069328,21.39606694375647,22.97307940319244,19.86193501563865,19.664038907092028,19.58463718752616,19.810866559819527,19.465937010710118,22.434772773304594,20.445539922646542,18.99187307244663,21.90849954742701,22.314131385426723,20.432733544527707,23.07558442006914,22.830034474576443,21.269107840654502,20.48988421076927,21.083491338486773,20.934278289661282,20.287989620862316,20.04722528013383,21.199347342918216,23.521067577919112,23.2611930742384,21.92570835834572,20.556276718935056,21.41646356230674,22.53034703709958,18.95340785965337,19.986508970321974,19.495183602853512,18.457821713581545,22.886338306822665,19.129172314836154,20.136171638349733,19.376016989288264,20.879430687458026,20.620771994361597,20.30679376611553,20.368798039683195,22.35947129540103,20.069728697762105,22.54894428694912,20.37273542865621,20.71779772905426,19.199159497364995,19.174850117652987,20.13094661361353,20.62659792573699,25.292356917614896,21.758700626292807,21.796149648767223,18.807339197843834,20.506265808043914,22.110290220817404,19.52840260028443,20.325926854481473,19.625476983112655,21.50252421557022,19.304125125188563,19.107865328534132,23.169806519640005,18.840768128990423,19.36444675592656,18.627786784209636,21.17955986366495,20.377078375453138,21.746512886013605,20.910669810022966,23.66715843905666,19.356125235979412,19.33650002206052,17.813568277801206,22.465861054219694,20.57561772108256,19.015107379674305,21.499214768467855,19.66178092422977,17.299495077910098,21.27615053752397,21.040198736231254,21.92098191704396,21.7224558930939,21.092338092530998,20.048671453648097,21.894206057556605,20.415195026786527,20.471143313052202,21.849984955018474,19.59156765743604,20.658934051201378,20.105603080001295,22.787613064091445,18.897059576214055,19.64405870233027,22.74129354031224,21.948058114314257,20.038176858751854,22.177258018525677,22.318758483797644,17.211802761238985,23.061132678317314,19.552166474373674,20.22058498039463,19.1389392082589,19.751662039949778,22.589266875609983,26.340632927208585,19.534708840769273,20.435276534084654,19.903908501005596,19.287439594230733,19.687016476370815,22.7859490833811,20.26081963393427,20.008806098312462,20.432875222497227,21.770367489208184,19.54263516359492,20.167399003287947,20.02036350615788,25.816840819488693,20.66031803587651,19.756693437676223,19.47969541290554,19.46354033785,24.169496968680054,18.739319147762597,21.559823540829672,20.841106100432793,21.572330613609903,23.06348379203432,23.90661007609995,20.544449448312626,18.828527980887348,21.643407148870626,20.295760247970698,21.926818619899166,18.887256320586236,20.26723932452398,20.649629842496918,22.847263558158666,20.639447033358472,21.651692029555726,19.3900555692985,21.40442957359937,20.067715321953298,20.687515795874024,20.646624626390846,19.38789150561276,21.443595032483266,21.960080436904104,20.986097829919892,20.116046923130224,20.38834949087981,21.51694698715695,23.463402138610206,22.107609158967936,21.13017429963753,21.059552411062068,19.918938919026616,19.132327452939787,21.94580529932221,19.159457479949786,19.567551127469855,19.27757204338643,19.477956222501692,20.02620191687006,20.455200776646638,23.044574887856943,24.184179585129318,20.388726808803085,20.116390923202314,21.32891538129463,20.93827950221911,19.878697565839243,20.238709984208988,18.95241706841831,19.63538512176151,20.604884938737765,22.740623426919786,20.3125128271046,24.597055201180662,23.117114991068007,20.78628428680642,19.932433926767427,19.826295549099655,21.26517033895163,22.496434708234126,19.328382384869762,19.427802084161115,20.18180618902739,19.980960001271505,18.996123380968683,22.600876020595507,20.092995805378262,20.046496736817677,21.151917099556776,18.585982112110514,20.695329089308867,22.520164246985647,19.3471325562148,18.999355953219133,23.20757904373024,21.23702785682451,21.08554669425927,20.768225524289374,20.2479251962018,19.3533467533098,22.213653662019038,20.24562900900173,18.67996002744327,21.27136955536682,21.42004604510025,21.047075972047644,21.8035683042629,19.22968489276112,21.077647966937757,23.387816453129698,18.706232935340964,19.58110217755027,23.538867158230946,21.96712491711981,19.68563168309507,20.03480567791853,18.855622835428132,19.611919934246885,20.89874787711282,20.381292625866948,20.623611830893093,21.463850115483186,22.47425899129698,21.46282774650559,20.250449939937386,19.804071134581005,22.940732615911422,24.1535698900103,22.049830938319623,19.98785104241217,22.944212504137,21.578861893090256,25.17177163891695,19.37427511207424,19.96383210404445,24.180920079085045,22.206977518330888,19.158269753875864,19.069924834450198,22.714272038785044,22.147044335773373,20.251879940231216,24.956261862764006,19.58788736648328,21.751091249893005,21.93032100053686,21.621710733177533,19.367732274650958,19.09456742423413,16.917281198342668,20.807982080155547,20.46876211568297,19.988975863285916,20.580262018382033,19.091740116853863,21.44414464440018,21.29242653675922,22.038981262669978,20.180643707721647,23.155520679582406,19.32638298997934,21.506597416475046,20.650611348398325,20.693713245656816,20.540019222244897,21.217521718500812,24.49306731779338,19.642596451766774,22.00972851632279,20.932965503614184,21.834232905374517,21.098649782686852,22.49932865299591,22.579718287600105,21.529871119919466,21.118854440357843,20.944082894927185],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Lasso() vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_pred_logLasso"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["alphaslasso = np.linspace(0.1, 1, 5)\n","param_gridLasso = {'lasso__alpha': alphaslasso}\n","\n","GridLasso, \\\n","BestParametresLasso, \\\n","ScoresLasso, \\\n","SiteEnergyUse_pred_logLasso, \\\n","figLasso = reg_modelGrid(model=Lasso(),\n","                            scaler=RobustScaler(quantile_range=(10, 90)),\n","                            X_train=BEBM_train,\n","                            X_test=BEBM_test,\n","                            y_train=SiteEnergyUse_train_log,\n","                            y_test=SiteEnergyUse_test_log,\n","                            y_test_name='SiteEnergyUse_test_log',\n","                            y_pred_name='SiteEnergyUse_pred_logLasso',\n","                            score=score,\n","                            param_grid=param_gridLasso)\n","\n","print(BestParametresLasso)\n","ScoresLasso\n","figLasso.show()\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[1.8799022301978152,1.8690467337529824,1.88296817939679,1.938468870419126,2.012193522389029]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[2.113560284884741,2.011691113628676,1.993232090268326,2.046761812359641,2.1219014586902247]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[1.6462441755108896,1.7264023538772888,1.772704268525254,1.830175928478611,1.902485586087833]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.9462996869455522,2.014117422573288,2.0624172682206936,2.1188726551187735,2.182965253067496],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[2.2907750194386804,2.0458423320346224,1.9105187230261556,1.9481048988876408,2.051862380855895],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.74621135174956,1.7908573148138591,1.8510584757503923,1.9257349819580312,2.0021255401379703],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.8150207302099814,1.829767714492614,1.8711964671930206,1.920941080854845,1.9800755634308735],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.6012043626453019,1.6646488848505276,1.719649962793687,1.7786907352763393,1.843938874452909],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Lasso en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Lasso pour tout les paramètres de GridSearchCV\n","fig1 = go.Figure([\n","    go.Scatter(name='RMSE moyenne',\n","               x=alphaslasso,\n","               y=GridLasso.ScoresMean,\n","               mode='lines',\n","               marker=dict(color='red', size=2),\n","               showlegend=True),\n","    go.Scatter(name='SDup RMSE',\n","               x=alphaslasso,\n","               y=GridLasso.ScoresMean + GridLasso.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               showlegend=False),\n","    go.Scatter(name='SDdown RMSE',\n","               x=alphaslasso,\n","               y=GridLasso.ScoresMean - GridLasso.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               fillcolor='rgba(68, 68, 68, .3)',\n","               fill='tonexty',\n","               showlegend=False)\n","])\n","\n","fig2 = px.line(GridLasso,\n","               x=alphaslasso,\n","               y=[\n","                   'ScoresSplit0', 'ScoresSplit1', 'ScoresSplit2',\n","                   'ScoresSplit3', 'ScoresSplit4'\n","               ])\n","\n","fig3 = go.Figure(data=fig1.data + fig2.data)\n","fig3.update_xaxes(type='log', title='alpha')\n","fig3.update_yaxes(title='RMSE')\n","fig3.update_layout(\n","    title=\"RMSE du modèle Lasso en fonction de l'hyperparamètre alpha\")\n","fig3.show()\n","if write_data is True:\n","    fig3.write_image('./Figures/graphRMSELasso.pdf')\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.005e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.709e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.843e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.365e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.098e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.732e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.390e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.124e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.875e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.416e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.033e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.756e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.908e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.178e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.781e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.151e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.943e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.471e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.807e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.062e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.093e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.237e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.443e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.979e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.834e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.861e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.207e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.156e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.016e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.529e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.054e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.124e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.500e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.267e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.890e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.298e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.919e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.093e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.189e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.223e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.592e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.133e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.560e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.950e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.363e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.330e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.175e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.657e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.980e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.293e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.258e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.430e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.624e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.217e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.012e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.366e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.259e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.396e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.725e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.044e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.691e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.329e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.302e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.077e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.499e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.464e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.441e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.110e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.346e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.403e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.796e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.760e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.390e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.570e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.534e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.144e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.518e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.479e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.178e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.434e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.868e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.479e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.832e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.213e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.606e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.595e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.523e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.556e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.247e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.904e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.941e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.713e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.567e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.282e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.677e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.014e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.317e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.673e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.611e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.634e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.654e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.977e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.749e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.784e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.712e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.352e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.386e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.087e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.750e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.051e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.697e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.738e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.421e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.854e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.819e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.779e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.159e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.455e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.827e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.789e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.820e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.123e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.922e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.488e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.889e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.901e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.228e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.194e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.522e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.864e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.897e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.988e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.555e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.956e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.973e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.296e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.937e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.587e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.934e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.262e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.969e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.052e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.618e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.041e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.003e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.649e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.020e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.328e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.360e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.036e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.007e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.111e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.679e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.082e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.106e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.068e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.074e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.708e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.391e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.098e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.421e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.168e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.736e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.140e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.764e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.127e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.167e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.478e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.155e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.450e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.137e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.194e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.220e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.790e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.196e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.224e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.815e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.181e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.531e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.206e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.505e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.268e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.840e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.229e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.276e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.863e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.580e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.556e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.312e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.886e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.291e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.324e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.907e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.272e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.301e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.625e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.603e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.292e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.352e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.333e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.927e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.947e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.367e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.665e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.311e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.346e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.328e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.646e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.389e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.371e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.965e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.406e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.387e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.983e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.684e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.345e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.702e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.360e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.421e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.999e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.405e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.734e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.015e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.441e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.424e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.375e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.719e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.388e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.450e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.029e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.436e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.763e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.472e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.401e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.043e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.457e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.749e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.475e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.056e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.412e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.500e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.423e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.463e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.068e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.486e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.776e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.789e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.434e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.498e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.080e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.487e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.443e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.524e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.091e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.811e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.512e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.800e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.452e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.517e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.508e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.101e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.545e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.831e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.110e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.535e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.460e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.821e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.468e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.119e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.526e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.534e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.475e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.127e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.554e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.848e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.482e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.549e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.135e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.840e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.542e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.579e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.488e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.571e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.863e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.142e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.856e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.562e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.493e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.149e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.593e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.155e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.556e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.499e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.504e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.870e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.586e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.574e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.568e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.161e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.605e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.166e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.887e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.599e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.512e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.508e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.882e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.579e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.583e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.171e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.610e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.897e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.176e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.516e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.615e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.520e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.592e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.892e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.180e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.587e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.624e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.905e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.184e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.523e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.901e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.620e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.526e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.599e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.187e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.595e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.191e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.632e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.529e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.532e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.912e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.628e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.909e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.194e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.605e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.602e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.639e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.635e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.534e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.197e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.918e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.915e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.536e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.610e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.200e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.608e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.538e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.644e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.923e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.202e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.921e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.205e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.540e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.615e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.542e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.612e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.207e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.928e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.649e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.647e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.618e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.544e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.209e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.926e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.653e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.617e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.932e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.930e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.545e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.651e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.547e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.622e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.212e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.620e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.657e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.548e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.214e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.935e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.655e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.933e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.549e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.625e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.215e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.623e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.660e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.550e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.217e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.938e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.658e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.936e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.551e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.627e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.218e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.626e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.662e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.219e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.940e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.552e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.661e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.939e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.553e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.220e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.629e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.628e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.942e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.221e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.554e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.663e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.664e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.555e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.631e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.941e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.222e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.630e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.666e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.555e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.223e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.944e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.556e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.665e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.943e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.632e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.223e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.631e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.668e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.556e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.224e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.667e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.945e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.945e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.633e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.557e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.225e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.669e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.225e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.946e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.633e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.557e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.946e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.558e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.669e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.226e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.635e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.634e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.558e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.670e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.670e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.948e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.226e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.947e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.635e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.559e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.227e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.671e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.227e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.635e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.948e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.559e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.559e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.948e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.671e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.636e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.636e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.228e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.228e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.672e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.672e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.949e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.560e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.560e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.949e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.228e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.637e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.560e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.637e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.673e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.229e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.950e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.560e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.950e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.637e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.673e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.229e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.637e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.673e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.229e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.950e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.673e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.561e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.950e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.561e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.638e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.230e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.230e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.638e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.674e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.951e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.561e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.561e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.674e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.951e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.230e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.638e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.638e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.561e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.951e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.674e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.674e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.230e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.951e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.639e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.562e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.230e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.675e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.231e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.562e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.639e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.952e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.952e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.562e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.675e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.639e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.639e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.231e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.675e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.231e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.952e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.562e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.675e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.952e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.639e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.562e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.231e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.675e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.231e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.639e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.952e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.562e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.952e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.562e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.675e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.639e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.231e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.639e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.676e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.562e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.675e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.952e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.231e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.952e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.562e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.231e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.676e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.676e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.562e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.676e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.676e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.676e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.676e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.676e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.676e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.676e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.676e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.676e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.676e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.676e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.676e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.647e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.641e+03, tolerance: 1.728e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.713e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.735e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.954e+03, tolerance: 1.791e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"name":"stdout","output_type":"stream","text":["              paramètre  ElasticNet()\n","0     elasticnet__alpha      1.911644\n","1  elasticnet__l1_ratio      0.000000\n"]},{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning:\n","\n","Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.081e+03, tolerance: 2.153e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_pred_logEN=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[20.981432729083796,20.756772576174576,20.932613487199127,20.724967066137385,20.817636294003382,20.71209588130378,20.946006691615263,20.716606131675192,21.031884187125453,20.75237512963124,20.76678228924654,20.78917943685984,20.709881908778513,20.79098998635022,20.741390479156788,20.7187825419538,20.689741302671624,20.826630142877615,20.6832526718304,20.66985600089483,20.80214227113576,20.787243465563424,21.62342983937024,20.915694061796913,20.673417198877907,20.78967029787584,20.9167006764537,20.672059144168045,20.694685432288672,21.05279346614308,20.96485588972211,20.70925182917449,20.96480454769614,20.801174979818775,24.152153668540194,21.266955017726715,20.678379465742754,20.66357309742695,20.706487730735518,20.829894344441175,20.63881186760909,20.69623518155952,20.77731007265746,20.8882569566187,21.096820158691465,22.28506379041582,20.74965658068058,21.063276711141906,21.01303913098287,20.70004948216406,20.839058144335183,20.73139343661409,20.78521208796309,21.852800721552477,20.79806402663804,20.680524716447245,20.690549627988915,20.759111554116423,20.76861728325217,20.663444016037307,20.704604314059605,20.863542957465384,20.87070524239722,20.669556879614913,20.721493209097112,20.836989630268423,21.026883298841067,20.97908649571807,20.791539142494525,20.719698372052193,20.782405119504133,21.78344934822353,20.682146846472076,20.99427688387584,21.174502035732253,20.988333330326512,20.84430035502883,21.042105079672215,21.077928106844414,23.169225899827328,20.797588230919555,20.804691864474513,20.925498853496972,20.636914646807536,20.784220452422943,21.317234875421793,23.992571819381006,20.683696616708815,20.706839526358355,20.68939616273326,20.752471848311973,20.78789593216562,20.705567829720867,20.65346574527959,20.753265416301026,20.84441081747849,20.663842441754486,21.172024286711995,20.81285796673236,20.833154901567855,20.65808407363934,21.481342944965412,21.361614730214775,20.813001564413295,20.668355813787894,20.890323266426016,20.713362239995565,20.805010421842614,20.7399754054095,21.101729240057132,21.222652791328105,20.732461509547715,20.802556492150124,20.70136237051951,20.760531543589938,21.51931767614838,20.957350711708788,20.73783410948556,22.178903686248724,20.8273479221049,20.703792730918817,20.877701467231535,20.756104945195663,20.700625809582387,21.375348761154967,20.708601680726673,20.7358984731397,20.765763111137517,20.820539281471458,20.723558418714433,21.813033586059053,20.662024704745775,22.44018022035682,20.776790673467033,20.685434887090775,21.044593897820796,20.73642311859314,20.756326084388583,20.659829685685384,21.026550560915574,21.12599382675155,21.52339723834534,20.88922652162482,20.977894037362727,20.701940885585092,20.693169692726027,20.787080239372887,20.97207406307758,20.8298402015699,23.140752681358244,21.220321991549785,20.803847783759384,20.729970315709156,20.683472786754095,20.712583942890795,20.691943941659204,20.65508553110938,22.404431339480105,21.53940711149493,20.646058787186583,25.01137058477083,20.942434158673972,20.81037697019443,20.746446779253805,20.728593514652456,20.639478949277432,21.03626488916359,20.73260269211739,20.733466296184936,20.741793175489345,20.631422974290082,20.666894054042846,20.711400708792507,20.975578302737727,20.701185857224466,20.974458717963795,21.16530590387735,20.753435192178454,20.751828931710566,20.72004674819085,20.82594399915545,20.71776702057685,20.774571654750616,20.852227692146496,20.788426285132502,20.787923297385177,20.880203619910315,26.447859161052612,20.924442068683387,20.691863118442786,20.776364659577567,20.664335126843365,20.783416514699148,20.689898987524405,20.97642052311449,20.821372079528924,20.72612108275678,20.783645251401722,21.217147611902345,20.668654267728495,21.309056724469063,20.65488066642525,20.939887220091055,20.65765631936455,20.81005988056309,20.703592993438875,20.745778708420648,21.20010111445241,20.741982465098676,20.766733735720756,20.782059920662288,20.669124282193028,20.792096150734956,20.75434277182337,20.680462054865135,20.828857737742815,20.705652253830156,20.82716930719595,20.642148535753073,20.78965576525572,20.73349076666132,20.68378713444923,20.748121092031354,20.64740101758934,21.068718195531115,20.818044865701655,21.251670420901466,22.312420822380133,20.847357348479463,20.56536090308968,21.361588535609556,20.72227412557668,20.690725725574293,20.68261078249264,20.853593977986986,20.79374956507883,20.88514663334022,22.05547973745275,21.62975467073325,21.315085061221584,20.820248698454783,21.0735165994991,20.75367306521987,20.71129826413984,20.66304111811517,20.755214618212314,20.66968732238265,20.860131609115623,21.1616901756026,20.680841940308532,20.72352475030505,21.33584174771058,20.689421543943002,20.687002051958345,20.74285456864674,20.90243835702285,20.737235721326737,20.663811200845455,21.50603662555607,20.768218852746568,20.695364006046738,20.866468391268562,20.701909228755124,20.760050374071735,20.661694008126982,21.212636622932212,21.333360738976406,20.700959365959267,21.951273296791555,20.717006342511446,20.715204981836,20.794777253205293,20.685742927433285,20.669666611107747,20.659191250455066,20.638133812177216,21.650074566518427,20.706175494990063,22.417892793769,20.906992513040336,20.765365695922767,20.64554210527898,21.306034887802817,20.85130257536895,20.73041974289348,20.7464410269215,20.642958541240628,20.69192955469613,22.727574792360244,20.67217150290356,22.253837233711458,20.672992391692066,20.782663804751508,21.004617276067204,20.875810343431414,20.713903413355236,20.702173207258838,20.812689622799418,20.656684175204827,20.84924415829596,21.771991066749276,20.68461646912242,20.793384896061198,20.732193515090266,20.985958882338508,20.752432893523586,20.710472708302625,20.817130423339687,20.691582049009558,20.70358242367462,20.758479812853608,20.79084936491208,22.430747082179344,20.756541276866468,20.740267948629352,20.72301313402863,21.507154351474732,20.6596057416441,20.707600990703643,20.690464007749604,20.686506250679464,20.69289802472017,21.493028660533245,20.70107676227279,21.307773283144563,20.786869263952866,20.736062489427997,20.834298800332945,20.82471288679427,20.73018063679556,21.78452975337383,20.717381259402188,20.772924260572914,20.78913070549721,20.892723886461088,20.738253598884917,20.72613040913953,20.632090074900542,20.66437978053947,20.68413430496929,20.927524789214747,20.742304902695544,20.695401597660776,20.81496568551731,21.28537670609601,20.689298884089272,21.297570752361217,21.186885604262436,21.13354525946953,22.342345646849143,20.966094766009054,21.95787693546327,20.77375676163255,20.679274429755132,21.475696139613508,20.7293560956869,20.69400178087816,21.387113848414185,20.710419417735384,20.693289987418822,20.74554941541406,20.691219760116596,21.568651202188303,21.8662074718815,20.70105147874127,20.661446091063382,21.320706098299613,20.623314895497433,20.833344387235137,21.666172242190456,20.71326833216821,21.12975784492961,21.83238140448168,20.66403514488674,20.753205475179207,20.71390533111285,20.89162680313381,21.60752294397749,20.687222796412424,20.937177031231876,21.63386205175909,20.95259209892797,20.77084901848052,20.816206669718934,20.706706948555002,20.682017278515445,20.756957646966626,20.7742975171621,20.687453684396385,20.802052810260214,21.354748573537137,20.741691461312005,20.678662952092797,20.729314161572482,20.75232655123271,20.751050444282193,20.93698089009691,22.31413833248221,20.70785489281774,20.659367638225596,20.697192944888474,20.690367802607348,20.731529462007735,20.828795137965503,20.984138311640347,20.815832178447543,21.193662135766626,21.121896474011578,20.776836151254813,20.64758342722903,20.564016635032807,20.905022022957905,20.69899105301975,20.70164812625951,20.71828962362091,20.665895601480088,20.69977337021337,20.973991300861265,20.83916221632406,21.186836893668364,20.75787514396944,20.77065563947021,20.66306370015832,20.84808400424363,20.68908950945025,20.83353671325755,20.691196138459418,21.739371268796265,20.805857014683312,20.79663097187623,20.778947456782983,20.806122324884395,20.65413330904543,21.09765924549946,20.766550676377815,20.71526288153708,20.71483541042573,20.834793967892807,20.80900314171802,20.62156343733953,20.96687145374791,20.868245962227874,20.73885370060982,20.80017039965871,20.740614679231804,21.13508935963316,20.644477155273023,22.719017117601013,21.184181856583745,20.88824124406634,20.65984442619364,20.69753600313711,21.393844795752482,20.669643668885136,20.744727994752235,20.762812164664513,20.70917674719351,20.693013875468132,21.023050294612492,20.775487606491556,20.885027720819213,20.6875324070661,28.315465855173557,20.692013984937727,20.722189193919196,21.11663223067475,21.35281061700821,20.748330933225784,20.660496956232887,20.958184890112385,22.336100786864034,20.72180378765469,20.701277484205832,20.753625117077497,20.81571053392687,20.666514400110646,20.893781567418465,20.72963941985847,20.661790069573,20.732617997782263,20.78786641992133,21.513798211381513,23.90635621445614,22.5387072724398,20.980957188714786,21.33656168731593,20.70208419161006,21.34401981758171,20.652143634711095,21.553095375797064,20.678125153107334,20.76270249000516,20.914663431647238,20.691311689391174,20.773361437828175,20.68952624730436,20.659321397991892,21.001431638827455,22.306504661533456,21.043319974800802,21.192766535088634,20.75723426320295,20.69712932793429,22.48640123993229,21.35759571190239,20.675826921828506,22.533919291405255,21.298502513127563,20.712903045939075,26.060978639107052,20.702280882813177,20.80835726222974,22.8067210928039,20.970300225210412,20.804487450960995,20.683655552942852,20.837013701254364,20.710629951224373,20.95914226711288,20.75051167572994,20.735833221863512,20.776095864240283,20.702462669802813,20.8102063215608,20.664049885394995,20.701970035788705,20.739363883674503,20.74036954681061,21.040470348364945,20.762226953065365,21.192513523512993,20.764798198823172,20.682015870115478,21.039012767386424,20.913919677726486,20.677752056098758,20.704662238296645,20.733618573925682,22.140019895008134,20.767027608954,20.72589081975814,20.980625982315846,20.93014128967697,20.827286507324835,20.804864126847022,20.74594761599571,20.960044584616142,20.944204522320586,20.71018746356784,20.707569137077737,21.400765772576925,21.080940868697866,20.861101032268998,21.11874256845611,20.8868960626622,20.753936608264887,20.66838966044242,21.06151640395633,20.689796321676805,20.64067249776136,20.882904545427753,20.734648241408465,20.655115938790534,20.701488754427547,20.61209179745659,21.06567423823968,20.671998449727685,20.668947066457203,20.730105389613012,20.74445649756856,20.738243107800926,21.92650877766434,22.56249897052361,20.84910849197226,20.71456735361368,20.759905717273476,20.749614794777955,20.752141588236793,20.704372304640057,20.752217186735514,20.744260311609743,20.72313782446406,20.85962720156672,20.783504526822732,20.7472515613034,20.753265817518624,20.723019901651785,20.732798973853814,20.746415317183416,20.709191748452042,20.74982390932267,20.690869136233353,22.218941205006868,20.735769946208144,20.76087329409814,20.700078243825594,20.71270591448051,21.066161069411223,20.74677285168969,20.82041212892029,20.775521163191478,23.057084896676603,20.673742633851244,20.98574272524405,21.03798048577995,20.762521806583074,20.782693112570204,20.82500689295831,20.716557660591572,21.03776336494342,20.840412760868958,20.669957339860225,20.692683237889717,20.685235365600683,20.912607988679813,20.768562836850187,20.775784185019873,20.661892381662703,20.778505167099624,20.943859212675825,20.750672469247625,21.914501910776504,20.710279520081414,21.416480138634217,20.80538043902013,21.08564628992812,24.489986199596924,20.78530057323578,20.679257336992254,20.663542366328635,20.673680456293898,20.664918822991673,20.71032839570934,20.715188132742615,20.730390669917536,21.154892964156424,20.668453443229318,20.656313219516647,20.7151365202061,20.76849673292994,21.00315408249416,20.784749699204482,20.74575349315384,20.670429294340494,21.617435442041774,20.959309948770553,21.170044618006354,20.678674162502624,20.985582814012503,20.659543588646567,20.63385131847976,20.761626679923836,20.872018332619557,20.88437210634929,20.801768146903303,20.726214283692126,20.697819607811017,22.40933421680393,21.279337499056783,22.097528215891217,20.723343786277145,20.66186127428027,20.717661478584187,20.650823052870596,20.984054381579263,20.65563599726488,20.7908810114507,22.645065291719856,20.683326947864746,21.401144466178774,20.69134452139536,20.707541504659485,20.741477120322678,20.674503885398114,20.721159533037113,20.733532463843908,20.766120567993873,20.645616981418836,21.05575914926802,21.43697975344179,22.51277599251929,20.772109190372806,20.725912983502912,20.700248379867162,20.809811728507423,21.326706391368067,20.80665874865613,20.677941387270717,20.72459625513406,20.66686918383832,20.732217659391782,20.643980149306916,20.789925875954456,20.93348532896261,20.702291372186938,22.853500724515015,21.06293821281863,20.71290207028498,21.088750611675703,20.664184201241063,20.75690117396861,20.660599099543482,20.822877015806974,20.71039628040082,21.02972664216916,20.79703228526757,21.065037721762547,20.99445733157592,20.957894924598303,20.793021959733274,20.718030257661923,20.849067278124096,20.947702297157846,20.643360715624716,20.73654126302279,20.71054986050312,22.173464104834164,21.658925874673013,20.70575816769218,23.298909696984968,22.14581047608621,22.190367885602825,22.6408598325185,20.644877956962716,20.81332711639454,20.75480957608082,21.00855097931661,20.71829723682852,20.941449082262405,20.71104498555971,20.72692442994324,20.840027323641163,20.8071919194109,22.464954048427224,20.719204312312275,21.120087347991802,20.796104803176625,20.636992322487583,20.682584924944113,20.791234438588702,20.706486342014532,20.70576253491991,22.516981451720646,20.92056059257773,20.783540381009324,21.124292807193157,20.825145597412664,21.1362138226523,20.652391865712804,20.653462869113437,20.737650958872223,20.810864207857485,20.715369803970376,20.701991571110213,20.70347060030968,20.879858864608217,20.80074063469713,20.758792671393348,20.980776315295568,20.841909001447423,20.86887366772345,20.765754900794704,20.726211089491976,20.73372854223153,21.911468951272145,20.852889801337724,21.03543143435425,20.93481463382632,20.946173662529105,20.920536511217584,20.639082038348313,20.70115850801573,21.383670095068545,27.09648545190791,20.825867219917914,20.69975036088415,20.67750395991864,20.679028599042457,23.204694623220767,20.68346500384009,21.04999923210069,21.702841202627106,21.89733115729322,20.90593080368558,20.8461215673668,20.650932252441216,20.804233302180595,20.65963698255313,20.699014181130057,20.740669381242284,20.652112107038796,20.81912868482401,20.540453515983074,20.716113885419556,20.646026973225243,20.697876702984928,20.696571692999424,20.712807139928028,20.757166250506128,20.76672726578443,20.94328656930919,20.687992593650584,20.793597427770152,20.705603835493793,21.659058972652254,20.66921173967655,20.718214332295716,21.321571351516056,20.684851315925847,20.688184792497204,27.03423772279472,20.70429132295414,20.681627520140385,20.758536175873434,20.81373897636954,20.689070449974928,20.976995805427453,21.73197698236738,20.796446582781336,20.72300027277435,20.718068666375324,20.827759419483705,21.22029401880549,20.75069739283438,20.803159364757963,20.843574311234452,20.892871336576253,21.01114356778163,20.729166034327296,20.78733368329317,20.674221169900257,20.652830360774328,20.862310256562246,20.83019469820626,20.65729286446997,26.95180533939304,20.866284002392973,22.722989802570947,20.835288787655426,21.407668509203578,20.878548927341043,21.70960568483987,21.39729591110999,21.363483804794143,20.937964724502315,20.691196138459418,20.736380066139734,20.680571614437948,20.86785397509662,20.79008715608862,20.733168881907552,20.84799321245771,21.087347282623913,20.821464183757513,20.67889189965678,20.757830576278852,20.75149966108316,21.205453290030547,21.11081033819154,20.90820253529233,20.705392383585533,20.712309228604322,20.721920634900997,20.864056247534183,20.658109738325876,20.77448889496016,20.75487792844898,20.63153188530063,20.679475420087027,20.873535165632585,20.718009620213216,20.94409267929241,20.836832969903128,20.709002141813976,20.75078589868787,20.780615633151736,20.78478034991997,21.34107603069629,20.67319573408412,20.754274353652253,20.742943104593788,20.708761943726362,20.91145868202376,20.697794567900093,21.454279765544367,21.36040968669883,20.723866509791627,20.663068572036686,20.697836360522302,20.728449088057847,20.930462875857344,20.662824278936316,20.64187191951675,21.18626593873379,21.06817879374965,20.99084521071113,21.120947590519556,20.724598916869798,20.669518132865257,22.535030354247944,20.831192845246278,20.83917358795943,20.756610228437893,20.75074592672956,20.891310921980637,21.186880863114514,20.70384716076307,20.760288120467017,20.893903226022974,20.871581966365344,21.24300388515028,20.71362363653625,20.73124710970471,20.839938197497517,20.70193078247363,20.64584678114023,23.582337201438115,20.915107364641287,20.86538464506529,20.636295534101897,20.87564416284767,20.725567923380254,20.865532919243304,20.766270602233394,20.78670786406906,20.77647536275408,20.794402958398486,20.657668249137952,20.972457839501008,20.698388176693452,20.618645522951635,20.697495446620177,21.379363050768365,20.702013647955823,20.75030896611168,20.83766777596835,21.72946536127627,21.15068750495507,20.727765670383647,21.0320743174562,21.178592080744544,20.754019729401495,20.68655552586492,20.62820325600604,20.85288942059089,21.348761105473354,20.936211950964545,21.004595874448594,21.152418457323105,21.199842745009228,21.06310979653566,20.786471154879724,22.793678276213313,20.76149187174514,20.815117160336754,20.99778731520347,22.511305384405297,21.413083777225673,20.981377354811144,20.636551012133655,20.852907091295123,20.898078429818803,20.73636272156668,20.643747684134155,21.325982727547856,21.079591512088463,20.65824274465305,20.758203490218705,24.48578074039557,20.68401827951956,20.815838307687482,21.446095496061236,20.86287923317491,20.716512606654,20.65088007190802,20.819956354834588,21.590725368627158,20.73110071265413,20.664686173728175,20.665405147533374,20.657801869462805,20.75959004722978,20.71678940209215,20.650602950656058,20.95000422315517,20.702545994384888,20.614387275758446,21.243083358693838,21.515036232634134,20.708378859343224,20.817063425933714,22.16553952154555,20.68331629589585,20.717368739446727,20.72443412981462,20.689939662316306,23.444576960220992,20.72426917305305,21.47545962749596,21.15385095441763,20.825032870585673,20.831272075941243,22.930371415754042,21.09793003506358,20.779433585993672,20.67998484279493,20.680045242284145,21.007150288423073,20.72726219521115,21.02267783963971,20.69183077561095,20.746095245917896,20.81326311088019,21.124712034206357,20.728783164899657,21.147995040822813,21.821833125213054,20.717287749895757,20.680151660431804,21.335988077276134,20.69858285063419,20.782201246286505,20.773992199834083,20.778787406769265,21.57996679667583,20.720428186958532,20.697709001887397,20.695354229132242,20.711774596279092,20.71975384269447,20.8729423418833,20.751459432572894,20.68519645586421,21.070208693126684,20.91758492869629,20.7511241904546,20.675647056867685,21.053115096434947,20.77756689702953,20.756328130320856,20.54840531444918,20.822113158718913,20.78724705700976,20.708058085323753,21.519517691974098,20.750502079515318,21.144224703328508,21.139294818834514,20.81583767486689,20.714623016061438,20.940910575569625,20.697786111908858,20.73694895069146,20.713456019382832,20.68233061230869,21.144983961034455,20.71845593025257,20.906151206075606,20.73748600211065,20.64818560850827,20.822616092816734,20.92474197041894,20.651699686144855,20.84921948400461,20.69201233054225,21.09901213004311,20.75451442531304,20.845775192401668,20.757477746509693,20.777126174220022,20.71736306464376,20.701770601326142,21.252035199789702,21.47191220530631,20.835229786442504,20.751484308109024,20.71678928017988,21.773126026811795,20.746434259298344,20.77777606105028,20.81772127005712,20.902132829141223,20.71395997826976,20.708348468086786,21.05385550952163,20.67831734348631,20.715315156813467,20.703260297744748,20.973361352190416,20.672761944000232,20.818461077535307,20.959365163346423,20.759119379735285,20.74727125982599,20.70025758141793,20.66339670965642,20.996982069196935,20.81670921818949,20.647319882496213,20.919312823842642,20.712329192060995,20.683846688353988,20.824491717522527,20.677918241100677,20.93319445388631,20.84137739020813,20.70803998274238,20.668311499670654,21.107600370920217,20.879419384009342,20.677016308853016,21.187923546996345,20.706704541458386,20.762538083295524,20.742235567720144,20.777773354059594,20.69912838070252,20.754913926885543,20.904567028091343,21.111805830121572,20.833294234752373,20.939020093027676,21.017529236014656,20.683597811768763,21.117691014810223,20.687657659241427,20.686633349935722,20.727348065123447,20.732747095581317,21.522945234652884,21.787654807424886,20.695890041621155,20.684896047768,20.674327382173214,20.695401597660776,20.637497673496448,20.930573014331735,20.723221688119843,20.732075285183868,20.6762520523779,20.69073933232672,20.775494000873113,20.78767551247648,20.699422965698417,21.84612512753176,20.72749961192853,20.74959526746385,20.64650880364106,20.66335061734232,21.140419281853656,20.647566174826075,20.553309344147358,20.925110781497793,20.708677135038684,20.9977844059202,21.765883922989662,20.667775176176523,21.149364570788755,20.67017869308864,20.68664029127303,20.737251845768984,20.686635043226357,20.65631756624015,20.817159185001216,21.537755715565527,20.793780356639346,20.649862413375484,20.745685257803085,21.1827975399459,20.672158423507124,20.689308016873365,20.671979115801392,20.873624589922613,20.646658211728283,21.042763446658505,20.75272308480492,20.67315252565856,20.686318664949525,21.225084187859498,20.783989956800017,20.71906669447809,20.786344247737443,20.761425852578757,20.66376766143451,20.712634065673214,20.967492160484447,20.846840734319155,20.711670693462068,20.7306402642676,20.70153551648746,20.65513771164257,20.70527431313342,21.819881095006217,20.718108872556595,20.684046493688633,20.66455860712659,21.084309478793667,20.79093350601874,20.742110285197416,20.773679099199903,20.719065677477012,20.732729483588933,20.693402573035232,20.759020472548027,20.663171790755197,21.10260204100797,20.878660461868122,20.844903032770905,20.687115362973913,20.79235873902473,20.837646455889423,21.14082206999383,20.718036153456836,20.691955446323817,20.934346748878326,20.721562700703053,20.738946218413115,20.93354748113918,20.95169287209979,20.72547696113776,20.91189636021785,20.69139509820961,20.873496008030177,20.66915097803178,20.696483036134314,20.6918121423018,21.35858576827286,20.868373416072476,20.68483196566403,20.656426338807226,20.77351739849978,20.796385119846626,20.91812513692784,20.837250210951918,20.680843970890233,20.83214123248371,21.26359506420853,20.76500259082133,20.973571637887535,20.817236594677006,20.66741787279542,20.689621726059904,20.648752213240417,20.72140590162234,20.860141243765842,20.78543997908121,20.68109812372752,20.78586585529653,20.68737997025999,20.7261420615043,20.799886457382218,20.859363569807353,20.79691774290708,21.424655260758858,20.820339470952952,20.737919224571527,20.759680743362544,20.86304231628234,21.285942975078267,21.547645552336526,20.935690491493663,20.985548029845308,21.483528554822332,21.026027419547518,22.938676674543423,20.770632801733406,20.68176267623169,21.343806019565356,21.30635847916068,20.740054852408388,20.65271506591365,20.77641017395038,20.69698039802311,20.770916696598103,24.258845869659687,20.760613758432754,20.999704374279435,20.917907711007288,20.874025976194524,20.69253643188673,20.72677962744896,20.646005083058995,20.953269914186418,20.80027775324511,20.792173132509003,20.668734715962515,20.719543121058415,20.886136061529264,20.907690901016494,21.23047492843042,20.777884558401258,21.2686387837511,20.65853161011072,20.85818803647293,20.677429729693657,20.705976060527497,20.8189819985088,20.716514687805677,20.874016422511456,20.688795882014972,21.45030095526259,20.9070714009211,20.954128310839373,20.722884721842593,20.957411211353932,21.28354295825814,20.65012215291549,20.82886923986044,20.81006757455871],"xaxis":"x","y":[22.839725872000276,20.91128637269999,22.88916655727953,19.430343691015025,21.11735552640613,21.280697719341468,22.124201158883572,18.136670762341843,22.576080863006137,20.10297814136792,20.137307637847517,21.968529606994224,20.90466916348112,21.65055481705341,19.466148173643653,19.644920713511322,20.60018787473792,19.883803436319816,19.908561207609715,19.88468822756935,20.68752092168076,19.37233862557095,24.197363319180216,20.375121118721335,18.385719188798834,18.92468784406244,21.354902012656122,22.075827573908075,19.724580465382388,20.25426974158722,20.75534115928083,23.747615973251953,20.91788210799568,20.92150383722997,24.937914667295647,22.368646863289158,19.83010122567355,18.16161414446258,21.756941677613227,21.220261555831726,19.921137323148557,19.756730386301125,19.076215392899233,21.77156523118196,21.51345533419679,28.08571411944019,20.221414835639845,22.43563961615873,0,19.8813455186455,20.446723265477033,20.65296343235214,19.469859640957857,24.8692968913913,20.808662576707462,19.741272758302454,21.063890409149668,20.444375957405665,20.737383520261705,20.772392205332533,19.30201857006515,20.50461437981694,20.844484149195353,19.768388190282472,22.01816825105002,20.77707066358558,22.615494713832,21.249591056381075,19.3101001682171,18.557194427644223,20.820077266859332,26.161807884063222,19.616841359021425,24.601371538637977,20.929756127701882,21.371449507411626,19.801676786988352,23.186489993216338,22.288287088869534,24.593075628702742,20.324838680602316,19.538495641550785,20.313003679890702,17.83704270507587,22.508853453023672,21.500237946648834,25.808363100543477,20.146924115160264,21.08058139373211,17.12438387523814,20.968249130032163,20.028493079904162,18.408984789492116,19.05702758537138,20.086616681313576,21.346006819966092,19.260460167342536,22.182304931495295,0,21.19880336254275,18.935012496616167,22.63391493571178,23.82192770536634,21.24843214899758,21.979683188147945,19.96496549198579,18.7932762528682,20.682137237753555,19.632965031850407,23.473975339042966,22.69657897049395,20.12759231185353,20.362922084924325,18.91123104739614,19.548169047796804,22.968053968168725,21.917396756553043,19.216324315987517,24.057859634036678,21.674989393981168,19.72022056098374,20.690574372087426,21.01024397957751,20.841336970061143,22.67830612646918,19.365836228253468,21.066918904158555,20.194293964879012,19.514174668391416,20.55950872954696,24.227674033652324,16.89837701160671,23.000908813982363,19.725383525541226,20.372113673311205,21.25165374313537,19.232324890468607,19.688668857104407,19.006769325349232,21.51252238785962,21.56101612004373,24.60236976749392,22.783863807855585,21.893038360020444,21.254697640624393,19.266317599579637,20.581937941283943,20.45335403901458,21.260660009666424,23.096986451321428,24.198260151329528,20.67873788317677,19.161993412283312,20.805625607806174,20.111067676706508,19.630957845101975,19.421904563712328,24.27931634919893,22.429090755701058,21.147722912842873,25.31201588491192,0,19.877134324376346,20.195578868662224,20.07860380095908,20.74807693982553,23.74195830377693,17.92351319630841,19.98475794826,19.330905939549268,21.75045501313216,19.64650597499701,21.21476614620658,22.11740118712243,20.373745608135653,21.286908529421844,22.031685219396664,20.241113097705927,19.58596744655146,19.47904998638186,21.40733571681439,19.113174941454258,19.88328018890598,21.622357074698563,20.24572329389297,22.858722725717435,21.004219094475406,26.772642502942784,20.915100345640315,19.607783613490604,22.164399063013658,20.790635654144218,20.499654561276497,19.533857847503334,22.238063995762765,19.371674563247215,21.64924126399572,21.665790435544952,22.72561505846151,20.402841366046307,21.38581238341487,19.76025356060854,20.786842719711426,19.14889856436821,19.895681776607635,22.093666112296297,20.459085985918723,22.982495350783758,19.331349158501606,20.55332894959407,20.31682150206961,20.014253448408297,20.016785168783276,17.008111246780107,21.536104422950213,20.603485087703003,19.59434829773337,20.049508251055094,19.642302060721956,21.52720398550173,18.99020093378567,20.22398061738597,21.01213158467734,20.053269667846333,22.108159109401438,21.79011956511657,22.084914601393155,23.135231909261865,21.190067594656295,22.67339901515593,23.695624962968736,20.984090887872558,19.114539527933296,19.182165482881256,21.52107548780855,21.22249010417508,21.043820980181792,22.894093531705956,23.85820974949918,23.265700738521485,21.89761800850832,24.611415306054898,19.96312680068985,20.173501472804098,20.216216542451658,20.253427701740367,20.770035525423065,22.54105973810428,21.560752172206048,20.50991100623185,19.66651702047476,22.893554470389834,19.678935343554116,18.37060227548128,20.996625195887564,20.88004158687887,20.017617555780475,18.785129945834015,22.472827160217914,21.123452379734417,21.834751391009725,22.36922218137253,20.638651229695757,19.920184186837908,20.233535275947663,21.775846548503456,23.23197611805285,20.98111639712373,24.69390393605354,18.972968571042593,19.85555820528158,19.343431218526263,18.83915024282446,23.64752507012282,17.79557360413656,21.996936490386865,23.456377353127298,21.603092935704957,24.298252303878165,21.481533394476365,20.408641755181446,16.772761777957808,22.20744305955703,21.11943827672747,20.998113148879906,20.11872947152901,21.236410750491928,19.699904832330084,24.209354374596852,20.24751150890443,25.07655356851684,19.568646661314627,19.776542871248843,21.727271525181536,21.062803097791257,24.076140757673382,19.409983373621806,21.192511957229144,19.312950102671177,20.179206907329647,22.432801454859,18.979888467927786,22.15617321227827,20.260215495083163,20.910464871041206,19.297632789142515,20.29925833471969,21.341220185389716,20.961489967791394,22.164351182655523,24.799241151178187,20.420977915907716,25.348057895064095,20.921713841616427,20.242927524338644,18.95868492479523,22.708968189549438,18.49440443743341,20.482328404305942,22.37000861087754,20.121388068881203,20.44167803431194,21.856025378041313,18.927945745238855,21.9419703223203,19.736694248820182,20.23558290561487,19.836668342627448,21.25562508835354,21.394257392515996,22.71500741252458,19.547704151356815,20.400995752923983,20.265469687416747,20.212441762527977,22.377553705906216,19.66793597026211,19.3315880738609,17.677595133772794,20.276225239062065,21.268637940423815,21.725285950225775,19.195369804190715,21.15736975857522,23.488081735909503,20.232817889146876,23.728782472563765,22.84612630027242,22.077517747249683,24.569997706071334,21.7711290432985,23.43665576011153,22.83375379636568,19.2148787700197,23.42842113265138,19.4136293492673,20.540968959330602,24.77613012919926,20.659638185399235,19.087533251252736,19.83361590003151,19.96019429668775,22.334373955189864,23.109422435163253,19.778390202711652,19.167836614935887,23.933068642871486,18.56499419954315,22.530048594882032,21.71290394044351,20.45930590947763,23.069120127461606,22.999636382856856,18.925763040720064,19.49519726866692,21.042555130756018,21.216321947424902,22.454539777644875,20.13393362882646,20.867172390851817,23.066997383640125,21.52825545142037,19.72029436584214,21.117385332870825,20.179272523647345,21.590649178257912,19.692513024244924,22.86005581934068,18.799932943985397,19.96989517504386,22.510505954413144,19.34599856610255,21.28740957905986,20.158837704444057,20.907660872216642,21.905619198419114,22.38172749868176,24.308788888311135,19.262765428559003,22.547404100685924,19.87902826477287,19.826795369611226,20.791833978792408,20.76154230516995,20.245177516562332,19.101442274030862,23.095721196617728,22.853333163501407,20.55222183482644,18.623284957897813,19.707161942553423,23.312427516610697,20.72798026434645,19.38519821479218,19.584766197834426,19.657602815047426,19.766824889008415,20.72718342775812,21.19739249261565,21.766933642509713,19.997824758905686,18.07487316385907,17.89367601727741,21.076958761045283,22.70946891023392,18.280325895793162,19.286378115164315,23.27164625070411,21.087259361308853,19.696262913257698,22.998819199975294,22.068276096052415,20.676958661968467,21.84293342285994,20.22886359790674,19.302304652102425,19.45315606988007,20.189004387901257,19.834023760961447,20.136832510309578,21.96825774467718,21.693760030133728,19.68327564708964,20.69046887217181,19.902947681155606,22.060735176692912,21.15375333067992,25.097138279698054,21.440477364164977,21.3505869180748,19.260072175979914,21.05711024152434,22.371116977331784,18.18526261240324,21.763695119906945,19.900159246109634,21.022878950161832,20.48383078099113,21.821470865389173,24.987649624596244,22.265101932257092,18.954616641321604,25.734123629818527,20.035932572983462,21.002454511024606,22.570203152455143,23.437077312046757,19.317032739803228,17.641741979338622,22.448170385491473,23.33275256138136,19.064590160696714,19.240852168960235,20.1483293247843,19.78119349991646,20.035822020141666,20.368090770815378,19.622542520355413,19.136726140380034,20.665117281017753,22.04592459909886,22.766122645362053,24.701481133697825,23.00797271520254,22.942154096547412,21.766675289013797,19.216601410973656,22.721359157406464,18.9576603836655,21.88381079939571,18.472822451162664,21.07184726592027,22.03833211483471,20.099462108962847,21.41678722119122,20.92948670168764,20.890428315584586,21.725547245838506,28.740163782092626,21.205026123435218,22.323663662536706,19.34101707633175,18.666140767995948,24.012052706607076,23.69806207100269,21.754438391935153,23.682062419502355,23.126467381893125,21.07179899498228,25.515113797875166,19.69217074150189,22.12431429427697,25.95498550595432,21.870103917395213,20.208896815397623,20.476170795773445,21.523407372759994,19.429205075454394,20.94805891660743,20.579612186732355,18.425777297525496,21.048120970639673,20.14580390451336,19.1678248265788,19.26180569731426,21.731372277489818,20.677454054144345,19.241454867705745,23.733038602815835,20.006473671385464,20.924659576737604,19.109062896280463,21.73683233585413,21.589387814254906,21.96355600303241,20.930649460210123,19.44163523375263,19.835220336552222,22.967539915594646,19.57718354371762,19.694551733623157,22.05857907784579,20.152641197123305,21.206937139516242,19.177040653531833,21.059144404213697,21.77992540342766,22.0971519060863,19.9033116727081,19.870869348950354,20.847191191309296,21.255773747330974,22.179827408990512,20.655014419256375,22.594985264708335,19.867378293238115,18.6010489412016,21.546971094057223,20.8121798521563,20.30528553843669,21.30235611574127,19.472993206549905,20.642314511859237,20.823420888524133,19.456579392491523,21.112413416140527,20.769205079804532,20.344212591822245,19.88957080391307,19.783879090807634,22.2606456541005,24.0347829295736,25.71484503270827,20.79497581994581,18.966254774482525,19.558951865783115,20.00118447409592,19.759231530600534,18.668189678924886,19.362035840585754,19.21669376414341,19.534513178135196,21.58943582111394,20.029295821796193,20.93634103691578,21.419330156983328,19.511493048750843,20.011894681671833,20.008240359915625,18.95284902418798,21.345469949872594,20.977820033541025,23.20923603756077,19.692880228017533,22.637005220660473,19.457649403400342,19.12974837607436,23.60775201354553,0,21.026538976608304,23.88396569488923,27.919301359795483,21.157605371717533,22.006125382801496,21.526914680071894,21.60107003042797,21.310840181677595,20.659294437479453,19.303395037499914,20.578169960300666,22.895450751816803,20.179768194947595,20.3718571876006,21.13235475498899,20.116418845808077,20.597315830475623,19.957464846805212,23.115072392844226,20.726116689654802,20.700075492290427,20.833447685328323,23.397238664878486,18.725486086935668,22.38037915102657,20.812817350665142,23.171040699108495,25.19463787743612,17.21138734793915,19.0336054859068,21.825732486030095,21.042046241460838,20.6322831501737,19.380751294097546,19.582601081862446,21.284194059773114,22.179583552994327,23.19729034551032,19.967482396438246,19.738441963085798,19.239293618022067,21.59285163447237,20.535950228874793,21.01158079776531,20.08203076321153,22.84732578883419,21.756353934255824,23.316254320731808,18.81898933836999,21.940012430656058,20.748003621067692,19.730383893455734,20.182461946627072,21.076706013187607,20.7269525620563,19.59081933226322,20.904152603600195,19.659163210227153,24.761947476774708,22.534274856548336,23.233448675874918,19.884197354704103,21.007268735063114,19.408841782080135,20.136824689258354,21.854300382519792,19.881144027962787,20.995690391681727,23.573228460474112,19.074352191519743,21.469722294142937,18.833019773765052,20.27464659506303,19.718574682047347,18.258239797953646,20.755317114550273,20.561150873374917,20.948677886535172,19.720177445507503,22.150900676319612,23.39909808655993,22.31935969126546,20.155632003058283,21.13370665837429,19.473136561032486,20.46100849412487,22.136011068345166,21.159614691783496,19.431106818709004,19.13213914887522,19.706696670705693,19.346767596320518,20.918060329643115,20.18711885177595,23.064648540231374,18.98912255599455,24.074524917146316,20.61646897501603,20.227612418267608,23.77281502683423,18.64679105166152,19.821061094559884,21.41515272305351,20.235862606471247,21.36550800836747,21.97455042987335,22.17732889464112,25.151779365365787,24.150527389957542,21.040691724844343,20.760535400558187,19.34725801943171,20.04287236504247,21.531748865354032,18.726909959036774,19.7936588697483,20.03811220566055,23.56945490190484,23.91740995998136,19.437420256199317,23.65829637497728,27.09570141066156,23.540007985307764,23.389995853863947,20.300248650151303,19.67235973078438,20.003307902740296,20.529649424214696,20.802720957968873,21.281868210231526,20.815053320149143,21.927740352835695,20.82479873735278,20.51237686567707,24.94868149530031,20.081935873735674,22.717589515695337,20.82980401444312,19.3661810523187,19.957403913237158,21.18136460499971,19.8915004159983,20.17210241586846,22.4307529287208,22.889842311610867,20.801380865239704,22.613937582771225,20.216218911187326,23.993435990731076,19.557332226454086,17.31416721288844,20.21067743703825,21.405123280549176,20.1770277231615,18.904187708332326,19.046718202293015,20.073998999202523,19.6302991245126,21.48379659194355,20.81804614111716,19.563852956879067,21.051151840896342,20.770721956713995,19.67623430456214,19.581897497422524,21.384446242838667,20.692911719555987,21.545170940853957,21.994462034082755,21.47969097288377,20.278388484789648,19.382106816183008,19.441821622178967,21.939001270486997,25.832615032960224,21.65340972782181,19.535812626400283,18.947642644803835,19.712127286089668,23.90063882300631,18.304477915148002,22.443056343683086,23.16226531069458,25.14188655199703,20.70748794380435,20.732604891283092,19.934400683437797,20.62113432421718,19.28434545274011,19.017540081468233,21.15041915835723,20.26686372980413,20.03253125195289,20.148876382348814,22.62198208884591,20.021014007950576,17.20778649543665,21.273482891420798,19.30063185166596,20.195062159996276,21.445335194484834,21.0819625214105,19.392355160192587,20.378347780631422,18.799302474087973,23.18300636577445,18.93991113136844,19.223167412657357,23.650902186871754,19.945176609895544,19.495627675541318,25.79058328300537,20.476746945456053,19.21271648764287,19.678332456797207,19.71871179715288,20.105565995835796,21.137414277628565,23.008367409641956,21.03393309328942,19.30271923785845,20.626830273876646,21.092597149125172,21.952512694511015,19.882805718288704,20.77592509493985,21.046183463307532,22.0435593555408,21.04785962557455,20.25569596256737,19.352118206703615,18.845717636328267,19.34199651526983,20.91962424027046,19.595170049801663,19.600097337632842,25.661642339745672,21.791142017552335,23.50747794569296,20.391385205003306,23.803560322980168,23.78648471341281,23.49397732898427,23.081833665589276,23.144648130459657,20.920832580620292,19.193623112656937,18.901120542977083,21.79790479470573,19.407912925215978,18.98289333455679,19.415382799115825,21.253000869012993,21.604198363469845,20.557571210143905,19.274253014992237,20.09246940100189,19.391954018620176,22.850833320712763,22.012401869246364,20.826658939867247,19.118363214083377,21.289967644645948,18.733922283625127,20.9115880183164,16.829816883719186,20.861088989018167,20.73330584327736,18.406398882361927,21.03390016791011,22.274116203708253,19.445206608830574,20.834279688605037,23.70344643795525,23.354012710201598,19.88516012413886,22.730738164708086,19.75792383693999,24.394611647917593,19.734888199833236,24.922685338201692,19.530254509942655,20.280991315768514,21.951122115143434,19.19842345841963,22.926085660752005,22.977548256996624,21.21128990053558,19.167558108393887,20.647437784958075,19.35146728073557,21.097845676874776,19.32409702234251,18.277738745265133,24.145550575002787,19.346013720701006,21.812205122435703,22.86783241905905,19.249124305809787,21.53558768926747,24.102450641699225,20.500920634779252,19.622850798933502,22.45828794986169,19.57219805726336,23.264639171673256,22.333293091972617,18.995877797816178,20.694084927736416,19.630595419370202,20.436904665975636,25.664004852328446,20.985299957568404,21.714666548374087,21.619000188699776,20.319316549429495,16.9587226208279,24.58750691743097,21.431401448181294,20.454850489915017,19.41323537187323,20.769097708402093,19.12410363757553,21.037456801472633,20.865493152029874,20.200550831447014,19.074330956681983,20.89335338152254,18.80103888605608,22.493051859930933,20.264370436179377,21.216120605388507,19.103940869449712,23.841976780210096,20.967615900689093,20.26093058536308,21.15453445100085,24.16646354231811,22.25017305320405,20.99185811169326,22.413732248617176,21.395751936133603,21.027619198526104,19.682163069515127,20.59828494694422,20.60161654204836,22.930920924409694,21.340491521133263,21.13532539299306,23.86936283890287,21.30122097904847,21.87678820576361,19.984953818018486,23.219303551688306,21.660324565067476,20.87994439864477,20.922214169866013,24.081396558763124,22.721264543402345,21.873381120752498,19.089555706982512,20.89641611390129,20.568873923302558,19.242152996635603,19.205602363695707,24.69282091354044,22.779994713980038,19.521220854266264,21.142478555307388,25.236220259743373,20.048804801912784,19.552903579436016,22.115896159241377,21.403353538154413,19.840625811644227,18.35486059310022,20.417051943879315,23.212857021000243,21.53372289651904,20.79356441567801,23.25433145453618,20.592440384261195,20.758465193484422,20.24512208583746,20.925901548056537,24.750691092265395,19.41019874513054,19.696673307271265,22.56844661795243,22.50456479926641,19.4801354006198,20.95523200641152,24.568652749155945,18.75491624208908,19.461087704959443,20.880059154822476,19.68504303470752,23.816825076283962,19.839377869097518,23.420195657862312,22.999884079036303,20.649943786599653,21.684940338873297,24.09117022808059,21.12734576220247,19.49242240707679,18.90953834843785,20.94197381362215,23.259273464150183,23.03775857233843,22.58649267850949,20.120777025175872,20.906486953193497,20.378669388865216,22.737839310980224,19.27074701059353,23.65992231174119,23.339497059809283,22.05115682063906,18.775074918844375,22.10097025846529,19.31545475085148,19.854767962698624,19.840511525069328,21.39606694375647,22.97307940319244,19.86193501563865,19.664038907092028,19.58463718752616,19.810866559819527,19.465937010710118,22.434772773304594,20.445539922646542,18.99187307244663,21.90849954742701,22.314131385426723,20.432733544527707,23.07558442006914,22.830034474576443,21.269107840654502,20.48988421076927,21.083491338486773,20.934278289661282,20.287989620862316,20.04722528013383,21.199347342918216,23.521067577919112,23.2611930742384,21.92570835834572,20.556276718935056,21.41646356230674,22.53034703709958,18.95340785965337,19.986508970321974,19.495183602853512,18.457821713581545,22.886338306822665,19.129172314836154,20.136171638349733,19.376016989288264,20.879430687458026,20.620771994361597,20.30679376611553,20.368798039683195,22.35947129540103,20.069728697762105,22.54894428694912,20.37273542865621,20.71779772905426,19.199159497364995,19.174850117652987,20.13094661361353,20.62659792573699,25.292356917614896,21.758700626292807,21.796149648767223,18.807339197843834,20.506265808043914,22.110290220817404,19.52840260028443,20.325926854481473,19.625476983112655,21.50252421557022,19.304125125188563,19.107865328534132,23.169806519640005,18.840768128990423,19.36444675592656,18.627786784209636,21.17955986366495,20.377078375453138,21.746512886013605,20.910669810022966,23.66715843905666,19.356125235979412,19.33650002206052,17.813568277801206,22.465861054219694,20.57561772108256,19.015107379674305,21.499214768467855,19.66178092422977,17.299495077910098,21.27615053752397,21.040198736231254,21.92098191704396,21.7224558930939,21.092338092530998,20.048671453648097,21.894206057556605,20.415195026786527,20.471143313052202,21.849984955018474,19.59156765743604,20.658934051201378,20.105603080001295,22.787613064091445,18.897059576214055,19.64405870233027,22.74129354031224,21.948058114314257,20.038176858751854,22.177258018525677,22.318758483797644,17.211802761238985,23.061132678317314,19.552166474373674,20.22058498039463,19.1389392082589,19.751662039949778,22.589266875609983,26.340632927208585,19.534708840769273,20.435276534084654,19.903908501005596,19.287439594230733,19.687016476370815,22.7859490833811,20.26081963393427,20.008806098312462,20.432875222497227,21.770367489208184,19.54263516359492,20.167399003287947,20.02036350615788,25.816840819488693,20.66031803587651,19.756693437676223,19.47969541290554,19.46354033785,24.169496968680054,18.739319147762597,21.559823540829672,20.841106100432793,21.572330613609903,23.06348379203432,23.90661007609995,20.544449448312626,18.828527980887348,21.643407148870626,20.295760247970698,21.926818619899166,18.887256320586236,20.26723932452398,20.649629842496918,22.847263558158666,20.639447033358472,21.651692029555726,19.3900555692985,21.40442957359937,20.067715321953298,20.687515795874024,20.646624626390846,19.38789150561276,21.443595032483266,21.960080436904104,20.986097829919892,20.116046923130224,20.38834949087981,21.51694698715695,23.463402138610206,22.107609158967936,21.13017429963753,21.059552411062068,19.918938919026616,19.132327452939787,21.94580529932221,19.159457479949786,19.567551127469855,19.27757204338643,19.477956222501692,20.02620191687006,20.455200776646638,23.044574887856943,24.184179585129318,20.388726808803085,20.116390923202314,21.32891538129463,20.93827950221911,19.878697565839243,20.238709984208988,18.95241706841831,19.63538512176151,20.604884938737765,22.740623426919786,20.3125128271046,24.597055201180662,23.117114991068007,20.78628428680642,19.932433926767427,19.826295549099655,21.26517033895163,22.496434708234126,19.328382384869762,19.427802084161115,20.18180618902739,19.980960001271505,18.996123380968683,22.600876020595507,20.092995805378262,20.046496736817677,21.151917099556776,18.585982112110514,20.695329089308867,22.520164246985647,19.3471325562148,18.999355953219133,23.20757904373024,21.23702785682451,21.08554669425927,20.768225524289374,20.2479251962018,19.3533467533098,22.213653662019038,20.24562900900173,18.67996002744327,21.27136955536682,21.42004604510025,21.047075972047644,21.8035683042629,19.22968489276112,21.077647966937757,23.387816453129698,18.706232935340964,19.58110217755027,23.538867158230946,21.96712491711981,19.68563168309507,20.03480567791853,18.855622835428132,19.611919934246885,20.89874787711282,20.381292625866948,20.623611830893093,21.463850115483186,22.47425899129698,21.46282774650559,20.250449939937386,19.804071134581005,22.940732615911422,24.1535698900103,22.049830938319623,19.98785104241217,22.944212504137,21.578861893090256,25.17177163891695,19.37427511207424,19.96383210404445,24.180920079085045,22.206977518330888,19.158269753875864,19.069924834450198,22.714272038785044,22.147044335773373,20.251879940231216,24.956261862764006,19.58788736648328,21.751091249893005,21.93032100053686,21.621710733177533,19.367732274650958,19.09456742423413,16.917281198342668,20.807982080155547,20.46876211568297,19.988975863285916,20.580262018382033,19.091740116853863,21.44414464440018,21.29242653675922,22.038981262669978,20.180643707721647,23.155520679582406,19.32638298997934,21.506597416475046,20.650611348398325,20.693713245656816,20.540019222244897,21.217521718500812,24.49306731779338,19.642596451766774,22.00972851632279,20.932965503614184,21.834232905374517,21.098649782686852,22.49932865299591,22.579718287600105,21.529871119919466,21.118854440357843,20.944082894927185],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle ElasticNet() vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_pred_logEN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["alphasEN = np.logspace(0, 7, 200)\n","l1ratioEN = np.linspace(0, 1, 6)\n","param_gridEN = {\n","    'elasticnet__alpha': alphasEN,\n","    'elasticnet__l1_ratio': l1ratioEN\n","}\n","\n","GridEN, \\\n","BestParametresEN, \\\n","ScoresEN, \\\n","SiteEnergyUse_pred_logEN, \\\n","figEN = reg_modelGrid(model=ElasticNet(),\n","                         scaler=scaler,\n","                         X_train=BEBM_train,\n","                         X_test=BEBM_test,\n","                         y_train=SiteEnergyUse_train_log,\n","                         y_test=SiteEnergyUse_test_log,\n","                         y_test_name='SiteEnergyUse_test_log',\n","                         y_pred_name='SiteEnergyUse_pred_logEN',\n","                         score=score,\n","                         param_grid=param_gridEN)\n","\n","print(BestParametresEN)\n","ScoresEN\n","figEN.show()\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"y":[1.8451362440057963,1.843566312811219,1.8421358183737915,1.840867341582113,1.8397838331285414,1.8389082777190964,1.8382633172158063,1.8378708390827025,1.8377515389331573,1.8379244683604135,1.8384065814039232,1.8392122947673593,1.8403530780582464,1.8418370906766093,1.843668881393015,1.8458491650437048,1.8483746881396386,1.8512381916470804,1.8544284749613766,1.8579305604601895,1.8617259533395452,1.8657929870710046,1.8701072411048305,1.8746420146443445,1.8793688385928757,1.8842580071766442,1.889279111217021,1.8944015564179864,1.899595052143568,1.9048300587515228,1.9100781843903012,1.915312525046891,1.9205079443821647,1.9256412923813084,1.9306915639976865,1.9356400007363614,1.9404701394978061,1.9451678139963584,1.949721114711141,1.9541203136592677,1.9583577603467688,1.9624277550984899,1.9663264056397647,1.9700514723433895,1.9736022070051207,1.9769791894052395,1.9801841652836238,1.983219888727381,1.9860899713644258,1.9887987401897145,1.9913511053348074,1.9937524386334275,1.9960084634394757,1.998125155820277,2.000108656974791,2.0019651965104694,2.0037010260483696,2.0053223625082643,2.00683534034754,2.008245971983558,2.0095601156127443,2.010783449645462,2.0119214529986627,2.0129793905239475,2.013962302893225,2.0148750003144125,2.0157220595030063,2.016507823389756,2.017236403098487,2.017911681780174,2.0185373199387517,2.019116761930314,2.019653243359971,2.020149799139498,2.020609272003992,2.0210343213172477,2.0214274320233896,2.0217909236269684,2.022126959105174,2.0224375536745685,2.0227245833508123,2.022989793253721,2.023234805621732,2.023461127509811,2.023670158153168,2.0238631959860762,2.0240414453108384,2.024206022616574,2.0243579625512957,2.0244982235537017,2.024627693153473,2.0247471929506142,2.0248574832857003,2.0249592676138066,2.025053196595502,2.025139871918607,2.0252198498645546,2.0252936446331105,2.02536173143904,2.0254245493939815,2.025482504186416,2.025535970572177,2.0255852946874318,2.025630796195567,2.0256727702788546,2.02571148948524,2.025747205440031,2.025780150431727,2.0258105388807106,2.0258385686989797,2.025864422548616,2.025888269006199,2.025910263639903,2.025930550005596,2.0259492605678187,2.025966517551131,2.0259824337269534,2.0259971131406536,2.026010651783322,2.026023138212334,2.0260346541245493,2.0260452748856768,2.026055070019112,2.026064103657302,2.0260724349584653,2.026080118491295,2.0260872045900773,2.0260937396824668,2.0260997665920177,2.0261053248173844,2.026110450789976,2.0261151781117284,2.026119537774497,2.0261235583624955,2.0261272662390826,2.0261306857190884,2.026133839227807,2.026136747447677,2.0261394294535977,2.026141902837761,2.0261441838248038,2.026146287378037,2.02614822729743,2.0261500163099955,2.0261516661531633,2.0261531876516816,2.0261545907885465,2.0261558847704304,2.0261570780880205,2.026158178571681,2.026159193442787,2.0261601293610685,2.0261609924682817,2.0261617884284804,2.0261625224651634,2.0261631993955325,2.0261638236620865,2.0261643993617633,2.026164930272812,2.026165419879581,2.0261658713953716,2.026166287783524,2.0261666717768567,2.026167025895596,2.026167352463914,2.0261676536251776,2.0261679313560115,2.02616818747927,2.026168423675998,2.0261686414964606,2.026168842370321,2.026169027616016,2.026169198449413,2.0261693559917893,2.026169501277186,2.0261696352592002,2.0261697588172405,2.026169872762296,2.026169977842265,2.026170074746857,2.026170164112124,2.0261702465246327,2.026170322525315,2.026170392613019,2.026170457247782,2.026170516853848,2.026170571822458,2.0261706225144116,2.02617066926244,2.0261707123733865,2.026170752130221,2.026170788793899,2.026170822605074,2.0261708537856746,2.0261708825403644,2.026170909057884,2.026170933512288,2.026170956064092,2.0261709768613203,2.026170996040483]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"y":[2.0092955260185614,2.0028167704281206,1.9965677607991097,1.9906076858407635,1.984996238007754,1.9797922040344869,1.975051872050998,1.9708273155155478,1.967164644045923,1.9641023385575016,1.9616698059498816,1.9598862895069664,1.9587602498568648,1.95828928731058,1.9584606156956041,1.9592520321908704,1.9606332712848231,1.962567595624392,1.965013467848383,1.9679261637676966,1.9712592209846915,1.9749656583290571,1.9789989412875904,1.9833137007863781,1.9878662345181446,1.9926148315564438,1.9975199640359018,2.0025443866695176,2.0076531782611817,2.0128137511873416,2.0179958465604253,2.023171525420563,2.0283151603655334,2.03340342771989,2.038415297622015,2.0433320180759917,2.0481370887913326,2.0528162212038663,2.0573572821315897,2.061750219804493,2.0659869723057995,2.0700613596230037,2.0739689614373225,2.07770698343716,2.0812741153214693,2.084670383787032,2.0878970037109656,2.090956230495344,2.093851216184465,2.096585871542896,2.0991647358329826,2.101592855584922,2.1038756732336177,2.1060189261195466,2.1080285560247196,2.1099106291430663,2.11167126616702,2.1133165820057327,2.1148526345302394,2.1162853816614087,2.1176206460712605,2.1188640867511883,2.120021176705942,2.121097186054711,2.1220971698557074,2.1230259600143264,2.123888160684135,2.124688146621866,2.1254300640102834,2.1261178333147006,2.1267551537888347,2.127345509292848,2.1278921751303237,2.1283982256512224,2.1288665424044613,2.1292998226567423,2.1297005881235385,2.1300711937841874,2.130413836675754,2.1307305645802437,2.131023284536864,2.1312937711258457,2.131543674482905,2.1317745280141374,2.1319877557900955,2.132184679605321,2.132366525695821,2.13253443111204,2.132689449749051,2.1328325580389262,2.132964660312923,2.133086593843047,2.1331991335741156,2.1333029965584784,2.1333988461063287,2.1334872956649367,2.133568912440402,2.13364422077548,2.133713705296971,2.1337778138458487,2.133836960203003,2.1338915266230414,2.1339418661881218,2.133988304993297,2.1340311441743207,2.134070661788341,2.134107114557339,2.1341407394836622,2.134171755346459,2.134200364087303,2.1342267520927902,2.134251091381432,2.134273540701665,2.1342942465473955,2.134313344097048,2.134330958081691,2.1343472035874496,2.1343621867970364,2.134376005674919,2.1343887506002925,2.134400504951775,2.134411345647419,2.134421343643406,2.134430564394527,2.1344390682793457,2.134446910992702,2.1344541439080498,2.1344608144119035,2.134466966212537,2.1344726396248817,2.1344778718334507,2.1344826971349726,2.134487147162283,2.1344912510909078,2.1344950358296897,2.1344985261966496,2.134501745081251,2.1345047135940947,2.1345074512050215,2.1345099758705155,2.134512304151229,2.1345144513204026,2.1345164314638683,2.134518257572303,2.13451994162632,2.1345214946749618,2.134522926908092,2.134524247723178,2.1345254657868744,2.1345265890918395,2.1345276250091265,2.1345285803365117,2.1345294613430648,2.134530273810257,2.1345310230698757,2.1345317140389923,2.134532351252209,2.1345329388914074,2.134533480813173,2.1345339805741004,2.134534441454118,2.134534866478012,2.1345352584352675,2.1345356198983714,2.134535953239691,2.134536260647039,2.13453654413803,2.1345368055733207,2.1345370466688163,2.1345372690069317,2.134537474046978,2.1345376631347355,2.134537837511286,2.134537998321161,2.1345381466198474,2.134538283380718,2.13453840950142,2.134538525809762,2.134538633069153,2.1345387319836084,2.1345388232023694,2.1345389073241696,2.134538984901159,2.134539056442532,2.134539122417867,2.134539183260208,2.1345392393689084,2.1345392911122527,2.1345393388298723,2.134539382834974,2.1345394234163972,2.1345394608405104,2.134539495352957,2.1345395271802694,2.134539556531356,2.134539583598871,2.1345396085604804,2.1345396315800285,2.134539652808609,2.1345396723855647]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"y":[1.6809769619930315,1.6843158551943176,1.6877038759484733,1.6911269973234626,1.694571428249329,1.698024351403706,1.7014747623806146,1.7049143626498573,1.7083384338203917,1.7117465981633253,1.7151433568579648,1.718538300027752,1.721945906259628,1.7253848940426386,1.728877147090426,1.7324462978965391,1.736116104994454,1.7399087876697688,1.7438434820743702,1.7479349571526823,1.7521926856943988,1.756620315812952,1.7612155409220707,1.765970328502311,1.7708714426676069,1.7759011827968445,1.7810382583981403,1.7862587261664553,1.7915369260259542,1.796846366315704,1.8021605222201773,1.8074535246732188,1.8127007283987961,1.8178791570427268,1.8229678303733579,1.827947983396731,1.8328031902042796,1.8375194067888507,1.8420849472906926,1.8464904075140427,1.8507285483877378,1.854794150573976,1.8586838498422071,1.862395961249619,1.865930298688772,1.8692879950234471,1.872471326856282,1.8754835469594182,1.8783287265443869,1.8810116088365325,1.8835374748366323,1.8859120216819332,1.888141253645334,1.8902313855210078,1.8921887579248624,1.8940197638778729,1.8957307859297194,1.897328143010796,1.8988180461648405,1.9002065623057076,1.901499585154228,1.9027028125397358,1.9038217292913837,1.9048615949931837,1.9058274359307423,1.9067240406144985,1.9075559583218775,1.9083275001576463,1.909042742186691,1.9097055302456472,1.9103194860886688,1.9108880145677798,1.9114143115896185,1.9119013726277738,1.912352001603523,1.912768819977753,1.9131542759232405,1.9135106534697492,1.913840081534594,1.9141445427688932,1.9144258821647604,1.9146858153815962,1.914925936760559,1.9151477270054846,1.9153525605162405,1.9155417123668315,1.9157163649258562,1.9158776141211078,1.9160264753535408,1.916163889068477,1.9162907259940232,1.9164077920581812,1.916515832997285,1.9166155386691346,1.9167075470846753,1.916792448172277,1.9168707872887074,1.916943068490741,1.9170097575811094,1.9170712849421143,1.917128048169829,1.9171804145213125,1.9172287231867418,1.9172732873978369,1.9173143963833883,1.9173523171821394,1.9173872963227236,1.9174195613797924,1.9174493224149622,1.9174767733106564,1.9175020930044417,1.9175254466309655,1.9175469865781407,1.9175668534637964,1.9175851770385894,1.9176020770205713,1.9176176638664573,1.9176320394842707,1.9176452978917247,1.9176575258243755,1.9176688032973237,1.9176792041239341,1.9176887963948177,1.917697642920077,1.917705801637585,1.917713325989888,1.9177202652721048,1.91772666495303,1.9177325669714986,1.917738010009887,1.9177430297465017,1.9177476590884839,1.917751928386711,1.9177558656340834,1.9177594966484754,1.917762845241527,1.9177659333743629,1.9177687813012592,1.917771407702174,1.9177738298050064,1.9177760634983783,1.9177781234356714,1.9177800231309914,1.917781775047688,1.9177833906800066,1.9177848806284015,1.9177862546690012,1.9177875218176832,1.9177886903891668,1.917789768051523,1.9177907618764476,1.9177916783856255,1.9177925235934987,1.9177933030467038,1.917794021860451,1.9177946847520728,1.9177952960719635,1.9177958598321192,1.917796379732451,1.9177968591850616,1.9177973013366252,1.9177977090890361,1.9177980851184457,1.9177984318928205,1.9177987516881374,1.9177990466033166,1.9177993185739928,1.9177995693852192,1.9177998006831796,1.9178000139859894,1.9178002106936636,1.9178003920972966,1.9178005593875396,1.9178007136624171,1.9178008559345245,1.9178009871376824,1.917801108133061,1.91780121971483,1.9178013226153765,1.9178014175101061,1.9178015050218784,1.9178015857250958,1.917801660149471,1.917801728783506,1.9178017920776964,1.9178018504474883,1.9178019042760073,1.9178019539165705,1.917801999695008,1.9178020419117987,1.9178020808440441,1.9178021167472878,1.9178021498571907,1.9178021803910799,1.9178022085493729,1.9178022345168968,1.9178022584640955,1.9178022805481554,1.9178023009140313,1.9178023196954013]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"xaxis":"x","y":[1.957409004159486,1.9600051833830785,1.9627466966557887,1.9656377821729176,1.9686822770051378,1.9718835185697696,1.975244238690828,1.978766452507871,1.982451345139678,1.9862991595619035,1.9903090895666062,1.9944791818938397,1.9988062516338525,2.003285814783407,2.007912041410987,2.0126777322708,2.0175743209463075,2.0225919027511114,2.0277192907218424,2.0329440981542963,2.0382528463026577,2.0436310951136245,2.0490635942224507,2.0545344509059382,2.0600273112704675,2.065525550649138,2.071012468988189,2.0764714869173697,2.0818863382224597,2.0872412545729686,2.092521138606477,2.0977117218329666,2.1027997042931763,2.10777287347313,2.112620200624099,2.11733191333792,2.1218995439513675,2.1263159540667114,2.130575336145662,2.13467319373053,2.138606302345218,2.1423726535126146,2.1459713845852977,2.149402697222638,2.152667767366323,2.155768649480876,2.1587081776531147,2.1614898659042727,2.1641178097810805,2.166596590976998,2.168931186409745,2.171126882861287,2.1731891979835223,2.1751238081959037,2.176936483755948,2.17863303107326,2.180219242163158,2.1817008509964664,2.1830834963952355,2.184372691047083,2.185573796159804,2.1866920012491162,2.187732308541997,2.1886995214824574,2.18959823684243,2.1904328399647137,2.1912075026951823,2.1919261835954393,2.1925926300631353,2.1932103820237967,2.1937827768941616,2.1943129555519256,2.1948038690797937,2.1952582860825545,2.1956788004041945,2.1960678390978603,2.196427670524643,2.1967604124778735,2.1970680402478964,2.197352394558336,2.197615189318845,2.1978580191513792,2.1980823666574025,2.198289609402213,2.1984810266000148,2.1986578054895904,2.1988210473955654,2.198971773474499,2.199110930148467,2.199239394231539,2.1993579777567107,2.1994674325125017,2.199568454299666,2.1996616869193337,2.19974772590449,2.1998271220070365,2.1999003844528158,2.199967983976968,2.2000303556518257,2.2000879015193187,2.2001409930395095,2.2001899733665136,2.200235159462608,2.200276844060885,2.2003152974863096,2.200350769344582,2.20038349008767,2.2004136724644376,2.200441512864277,2.2004671925612076,2.2004908788654443,2.200512726189006,2.2005328770315047,2.20055146289188,2.200568605111437,2.2005844156532106,2.2005989978223144,2.2006124469316326,2.2006248509169017,2.200636290904934,2.2006468417384966,2.2006565724610687,2.2006655467645135,2.2006738234024406,2.2006814565718567,2.2006884962655064,2.2006949885971245,2.200700976101656,2.200706498012355,2.2007115905165295,2.2007162869915478,2.200720618222637,2.2007246126038593,2.200728296323546,2.2007316935354106,2.2007348265164066,2.200737715812383,2.200740380372452,2.200742837672955,2.20074510383182,2.2007471937140597,2.200749121029086,2.2007508984204778,2.200752537548791,2.2007540491679345,2.200755443195628,2.2007567287783854,2.2007579143514526,2.2007590076940944,2.200760015980588,2.2007609458272506,2.2007618033358227,2.200762594133471,2.2007633234096895,2.2007639959503327,2.200764616169002,2.2007651881359918,2.2007657156049887,2.200766202037695,2.2007666506265346,2.2007670643155968,2.200767445819951,2.20076779764346,2.200768122095207,2.200768421304645,2.2007686972355707,2.20076895169901,2.2007691863651004,2.2007694027740508,2.2007696023462486,2.200769786391581,2.20076995611803,2.2007701126395993,2.200770256983628,2.2007703900975293,2.200770512855009,2.2007706260618,2.2007707304609516,2.2007708267377026,2.2007709155239827,2.2007709974025564,2.20077107291085,2.200771142544477,2.200771206760494,2.2007712659803977,2.200771320592891,2.200771370956437,2.2007714174016106,2.2007714602332658,2.2007714997325407,2.2007715361586984,2.200771569750833,2.200771600729437,2.200771629297846,2.2007716556435777,2.2007716799395607,2.2007717023452686,2.200771723007769,2.2007717420626856,2.200771759635092],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"xaxis":"x","y":[2.0887047781660253,2.073803742743385,2.058925858136688,2.044154234760571,2.029576216595805,2.0152822635653607,2.0013646474433697,1.9879159794986137,1.9750275977543836,1.9627878518361686,1.9512803332383664,1.9405821076535212,1.9307620128321517,1.921879089310398,1.9139812113926753,1.907103981394414,1.9012699411133167,1.896488141073427,1.8927540910641123,1.8900500961365345,1.8883459621207828,1.887600035623176,1.8877605270003064,1.8887670523236737,1.8905523227167016,1.893043906982919,1.8961659959211872,1.899841103448725,1.9039916496080758,1.9085413825406008,1.9134166093818543,1.918547218711679,1.92386748882263,1.9293166860554296,1.934839465446671,1.9403860918281637,1.9459125033802973,1.951380241677149,1.9567562727535617,1.962012722995252,1.9671265520242884,1.9720791825221307,1.9768561043593569,1.9814464676967263,1.9858426770494775,1.9900399957834947,1.9940361682158843,1.9978310644682025,2.001426351486349,2.0048251921957605,2.008031973589023,2.011052063621566,2.0138915960909607,2.0165572821658793,2.0190562468814823,2.021395888700119,2.023583760123463,2.02562746731147,2.027534586694692,2.029312596642895,2.030968822360497,2.032510392307096,2.0339442045802882,2.0352769018413714,2.0365148535072106,2.0376641440698613,2.0387305665368785,2.039719620107934,2.0406365113165186,2.041486157968563,2.042273195302922,2.043001983881921,2.043676618794163,2.0443009398170324,2.0448785422435534,2.0454127881280657,2.045906817748413,2.046363561119514,2.046785749425142,2.047175926261917,2.0475364586126275,2.0478695474854525,2.0481772381720136,2.0484614300907764,2.048723886193623,2.0489662419226398,2.0491900137117454,2.0493966070338345,2.0495873239989626,2.049763370512894,2.0499258630082493,2.0500758347626737,2.0502142418200315,2.050341968531696,2.050459832735689,2.0505685905917206,2.050668941090288,2.0507615302537867,2.0508469550473047,2.050925767016298,2.050998475667802,2.0510655516112064,2.0511274294739645,2.051184510606875,2.0512371655928976,2.0512857365726904,2.051330539399378,2.051371865634319,2.051409984394959,2.0514451440651955,2.051477573877998,2.0515074853794504,2.0515350737827505,2.051560519220157,2.051583987900338,2.0516056331780534,2.051625596542649,2.0516440085313654,2.051660989573064,2.0516766507675515,2.051691094605342,2.0517044156323263,2.0517167010634862,2.051728031349533,2.0517384807000028,2.0517481175661367,2.051757005086591,2.051765201498815,2.051772760518714,2.0517797316910222,2.0517861607126275,2.051792089730917,2.05179755761907,2.0518026002300576,2.0518072506309903,2.0518115393193317,2.051815494422366,2.0518191418812157,2.0518225056206028,2.0518256077054455,2.0518284684853203,2.0518311067277146,2.0518335397409446,2.0518357834875367,2.051837852688807,2.051839760921324,2.0518415207058838,2.0518431435895725,2.051844640221461,2.051846020422414,2.05184729324948,2.051848467055278,2.05184954954277,2.0518505478157762,2.0518514684255633,2.0518523174138172,2.051853100352267,2.0518538223792375,2.0518544882333534,2.0518551022846263,2.051855668563126,2.051856190785419,2.0518566723789524,2.0518571165045447,2.0518575260771232,2.051857903784851,2.0518582521067668,2.0518585733290524,2.051858869560034,2.051859142744016,2.051859394674046,2.0518596270036755,2.0518598412578126,2.051860038842734,2.051860221055307,2.051860389091507,2.0518605440542634,2.051860686960699,2.051860818748804,2.051860940283594,2.0518610523627876,2.0518611557220394,2.0518612510397713,2.051861338941623,2.0518614200045597,2.051861494760659,2.0518615637006,2.0518616272768893,2.051861685906827,2.0518617399752475,2.0518617898370435,2.0518618358194973,2.0518618782244284,2.051861917330174,2.0518619533934177,2.05186198665087,2.051862017320827,2.051862045604601,2.051862071687841,2.0518620957417526],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"xaxis":"x","y":[1.7548081840753194,1.7563101448088623,1.758007514060676,1.7599116773051389,1.7620333421292909,1.7643823280076871,1.7669673488581814,1.7697957938496447,1.7728735128180486,1.7762046132695624,1.7797912762268862,1.783633598053024,1.7877294648453275,1.7920744650514662,1.796661844675794,1.8014825079090249,1.806525064336811,1.811775922181797,1.8172194254208116,1.8228380311872128,1.8286125226845538,1.8345222519374844,1.8405454060958628,1.8466592906709576,1.8528406229850496,1.8590658292168694,1.8653113386850795,1.8715538693965557,1.8777706993708239,1.8839399188209727,1.8900406589156695,1.8960532935604466,1.901959611412039,1.9077429561657167,1.9133883340145172,1.9188824880468283,1.9242139401955995,1.929373002146795,1.9343517573253695,1.9391440166768288,1.9437452514305302,1.9481525063549123,1.9523642971913575,1.956380495987789,1.9602022079582306,1.9638316432892482,1.967271987020863,1.970527269772467,1.9736022416874124,1.9765022515556327,1.9792331326612613,1.9818010965076749,1.9842126352078495,1.9864744330017021,1.9885932870792296,1.9905760376506623,1.9924295070121834,1.9941604472056833,1.9957754957600085,1.99728113892486,1.998683681762061,1.9999892244375614,2.0012036440565377,2.002332581398923,2.0033814319397973,2.0043553405747945,2.0052591995121736,2.006097648838092,2.006875079307862,2.0075956369621584,2.008263229212082,2.0088815320797972,2.0094539983216735,2.0099838661980125,2.0104741686874177,2.0109277429745935,2.011347240067863,2.011735134427129,2.0120937335045226,2.0124251871187786,2.0127314966006846,2.0130145236610044,2.0132759989442612,2.013517530241962,2.013740610347404,2.0139466245413504,2.014136857703782,2.0143125010517484,2.0144746585072624,2.014624352702265,2.0147625306301467,2.0148900689551192,2.0150077789921452,2.015116411371044,2.0152166603990613,2.015309168136492,2.0153945282001033,2.0154732893090075,2.015545958587456,2.0156130046386735,2.015674860403463,2.0157319258168216,2.0157845702752923,2.0158331349272163,2.015877934797478,2.0159192607577565,2.015957381352705,2.0159925444919065,2.0160249790168923,2.0160548961519487,2.0160824908469115,2.0161079430196267,2.0161314187052786,2.016153071119298,2.016173041640136,2.0161914607177502,2.0162084487132685,2.0162241166749015,2.016238567054831,2.016251894371472,2.016264185821174,2.0162755218431694,2.0162859766412677,2.01629561866556,2.0163045110571627,2.016312712058784,2.016320275393729,2.0163272506157233,2.016333683431788,2.0163396160002285,2.016345087205625,2.0163501329125992,2.0163547861999724,2.01635907757683,2.016363035181872,2.016366684967346,2.0163700508687366,2.01637315496132,2.0163760176045895,2.01637865757549,2.0163810921913248,2.016383337423133,2.0163854080002723,2.016387317506892,2.016389078470915,2.0163907024461216,2.0163922000878522,2.016393581222841,2.0163948549136217,2.0163960295179337,2.0163971127435123,2.016398111698625,2.016399032938679,2.0163998825092078,2.01640066598552,2.0164013885092613,2.016402054822141,2.0164026692970323,2.016403235966657,2.016403758550039,2.016404240476905,2.016404684910182,2.01640509476675,2.016405472736579,2.0164058213003804,2.0164061427458817,2.016406439182839,2.0164067125568805,2.016406964662275,2.0164071971537023,2.0164074115571173,2.0164076092797574,2.016407791619383,2.016407959772793,2.0164081148436734,2.016408257849851,2.0164083897299627,2.0164085113496224,2.0164086235071013,2.0164087269385633,2.0164088223228998,2.016408910286186,2.0164089914057866,2.0164090662141487,2.0164091352022946,2.0164091988230437,2.016409257493987,2.016409311600227,2.016409361496903,2.016409407511527,2.016409449946128,2.0164094890792374,2.016409525167717,2.0164095584484443,2.0164095891398666,2.0164096174434363,2.0164096435449337,2.0164096676156826,2.01640968981368,2.0164097102846292],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"xaxis":"x","y":[1.8108751815920947,1.8112554639753504,1.811806673458786,1.8125378085458763,1.8134571546633789,1.8145721543205584,1.8158892770166144,1.8174138916500788,1.8191501445402996,1.8211008464060998,1.8232673717266021,1.8256495738134708,1.8282457186487517,1.8310524401025297,1.8340647185710146,1.8372758844119685,1.840677646850469,1.8442601483324808,1.8480120436573064,1.8519206026502721,1.85597183465585,1.8601506327358033,1.8644409351323001,1.8688259012818331,1.8732880994223549,1.8778097026098317,1.8823726897485042,1.886959048050775,1.891550973197789,1.8961310633977277,1.900682503564495,1.905189235989903,1.9096361141733131,1.914009036906831,1.9182950602799462,1.9224824859397656,1.9265609246855582,1.9305213352464206,1.9343560388436953,1.9380587108334026,1.9416243513229763,1.94504923713522,1.948330857835343,1.9514678387399187,1.9544598538949465,1.9573075319571749,1.9600123577573083,1.96257657208783,1.9650030719650915,1.967295313287518,1.9694572174694713,1.9714930832907946,1.9734075048789432,1.9752052964440776,1.976891424124148,1.978470945070549,1.9799489537164892,1.981330535018918,1.982620724348437,1.9838244736167177,1.9849466231737998,1.9859918789740527,1.9869647944956097,1.9878697568997497,1.9887109769305966,1.9894924820784214,1.9902181125591365,1.9908915196959762,1.9915161663250376,1.9920953288828105,1.9926321008699646,1.9931293974205966,1.993589960739282,1.9940163661992414,1.994411028923439,1.994776210696415,1.9951140270780678,1.9954264546115008,1.9957153380355792,1.9959823974291053,1.9962292352276931,1.996457343066721,1.996668108414265,1.9968628209669397,1.9970426787891717,1.997208794182845,1.9973621992795825,1.997503851352328,1.9976346378464855,1.997755381133757,1.9978668429941357,1.9979697288332836,1.9980646916438964,1.9981523357206539,1.9982332201390491,1.9983078620088508,1.9983767395131773,1.998440294744257,1.9984989363468726,1.9985530419803323,1.9986029606095579,1.9986490146355584,1.9986915018752078,1.9987306973998369,1.99876685524174,1.998800209977257,1.9988309781946614,1.9988593598546327,1.998885539550676,1.998909687676411,1.9989319615062509,1.9989525061955795,1.9989714557061702,1.9989889336621978,1.999005054141868,1.9990199224093401,1.999033635591308,1.999046283302309,1.9990579482225457,1.9990687066317383,1.999078628902291,1.999087779954799,1.9990962196787327,1.999104003320906,1.9991111818441663,1.999117802258552,1.9991239079270067,1.9991295388475778,1.9991347319138946,1.9991395211555745,1.9991439379600955,1.999148011277547,1.9991517678095727,1.9991552321837176,1.9991584271142961,1.9991613735508211,1.9991640908149475,1.9991665967268146,1.9991689077216055,1.9991710389570794,1.9991730044127642,1.9991748169814694,1.9991764885536938,1.9991780300954938,1.9991794517203068,1.9991807627552018,1.999181971801991,1.9991830867935931,1.9991841150460234,1.9991850633063482,1.9991859377969088,1.9991867442561178,1.9991874879760818,1.9991881738373005,1.999188806340672,1.9991893896370108,1.999189927554269,1.9991904236226448,1.9991908810977381,1.9991913029819057,1.999191692043957,1.9991920508373178,1.9991923817167827,1.9991926868539645,1.9991929682515415,1.9991932277563984,1.9991934670717444,1.9991936877682892,1.999193891294549,1.9991940789863525,1.9991942520756065,1.9991944116983797,1.9991945589023588,1.9991946946537225,1.9991948198434841,1.9991949352933371,1.9991950417610496,1.9991951399454357,1.9991952304909424,1.9991953139918797,1.999195390996321,1.9991954620096988,1.9991955274981243,1.9991955878914445,1.9991956435860645,1.9991956949475491,1.9991957423130213,1.9991957859933769,1.9991958262753229,1.9991958634232607,1.9991958976810205,1.9991959292734633,1.999195958407955,1.999195985275728,1.9991960100531367,1.9991960329028153,1.9991960539747444,1.9991960734072367,1.9991960913278424,1.9991961078541898],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"xaxis":"x","y":[1.6138840720360559,1.6164570291454177,1.6191923495570184,1.6220952051260615,1.6251701752490944,1.6284211241321072,1.6318510740700378,1.6354620779073037,1.6392550944133781,1.6432298707283328,1.6473848362611558,1.6517170124229392,1.656221942331149,1.6608936441352453,1.6657245909146041,1.6707057192323154,1.6758264674512897,1.6810748438965868,1.6864375239428107,1.691899974172632,1.697446600933882,1.7030609199449338,1.708725743073232,1.7144233780393199,1.720135836569804,1.725845046424463,1.7315330627421464,1.7371822742765068,1.742775600318693,1.7482966744253448,1.75373001148301,1.7590611551394608,1.7642768032096656,1.7693649093054338,1.7743147596231998,1.7791170245291295,1.7837637852762098,1.788248536844715,1.7925661684874168,1.7967129240603248,1.800686344610831,1.8044851959675734,1.8081093842274691,1.8115598620698738,1.8148385287566262,1.8179481265154034,1.8208921357709464,1.8236746714041332,1.8263003819021977,1.8287743529326626,1.8311020165445355,1.8332890668858157,1.8353413830361036,1.8372649592938233,1.8390658430331495,1.8407500800577568,1.8423236672265544,1.8437925120087846,1.8451623985393266,1.8464389596862345,1.8476276546075598,1.8487337512594832,1.84976231331888,1.8507181909972354,1.85160601524609,1.8524301948842696,1.8531949162116617,1.853904144711339,1.8545616284798805,1.855170903063542,1.855735297414628,1.8562579407173272,1.856741769864945,1.857189537400648,1.8576038197613571,1.8579870256893034,1.8583414046979638,1.8586690554988243,1.8589719343127307,1.859251863004707,1.8595105369942122,1.8597495329040463,1.859970315920715,1.8601742468471643,1.8603625888356263,1.8605365137939556,1.8606971084635158,1.8608453801704614,1.860982262255302,1.8611086191880528,1.861225251378122,1.8613328996894918,1.8614322496727616,1.8615239355263056,1.8616085437992203,1.8616866168489354,1.861758656066388,1.8618251248815347,1.8618864515617437,1.8619430318152843,1.8619952312117474,1.8620433874307847,1.8620878123500866,1.8621287939830202,1.8621665982758464,1.8622014707739152,1.8622336381657405,1.862263309713339,1.8622906785767486,1.8623159230401356,1.862339207646477,1.8623606842473313,1.8623804929738088,1.8623987631344474,1.862415614045315,1.8624311557973015,1.862445489965226,1.8624587102630599,1.862470903149266,1.862482148385974,1.8624925195554438,1.8625020845370208,1.8625109059475597,1.8625190415480701,1.8625265446191377,1.862533464307497,1.8625398459459341,1.8625457313485625,1.8625511590833381,1.8625561647235653,1.862560781079984,1.8625650384149404,1.8625689646400077,1.8625725854983268,1.862575924732845,1.8625790042415364,1.8625818442206015,1.862584463296582,1.8625868786482362,1.862589106118969,1.86259116032055,1.8625930547287834,1.8625948017717597,1.862596412911262,1.8625978987178526,1.862599268940132,1.8626005325686232,1.8626016978946933,1.8626027725649008,1.8626037636311221,1.862604677596783,1.8626055204595,1.862606297750407,1.8626070145704279,1.8626076756237284,1.86260828524857,1.8626088474457632,1.8626093659049132,1.8626098440286192,1.8626102849547994,1.8626106915772733,1.8626110665647504,1.8626114123783366,1.862611731287684,1.8626120253858809,1.8626122966031866,1.8626125467196977,1.8626127773770278,1.8626129900890809,1.8626131862519846,1.8626133671532537,1.8626135339802368,1.8626136878279111,1.8626138297060673,1.8626139605459353,1.8626140812062977,1.8626141924791246,1.8626142950947717,1.8626143897267737,1.8626144769962647,1.8626145574760549,1.8626146316943895,1.8626147001384175,1.8626147632573857,1.862614821465591,1.8626148751450968,1.8626149246482426,1.862614970299955,1.862615012399881,1.8626150512243558,1.8626150870282139,1.8626151200464647,1.8626151504958326,1.8626151785761818,1.862615204471825,1.862615228352737,1.8626152503756672,1.8626152706851702,1.8626152894145536,1.8626153066867532],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle EN pour le paramètre l1=0.0 en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE ElasticNet pour tout le meilleur paramètre l1 ratio\n","for i in BestParametresEN['ElasticNet()'][BestParametresEN['paramètre'] ==\n","                                          'elasticnet__l1_ratio']:\n","    fig1 = go.Figure([\n","        go.Scatter(name='RMSE moyenne',\n","                   x=alphasEN,\n","                   y=GridEN.ScoresMean.where(\n","                       GridEN.elasticnet__l1_ratio == i).dropna(),\n","                   mode='lines',\n","                   marker=dict(color='red', size=2),\n","                   showlegend=True),\n","        go.Scatter(\n","            name='SDup RMSE',\n","            x=alphasEN,\n","            y=GridEN.ScoresMean.where(\n","                GridEN.elasticnet__l1_ratio == i).dropna() +\n","            GridEN.ScoresSD.where(GridEN.elasticnet__l1_ratio == i).dropna(),\n","            mode='lines',\n","            marker=dict(color=\"#444\"),\n","            line=dict(width=1),\n","            showlegend=False),\n","        go.Scatter(\n","            name='SDdown RMSE',\n","            x=alphasEN,\n","            y=GridEN.ScoresMean.where(\n","                GridEN.elasticnet__l1_ratio == i).dropna() -\n","            GridEN.ScoresSD.where(GridEN.elasticnet__l1_ratio == i).dropna(),\n","            mode='lines',\n","            marker=dict(color=\"#444\"),\n","            line=dict(width=1),\n","            fillcolor='rgba(68, 68, 68, .3)',\n","            fill='tonexty',\n","            showlegend=False)\n","    ])\n","\n","    fig2 = px.line(GridEN.where(GridEN.elasticnet__l1_ratio == i).dropna(),\n","                   x=alphasEN,\n","                   y=[\n","                       'ScoresSplit0', 'ScoresSplit1', 'ScoresSplit2',\n","                       'ScoresSplit3', 'ScoresSplit4'\n","                   ])\n","\n","    fig3 = go.Figure(data=fig1.data + fig2.data)\n","    fig3.update_xaxes(type='log', title='alpha')\n","    fig3.update_yaxes(title='RMSE')\n","    fig3.update_layout(\n","        title=\n","        \"RMSE du modèle EN pour le paramètre l1={:.2} en fonction de l'hyperparamètre alpha\"\n","        .format(i))\n","    fig3.show()\n","    if write_data is True:\n","        fig3.write_image('./Figures/graphRMSEEN{:.2}.pdf'.format(i))\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                          paramètre  KNeighborsRegressor(n_jobs=-1)\n","0  kneighborsregressor__n_neighbors                              34\n"]},{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning:\n","\n","\n","5 fits failed out of a total of 250.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n","    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n","  File \"/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/neighbors/_regression.py\", line 213, in fit\n","    return self._fit(X, y)\n","  File \"/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/neighbors/_base.py\", line 566, in _fit\n","    raise ValueError(\"Expected n_neighbors > 0. Got %d\" % self.n_neighbors)\n","ValueError: Expected n_neighbors > 0. Got 0\n","\n","\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning:\n","\n","One or more of the test scores are non-finite: [        nan -1.83486417 -1.73226896 -1.64635209 -1.61775247 -1.60390544\n"," -1.58881498 -1.57812224 -1.57576442 -1.56632828 -1.56530227 -1.56418737\n"," -1.5602916  -1.55765443 -1.55719316 -1.55387218 -1.5545984  -1.55311788\n"," -1.55380903 -1.55462925 -1.55545593 -1.55554503 -1.55673002 -1.55613559\n"," -1.55568468 -1.55606633 -1.55662624 -1.5574524  -1.55772324 -1.55706684\n"," -1.55792302 -1.55799639 -1.55753955 -1.55859802 -1.55934457 -1.55934785\n"," -1.55973635 -1.55916834 -1.55805682 -1.55797582 -1.5587523  -1.55888344\n"," -1.5590723  -1.55848644 -1.5584212  -1.55906391 -1.56012777 -1.56074139\n"," -1.56127781 -1.56271816]\n","\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_pred_logkNN=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[21.825503533274688,19.32654514287538,21.37615947341751,19.729154000296838,20.666714809406773,20.762597467383355,21.60123936449384,20.94130349649861,22.02268217417875,20.307112127384237,20.78055238338693,21.31328939251867,19.71534966174964,20.996866009942277,19.92754455541128,20.41210010652392,19.555283152197507,20.977396715246815,19.748312486247134,19.62982714515404,19.801068946976844,20.20055508642022,22.823320198137814,20.665988555899713,19.383708226025846,19.727186746064326,21.37196676880379,19.697596520020067,19.801609379416057,22.544042351955017,22.158958601284976,20.690664916765392,21.14550390429177,20.366387256334384,24.624725091141872,22.181149086884957,20.012986055061806,20.451512588660023,20.941697932550287,20.94608035084604,19.388305563698705,19.79800703324688,21.08790735492219,22.118624209999435,22.039419159496898,23.31636482868327,19.9926157119272,21.710483705649793,22.364460609693978,19.789175515743725,21.73654819793964,20.468266529967934,20.45063757183493,23.36315419991086,20.94536169238682,20.455410014871426,20.289576530494735,20.756750295804608,20.73131693867927,19.46421714698623,18.98420062505105,20.844987820916327,20.842407596792718,19.629827145154042,21.131493453394082,21.849028681853003,22.0901857606312,21.862091878145378,20.14614251633097,19.54649679725148,20.45551737082713,23.345004178654108,19.829790212650558,21.994095262054802,21.738350929722,21.84186132554031,20.20674899091208,22.16260848217371,22.12955844199721,24.25430342732519,20.435058970057135,20.483958421570605,20.725460563011225,20.012455656942247,19.18830203606279,21.951374234718962,25.122589703373972,20.27116397128398,20.3649873534185,19.516379824848237,19.94359483799899,20.612132920418336,18.98420062505105,20.309355136589573,19.81760681753027,20.672827345103162,19.26147090359674,22.817362826757254,20.42283990119977,20.603544153705727,19.747812804031966,23.65305793418022,22.495357429437064,20.37202323384337,22.767438131152865,22.092931681287933,19.40309578011041,20.501994149695427,19.90240307808412,22.316149646507462,22.452799104419114,20.867729110024005,20.71696468072924,19.032372911059102,19.457042051138878,22.45835966845075,21.75593881773551,19.423754740510912,24.73784983382155,20.563154973263046,19.00030944402647,21.190589765266637,20.1698899860832,19.914889763451967,23.32605103392384,19.308473809901884,20.867093455559814,20.31029903990393,20.172534648605403,20.930216540742265,23.400043942435826,19.343864317689537,23.856531498880784,20.421090033049172,19.869562077107663,21.845430804058516,19.686207734681968,19.86006195355334,19.487189347062248,21.548449294291487,22.066609096486957,23.48018147565227,21.286289976399637,21.515463984516686,20.974723284839765,19.888466738697336,20.329129685242524,20.94424252898533,20.799281881060597,23.844443594472068,22.57624577313698,20.75297797962578,21.025218644036404,19.894275774998917,21.19614999363628,19.92615212794991,20.16894251061566,23.45477060062665,22.56971000989962,19.612486204571734,24.9796365607219,21.47179736623937,20.370117948877077,19.89301774688651,20.435232479877108,19.42455844470637,22.351567858153594,19.75818807924976,20.41318726590447,19.606212942208188,20.95955505089218,19.365525095641793,19.91784324913621,21.567313082303002,19.815256470423904,21.359688458736834,22.38787949803047,20.109810044184947,20.071881318646902,19.56618728553239,20.006605298062595,19.697801753812914,20.52431381239227,21.875925674394296,20.38763652777926,20.668193160281934,20.31399541876355,25.121723480864,20.57590915994446,19.91171134056117,20.59365484154154,20.434363755673957,20.143173154996553,19.76095559701483,21.873688940839397,20.023877797221434,20.846002001296412,20.81552708591919,22.92909017743245,19.366082224370068,22.419464731360645,20.64518637915328,21.441094938705852,20.278920309083233,20.623390394387066,19.501641704140283,20.50702084956827,22.632025274742766,20.03480472570311,20.43384847044038,20.379520990658843,19.334256018863357,20.567532876702604,19.872744350003945,20.508676061772444,21.958061287785398,19.644264903496094,20.19223579510256,19.533661061582517,21.54197353191368,19.560596081596746,19.45387652921155,19.96438117395699,19.650797484087494,21.444716498678254,21.80093280025966,22.44996297954525,23.48243447918033,21.319072021620357,22.013968665060872,22.588832313788092,20.21238828140574,19.827393808150088,19.714464251311316,20.93892840413476,19.78284197524394,21.095081456494,23.66165370683763,23.708378284105827,22.487557323609007,20.91415516084195,22.13024526950072,20.074568364777672,20.889992878652407,20.230920316768252,20.149166418453447,20.2430880258413,22.028976452306928,21.748916563318822,19.740562034839165,20.19817849024831,23.225564392064747,19.82844773064793,20.660925599180622,19.950425607848693,21.393206882131814,20.374576687711613,20.454196536753276,22.521944567145542,20.46204305517281,19.400906668032015,21.307581242046723,19.57510980229059,20.262965664234308,20.16568513441831,22.508835766604903,22.979116800785647,20.739115918889652,24.436369457296465,19.659646214715053,19.637876927535324,20.496284891822604,19.678032914197743,20.2430880258413,20.27978920406959,19.971277874689097,22.409892686240283,20.6796510941552,23.514251006143486,21.600577781244972,20.469615584321385,19.3673110071309,21.885495466682766,20.90184843667634,19.560596081596753,19.89301774688651,19.60206473518316,19.39392470084218,24.08379828349088,20.684174332798584,24.025076726613133,20.014811643230164,20.433203877853618,22.165544545528043,20.80563259979306,20.836526284977978,19.379419393018953,20.64805575787121,20.799163439378372,20.235655084476274,23.62726863447211,19.745844381687185,21.397376887730093,20.41403471870836,21.7087378599494,20.218236176635788,19.79242078858436,20.701525114547675,19.810240957126595,20.8330779164156,21.024838945845598,21.071889219291943,23.35224873322406,20.94574319876227,20.51818642155889,19.46647315232546,23.504596649104116,20.28421593618215,19.720164627998447,20.41412465696876,20.45378930927386,19.21384872020381,22.453283918553986,20.4745807436095,22.314106939860462,20.44439512438569,20.516880535882354,20.03745603485876,20.921956449422137,20.699425162174688,22.90412826339436,20.108287860313457,19.829623741449236,20.227109391506104,20.363168423509507,19.986968085923554,19.647781619143498,19.91328737919256,19.455118223835218,19.847169451288174,21.564604862868094,19.932815635780248,19.825636432118323,21.086920880113613,22.83921055120716,20.692051785869523,23.238655331652815,22.65716533052636,22.445424610383345,23.92349453260206,21.67595385709207,22.718458438863554,21.56835766074883,19.74687410384946,23.575787361325684,19.75207794810231,20.670253651491908,22.873705922951505,19.484160089486032,19.805681461780047,19.88827049782451,20.387589444928153,22.435096656137418,23.5950729707771,19.763948076679743,19.379226249104754,22.62848704222603,20.10129309406651,21.03704074356565,22.789274259244625,21.09734190709297,21.823264556305453,23.8367148848892,19.2546079412584,19.901685029568846,19.629634525458044,21.728010180936653,23.026091687446915,20.559584563561685,22.228089175917855,22.760014343768322,21.918294602781433,20.22999079856127,20.898132864537235,19.077207955400883,20.46724629799677,20.267616951461555,20.812538946469296,20.64370663690388,20.16446452751464,22.702903976196637,19.607373362999198,20.49263779522474,20.354232838679284,20.057740897524724,21.160509359105916,21.441094938705856,23.117525288204614,19.87391599020888,20.34091658572052,19.84791584994665,19.720360600406472,19.47234653339706,20.657084666475615,21.88762910731591,20.667518573946417,21.747147554429354,21.79772637016592,20.55206323863237,19.483627347173424,21.19933484946157,22.126833563905542,19.76485054309166,19.563562944612322,20.401001675651358,19.262967724639996,19.76485054309166,21.33509516492437,20.833412180949715,22.97196852904565,20.330879559324398,21.434290892977174,20.08227123443016,21.118749464773096,21.01046908178925,21.499198388405837,19.82106651214298,23.800801253510457,20.48493584122551,20.470899400143352,20.676214761332517,20.912979280211626,19.770713775971647,22.088320081170725,20.516847500584085,19.586507506724843,19.474061442648527,20.61112962930062,20.312003196353622,20.006523480166067,22.468540757619706,20.413148198360933,19.872272741336317,20.441119211287216,19.63183639087742,21.98226737311665,20.18172994163291,23.66684275318142,22.329705158267018,20.520308813567816,20.23857997464239,19.813302830511326,22.267704295015022,19.223098012510235,20.057426320025392,20.283095030572728,20.659412172114628,19.828944387559122,21.492054802700874,19.885237564075258,21.228012790498493,19.428261815482713,25.27806693152357,20.68249927890336,19.686324704274966,22.55999116757275,23.34900146610441,19.91264014484591,19.487189347062245,21.96232605105944,23.689692397460686,19.690117749117107,19.02617558668339,20.074568364777672,20.402927825610938,19.294357494298385,20.407783953723566,19.73316543504524,19.698469771317676,20.45024734537333,21.08116487930122,23.67094041360367,24.574712912854384,23.484781522212273,21.82097402505969,22.218672643150022,19.692589750457405,22.36886072683654,20.447137337540653,22.528580127043217,20.68803243315636,20.61299702326763,21.45175337345292,19.782280900578257,19.79209228718356,19.769563816734582,19.74781280403197,22.193899104745853,24.997295967909615,22.275937186963912,22.731683530344707,20.00982139325204,20.27663269999623,23.471086432378574,23.383959791896874,20.18429713857216,23.5180752207941,23.019737586023638,20.09609486304411,25.298439444093656,19.796126127974954,20.775376859681984,25.147724416021987,21.52225297081378,20.411797709972205,19.51571682775187,20.529305733794466,19.385700300278707,21.663525144726346,20.264489841492612,19.963238147943482,19.705913730095414,19.871998169215477,19.89675336319365,20.354450340590756,19.81372791655112,20.37638504298357,19.630802003167645,21.942535788631353,18.660951496519004,22.843612464330448,19.485835518705034,20.550207589879818,21.877363309158937,21.564187513916295,20.70583375714464,19.051376230190744,19.950373580823477,23.620658043033497,20.162716200466456,19.59911968233027,21.97372620114395,21.47714548485465,20.50832817028013,19.96423570690311,21.0544056745623,21.750670022672978,21.58579053475883,20.028117440701838,19.19378191088317,22.425083118158515,21.471763029756303,20.511855390050503,21.807107956370125,22.26604743109784,20.173148704396578,19.336582383017266,21.889284640376395,20.95530438208973,20.16195570998411,21.208217704965588,19.964726742326803,19.771518703920556,20.896051640841346,20.09279666969369,21.921624062618303,19.72871128001904,20.28285042241091,20.791767666945468,20.591762401804864,20.99036509950339,23.681173728835322,23.463929463532594,21.049714537481503,19.492062860867147,20.404837415940975,19.809727147558355,20.095362084809274,19.274341578167192,19.96911779864388,19.415118136161425,19.598652455711076,21.793157864603614,20.140169989949012,19.71186663502086,20.2585799605242,19.58933629861405,20.39506996083454,19.99457399918068,19.469734614711996,20.598717743748526,20.681730005153774,23.713266547132104,19.96323814794348,20.895277858260982,19.789175515743725,19.568024996083658,22.258802161839366,21.122425771378293,20.721838200518434,20.76338214829194,25.729356700716004,20.035655462155894,21.44980913471527,22.40532076766172,20.97523166583944,20.584699491318702,21.022651386393683,19.64237334101332,21.30853380907703,20.660419304717216,20.550221380318476,19.829089162702413,20.43029046690302,21.518305735061865,18.53461732197901,20.37050284533897,19.307248871786964,20.265914072512647,21.752768003036046,20.75929963225114,22.611286692630138,19.130282712715253,22.017993404137187,20.895717969570928,21.952720458542107,24.8428606206281,21.612621866465375,19.74687410384946,20.211419081070307,19.28238695247732,19.49530650358626,20.078499795778015,19.624786780813306,20.917180240738727,22.35226642071881,20.318738826213703,20.226775942963865,20.2461423317847,20.22423123529636,21.984894496895723,20.70973809243777,19.976032373732657,19.27881410277027,23.079713691065233,20.8112030053811,22.845219261875908,19.67277896064215,21.333983664290088,20.307051412104215,19.954575873742606,20.341886055836312,21.533478878554828,21.218807051149327,20.476114082883754,19.466473152325463,20.276632699996227,24.612261026105536,22.85978479877928,24.190050148288478,19.63773139290275,20.030518877217425,19.97516627029314,19.55452338893807,21.950909005522306,19.75687974994092,20.39463537294208,23.600415754068493,19.468442862052285,22.308237000300124,19.769965067955564,20.5683768614036,19.62878406645411,19.99171873380379,20.369706346101744,19.400288692023974,20.13492312166676,19.47815913988265,21.6184298263012,22.61582024723075,23.56785017096256,20.272185146212447,21.000086911028948,19.55349262152733,20.881411233467567,22.132922340282654,20.468132924661376,19.72656393265062,19.63039175978751,20.41821014286177,20.184271019349097,19.339104376870814,20.439849254425898,21.529686381275663,19.032372911059102,25.186956956570565,21.111187024885403,19.739364649865863,22.14296075468328,19.449923235571795,20.23623185445978,19.487189347062245,20.482937518729383,20.60634952754369,21.814099503279916,20.908064868162302,21.58957010645704,22.40637780793874,21.068527359556764,20.68159691487718,19.673575145930798,20.441761066178273,21.496295866837357,19.42455095585464,19.823842509172533,20.037966208624624,23.737683578547006,24.023868505817415,19.861575676956452,23.995845962637475,24.79643479590816,23.48179700345873,23.569743651966217,20.15027910912084,20.6491473554376,20.130918356428825,21.348803593301632,19.61481718129712,21.813616244952954,19.87774403934149,20.393086154015197,21.24134669131845,20.489005989337404,24.262064612603545,19.711425384820465,22.270806686722306,20.814701547561864,19.595335508038396,20.13139480976436,20.974263719990674,19.716812690762694,19.649755823255006,23.5673907600298,21.593323817167573,20.1796652662442,22.753186918507836,21.002436211322554,21.642931904326723,20.321355181733416,20.309355136589573,19.919963767911753,20.45211723343655,19.53103314215187,19.15848302089389,19.0074229406659,20.467305790979882,20.18264615785467,21.26242461040316,21.37183771051562,20.623951016392454,21.349188437240223,20.38839071916172,20.357970239362594,20.66184478137983,23.713730032672373,20.07582753056927,22.353650287961372,21.661756427874803,21.763696324809892,20.79407879465573,19.424558444706374,19.68433140326564,22.365705801627694,25.46191753760092,21.131421590895574,19.76485054309166,19.702035354485787,19.964678032763626,23.86575214510965,19.462706910364297,21.851676634671474,23.913674249016665,24.4922765705823,21.26247956701937,21.235470661096013,19.91241945318063,20.71813107013783,19.527022313892516,19.211140318175268,19.91911075896287,19.974472755904124,20.765747542911583,21.266869812113935,20.832808158386566,19.82363318810292,20.625299602898604,19.779294099035205,19.31989566967363,20.20133351009138,21.48944253541467,20.589830261116628,19.364843339056133,20.78767640891565,19.077937687137876,22.90354524432906,19.636231682141254,19.673575145930798,22.900828138848702,19.610845182473483,19.852743251187245,25.368370024093444,19.866180351805426,19.732145903242973,20.218810951285672,19.976394049006352,20.69205178586952,21.664746390939996,23.769239995002234,20.943045425241145,19.649928762237103,20.444047367141206,21.126432443359924,23.138311005148335,20.118348620739777,20.456709940427988,20.96120771297146,20.717314612664374,21.874125677835817,19.71911583961168,20.20055508642022,19.657827270060118,19.87527409070141,21.48503535220986,20.183934682008527,20.81576680961163,25.368370024093444,21.207367694253957,23.56277963767396,20.782680152004396,23.35171053504048,21.329711100572155,23.837555811231667,22.017993404137187,23.322087756900675,21.81617732127767,19.82106651214298,19.657342585753995,20.58123457903205,20.71835957167536,19.589801986312285,19.73129861152606,20.74003992402759,22.00211559055366,20.501168649469157,19.74687410384946,20.038863446281795,20.078029241564955,22.708151749060473,22.363491306519126,21.353333680341684,18.98420062505105,20.73513976468682,19.600927342663162,21.39907303056101,19.527022313892516,19.874618668103498,20.65432832016339,19.91664952736133,20.505048052639324,21.09576804187796,19.60130495372587,22.30139947998126,21.0826621740499,20.64965252527103,19.81792287822654,20.694337384399283,19.662645325076316,22.952530239327412,19.383708226025846,19.270784745501334,20.013331348665105,19.94232172312857,21.63985712833841,19.718129580258356,23.673773485524933,22.495600748894628,20.527867217604886,19.440117391898696,19.561846091708595,19.46647315232546,21.444844305275247,19.343864317689537,19.546575327243065,22.82152170653835,22.55167213596464,21.09881105594442,21.428201890563898,19.715593564047865,20.550221380318476,23.52868229448483,20.594766240115735,20.420352862104355,20.659738507814307,19.865579946889152,21.599013805714534,22.329705158267018,19.699145392564496,20.24946010596536,20.56185096105169,20.372984159388956,22.77568159672965,21.393580741712995,21.06785827655162,20.716805121045592,19.820470676470027,19.478159139882646,24.443045750984282,21.330751304784645,20.516323554639182,19.97389807510543,21.493239550911138,19.597234924776743,21.222848074770738,20.190627723624846,20.391052582097082,20.100160393063724,20.476591732585476,19.594411934341462,21.953551327818296,19.578011056464106,19.88691504986557,19.719722489959878,23.415504460247693,20.738720658411157,20.10147489336166,20.728618737712047,23.177804026715165,22.286171398237244,20.748720814151472,22.04828697370366,22.049903817876224,20.958626731398216,20.601034405489823,20.34086065141563,20.993717440190295,22.903782390116604,21.293781748428454,21.53608189896851,22.168150731701257,22.06396840652548,22.350909587437968,20.43552244242081,23.681792129748374,20.68505782466323,19.82750025767231,20.96664096171697,23.699074737209607,23.255796117956447,21.607864179178016,19.486276021389585,22.100851696709125,21.327961427356435,20.011335577411533,19.532628958142286,22.256943898592517,21.61093308812751,20.296494481247986,20.712746002103206,24.843733182965455,19.738143385975313,19.926319527720032,23.343730727706134,20.78835032404268,19.75835966928034,20.342547715148243,20.873274011149675,23.656665515246623,20.409461482478463,20.441500174367626,20.222046527486917,19.439129746241036,20.292778117742348,21.026857492587133,20.30573590275155,21.539715976680075,19.629701208146617,20.07154786787032,22.689762475545177,22.711621748335034,19.902205380964844,21.140071123000805,24.265808086706606,19.665032901529745,19.680376701609173,20.730464512656205,19.70283966658526,23.95338348290329,20.564971229449768,22.977971497109326,22.096967843575296,20.603176292816833,21.171159243442936,23.74049826975604,22.62276665749781,19.61306305945954,19.74687410384946,19.74687410384946,21.898419451269966,20.831473243262725,22.039155470315155,20.134076907873833,19.937614055614763,20.84124100751699,21.630836221096754,19.561312141519448,21.785437000296106,23.847051410085772,21.52814612369996,19.74687410384946,22.195796742014142,19.71919261071553,20.393094470550245,20.01669918065487,20.730913367654104,23.552961467270535,20.23181119370321,19.586263402491287,19.45167489784407,19.25679747614539,19.12572508215222,21.277963210167286,20.176047414797193,19.61942724797676,21.90919542088943,21.821136860315193,20.746686784912384,20.059405835228585,21.73895315773884,20.676214761332513,21.040928389781016,20.803571511918022,21.183457318875117,20.387636527779264,19.287288681826542,22.273648657636283,20.97960926481325,21.65800612862924,21.624915695726912,20.45835537133392,20.023605380163687,21.742150891863997,19.223524805878853,19.95508353852182,20.057085443461048,20.728203037164434,22.24421751447102,19.636928074681126,20.898678846443808,19.49981554083181,19.456806748774326,20.526818961185747,20.613263729862403,20.227775493995935,21.232600251234636,19.773679582250715,22.222270167188462,20.102614631752814,20.744726768130615,19.45143501715893,19.615779875292052,20.239222180151984,19.48979351566437,22.106620134674206,22.487888263901574,20.659613995736766,19.458171498920173,20.791823157133745,23.008826034206148,19.71186663502086,20.26591407251265,19.95999571463005,21.238347017964912,19.629634525458048,19.988591666701023,21.835302969693576,20.68803243315636,19.53103314215187,19.657714078134646,21.81158802728597,20.178081845482016,20.92316509560519,21.636637964943212,21.330154530858756,19.77239349221674,20.691891655787906,20.452613071377602,21.819515033428722,21.09144574879138,19.544761857868703,21.40842256204602,19.583930953509,19.894275774998917,21.898642294333374,19.924946311415084,21.822588696076927,20.368252065527425,20.62593553615035,19.52033276500285,22.188597671671932,21.104986579600116,19.68898487198758,22.70874961347502,19.873387034819594,21.108339210892066,20.034804725703108,20.688720838686915,19.35013257648432,19.921669193284984,21.38249450313706,21.634349456564685,20.225707022635795,21.725696279490247,22.256309274495123,19.748312486247134,22.43109797365151,19.72748425402021,20.695411936849933,19.694408444738457,19.9421338616318,23.607984693801665,23.53950617102484,19.80343167640524,20.48428385391352,20.021323720155472,19.825636432118323,19.68310095821442,21.521911659243468,19.648720162667185,19.86674425279987,19.62625898164031,19.735557452883004,20.16052850522496,20.01922642496926,20.615816386676162,24.105888356518484,20.40659122568972,20.072848351710054,19.65134411337735,19.343864317689537,22.59736247967775,19.379690067771307,22.013968665060872,21.511208195717714,20.64965252527103,22.000530938504667,24.038880967500134,20.373122452411124,22.226039358018138,20.426443658446058,19.697243673274105,20.985929418449707,19.59524564339374,20.13356941651604,20.701525114547675,23.504020665531385,20.350505664129713,19.50646966885691,19.530156057950485,21.5927250415823,19.23941870414794,20.626595403243254,19.264980728724254,21.559706684511486,19.432430624371577,22.32357637374648,20.744685657135875,20.682757734913537,19.70907311577635,21.8122890813355,20.709738092437767,20.32516293090548,19.196731966529715,20.18861895047439,19.261470903596738,19.559986751378148,21.65169926265904,20.29940308817388,20.071950970413756,19.466473152325463,19.123831031404226,19.87101358320101,20.56394521519571,23.40651233235596,20.99840102060523,19.748312486247134,19.254607941258403,22.632319060099803,20.742250747166995,19.79408596602273,20.492931388576142,19.621287908124554,19.731042111402253,20.730330979947915,20.977133857309227,19.44736929196211,22.315527938578555,21.787118633010525,21.13205337270674,19.626936087894723,19.717031262017926,21.078539585502522,22.049617154740247,19.48373460020881,19.92615212794991,21.353590959011314,20.49027761417971,19.98929700900034,21.62677683689993,21.64999453677418,20.6897836639149,21.284381636496533,19.797556208921254,21.17565199184767,20.59319959827859,19.776128802916283,19.832043438472585,23.064354420525497,21.282480713556357,19.475565808009193,20.25583126337571,20.56726925247393,20.00231534551757,21.496167848638787,20.44139726163988,20.433359810993306,20.971774704667638,21.913958903977488,20.0616954685213,21.23423479221057,21.140071123000805,20.48071459500099,20.586976538920226,19.472405593643927,19.555312869652727,21.420208451534574,20.709738092437767,19.88826057043386,21.086518025226834,19.491103622663005,20.357970239362594,20.436022833643566,20.24139487495439,20.86407456141283,22.575628809021197,20.80614496777358,19.229615918781505,19.329000804091024,20.52692204492928,23.134664764604484,23.222788760482487,20.803501949051398,21.41476873813837,22.416466169261817,21.203154892445376,25.21102418226466,20.32371999766054,19.744034811567005,23.27663025083581,22.4527410900171,19.570785664351362,20.30625620645808,20.008417454631253,19.103812079189158,19.831912330993116,24.681411935999012,20.09129334044048,21.83343453082173,20.881013896171442,21.192995433072156,19.824057166746833,19.644999449343167,20.438956000966297,21.73280638376719,20.934603184128477,21.113590087788634,20.606301514343834,19.67028459740463,21.24878130676755,21.47233474453144,22.361631002632226,20.605843764681392,22.292409689588567,19.527022313892516,20.79263019501872,19.858463682714245,20.681676190670743,20.043490573706592,20.73368644355898,22.001152032993044,19.4207515478561,23.628288047334856,21.25762877748899,21.685419710280964,19.579008703285776,21.651253557148397,23.059387565273145,20.39555052444093,20.977031495794265,21.055299806523745],"xaxis":"x","y":[22.839725872000276,20.91128637269999,22.88916655727953,19.430343691015025,21.11735552640613,21.280697719341468,22.124201158883572,18.136670762341843,22.576080863006137,20.10297814136792,20.137307637847517,21.968529606994224,20.90466916348112,21.65055481705341,19.466148173643653,19.644920713511322,20.60018787473792,19.883803436319816,19.908561207609715,19.88468822756935,20.68752092168076,19.37233862557095,24.197363319180216,20.375121118721335,18.385719188798834,18.92468784406244,21.354902012656122,22.075827573908075,19.724580465382388,20.25426974158722,20.75534115928083,23.747615973251953,20.91788210799568,20.92150383722997,24.937914667295647,22.368646863289158,19.83010122567355,18.16161414446258,21.756941677613227,21.220261555831726,19.921137323148557,19.756730386301125,19.076215392899233,21.77156523118196,21.51345533419679,28.08571411944019,20.221414835639845,22.43563961615873,0,19.8813455186455,20.446723265477033,20.65296343235214,19.469859640957857,24.8692968913913,20.808662576707462,19.741272758302454,21.063890409149668,20.444375957405665,20.737383520261705,20.772392205332533,19.30201857006515,20.50461437981694,20.844484149195353,19.768388190282472,22.01816825105002,20.77707066358558,22.615494713832,21.249591056381075,19.3101001682171,18.557194427644223,20.820077266859332,26.161807884063222,19.616841359021425,24.601371538637977,20.929756127701882,21.371449507411626,19.801676786988352,23.186489993216338,22.288287088869534,24.593075628702742,20.324838680602316,19.538495641550785,20.313003679890702,17.83704270507587,22.508853453023672,21.500237946648834,25.808363100543477,20.146924115160264,21.08058139373211,17.12438387523814,20.968249130032163,20.028493079904162,18.408984789492116,19.05702758537138,20.086616681313576,21.346006819966092,19.260460167342536,22.182304931495295,0,21.19880336254275,18.935012496616167,22.63391493571178,23.82192770536634,21.24843214899758,21.979683188147945,19.96496549198579,18.7932762528682,20.682137237753555,19.632965031850407,23.473975339042966,22.69657897049395,20.12759231185353,20.362922084924325,18.91123104739614,19.548169047796804,22.968053968168725,21.917396756553043,19.216324315987517,24.057859634036678,21.674989393981168,19.72022056098374,20.690574372087426,21.01024397957751,20.841336970061143,22.67830612646918,19.365836228253468,21.066918904158555,20.194293964879012,19.514174668391416,20.55950872954696,24.227674033652324,16.89837701160671,23.000908813982363,19.725383525541226,20.372113673311205,21.25165374313537,19.232324890468607,19.688668857104407,19.006769325349232,21.51252238785962,21.56101612004373,24.60236976749392,22.783863807855585,21.893038360020444,21.254697640624393,19.266317599579637,20.581937941283943,20.45335403901458,21.260660009666424,23.096986451321428,24.198260151329528,20.67873788317677,19.161993412283312,20.805625607806174,20.111067676706508,19.630957845101975,19.421904563712328,24.27931634919893,22.429090755701058,21.147722912842873,25.31201588491192,0,19.877134324376346,20.195578868662224,20.07860380095908,20.74807693982553,23.74195830377693,17.92351319630841,19.98475794826,19.330905939549268,21.75045501313216,19.64650597499701,21.21476614620658,22.11740118712243,20.373745608135653,21.286908529421844,22.031685219396664,20.241113097705927,19.58596744655146,19.47904998638186,21.40733571681439,19.113174941454258,19.88328018890598,21.622357074698563,20.24572329389297,22.858722725717435,21.004219094475406,26.772642502942784,20.915100345640315,19.607783613490604,22.164399063013658,20.790635654144218,20.499654561276497,19.533857847503334,22.238063995762765,19.371674563247215,21.64924126399572,21.665790435544952,22.72561505846151,20.402841366046307,21.38581238341487,19.76025356060854,20.786842719711426,19.14889856436821,19.895681776607635,22.093666112296297,20.459085985918723,22.982495350783758,19.331349158501606,20.55332894959407,20.31682150206961,20.014253448408297,20.016785168783276,17.008111246780107,21.536104422950213,20.603485087703003,19.59434829773337,20.049508251055094,19.642302060721956,21.52720398550173,18.99020093378567,20.22398061738597,21.01213158467734,20.053269667846333,22.108159109401438,21.79011956511657,22.084914601393155,23.135231909261865,21.190067594656295,22.67339901515593,23.695624962968736,20.984090887872558,19.114539527933296,19.182165482881256,21.52107548780855,21.22249010417508,21.043820980181792,22.894093531705956,23.85820974949918,23.265700738521485,21.89761800850832,24.611415306054898,19.96312680068985,20.173501472804098,20.216216542451658,20.253427701740367,20.770035525423065,22.54105973810428,21.560752172206048,20.50991100623185,19.66651702047476,22.893554470389834,19.678935343554116,18.37060227548128,20.996625195887564,20.88004158687887,20.017617555780475,18.785129945834015,22.472827160217914,21.123452379734417,21.834751391009725,22.36922218137253,20.638651229695757,19.920184186837908,20.233535275947663,21.775846548503456,23.23197611805285,20.98111639712373,24.69390393605354,18.972968571042593,19.85555820528158,19.343431218526263,18.83915024282446,23.64752507012282,17.79557360413656,21.996936490386865,23.456377353127298,21.603092935704957,24.298252303878165,21.481533394476365,20.408641755181446,16.772761777957808,22.20744305955703,21.11943827672747,20.998113148879906,20.11872947152901,21.236410750491928,19.699904832330084,24.209354374596852,20.24751150890443,25.07655356851684,19.568646661314627,19.776542871248843,21.727271525181536,21.062803097791257,24.076140757673382,19.409983373621806,21.192511957229144,19.312950102671177,20.179206907329647,22.432801454859,18.979888467927786,22.15617321227827,20.260215495083163,20.910464871041206,19.297632789142515,20.29925833471969,21.341220185389716,20.961489967791394,22.164351182655523,24.799241151178187,20.420977915907716,25.348057895064095,20.921713841616427,20.242927524338644,18.95868492479523,22.708968189549438,18.49440443743341,20.482328404305942,22.37000861087754,20.121388068881203,20.44167803431194,21.856025378041313,18.927945745238855,21.9419703223203,19.736694248820182,20.23558290561487,19.836668342627448,21.25562508835354,21.394257392515996,22.71500741252458,19.547704151356815,20.400995752923983,20.265469687416747,20.212441762527977,22.377553705906216,19.66793597026211,19.3315880738609,17.677595133772794,20.276225239062065,21.268637940423815,21.725285950225775,19.195369804190715,21.15736975857522,23.488081735909503,20.232817889146876,23.728782472563765,22.84612630027242,22.077517747249683,24.569997706071334,21.7711290432985,23.43665576011153,22.83375379636568,19.2148787700197,23.42842113265138,19.4136293492673,20.540968959330602,24.77613012919926,20.659638185399235,19.087533251252736,19.83361590003151,19.96019429668775,22.334373955189864,23.109422435163253,19.778390202711652,19.167836614935887,23.933068642871486,18.56499419954315,22.530048594882032,21.71290394044351,20.45930590947763,23.069120127461606,22.999636382856856,18.925763040720064,19.49519726866692,21.042555130756018,21.216321947424902,22.454539777644875,20.13393362882646,20.867172390851817,23.066997383640125,21.52825545142037,19.72029436584214,21.117385332870825,20.179272523647345,21.590649178257912,19.692513024244924,22.86005581934068,18.799932943985397,19.96989517504386,22.510505954413144,19.34599856610255,21.28740957905986,20.158837704444057,20.907660872216642,21.905619198419114,22.38172749868176,24.308788888311135,19.262765428559003,22.547404100685924,19.87902826477287,19.826795369611226,20.791833978792408,20.76154230516995,20.245177516562332,19.101442274030862,23.095721196617728,22.853333163501407,20.55222183482644,18.623284957897813,19.707161942553423,23.312427516610697,20.72798026434645,19.38519821479218,19.584766197834426,19.657602815047426,19.766824889008415,20.72718342775812,21.19739249261565,21.766933642509713,19.997824758905686,18.07487316385907,17.89367601727741,21.076958761045283,22.70946891023392,18.280325895793162,19.286378115164315,23.27164625070411,21.087259361308853,19.696262913257698,22.998819199975294,22.068276096052415,20.676958661968467,21.84293342285994,20.22886359790674,19.302304652102425,19.45315606988007,20.189004387901257,19.834023760961447,20.136832510309578,21.96825774467718,21.693760030133728,19.68327564708964,20.69046887217181,19.902947681155606,22.060735176692912,21.15375333067992,25.097138279698054,21.440477364164977,21.3505869180748,19.260072175979914,21.05711024152434,22.371116977331784,18.18526261240324,21.763695119906945,19.900159246109634,21.022878950161832,20.48383078099113,21.821470865389173,24.987649624596244,22.265101932257092,18.954616641321604,25.734123629818527,20.035932572983462,21.002454511024606,22.570203152455143,23.437077312046757,19.317032739803228,17.641741979338622,22.448170385491473,23.33275256138136,19.064590160696714,19.240852168960235,20.1483293247843,19.78119349991646,20.035822020141666,20.368090770815378,19.622542520355413,19.136726140380034,20.665117281017753,22.04592459909886,22.766122645362053,24.701481133697825,23.00797271520254,22.942154096547412,21.766675289013797,19.216601410973656,22.721359157406464,18.9576603836655,21.88381079939571,18.472822451162664,21.07184726592027,22.03833211483471,20.099462108962847,21.41678722119122,20.92948670168764,20.890428315584586,21.725547245838506,28.740163782092626,21.205026123435218,22.323663662536706,19.34101707633175,18.666140767995948,24.012052706607076,23.69806207100269,21.754438391935153,23.682062419502355,23.126467381893125,21.07179899498228,25.515113797875166,19.69217074150189,22.12431429427697,25.95498550595432,21.870103917395213,20.208896815397623,20.476170795773445,21.523407372759994,19.429205075454394,20.94805891660743,20.579612186732355,18.425777297525496,21.048120970639673,20.14580390451336,19.1678248265788,19.26180569731426,21.731372277489818,20.677454054144345,19.241454867705745,23.733038602815835,20.006473671385464,20.924659576737604,19.109062896280463,21.73683233585413,21.589387814254906,21.96355600303241,20.930649460210123,19.44163523375263,19.835220336552222,22.967539915594646,19.57718354371762,19.694551733623157,22.05857907784579,20.152641197123305,21.206937139516242,19.177040653531833,21.059144404213697,21.77992540342766,22.0971519060863,19.9033116727081,19.870869348950354,20.847191191309296,21.255773747330974,22.179827408990512,20.655014419256375,22.594985264708335,19.867378293238115,18.6010489412016,21.546971094057223,20.8121798521563,20.30528553843669,21.30235611574127,19.472993206549905,20.642314511859237,20.823420888524133,19.456579392491523,21.112413416140527,20.769205079804532,20.344212591822245,19.88957080391307,19.783879090807634,22.2606456541005,24.0347829295736,25.71484503270827,20.79497581994581,18.966254774482525,19.558951865783115,20.00118447409592,19.759231530600534,18.668189678924886,19.362035840585754,19.21669376414341,19.534513178135196,21.58943582111394,20.029295821796193,20.93634103691578,21.419330156983328,19.511493048750843,20.011894681671833,20.008240359915625,18.95284902418798,21.345469949872594,20.977820033541025,23.20923603756077,19.692880228017533,22.637005220660473,19.457649403400342,19.12974837607436,23.60775201354553,0,21.026538976608304,23.88396569488923,27.919301359795483,21.157605371717533,22.006125382801496,21.526914680071894,21.60107003042797,21.310840181677595,20.659294437479453,19.303395037499914,20.578169960300666,22.895450751816803,20.179768194947595,20.3718571876006,21.13235475498899,20.116418845808077,20.597315830475623,19.957464846805212,23.115072392844226,20.726116689654802,20.700075492290427,20.833447685328323,23.397238664878486,18.725486086935668,22.38037915102657,20.812817350665142,23.171040699108495,25.19463787743612,17.21138734793915,19.0336054859068,21.825732486030095,21.042046241460838,20.6322831501737,19.380751294097546,19.582601081862446,21.284194059773114,22.179583552994327,23.19729034551032,19.967482396438246,19.738441963085798,19.239293618022067,21.59285163447237,20.535950228874793,21.01158079776531,20.08203076321153,22.84732578883419,21.756353934255824,23.316254320731808,18.81898933836999,21.940012430656058,20.748003621067692,19.730383893455734,20.182461946627072,21.076706013187607,20.7269525620563,19.59081933226322,20.904152603600195,19.659163210227153,24.761947476774708,22.534274856548336,23.233448675874918,19.884197354704103,21.007268735063114,19.408841782080135,20.136824689258354,21.854300382519792,19.881144027962787,20.995690391681727,23.573228460474112,19.074352191519743,21.469722294142937,18.833019773765052,20.27464659506303,19.718574682047347,18.258239797953646,20.755317114550273,20.561150873374917,20.948677886535172,19.720177445507503,22.150900676319612,23.39909808655993,22.31935969126546,20.155632003058283,21.13370665837429,19.473136561032486,20.46100849412487,22.136011068345166,21.159614691783496,19.431106818709004,19.13213914887522,19.706696670705693,19.346767596320518,20.918060329643115,20.18711885177595,23.064648540231374,18.98912255599455,24.074524917146316,20.61646897501603,20.227612418267608,23.77281502683423,18.64679105166152,19.821061094559884,21.41515272305351,20.235862606471247,21.36550800836747,21.97455042987335,22.17732889464112,25.151779365365787,24.150527389957542,21.040691724844343,20.760535400558187,19.34725801943171,20.04287236504247,21.531748865354032,18.726909959036774,19.7936588697483,20.03811220566055,23.56945490190484,23.91740995998136,19.437420256199317,23.65829637497728,27.09570141066156,23.540007985307764,23.389995853863947,20.300248650151303,19.67235973078438,20.003307902740296,20.529649424214696,20.802720957968873,21.281868210231526,20.815053320149143,21.927740352835695,20.82479873735278,20.51237686567707,24.94868149530031,20.081935873735674,22.717589515695337,20.82980401444312,19.3661810523187,19.957403913237158,21.18136460499971,19.8915004159983,20.17210241586846,22.4307529287208,22.889842311610867,20.801380865239704,22.613937582771225,20.216218911187326,23.993435990731076,19.557332226454086,17.31416721288844,20.21067743703825,21.405123280549176,20.1770277231615,18.904187708332326,19.046718202293015,20.073998999202523,19.6302991245126,21.48379659194355,20.81804614111716,19.563852956879067,21.051151840896342,20.770721956713995,19.67623430456214,19.581897497422524,21.384446242838667,20.692911719555987,21.545170940853957,21.994462034082755,21.47969097288377,20.278388484789648,19.382106816183008,19.441821622178967,21.939001270486997,25.832615032960224,21.65340972782181,19.535812626400283,18.947642644803835,19.712127286089668,23.90063882300631,18.304477915148002,22.443056343683086,23.16226531069458,25.14188655199703,20.70748794380435,20.732604891283092,19.934400683437797,20.62113432421718,19.28434545274011,19.017540081468233,21.15041915835723,20.26686372980413,20.03253125195289,20.148876382348814,22.62198208884591,20.021014007950576,17.20778649543665,21.273482891420798,19.30063185166596,20.195062159996276,21.445335194484834,21.0819625214105,19.392355160192587,20.378347780631422,18.799302474087973,23.18300636577445,18.93991113136844,19.223167412657357,23.650902186871754,19.945176609895544,19.495627675541318,25.79058328300537,20.476746945456053,19.21271648764287,19.678332456797207,19.71871179715288,20.105565995835796,21.137414277628565,23.008367409641956,21.03393309328942,19.30271923785845,20.626830273876646,21.092597149125172,21.952512694511015,19.882805718288704,20.77592509493985,21.046183463307532,22.0435593555408,21.04785962557455,20.25569596256737,19.352118206703615,18.845717636328267,19.34199651526983,20.91962424027046,19.595170049801663,19.600097337632842,25.661642339745672,21.791142017552335,23.50747794569296,20.391385205003306,23.803560322980168,23.78648471341281,23.49397732898427,23.081833665589276,23.144648130459657,20.920832580620292,19.193623112656937,18.901120542977083,21.79790479470573,19.407912925215978,18.98289333455679,19.415382799115825,21.253000869012993,21.604198363469845,20.557571210143905,19.274253014992237,20.09246940100189,19.391954018620176,22.850833320712763,22.012401869246364,20.826658939867247,19.118363214083377,21.289967644645948,18.733922283625127,20.9115880183164,16.829816883719186,20.861088989018167,20.73330584327736,18.406398882361927,21.03390016791011,22.274116203708253,19.445206608830574,20.834279688605037,23.70344643795525,23.354012710201598,19.88516012413886,22.730738164708086,19.75792383693999,24.394611647917593,19.734888199833236,24.922685338201692,19.530254509942655,20.280991315768514,21.951122115143434,19.19842345841963,22.926085660752005,22.977548256996624,21.21128990053558,19.167558108393887,20.647437784958075,19.35146728073557,21.097845676874776,19.32409702234251,18.277738745265133,24.145550575002787,19.346013720701006,21.812205122435703,22.86783241905905,19.249124305809787,21.53558768926747,24.102450641699225,20.500920634779252,19.622850798933502,22.45828794986169,19.57219805726336,23.264639171673256,22.333293091972617,18.995877797816178,20.694084927736416,19.630595419370202,20.436904665975636,25.664004852328446,20.985299957568404,21.714666548374087,21.619000188699776,20.319316549429495,16.9587226208279,24.58750691743097,21.431401448181294,20.454850489915017,19.41323537187323,20.769097708402093,19.12410363757553,21.037456801472633,20.865493152029874,20.200550831447014,19.074330956681983,20.89335338152254,18.80103888605608,22.493051859930933,20.264370436179377,21.216120605388507,19.103940869449712,23.841976780210096,20.967615900689093,20.26093058536308,21.15453445100085,24.16646354231811,22.25017305320405,20.99185811169326,22.413732248617176,21.395751936133603,21.027619198526104,19.682163069515127,20.59828494694422,20.60161654204836,22.930920924409694,21.340491521133263,21.13532539299306,23.86936283890287,21.30122097904847,21.87678820576361,19.984953818018486,23.219303551688306,21.660324565067476,20.87994439864477,20.922214169866013,24.081396558763124,22.721264543402345,21.873381120752498,19.089555706982512,20.89641611390129,20.568873923302558,19.242152996635603,19.205602363695707,24.69282091354044,22.779994713980038,19.521220854266264,21.142478555307388,25.236220259743373,20.048804801912784,19.552903579436016,22.115896159241377,21.403353538154413,19.840625811644227,18.35486059310022,20.417051943879315,23.212857021000243,21.53372289651904,20.79356441567801,23.25433145453618,20.592440384261195,20.758465193484422,20.24512208583746,20.925901548056537,24.750691092265395,19.41019874513054,19.696673307271265,22.56844661795243,22.50456479926641,19.4801354006198,20.95523200641152,24.568652749155945,18.75491624208908,19.461087704959443,20.880059154822476,19.68504303470752,23.816825076283962,19.839377869097518,23.420195657862312,22.999884079036303,20.649943786599653,21.684940338873297,24.09117022808059,21.12734576220247,19.49242240707679,18.90953834843785,20.94197381362215,23.259273464150183,23.03775857233843,22.58649267850949,20.120777025175872,20.906486953193497,20.378669388865216,22.737839310980224,19.27074701059353,23.65992231174119,23.339497059809283,22.05115682063906,18.775074918844375,22.10097025846529,19.31545475085148,19.854767962698624,19.840511525069328,21.39606694375647,22.97307940319244,19.86193501563865,19.664038907092028,19.58463718752616,19.810866559819527,19.465937010710118,22.434772773304594,20.445539922646542,18.99187307244663,21.90849954742701,22.314131385426723,20.432733544527707,23.07558442006914,22.830034474576443,21.269107840654502,20.48988421076927,21.083491338486773,20.934278289661282,20.287989620862316,20.04722528013383,21.199347342918216,23.521067577919112,23.2611930742384,21.92570835834572,20.556276718935056,21.41646356230674,22.53034703709958,18.95340785965337,19.986508970321974,19.495183602853512,18.457821713581545,22.886338306822665,19.129172314836154,20.136171638349733,19.376016989288264,20.879430687458026,20.620771994361597,20.30679376611553,20.368798039683195,22.35947129540103,20.069728697762105,22.54894428694912,20.37273542865621,20.71779772905426,19.199159497364995,19.174850117652987,20.13094661361353,20.62659792573699,25.292356917614896,21.758700626292807,21.796149648767223,18.807339197843834,20.506265808043914,22.110290220817404,19.52840260028443,20.325926854481473,19.625476983112655,21.50252421557022,19.304125125188563,19.107865328534132,23.169806519640005,18.840768128990423,19.36444675592656,18.627786784209636,21.17955986366495,20.377078375453138,21.746512886013605,20.910669810022966,23.66715843905666,19.356125235979412,19.33650002206052,17.813568277801206,22.465861054219694,20.57561772108256,19.015107379674305,21.499214768467855,19.66178092422977,17.299495077910098,21.27615053752397,21.040198736231254,21.92098191704396,21.7224558930939,21.092338092530998,20.048671453648097,21.894206057556605,20.415195026786527,20.471143313052202,21.849984955018474,19.59156765743604,20.658934051201378,20.105603080001295,22.787613064091445,18.897059576214055,19.64405870233027,22.74129354031224,21.948058114314257,20.038176858751854,22.177258018525677,22.318758483797644,17.211802761238985,23.061132678317314,19.552166474373674,20.22058498039463,19.1389392082589,19.751662039949778,22.589266875609983,26.340632927208585,19.534708840769273,20.435276534084654,19.903908501005596,19.287439594230733,19.687016476370815,22.7859490833811,20.26081963393427,20.008806098312462,20.432875222497227,21.770367489208184,19.54263516359492,20.167399003287947,20.02036350615788,25.816840819488693,20.66031803587651,19.756693437676223,19.47969541290554,19.46354033785,24.169496968680054,18.739319147762597,21.559823540829672,20.841106100432793,21.572330613609903,23.06348379203432,23.90661007609995,20.544449448312626,18.828527980887348,21.643407148870626,20.295760247970698,21.926818619899166,18.887256320586236,20.26723932452398,20.649629842496918,22.847263558158666,20.639447033358472,21.651692029555726,19.3900555692985,21.40442957359937,20.067715321953298,20.687515795874024,20.646624626390846,19.38789150561276,21.443595032483266,21.960080436904104,20.986097829919892,20.116046923130224,20.38834949087981,21.51694698715695,23.463402138610206,22.107609158967936,21.13017429963753,21.059552411062068,19.918938919026616,19.132327452939787,21.94580529932221,19.159457479949786,19.567551127469855,19.27757204338643,19.477956222501692,20.02620191687006,20.455200776646638,23.044574887856943,24.184179585129318,20.388726808803085,20.116390923202314,21.32891538129463,20.93827950221911,19.878697565839243,20.238709984208988,18.95241706841831,19.63538512176151,20.604884938737765,22.740623426919786,20.3125128271046,24.597055201180662,23.117114991068007,20.78628428680642,19.932433926767427,19.826295549099655,21.26517033895163,22.496434708234126,19.328382384869762,19.427802084161115,20.18180618902739,19.980960001271505,18.996123380968683,22.600876020595507,20.092995805378262,20.046496736817677,21.151917099556776,18.585982112110514,20.695329089308867,22.520164246985647,19.3471325562148,18.999355953219133,23.20757904373024,21.23702785682451,21.08554669425927,20.768225524289374,20.2479251962018,19.3533467533098,22.213653662019038,20.24562900900173,18.67996002744327,21.27136955536682,21.42004604510025,21.047075972047644,21.8035683042629,19.22968489276112,21.077647966937757,23.387816453129698,18.706232935340964,19.58110217755027,23.538867158230946,21.96712491711981,19.68563168309507,20.03480567791853,18.855622835428132,19.611919934246885,20.89874787711282,20.381292625866948,20.623611830893093,21.463850115483186,22.47425899129698,21.46282774650559,20.250449939937386,19.804071134581005,22.940732615911422,24.1535698900103,22.049830938319623,19.98785104241217,22.944212504137,21.578861893090256,25.17177163891695,19.37427511207424,19.96383210404445,24.180920079085045,22.206977518330888,19.158269753875864,19.069924834450198,22.714272038785044,22.147044335773373,20.251879940231216,24.956261862764006,19.58788736648328,21.751091249893005,21.93032100053686,21.621710733177533,19.367732274650958,19.09456742423413,16.917281198342668,20.807982080155547,20.46876211568297,19.988975863285916,20.580262018382033,19.091740116853863,21.44414464440018,21.29242653675922,22.038981262669978,20.180643707721647,23.155520679582406,19.32638298997934,21.506597416475046,20.650611348398325,20.693713245656816,20.540019222244897,21.217521718500812,24.49306731779338,19.642596451766774,22.00972851632279,20.932965503614184,21.834232905374517,21.098649782686852,22.49932865299591,22.579718287600105,21.529871119919466,21.118854440357843,20.944082894927185],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle KNeighborsRegressor(n_jobs=-1) vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_pred_logkNN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["n_neighbors = np.linspace(0, 100, dtype=int)\n","param_gridkNN = {'kneighborsregressor__n_neighbors': n_neighbors}\n","\n","\n","GridkNN, \\\n","BestParametreskNN, \\\n","ScoreskNN, \\\n","SiteEnergyUse_pred_logkNN, \\\n","figkNN = reg_modelGrid(model=KNeighborsRegressor(n_jobs=-1),\n","                         scaler=scaler,\n","                         X_train=BEBM_train,\n","                         X_test=BEBM_test,\n","                         y_train=SiteEnergyUse_train_log,\n","                         y_test=SiteEnergyUse_test_log,\n","                         y_test_name='SiteEnergyUse_test_log',\n","                         y_pred_name='SiteEnergyUse_pred_logkNN',\n","                         score=score,\n","                         param_grid=param_gridkNN)\n","\n","print(BestParametreskNN)\n","ScoreskNN\n","figkNN.show()\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[null,1.8348641693167742,1.7322689550461117,1.646352094432038,1.6177524690731186,1.6039054385462137,1.588814976153475,1.5781222363450775,1.5757644169796354,1.5663282833062315,1.5653022717253962,1.5641873741769445,1.5602915989600092,1.557654434022118,1.5571931576155555,1.553872182847647,1.5545983995426627,1.5531178812491393,1.5538090314030693,1.5546292494699436,1.5554559256434017,1.5555450302600404,1.5567300225959508,1.5561355940452366,1.555684680549835,1.5560663304223472,1.5566262357151657,1.557452400576876,1.5577232353844246,1.557066844568575,1.557923022076383,1.5579963918987214,1.5575395458418155,1.5585980153395795,1.5593445673519482,1.5593478533840006,1.5597363482929836,1.5591683432434595,1.5580568238838526,1.557975821549801,1.5587523007888062,1.558883435024461,1.5590722996571729,1.5584864373313063,1.5584211993613664,1.559063911710229,1.5601277679680787,1.5607413913472095,1.5612778075470273,1.5627181562756434]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[null,2.0235896863712606,1.8781840318425318,1.7669422926605276,1.7399453533180655,1.7193658383934123,1.7030702048830264,1.6931841420057043,1.6973228768066602,1.6982483736990468,1.6958028648215773,1.6944994775041848,1.6915806651130232,1.689682704644189,1.6890047755574646,1.6830148202358417,1.6859296913155213,1.6852049392160122,1.6865806056049553,1.6886029356905041,1.6890518006319186,1.6898249974611723,1.6901705942413479,1.6898607663383938,1.6876271735316704,1.6875289871667027,1.6875440881057129,1.6882408099682717,1.6873613295594638,1.6864494342527727,1.686758018054847,1.6853483017455517,1.6849376660161592,1.6861594179440682,1.6868625080827018,1.685859763372425,1.6849324059741801,1.6856975604518363,1.6850683331933514,1.6833539479302704,1.6845410749297174,1.6857467664784065,1.6855880270766832,1.6862087899842964,1.6850297938999872,1.6857008616149436,1.6869526209774968,1.6880305061883893,1.6885263304487934,1.6904777057979277]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[null,1.646138652262288,1.5863538782496915,1.5257618962035484,1.4955595848281718,1.4884450386990151,1.4745597474239234,1.4630603306844507,1.4542059571526107,1.4344081929134163,1.434801678629215,1.4338752708497042,1.4290025328069953,1.4256261634000469,1.4253815396736464,1.4247295454594522,1.423267107769804,1.4210308232822664,1.4210374572011832,1.4206555632493831,1.4218600506548849,1.4212650630589085,1.4232894509505538,1.4224104217520794,1.4237421875679996,1.4246036736779917,1.4257083833246185,1.4266639911854802,1.4280851412093853,1.4276842548843771,1.4290880260979189,1.430644482051891,1.4301414256674718,1.4310366127350909,1.4318266266211945,1.4328359433955762,1.434540290611787,1.4326391260350828,1.4310453145743538,1.4325976951693316,1.432963526647895,1.4320201035705153,1.4325565722376625,1.4307640846783163,1.4318126048227455,1.4324269618055143,1.4333029149586607,1.4334522765060296,1.4340292846452611,1.434958606753359]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[null,1.99522357770901,1.9404899847570307,1.8087075874299514,1.7921074891025885,1.7561068581586943,1.7477211041882603,1.7493712706572722,1.7521993892908685,1.748523221747453,1.7482606868595156,1.7493967535069133,1.7506296132422698,1.7510973594899335,1.7531275896160827,1.7466024533753395,1.7483186026755624,1.7508456637170968,1.7539258517422949,1.7573044292172832,1.7584133969922755,1.7599607548627874,1.7607788565125966,1.7608154702242245,1.758202363686115,1.7573107245610817,1.7576555489481167,1.759075713376466,1.754968906415441,1.7540002559333803,1.7531301336681602,1.7495258606877901,1.7508258976108326,1.7526308954549366,1.7533339652116584,1.7508845048624155,1.7479816452767505,1.748062381408229,1.7494206913206078,1.7466300109746788,1.7484415744411064,1.7498157193389394,1.7499975055286294,1.7506218774070892,1.7481073752496055,1.7484542150568552,1.7497697901052558,1.750915253174208,1.751480908757162,1.7536393653615445],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[null,1.7442646305652483,1.6880799516427445,1.6452087408382445,1.627300042144341,1.619965977052269,1.6052843248042756,1.5924215243403692,1.5846802355480525,1.589041996949712,1.583913916260326,1.5791741520436886,1.5719506208133271,1.5617119203745857,1.5584687196342182,1.558458951070834,1.5632571603294332,1.560430597546811,1.559626030274227,1.5585578405043545,1.559460727556882,1.5610306408892918,1.5625225614587603,1.5625113844422807,1.5610184991746332,1.5630402532603278,1.5637800874708179,1.5637919761024732,1.56471025467713,1.560378152299529,1.560852048350381,1.5624891295035606,1.5594020965602873,1.5599557755457503,1.5602839016104262,1.5612954932373455,1.562362830078831,1.5626810502767137,1.5599137257853721,1.5602692052272802,1.5611458966324114,1.5616438938484456,1.5631441658903344,1.5636647640041188,1.5655216323584802,1.5666757255779606,1.567741523093143,1.5683950508186517,1.5699666398855505,1.571904251831533],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[null,2.1069584906248915,1.8458707026692078,1.7067626060421566,1.647006736979225,1.6311407439125516,1.6182681010911129,1.5950734956196266,1.5953120212253404,1.5879205156714304,1.5904751338893426,1.5917867659673448,1.5851068671934134,1.5852872873633588,1.583748010661166,1.5707679442171614,1.5733440378364774,1.5691634852755145,1.5690831582230158,1.5683497404222304,1.5686282981160289,1.5687124748614596,1.5699256037627491,1.5697214219722606,1.569487464369392,1.5717449203776583,1.5719675927812757,1.572758160474137,1.5762763560549182,1.5776496087749348,1.5802313698109516,1.5800837706256752,1.5795514665547172,1.5810281416186804,1.583146341824944,1.58298035420264,1.5844148557900009,1.584755737633045,1.583994207580807,1.5843115005866513,1.584270202071398,1.5852194400572803,1.5822206846286893,1.5808993508635076,1.5806300549165395,1.5815387192952117,1.5835597629328502,1.585201107817903,1.5849671200322513,1.5867089961004384],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[null,1.740408388832869,1.6631003619233478,1.6307816124365402,1.6126406334460863,1.6142793177850283,1.581015750510058,1.5659654414061486,1.5765133771237743,1.5696670623634343,1.5637163190856525,1.559082915582982,1.555241096699927,1.5537492584610912,1.5519387576258579,1.5539900331171148,1.5532093688512227,1.5507141122622696,1.551281648540665,1.554449932392621,1.554081686576242,1.551914406967409,1.5508420002769878,1.5488413455014016,1.547926489602443,1.545492129529543,1.544905203144851,1.545007740110506,1.5461164187244394,1.546901545357112,1.5479064111999947,1.549228873562247,1.5482677089580585,1.5483864196390758,1.5479163231115554,1.5487891738192356,1.54965225481212,1.550019859000501,1.5468346071839725,1.5460377017679943,1.546905815919341,1.546615817507784,1.5480622147263758,1.5485728563932621,1.548031185295499,1.548491424972809,1.5485260868219273,1.548298916813693,1.548414386860098,1.5491365163946729],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[null,1.5874657588518521,1.5238037742382282,1.4402999254132973,1.4097074436933519,1.3980342958225265,1.3917856001736673,1.3877794497019709,1.370117061710142,1.3364886197991275,1.3401453025321435,1.341496283783795,1.3385297968511087,1.3364263444216204,1.338682710540454,1.3395415324577846,1.3348628280206174,1.3344355474440044,1.3351284682351443,1.334484304813229,1.3366955189755805,1.3361068737192534,1.3395810909686607,1.3387883480860159,1.3417885859165908,1.3427436243831243,1.3448227462307674,1.3466284128207981,1.3465442410501949,1.3464046604779185,1.347495147352428,1.3486543251143341,1.3496505595251822,1.3509888444394547,1.3520423050011567,1.3527897407983667,1.3542701555072156,1.3503226878988102,1.3501208875485036,1.3526306891924007,1.352998014879775,1.351122304369855,1.3519369275118351,1.3486733379885543,1.3498157489867073,1.3501594736483076,1.3510416768872173,1.3508966281115922,1.351559982200074,1.3522016516900277],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle kNN en fonction de l'hyperparamètre n le nombre de voisins"},"xaxis":{"title":{"text":"n neighbors"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE kNN pour tout les paramètres de GridSearchCV\n","fig1 = go.Figure([\n","    go.Scatter(name='RMSE moyenne',\n","               x=n_neighbors,\n","               y=GridkNN.ScoresMean,\n","               mode='lines',\n","               marker=dict(color='red', size=2),\n","               showlegend=True),\n","    go.Scatter(name='SDup RMSE',\n","               x=n_neighbors,\n","               y=GridkNN.ScoresMean + GridkNN.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               showlegend=False),\n","    go.Scatter(name='SDdown RMSE',\n","               x=n_neighbors,\n","               y=GridkNN.ScoresMean - GridkNN.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               fillcolor='rgba(68, 68, 68, .3)',\n","               fill='tonexty',\n","               showlegend=False)\n","])\n","\n","fig2 = px.line(GridkNN,\n","               x=n_neighbors,\n","               y=[\n","                   'ScoresSplit0', 'ScoresSplit1', 'ScoresSplit2',\n","                   'ScoresSplit3', 'ScoresSplit4'\n","               ])\n","\n","fig3 = go.Figure(data=fig1.data + fig2.data)\n","fig3.update_xaxes(type='log', title='n neighbors')\n","fig3.update_yaxes(title='RMSE')\n","fig3.update_layout(\n","    title=\n","    \"RMSE du modèle kNN en fonction de l'hyperparamètre n le nombre de voisins\"\n",")\n","fig3.show()\n","if write_data is True:\n","    fig3.write_image('./Figures/graphRMSEkNN.pdf')\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                             paramètre RandomForestRegressor()\n","0  randomforestregressor__n_estimators                     215\n","1  randomforestregressor__max_features                    log2\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_pred_log_logRF=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[22.571557360121595,22.01989409082848,21.62669463292471,19.589026226213377,19.661359472351403,20.815337769988535,21.90058383838967,20.27660015129284,21.958320839362145,20.193830045198673,20.790862911414827,21.202177195849735,20.291905182580873,21.11398985281054,19.934643957297695,19.938349432221862,18.91347223669258,20.47023575726051,19.015439514774414,20.144902272821945,20.42869195013107,19.593130314079016,23.396521838122656,20.60561590517945,19.885427394327962,19.995602970341682,21.298345746357,19.343768656721473,20.15300004072263,22.58336513110135,21.645132220288236,20.481261596737376,21.13138819460388,20.959859971459956,24.903912379957582,22.110499192434386,19.706267210844644,20.146840505772296,19.859284929099367,20.852369604779966,19.117717491831087,19.883018638359324,20.662631605223442,22.39199048807033,21.986190020577798,25.77939943011339,20.035196939688632,22.723559353782534,22.11908246881397,19.524046261297787,21.658967895159417,20.68441822302757,20.51297684738493,24.1883265843766,21.120110377201616,19.69330559917147,20.33955569688539,20.837133699409492,20.406748011861477,20.145298762347153,19.005385652263097,21.2609507873188,21.649724850183954,19.571664861586086,21.44066448160765,22.673489703143147,22.017570044715967,21.68540804885581,19.588420344252416,18.925236869173816,21.485875486037575,23.385984023045612,20.229972865527635,22.45730502611271,21.52642274748071,21.784197708749787,19.89151343826819,22.5693574599178,22.28750439217885,24.42909276576251,20.061497647316255,20.365890687520437,20.814490827845372,20.07651990629316,20.993400228567275,21.730222178688233,23.94435620848093,19.639704145077786,20.07189001636977,18.786436605710445,20.03280732004991,20.430820541876237,19.122935740752485,21.099453456977795,19.148770070424508,20.930401743686712,19.86318563378498,22.685641563296457,20.46509004314592,20.066321919197325,19.777073587073676,22.97782510556525,22.40622508826314,21.439881380459017,22.875349519913765,21.576164641775712,19.473426639365705,20.65670140614694,19.12676093068137,22.80714028479474,22.67615406198032,21.47953469565398,21.434888993559046,19.150434605906398,19.277283328834102,22.53807422843864,21.662855202151185,19.312849299385558,24.125806883115946,21.141313164118525,18.820147323596228,21.147815641042477,20.146701047505033,20.059760243147185,23.242229102308023,19.510424730135398,20.77694108757146,20.231611034395833,19.64473530576997,21.241597458478108,23.48302418208889,17.323848905740924,23.50945010104767,20.178898680857277,19.71017839469274,21.32131990559936,19.588326099515903,19.26905044179325,19.74650480529163,21.594592593076253,22.69376437924368,23.213462378236954,21.608767641404075,22.348561638368626,20.726982920644044,20.234510795948992,20.428360920376356,20.856125155391585,20.777713104671385,23.339779621492916,22.41508999116351,20.70043906901553,20.463298639690052,20.375265293588416,21.207138296834938,19.590235157918386,19.4582133108555,23.18374749639364,22.685440661598946,19.6489876733064,25.545048885172555,21.820777889899084,20.62259175560564,20.295426259753622,20.48390236509558,19.81442203404193,22.502403861710466,19.411377375445092,20.836977123106937,19.46850979590738,20.539259308708164,19.532431976855975,20.116506220525775,21.690104102609766,21.505230681392074,21.355752433482422,22.499621546888665,20.16273873494558,19.39214753758367,19.37119839499259,20.004394293891213,19.58598846671986,20.292061037047556,21.66756689239146,20.28442928415552,21.31704929169676,20.55831458797103,20.646761129527615,20.946203706670378,19.68497259859348,20.47206536070835,20.589411836284757,20.469580447155728,19.304739152041257,21.635508539890502,19.573786618957257,20.10507467108858,21.208722603162006,22.845448879373272,19.543308681371425,22.456856646692476,19.726497891254876,21.3426679787905,20.147298217011503,20.088318007852248,19.69854413660194,20.983185137704847,22.488212214897526,19.784350886168,21.17055864134268,20.61181249704472,19.694495242582928,21.165304294014415,18.998174247457122,20.514803351407217,21.045118663973067,19.79663287648258,19.6367808040508,19.939301051848386,22.043832067427516,19.345547792698294,19.571703828517812,19.93952980609523,19.299555860150903,21.713387639509484,22.284832114042803,22.44796325446799,24.04464639182157,21.10439585767915,21.39378744570955,22.403323264577367,20.26993706163902,19.061651880376193,19.43540137941497,21.334024196697584,20.743333211774157,21.079721773534295,23.129394553910487,24.264570937208617,23.768769138567528,20.88522119390223,22.839533094870315,19.71665506521425,20.878112242226003,20.185512665334237,20.288050369902944,21.145863568293002,22.197941260714945,21.79870541944662,20.149986606371115,21.134682206907378,23.002022870992775,19.747621077695797,20.290377502424896,20.247752527816306,21.424304995449134,14.295303313454076,20.18907135884901,22.30629512953546,21.30736660897104,18.0524585327896,22.315258262051287,19.781086948135105,19.97712354739484,20.081194636713875,22.623890609510024,23.305570054545733,19.435230548135287,24.609363802314004,19.38489937602369,19.618884647464952,20.162102266787276,19.55652895439752,21.163380788075344,20.532509860135036,21.099065503619975,22.23149444153365,21.221743871775203,23.13853220371993,21.647920303803048,19.996281689457962,18.906171720252438,21.85956367351776,21.487646478350392,18.865844273828092,20.287445647271017,19.587394010172225,19.013646542245098,24.268907466341314,20.09390295142744,24.764485606777242,20.340402730766098,20.850650689358737,22.1742108525563,20.619055539906405,21.115439974008257,19.58411497592435,21.064459474614175,20.453260286134405,20.646986225170217,23.072296036161294,19.58178880630676,21.060876065803896,20.024490075298598,21.12775049485907,19.480718865950937,20.074057348912344,20.902790996436472,20.234557994325524,21.32343472954765,20.987923206566045,20.60579801972911,23.54870471833965,20.269054302176265,20.69640305140527,19.001925668863045,23.316590857013374,19.955821985224333,19.90848701698345,20.914404765880068,21.11069646048334,17.25961787845096,22.612994595158977,19.812212496012503,23.224334472078777,20.710579965090137,20.675715311785446,20.236158443438725,21.015033348884025,20.68834638814612,23.35780383722099,20.006430433717334,21.156934763643665,19.67720973431115,20.500096798106178,20.88522788863702,19.386270663952946,21.115789258828517,19.78453121550563,20.046561948476196,21.387564801822293,20.873370596873645,20.017328753471865,21.203378111081342,23.177804414303015,20.693396142723024,23.487114081450496,22.53310028761764,22.687381055825583,24.221968698444485,21.340778930200713,23.23045158801122,22.620345852037595,19.017774917387282,23.334179639495236,19.671364004268675,20.825186075272132,23.333291022656834,20.031451314397692,19.356121437344623,19.81383416728762,20.741151929241195,22.251205256465695,24.19171609860848,19.60975134886287,19.37572665452995,22.82524899097352,19.550474769605625,21.34649455787276,22.914162932766324,20.55636351088024,22.656920165817315,23.995154410970844,19.7370511158502,19.678361167833025,19.860429752326667,21.6098862989624,23.128160646088546,20.72306879821247,21.565692363490946,22.90015152171797,21.03326411217071,19.5507693415006,19.858992250529038,19.435978076075518,21.034889684788997,20.049875078441104,21.248844117964214,20.59269969688195,20.60465658991742,22.34381211431196,19.599873062437897,20.592796596229633,20.305109581204654,20.04238159064842,21.155883812786733,21.477885865218852,24.291299493734204,20.191808382138127,20.364319261040585,19.88701838851311,19.67187671169015,20.083579801304868,20.614349076482487,20.91667087152001,20.567185041594044,22.428284973955474,22.88268579183834,20.42554574276941,19.43004412070912,19.987822830451442,23.072829260551647,20.375297136553726,19.83712446789179,20.107622503028505,19.617675672050833,19.497547365467927,21.076113962230696,21.225471708073293,22.808270214773145,20.17670340596687,20.76558458370036,20.37205173481324,21.35850291455029,21.46546078028432,21.20680975042158,20.016106525533672,23.939738204126616,19.750588300803102,20.417851428126017,20.959043216425435,21.63805706621179,19.33783434450671,22.31741554437487,20.72308695216273,19.995227943014473,19.94992264455323,20.919596269046142,19.999010064873897,20.212583430085516,23.170237764388105,21.117364932262692,19.887634830447766,19.81448132087479,19.734680026198607,22.082028337775085,20.74652034178187,25.4729155055975,22.136839422461183,20.90764390607668,20.14822781165667,19.69396024693388,22.179121777968167,18.76946231466343,21.58066567904044,20.71903636207884,20.270377078263724,19.552514331230867,21.397836620224634,21.748670771882463,21.571131488180672,18.606858975230583,25.64936846102603,20.674475243504823,19.710517987417493,22.614330710596516,23.33005733823641,20.188677585238448,19.977945651930337,21.7687901610566,23.449465586205736,19.59137353384649,19.035311714149444,19.87292721177068,20.114267244896578,19.189630783666832,20.551258527892507,19.598998306015257,19.68422411584536,20.630159697191807,21.561284833178373,23.061382640761682,24.65179530697985,23.18098080491645,22.43692659752699,22.432769745401078,19.533317329562077,22.604447151645708,19.975353868847176,22.93340999549481,20.54767624291102,20.880307733331506,21.225874207871048,19.991873236954575,20.86699319619747,20.387942998426958,20.27008604689084,22.14730536276602,25.12239952607957,21.964140789154992,22.310142884195496,19.762958222597458,19.094455986331404,23.788055651950476,23.787004349411305,20.618170410805835,23.55544704625563,22.801519313112955,19.893672161561827,25.31126256940219,19.37197116243014,20.967377569433115,25.705598768613918,21.575204302714624,20.13447909666686,19.71806586786666,20.30395218131729,19.821512670871634,21.456556484711296,20.3995653345144,19.55776983197437,19.766589615763074,20.164744100328583,19.601266431924156,20.196863886534416,20.991359133441613,20.35519410945431,19.540038637499542,22.614694448109333,21.074183943177687,23.145899745854507,19.59474925350585,20.826409968097273,21.583530886861645,20.841336784434578,21.27602822592995,19.14083646562272,19.633868966344373,22.98772755123452,19.82046280090468,19.652083821290326,22.085684961491747,21.40674029875972,20.69488461047212,19.391878179834865,20.60618654406096,21.71947338719104,21.350253365772787,19.775569889593342,19.747862406958046,22.31848140344541,21.721470939672447,21.510693237124094,21.84750279191774,22.335688118266955,20.02927568012094,19.470695199634374,22.084513597282267,20.933032598685312,19.333627835621773,21.3196593335041,20.038165303343135,20.06351677810908,20.999869365214643,20.98555306859686,22.37518874382263,20.903546380909933,19.619259604128015,21.397207876716173,20.45241479985117,20.635878630279255,24.05742130936072,23.404347039641948,21.01975567941608,20.29523439130977,20.05394447038447,19.646806706329162,19.940848624192377,19.35871647199414,19.52242108683545,19.36289310489416,19.610990576445243,21.855742898597494,20.560734124885684,20.318472339939195,20.38765929466566,19.81637619436986,20.08623599420519,20.167025758299946,19.66454947501665,20.24813269694426,20.91699539343583,23.17251213934614,19.520890363404554,21.529579641002137,19.46192578640825,19.389426846276365,22.346626537948143,21.34998299446333,20.681531028795796,21.821167114525203,26.64392092748784,20.90497550361643,21.588338246759353,22.259882822534955,19.70887126024989,20.851851151429884,21.174448837576655,19.532257802976584,20.921616825325657,21.981944941950523,20.030464173166234,19.88198719463225,20.1463652740204,21.444690631968342,18.719991697325526,20.924068345930504,20.391316121287826,20.401043813344764,20.588936723225746,19.50977900652692,22.26859468917027,19.031916202697154,22.44194268831979,20.993711229300356,22.383138575686143,25.336542757753033,21.277375311354817,18.693762695907775,20.09805770503944,19.73995274393209,20.127543979205523,19.743555329491397,19.536624690627583,21.451477909313844,22.26007211807776,21.01907620055567,20.00265751331386,19.903896061402882,19.801201702595307,21.658985872635288,15.444196866143798,19.918667772014263,20.030708093610944,23.021391476387844,21.234315953993477,23.086305028709702,19.376113298863974,21.59451289436616,20.097858920138286,21.295377228347615,20.36633561628724,21.367656872299545,21.57913339081652,20.051992049070737,18.91331971955275,19.652393494321988,24.43795964429401,22.613308021553095,24.12859041838318,19.67519864964472,20.82857192738042,20.332393113633994,20.030284105433388,21.764787235604544,19.70086094355018,20.356022590268875,24.107668504697322,18.91743259791267,21.89965932876114,19.06306384629148,21.27765666253074,19.67791156893113,19.496283619583462,20.527594947784497,19.848948668454558,19.958401435406778,20.072114044937297,22.01740719843017,22.97197168809127,23.262955009071383,21.03520327133375,20.483307464636738,19.716956467369048,20.73806754545006,22.896209091236745,20.514766183974185,20.146932173220133,19.75243181591484,21.624023540311235,19.715200867597886,19.49612020285431,20.586728917863443,21.870072283553352,19.128210981824612,25.302487348338236,20.855804894247516,19.956869804168356,22.266379191198634,19.701252190851886,19.953363154726716,20.025855491201042,20.47767221078565,20.462156178112597,21.60361780226381,21.104701670905555,22.747197248044028,23.21396129578083,20.949989992686167,20.50940152209444,20.2982787126458,20.673744782886224,21.456790971795474,20.133523482120584,19.84751589836098,19.541737998434943,23.799373815049425,24.25359257218359,20.073882315291076,24.204551092967485,26.207055820931725,23.563331091633838,24.19691141568034,19.09283092147907,20.027938893829152,20.0255917900437,21.185745904770428,19.911516290610756,21.60802412530416,20.001977570203834,20.82212797085286,16.31858502641611,20.27254588287806,24.59120046577116,20.57806858851384,22.41828404300129,20.828163295484703,19.516607401228654,19.696394863153177,21.13148795382755,19.315000779753248,19.877291029175773,23.306064455303847,21.4740180047853,20.761472286589722,22.89995856704731,21.209368410560003,22.3190545788835,20.62314606265527,21.042541543672012,20.055834825291132,20.287377712165245,19.50789576198981,19.526828048704864,18.95422444827814,20.491143193415994,20.42761911888701,20.50602102114128,21.46632522868298,20.801689757070214,21.43938656407136,20.46543881480736,20.281940847535935,20.382931426469998,22.40961713549143,20.504127328636347,21.97444258865395,22.062669735215405,21.609514138607036,20.68714829370381,20.348046475797172,19.64441429450667,22.991000496223492,23.375326420537096,21.259810460459068,19.450091358161682,19.376130985316223,19.785050480938377,23.99437231412775,19.42102014325926,22.329636655255005,24.584336958744192,25.025382598315645,21.165761484340777,21.18719397266264,20.46000870650338,20.93725335022929,19.808053667234457,19.711986755888983,20.111616020713985,20.486330764944913,20.193467065239247,21.05924414000716,21.55028126504598,20.321153633136635,19.73956710367901,20.0089533715761,19.468341488651685,20.63338779593361,20.169594577835575,20.772168628127933,19.93813460165869,20.708410057333822,19.11383719361769,23.317417267299614,19.88124778213483,19.856191710231172,23.397526140012456,20.212586387591617,19.473732808479593,25.538386065697974,19.808133812272462,19.51797511695259,19.661673851402472,19.615600080885013,20.57852601979615,21.31351712246326,23.914801051520595,21.093170920097315,19.198315171594377,20.411876404633617,21.04139715855829,22.74140915694629,20.03211812719001,20.414586642995754,21.177719868687987,21.514767583335747,21.522185888932796,20.073348094417227,19.621620108731538,19.602376550185014,19.015169230767544,21.668919570222315,19.80478689579421,20.359640825210057,24.907177861994544,19.412141333463182,23.467088854815973,21.016042358565752,23.218615349449422,21.8372214498241,23.973490084246038,22.512200259397034,23.18323714332879,22.10934178470979,20.016106525533672,19.120518453542836,20.531699809604046,20.467287150832412,19.5137459907828,19.601147951287274,20.99837148105303,21.4010744453569,20.571882483603034,19.747515394384127,19.678812484633944,20.304714866365998,22.77879656453744,21.981761379657126,21.17140975542431,19.28931901112053,20.873839304014727,19.687107282498864,20.97642251620081,18.648955221798474,20.518720501902028,20.618103031195588,19.80351906679643,21.02429982209227,21.389578859135895,19.188621932293973,22.064801957656993,21.914196402094124,21.654658213521106,19.749767581385832,20.426442056111878,19.71813430400865,23.66285436698515,19.199812304200652,20.959347091877525,19.665173851250113,19.63339674578488,21.839545194900666,19.452677175224196,23.41688437753457,22.84659931967856,20.446425859360463,19.73264930913517,20.306409728151284,19.42181626472251,22.478834558632318,19.53615604541013,19.637257555594527,23.650175963125946,21.695771058991546,20.905820681263943,22.43072164550241,19.540802241637994,19.77850480369841,24.037419163923335,20.870705119228997,20.313457594025312,21.296069029432076,20.272649602107638,22.185485190739303,21.996124822814483,19.885002553713168,20.757840694227614,20.56863810645272,20.254795701964795,23.92179876933161,20.447396985420507,20.137107074900857,20.88304540945791,19.49033255381238,19.927102355112055,24.747924895955894,21.486173666122667,20.384422032317715,20.892138480661483,21.482175229568025,19.57231722186375,20.981472800795366,20.863254595252847,20.749327192199033,19.98231714262516,20.71059928707389,19.912293001298053,22.304722557496536,19.86557930352131,20.589666762490047,19.419355815510176,23.71510075144522,20.30326242287252,20.002338555689274,21.12151347867931,23.8360920844124,22.327538655460057,21.531993918464234,21.613354254863165,21.908043121693332,21.163154172868158,21.094896874187377,20.14944126124495,21.01309246602851,23.835916751493095,21.541773289831124,21.74484768836312,22.76688455912435,22.41987303470543,22.542503344118334,20.483225523326478,23.40997624753107,21.155713241502,20.65766944561269,20.85331122040382,23.980807556001473,23.90879684957334,21.33659450285054,19.41528953172751,21.11052359050039,21.53425995515233,20.358623480220167,19.682043010420028,23.56900799230917,21.963445527209437,20.006838073089327,20.565755365142188,25.313013508861058,19.80134952305921,19.82877996130257,23.394344641213422,21.39592382156242,20.31234145443548,19.364313253088042,20.838884036669697,23.66668790732349,20.51311769632363,20.44210666787987,21.219737076465627,19.621309174394213,20.172809842012015,21.44291634950221,20.70665047464749,22.03511332969234,19.47178776404145,19.945280203481907,22.840207514407247,22.76423641820401,20.284204143512934,21.341913868726323,24.494344136623916,19.51696376875278,19.483437685498476,20.270584058133156,20.009199152015803,24.362516014159624,20.272458182971203,22.86554585472417,22.54875207818894,20.507998821405682,21.259955405783977,23.855732479411646,22.501770527924464,19.59438133106417,19.2882328629362,20.736771385749595,22.087745448566366,21.04882084208214,22.118623010160324,20.364779984165025,20.409992566597833,20.689176802375474,22.476107488792834,19.605674902243486,21.975120837869156,23.77593502155051,21.85387817516889,19.926525866599576,22.66285790718358,19.99541481928665,19.870592007245204,20.479160441691693,20.485430848794113,23.91955588796962,20.294540468854855,19.583798837699902,19.557534099772425,19.737076033087508,19.167070328393006,20.60788011666532,20.436797692205133,18.133108251451784,22.12796128054369,22.239430748887365,20.888822609382697,22.133120369383867,22.218961551004146,20.609591074138375,14.391618684921367,19.665433315079433,21.14228018114189,20.300120670285775,19.68461590726228,22.195608491523632,20.76162689608457,22.64848386863945,21.60044087105598,21.280391220410035,18.712861308680615,21.885570924680522,19.649882058985526,20.378820726820358,20.08552543221928,20.95934376786774,23.073816669392155,19.949220373190247,20.768733166156288,19.611288018796827,19.497738898291352,20.490630753615143,20.672464996252913,20.44074588494661,21.525062866287335,20.518135080891426,22.195692853061857,19.915828307113895,20.970857282053263,19.261448415208164,19.48950172805867,20.21012763047448,20.487817625995888,23.462182866374132,22.472837236662038,20.466436072445095,19.188789795698654,20.89745393585262,22.553035929603695,19.976110441453603,20.545257682841036,19.826590025996005,21.11365920663734,19.518188236447518,19.22937544491886,22.30489485600458,20.392951332303983,19.40773517305665,19.650388649914927,21.36719931303302,20.2971003167727,21.168812752968023,22.2400391804629,22.129909177542064,19.7413576839306,19.712521023090126,20.616351571959765,21.944375032398526,20.802530363803708,18.660050493933305,21.46696313735178,19.43805517862409,19.95311756502245,20.92584248800329,20.710741300930035,21.548816980433305,20.86441239420612,20.72489228003571,19.396293200895713,22.25986366238162,20.745261401977427,20.2707290224464,22.464183560044983,19.81226773628161,21.357783649009924,20.260846338960082,21.3483899037327,19.011541066502744,20.365514006579424,21.918249503538508,22.16201291329869,20.64990385409569,21.810417044290883,22.71955737269468,18.55272109401809,22.828008122092776,19.593641849125223,20.55869306209428,19.986897839807472,19.648383488028824,23.893601816864297,23.222783100583275,19.53866509177013,20.351869038435677,20.385686592156187,20.017328753471865,19.971992038281922,21.908436573603563,20.059572104804364,19.773502866505552,20.309989524371613,20.070169059946128,20.135644052673587,20.550923918537787,21.004782855582548,24.640485940399067,20.444436621554633,20.23452629480856,18.265845343889826,19.93657758377925,22.742002597815663,19.939066116872574,21.00058084992906,21.44480293222149,21.06434282001912,22.376324142765206,24.047032390234776,21.095265072659586,20.947350209560646,20.477421018582813,20.19436528224682,21.089059259043204,18.995907348339436,20.832211706851258,20.82415789829486,23.059793106115766,20.92315777116331,20.18800227258898,19.625487600374534,21.84422829203089,19.872564928054704,20.435478539844013,20.117188699870248,21.07071810063555,19.370137191215232,22.05024367427634,21.133435230820194,20.39412275081263,19.85297377683368,22.036061515482015,21.40192924459073,20.63307869677085,21.03501276246769,20.463223204335584,19.894094204389386,19.49345423815119,21.922035171967543,20.54266770978533,19.76750933671994,19.34296534740901,19.701754089884346,20.348617092201003,20.74863763965672,23.55867517647249,21.343570210503355,19.451935472081743,19.885025856488696,22.042230965271173,20.893219436536224,19.4662804251613,21.371207390558553,19.754477988582984,19.931218187426484,20.546985042615823,21.916862703878753,19.675639337627597,23.835541564284277,20.621319476429115,21.080240719329417,19.536453364758838,19.750719759919885,21.038433417700418,22.277388431716087,19.649259707730298,19.568215215410934,21.414237937177646,19.978309986289982,20.40227971763999,21.952004243102014,20.888128070539913,20.272538656325697,21.895659692649687,19.771274785646384,21.043287618250496,21.101446131997815,19.605974521771138,19.24478328952352,23.142428566531617,21.322078604148874,19.907558582076504,20.392092559705056,20.407229687432245,19.484017894273023,20.540249641462143,20.414280945075646,19.84940387932491,20.985198459171713,22.203464897834895,20.64828617562127,21.92687812022749,21.153838309082104,20.27438636149043,21.108350302090308,20.128407327006922,19.419025747206945,22.172009752060102,20.857270666062398,19.28240073178116,20.47879379774274,18.896807032485746,20.118940195865097,20.757301049335826,20.31788119358751,20.623438342947694,22.052119496194415,21.533189228637564,20.25679418562993,21.053903565871686,20.371746931797468,23.021555966939744,23.44899720088119,21.403244289900076,21.291022998280532,23.096158905697923,21.592723510505152,25.14606156562445,19.993279973667175,20.35289156950601,23.475382491309603,22.31129036974153,19.324465820307736,20.088588983161745,20.00599030606661,20.47792262524424,20.360761919425954,25.169298774038722,19.827864812657445,21.966154166753416,21.32899576232554,21.302821928355268,20.126477539826492,19.45000354610134,19.6632733036404,21.52167770886922,16.63208090062882,20.406942737198936,20.23528558664527,19.73217459580386,21.323452848388918,21.935505165391806,22.502411661736346,21.66530341835104,22.82304498966138,18.79148921252967,18.746312145482374,20.42833423990064,20.579967004884416,20.635726123966624,20.769099939534318,22.963867334593626,19.575977991926457,23.39351150640709,21.577399464180164,21.755966649691203,20.015094392828086,21.782649767538587,22.737108993841964,21.5115948420812,21.087135244546968,20.824554680203047],"xaxis":"x","y":[22.839725872000276,20.91128637269999,22.88916655727953,19.430343691015025,21.11735552640613,21.280697719341468,22.124201158883572,18.136670762341843,22.576080863006137,20.10297814136792,20.137307637847517,21.968529606994224,20.90466916348112,21.65055481705341,19.466148173643653,19.644920713511322,20.60018787473792,19.883803436319816,19.908561207609715,19.88468822756935,20.68752092168076,19.37233862557095,24.197363319180216,20.375121118721335,18.385719188798834,18.92468784406244,21.354902012656122,22.075827573908075,19.724580465382388,20.25426974158722,20.75534115928083,23.747615973251953,20.91788210799568,20.92150383722997,24.937914667295647,22.368646863289158,19.83010122567355,18.16161414446258,21.756941677613227,21.220261555831726,19.921137323148557,19.756730386301125,19.076215392899233,21.77156523118196,21.51345533419679,28.08571411944019,20.221414835639845,22.43563961615873,0,19.8813455186455,20.446723265477033,20.65296343235214,19.469859640957857,24.8692968913913,20.808662576707462,19.741272758302454,21.063890409149668,20.444375957405665,20.737383520261705,20.772392205332533,19.30201857006515,20.50461437981694,20.844484149195353,19.768388190282472,22.01816825105002,20.77707066358558,22.615494713832,21.249591056381075,19.3101001682171,18.557194427644223,20.820077266859332,26.161807884063222,19.616841359021425,24.601371538637977,20.929756127701882,21.371449507411626,19.801676786988352,23.186489993216338,22.288287088869534,24.593075628702742,20.324838680602316,19.538495641550785,20.313003679890702,17.83704270507587,22.508853453023672,21.500237946648834,25.808363100543477,20.146924115160264,21.08058139373211,17.12438387523814,20.968249130032163,20.028493079904162,18.408984789492116,19.05702758537138,20.086616681313576,21.346006819966092,19.260460167342536,22.182304931495295,0,21.19880336254275,18.935012496616167,22.63391493571178,23.82192770536634,21.24843214899758,21.979683188147945,19.96496549198579,18.7932762528682,20.682137237753555,19.632965031850407,23.473975339042966,22.69657897049395,20.12759231185353,20.362922084924325,18.91123104739614,19.548169047796804,22.968053968168725,21.917396756553043,19.216324315987517,24.057859634036678,21.674989393981168,19.72022056098374,20.690574372087426,21.01024397957751,20.841336970061143,22.67830612646918,19.365836228253468,21.066918904158555,20.194293964879012,19.514174668391416,20.55950872954696,24.227674033652324,16.89837701160671,23.000908813982363,19.725383525541226,20.372113673311205,21.25165374313537,19.232324890468607,19.688668857104407,19.006769325349232,21.51252238785962,21.56101612004373,24.60236976749392,22.783863807855585,21.893038360020444,21.254697640624393,19.266317599579637,20.581937941283943,20.45335403901458,21.260660009666424,23.096986451321428,24.198260151329528,20.67873788317677,19.161993412283312,20.805625607806174,20.111067676706508,19.630957845101975,19.421904563712328,24.27931634919893,22.429090755701058,21.147722912842873,25.31201588491192,0,19.877134324376346,20.195578868662224,20.07860380095908,20.74807693982553,23.74195830377693,17.92351319630841,19.98475794826,19.330905939549268,21.75045501313216,19.64650597499701,21.21476614620658,22.11740118712243,20.373745608135653,21.286908529421844,22.031685219396664,20.241113097705927,19.58596744655146,19.47904998638186,21.40733571681439,19.113174941454258,19.88328018890598,21.622357074698563,20.24572329389297,22.858722725717435,21.004219094475406,26.772642502942784,20.915100345640315,19.607783613490604,22.164399063013658,20.790635654144218,20.499654561276497,19.533857847503334,22.238063995762765,19.371674563247215,21.64924126399572,21.665790435544952,22.72561505846151,20.402841366046307,21.38581238341487,19.76025356060854,20.786842719711426,19.14889856436821,19.895681776607635,22.093666112296297,20.459085985918723,22.982495350783758,19.331349158501606,20.55332894959407,20.31682150206961,20.014253448408297,20.016785168783276,17.008111246780107,21.536104422950213,20.603485087703003,19.59434829773337,20.049508251055094,19.642302060721956,21.52720398550173,18.99020093378567,20.22398061738597,21.01213158467734,20.053269667846333,22.108159109401438,21.79011956511657,22.084914601393155,23.135231909261865,21.190067594656295,22.67339901515593,23.695624962968736,20.984090887872558,19.114539527933296,19.182165482881256,21.52107548780855,21.22249010417508,21.043820980181792,22.894093531705956,23.85820974949918,23.265700738521485,21.89761800850832,24.611415306054898,19.96312680068985,20.173501472804098,20.216216542451658,20.253427701740367,20.770035525423065,22.54105973810428,21.560752172206048,20.50991100623185,19.66651702047476,22.893554470389834,19.678935343554116,18.37060227548128,20.996625195887564,20.88004158687887,20.017617555780475,18.785129945834015,22.472827160217914,21.123452379734417,21.834751391009725,22.36922218137253,20.638651229695757,19.920184186837908,20.233535275947663,21.775846548503456,23.23197611805285,20.98111639712373,24.69390393605354,18.972968571042593,19.85555820528158,19.343431218526263,18.83915024282446,23.64752507012282,17.79557360413656,21.996936490386865,23.456377353127298,21.603092935704957,24.298252303878165,21.481533394476365,20.408641755181446,16.772761777957808,22.20744305955703,21.11943827672747,20.998113148879906,20.11872947152901,21.236410750491928,19.699904832330084,24.209354374596852,20.24751150890443,25.07655356851684,19.568646661314627,19.776542871248843,21.727271525181536,21.062803097791257,24.076140757673382,19.409983373621806,21.192511957229144,19.312950102671177,20.179206907329647,22.432801454859,18.979888467927786,22.15617321227827,20.260215495083163,20.910464871041206,19.297632789142515,20.29925833471969,21.341220185389716,20.961489967791394,22.164351182655523,24.799241151178187,20.420977915907716,25.348057895064095,20.921713841616427,20.242927524338644,18.95868492479523,22.708968189549438,18.49440443743341,20.482328404305942,22.37000861087754,20.121388068881203,20.44167803431194,21.856025378041313,18.927945745238855,21.9419703223203,19.736694248820182,20.23558290561487,19.836668342627448,21.25562508835354,21.394257392515996,22.71500741252458,19.547704151356815,20.400995752923983,20.265469687416747,20.212441762527977,22.377553705906216,19.66793597026211,19.3315880738609,17.677595133772794,20.276225239062065,21.268637940423815,21.725285950225775,19.195369804190715,21.15736975857522,23.488081735909503,20.232817889146876,23.728782472563765,22.84612630027242,22.077517747249683,24.569997706071334,21.7711290432985,23.43665576011153,22.83375379636568,19.2148787700197,23.42842113265138,19.4136293492673,20.540968959330602,24.77613012919926,20.659638185399235,19.087533251252736,19.83361590003151,19.96019429668775,22.334373955189864,23.109422435163253,19.778390202711652,19.167836614935887,23.933068642871486,18.56499419954315,22.530048594882032,21.71290394044351,20.45930590947763,23.069120127461606,22.999636382856856,18.925763040720064,19.49519726866692,21.042555130756018,21.216321947424902,22.454539777644875,20.13393362882646,20.867172390851817,23.066997383640125,21.52825545142037,19.72029436584214,21.117385332870825,20.179272523647345,21.590649178257912,19.692513024244924,22.86005581934068,18.799932943985397,19.96989517504386,22.510505954413144,19.34599856610255,21.28740957905986,20.158837704444057,20.907660872216642,21.905619198419114,22.38172749868176,24.308788888311135,19.262765428559003,22.547404100685924,19.87902826477287,19.826795369611226,20.791833978792408,20.76154230516995,20.245177516562332,19.101442274030862,23.095721196617728,22.853333163501407,20.55222183482644,18.623284957897813,19.707161942553423,23.312427516610697,20.72798026434645,19.38519821479218,19.584766197834426,19.657602815047426,19.766824889008415,20.72718342775812,21.19739249261565,21.766933642509713,19.997824758905686,18.07487316385907,17.89367601727741,21.076958761045283,22.70946891023392,18.280325895793162,19.286378115164315,23.27164625070411,21.087259361308853,19.696262913257698,22.998819199975294,22.068276096052415,20.676958661968467,21.84293342285994,20.22886359790674,19.302304652102425,19.45315606988007,20.189004387901257,19.834023760961447,20.136832510309578,21.96825774467718,21.693760030133728,19.68327564708964,20.69046887217181,19.902947681155606,22.060735176692912,21.15375333067992,25.097138279698054,21.440477364164977,21.3505869180748,19.260072175979914,21.05711024152434,22.371116977331784,18.18526261240324,21.763695119906945,19.900159246109634,21.022878950161832,20.48383078099113,21.821470865389173,24.987649624596244,22.265101932257092,18.954616641321604,25.734123629818527,20.035932572983462,21.002454511024606,22.570203152455143,23.437077312046757,19.317032739803228,17.641741979338622,22.448170385491473,23.33275256138136,19.064590160696714,19.240852168960235,20.1483293247843,19.78119349991646,20.035822020141666,20.368090770815378,19.622542520355413,19.136726140380034,20.665117281017753,22.04592459909886,22.766122645362053,24.701481133697825,23.00797271520254,22.942154096547412,21.766675289013797,19.216601410973656,22.721359157406464,18.9576603836655,21.88381079939571,18.472822451162664,21.07184726592027,22.03833211483471,20.099462108962847,21.41678722119122,20.92948670168764,20.890428315584586,21.725547245838506,28.740163782092626,21.205026123435218,22.323663662536706,19.34101707633175,18.666140767995948,24.012052706607076,23.69806207100269,21.754438391935153,23.682062419502355,23.126467381893125,21.07179899498228,25.515113797875166,19.69217074150189,22.12431429427697,25.95498550595432,21.870103917395213,20.208896815397623,20.476170795773445,21.523407372759994,19.429205075454394,20.94805891660743,20.579612186732355,18.425777297525496,21.048120970639673,20.14580390451336,19.1678248265788,19.26180569731426,21.731372277489818,20.677454054144345,19.241454867705745,23.733038602815835,20.006473671385464,20.924659576737604,19.109062896280463,21.73683233585413,21.589387814254906,21.96355600303241,20.930649460210123,19.44163523375263,19.835220336552222,22.967539915594646,19.57718354371762,19.694551733623157,22.05857907784579,20.152641197123305,21.206937139516242,19.177040653531833,21.059144404213697,21.77992540342766,22.0971519060863,19.9033116727081,19.870869348950354,20.847191191309296,21.255773747330974,22.179827408990512,20.655014419256375,22.594985264708335,19.867378293238115,18.6010489412016,21.546971094057223,20.8121798521563,20.30528553843669,21.30235611574127,19.472993206549905,20.642314511859237,20.823420888524133,19.456579392491523,21.112413416140527,20.769205079804532,20.344212591822245,19.88957080391307,19.783879090807634,22.2606456541005,24.0347829295736,25.71484503270827,20.79497581994581,18.966254774482525,19.558951865783115,20.00118447409592,19.759231530600534,18.668189678924886,19.362035840585754,19.21669376414341,19.534513178135196,21.58943582111394,20.029295821796193,20.93634103691578,21.419330156983328,19.511493048750843,20.011894681671833,20.008240359915625,18.95284902418798,21.345469949872594,20.977820033541025,23.20923603756077,19.692880228017533,22.637005220660473,19.457649403400342,19.12974837607436,23.60775201354553,0,21.026538976608304,23.88396569488923,27.919301359795483,21.157605371717533,22.006125382801496,21.526914680071894,21.60107003042797,21.310840181677595,20.659294437479453,19.303395037499914,20.578169960300666,22.895450751816803,20.179768194947595,20.3718571876006,21.13235475498899,20.116418845808077,20.597315830475623,19.957464846805212,23.115072392844226,20.726116689654802,20.700075492290427,20.833447685328323,23.397238664878486,18.725486086935668,22.38037915102657,20.812817350665142,23.171040699108495,25.19463787743612,17.21138734793915,19.0336054859068,21.825732486030095,21.042046241460838,20.6322831501737,19.380751294097546,19.582601081862446,21.284194059773114,22.179583552994327,23.19729034551032,19.967482396438246,19.738441963085798,19.239293618022067,21.59285163447237,20.535950228874793,21.01158079776531,20.08203076321153,22.84732578883419,21.756353934255824,23.316254320731808,18.81898933836999,21.940012430656058,20.748003621067692,19.730383893455734,20.182461946627072,21.076706013187607,20.7269525620563,19.59081933226322,20.904152603600195,19.659163210227153,24.761947476774708,22.534274856548336,23.233448675874918,19.884197354704103,21.007268735063114,19.408841782080135,20.136824689258354,21.854300382519792,19.881144027962787,20.995690391681727,23.573228460474112,19.074352191519743,21.469722294142937,18.833019773765052,20.27464659506303,19.718574682047347,18.258239797953646,20.755317114550273,20.561150873374917,20.948677886535172,19.720177445507503,22.150900676319612,23.39909808655993,22.31935969126546,20.155632003058283,21.13370665837429,19.473136561032486,20.46100849412487,22.136011068345166,21.159614691783496,19.431106818709004,19.13213914887522,19.706696670705693,19.346767596320518,20.918060329643115,20.18711885177595,23.064648540231374,18.98912255599455,24.074524917146316,20.61646897501603,20.227612418267608,23.77281502683423,18.64679105166152,19.821061094559884,21.41515272305351,20.235862606471247,21.36550800836747,21.97455042987335,22.17732889464112,25.151779365365787,24.150527389957542,21.040691724844343,20.760535400558187,19.34725801943171,20.04287236504247,21.531748865354032,18.726909959036774,19.7936588697483,20.03811220566055,23.56945490190484,23.91740995998136,19.437420256199317,23.65829637497728,27.09570141066156,23.540007985307764,23.389995853863947,20.300248650151303,19.67235973078438,20.003307902740296,20.529649424214696,20.802720957968873,21.281868210231526,20.815053320149143,21.927740352835695,20.82479873735278,20.51237686567707,24.94868149530031,20.081935873735674,22.717589515695337,20.82980401444312,19.3661810523187,19.957403913237158,21.18136460499971,19.8915004159983,20.17210241586846,22.4307529287208,22.889842311610867,20.801380865239704,22.613937582771225,20.216218911187326,23.993435990731076,19.557332226454086,17.31416721288844,20.21067743703825,21.405123280549176,20.1770277231615,18.904187708332326,19.046718202293015,20.073998999202523,19.6302991245126,21.48379659194355,20.81804614111716,19.563852956879067,21.051151840896342,20.770721956713995,19.67623430456214,19.581897497422524,21.384446242838667,20.692911719555987,21.545170940853957,21.994462034082755,21.47969097288377,20.278388484789648,19.382106816183008,19.441821622178967,21.939001270486997,25.832615032960224,21.65340972782181,19.535812626400283,18.947642644803835,19.712127286089668,23.90063882300631,18.304477915148002,22.443056343683086,23.16226531069458,25.14188655199703,20.70748794380435,20.732604891283092,19.934400683437797,20.62113432421718,19.28434545274011,19.017540081468233,21.15041915835723,20.26686372980413,20.03253125195289,20.148876382348814,22.62198208884591,20.021014007950576,17.20778649543665,21.273482891420798,19.30063185166596,20.195062159996276,21.445335194484834,21.0819625214105,19.392355160192587,20.378347780631422,18.799302474087973,23.18300636577445,18.93991113136844,19.223167412657357,23.650902186871754,19.945176609895544,19.495627675541318,25.79058328300537,20.476746945456053,19.21271648764287,19.678332456797207,19.71871179715288,20.105565995835796,21.137414277628565,23.008367409641956,21.03393309328942,19.30271923785845,20.626830273876646,21.092597149125172,21.952512694511015,19.882805718288704,20.77592509493985,21.046183463307532,22.0435593555408,21.04785962557455,20.25569596256737,19.352118206703615,18.845717636328267,19.34199651526983,20.91962424027046,19.595170049801663,19.600097337632842,25.661642339745672,21.791142017552335,23.50747794569296,20.391385205003306,23.803560322980168,23.78648471341281,23.49397732898427,23.081833665589276,23.144648130459657,20.920832580620292,19.193623112656937,18.901120542977083,21.79790479470573,19.407912925215978,18.98289333455679,19.415382799115825,21.253000869012993,21.604198363469845,20.557571210143905,19.274253014992237,20.09246940100189,19.391954018620176,22.850833320712763,22.012401869246364,20.826658939867247,19.118363214083377,21.289967644645948,18.733922283625127,20.9115880183164,16.829816883719186,20.861088989018167,20.73330584327736,18.406398882361927,21.03390016791011,22.274116203708253,19.445206608830574,20.834279688605037,23.70344643795525,23.354012710201598,19.88516012413886,22.730738164708086,19.75792383693999,24.394611647917593,19.734888199833236,24.922685338201692,19.530254509942655,20.280991315768514,21.951122115143434,19.19842345841963,22.926085660752005,22.977548256996624,21.21128990053558,19.167558108393887,20.647437784958075,19.35146728073557,21.097845676874776,19.32409702234251,18.277738745265133,24.145550575002787,19.346013720701006,21.812205122435703,22.86783241905905,19.249124305809787,21.53558768926747,24.102450641699225,20.500920634779252,19.622850798933502,22.45828794986169,19.57219805726336,23.264639171673256,22.333293091972617,18.995877797816178,20.694084927736416,19.630595419370202,20.436904665975636,25.664004852328446,20.985299957568404,21.714666548374087,21.619000188699776,20.319316549429495,16.9587226208279,24.58750691743097,21.431401448181294,20.454850489915017,19.41323537187323,20.769097708402093,19.12410363757553,21.037456801472633,20.865493152029874,20.200550831447014,19.074330956681983,20.89335338152254,18.80103888605608,22.493051859930933,20.264370436179377,21.216120605388507,19.103940869449712,23.841976780210096,20.967615900689093,20.26093058536308,21.15453445100085,24.16646354231811,22.25017305320405,20.99185811169326,22.413732248617176,21.395751936133603,21.027619198526104,19.682163069515127,20.59828494694422,20.60161654204836,22.930920924409694,21.340491521133263,21.13532539299306,23.86936283890287,21.30122097904847,21.87678820576361,19.984953818018486,23.219303551688306,21.660324565067476,20.87994439864477,20.922214169866013,24.081396558763124,22.721264543402345,21.873381120752498,19.089555706982512,20.89641611390129,20.568873923302558,19.242152996635603,19.205602363695707,24.69282091354044,22.779994713980038,19.521220854266264,21.142478555307388,25.236220259743373,20.048804801912784,19.552903579436016,22.115896159241377,21.403353538154413,19.840625811644227,18.35486059310022,20.417051943879315,23.212857021000243,21.53372289651904,20.79356441567801,23.25433145453618,20.592440384261195,20.758465193484422,20.24512208583746,20.925901548056537,24.750691092265395,19.41019874513054,19.696673307271265,22.56844661795243,22.50456479926641,19.4801354006198,20.95523200641152,24.568652749155945,18.75491624208908,19.461087704959443,20.880059154822476,19.68504303470752,23.816825076283962,19.839377869097518,23.420195657862312,22.999884079036303,20.649943786599653,21.684940338873297,24.09117022808059,21.12734576220247,19.49242240707679,18.90953834843785,20.94197381362215,23.259273464150183,23.03775857233843,22.58649267850949,20.120777025175872,20.906486953193497,20.378669388865216,22.737839310980224,19.27074701059353,23.65992231174119,23.339497059809283,22.05115682063906,18.775074918844375,22.10097025846529,19.31545475085148,19.854767962698624,19.840511525069328,21.39606694375647,22.97307940319244,19.86193501563865,19.664038907092028,19.58463718752616,19.810866559819527,19.465937010710118,22.434772773304594,20.445539922646542,18.99187307244663,21.90849954742701,22.314131385426723,20.432733544527707,23.07558442006914,22.830034474576443,21.269107840654502,20.48988421076927,21.083491338486773,20.934278289661282,20.287989620862316,20.04722528013383,21.199347342918216,23.521067577919112,23.2611930742384,21.92570835834572,20.556276718935056,21.41646356230674,22.53034703709958,18.95340785965337,19.986508970321974,19.495183602853512,18.457821713581545,22.886338306822665,19.129172314836154,20.136171638349733,19.376016989288264,20.879430687458026,20.620771994361597,20.30679376611553,20.368798039683195,22.35947129540103,20.069728697762105,22.54894428694912,20.37273542865621,20.71779772905426,19.199159497364995,19.174850117652987,20.13094661361353,20.62659792573699,25.292356917614896,21.758700626292807,21.796149648767223,18.807339197843834,20.506265808043914,22.110290220817404,19.52840260028443,20.325926854481473,19.625476983112655,21.50252421557022,19.304125125188563,19.107865328534132,23.169806519640005,18.840768128990423,19.36444675592656,18.627786784209636,21.17955986366495,20.377078375453138,21.746512886013605,20.910669810022966,23.66715843905666,19.356125235979412,19.33650002206052,17.813568277801206,22.465861054219694,20.57561772108256,19.015107379674305,21.499214768467855,19.66178092422977,17.299495077910098,21.27615053752397,21.040198736231254,21.92098191704396,21.7224558930939,21.092338092530998,20.048671453648097,21.894206057556605,20.415195026786527,20.471143313052202,21.849984955018474,19.59156765743604,20.658934051201378,20.105603080001295,22.787613064091445,18.897059576214055,19.64405870233027,22.74129354031224,21.948058114314257,20.038176858751854,22.177258018525677,22.318758483797644,17.211802761238985,23.061132678317314,19.552166474373674,20.22058498039463,19.1389392082589,19.751662039949778,22.589266875609983,26.340632927208585,19.534708840769273,20.435276534084654,19.903908501005596,19.287439594230733,19.687016476370815,22.7859490833811,20.26081963393427,20.008806098312462,20.432875222497227,21.770367489208184,19.54263516359492,20.167399003287947,20.02036350615788,25.816840819488693,20.66031803587651,19.756693437676223,19.47969541290554,19.46354033785,24.169496968680054,18.739319147762597,21.559823540829672,20.841106100432793,21.572330613609903,23.06348379203432,23.90661007609995,20.544449448312626,18.828527980887348,21.643407148870626,20.295760247970698,21.926818619899166,18.887256320586236,20.26723932452398,20.649629842496918,22.847263558158666,20.639447033358472,21.651692029555726,19.3900555692985,21.40442957359937,20.067715321953298,20.687515795874024,20.646624626390846,19.38789150561276,21.443595032483266,21.960080436904104,20.986097829919892,20.116046923130224,20.38834949087981,21.51694698715695,23.463402138610206,22.107609158967936,21.13017429963753,21.059552411062068,19.918938919026616,19.132327452939787,21.94580529932221,19.159457479949786,19.567551127469855,19.27757204338643,19.477956222501692,20.02620191687006,20.455200776646638,23.044574887856943,24.184179585129318,20.388726808803085,20.116390923202314,21.32891538129463,20.93827950221911,19.878697565839243,20.238709984208988,18.95241706841831,19.63538512176151,20.604884938737765,22.740623426919786,20.3125128271046,24.597055201180662,23.117114991068007,20.78628428680642,19.932433926767427,19.826295549099655,21.26517033895163,22.496434708234126,19.328382384869762,19.427802084161115,20.18180618902739,19.980960001271505,18.996123380968683,22.600876020595507,20.092995805378262,20.046496736817677,21.151917099556776,18.585982112110514,20.695329089308867,22.520164246985647,19.3471325562148,18.999355953219133,23.20757904373024,21.23702785682451,21.08554669425927,20.768225524289374,20.2479251962018,19.3533467533098,22.213653662019038,20.24562900900173,18.67996002744327,21.27136955536682,21.42004604510025,21.047075972047644,21.8035683042629,19.22968489276112,21.077647966937757,23.387816453129698,18.706232935340964,19.58110217755027,23.538867158230946,21.96712491711981,19.68563168309507,20.03480567791853,18.855622835428132,19.611919934246885,20.89874787711282,20.381292625866948,20.623611830893093,21.463850115483186,22.47425899129698,21.46282774650559,20.250449939937386,19.804071134581005,22.940732615911422,24.1535698900103,22.049830938319623,19.98785104241217,22.944212504137,21.578861893090256,25.17177163891695,19.37427511207424,19.96383210404445,24.180920079085045,22.206977518330888,19.158269753875864,19.069924834450198,22.714272038785044,22.147044335773373,20.251879940231216,24.956261862764006,19.58788736648328,21.751091249893005,21.93032100053686,21.621710733177533,19.367732274650958,19.09456742423413,16.917281198342668,20.807982080155547,20.46876211568297,19.988975863285916,20.580262018382033,19.091740116853863,21.44414464440018,21.29242653675922,22.038981262669978,20.180643707721647,23.155520679582406,19.32638298997934,21.506597416475046,20.650611348398325,20.693713245656816,20.540019222244897,21.217521718500812,24.49306731779338,19.642596451766774,22.00972851632279,20.932965503614184,21.834232905374517,21.098649782686852,22.49932865299591,22.579718287600105,21.529871119919466,21.118854440357843,20.944082894927185],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle RandomForestRegressor() vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_pred_log_logRF"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["n_estimatorsRF = np.logspace(0, 3, 10, dtype=int)\n","param_gridRF = {\n","    'randomforestregressor__n_estimators': n_estimatorsRF,\n","    'randomforestregressor__max_features': ['auto', 'sqrt', 'log2'],\n","}\n","\n","GridRF, \\\n","BestParametresRF, \\\n","ScoresRF, \\\n","SiteEnergyUse_pred_logRF, \\\n","figRF = reg_modelGrid(model=RandomForestRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBM_train,\n","                         X_test=BEBM_test,\n","                         y_train=SiteEnergyUse_train_log.ravel(),\n","                         y_test=SiteEnergyUse_test_log,\n","                         y_test_name='SiteEnergyUse_test_log',\n","                         y_pred_name='SiteEnergyUse_pred_log_logRF',\n","                         score=score,\n","                         param_grid=param_gridRF)\n","\n","print(BestParametresRF)\n","ScoresRF\n","figRF.show()\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[2.0347880074677684,1.8335369229533076,1.7209193128228908,1.6345335227404143,1.5861509383343861,1.558696976206027,1.554611418157926,1.5466852147246057,1.5530472251714849,1.5487408497643589]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[2.1641913304108296,2.0125960434902153,1.8676719792745509,1.7771205288914784,1.740175048436381,1.6838180466028119,1.6731207339910266,1.658920639393809,1.673407805968151,1.6691731949296744]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[1.7585053448437225,1.7671704231268106,1.8702824701889809,1.8835842172225419,1.6857597230265466,1.7287354728442708,1.778389848863069,1.8555607014318314,1.9058106425769399,1.9057388782082905]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[2.2714190260819915,2.0807061950825103,1.8535208142657904,1.913906662923029,1.8470777371215243,1.7779177646748294,1.7591599170619414,1.7448747920377414,1.754794620345894,1.755119585137849],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.9712231776967974,1.9582475070880907,1.5986337691043204,1.5257838460740214,1.4533301777386771,1.4664731319756807,1.463642198300412,1.4665497928228715,1.4763801469376985,1.4691079971466938],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.8919296750837482,1.6669570459075953,1.9136168527703148,1.567429999985236,1.5583703882050872,1.55251086270349,1.5755913785179718,1.5544786336744514,1.5796426740347032,1.56844347442044],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[2.0572780479155846,1.862876645418748,1.7127003744063158,1.6137441525424716,1.65236822469963,1.5830781262032882,1.560179410423865,1.5519655879347565,1.5598029492790118,1.55449253891373],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.9820901105607196,1.5988972212695942,1.5261247535677132,1.5518029521773133,1.4196081639070108,1.4135049954728465,1.4144841864854403,1.415557267153208,1.3946157352601158,1.396540653203082],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle RF pour le paramètre max_features=log2 en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"n_estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE ElasticNet pour tout le meilleur paramètre max features\n","for i in BestParametresRF['RandomForestRegressor()'][\n","        BestParametresRF['paramètre'] ==\n","        'randomforestregressor__max_features']:\n","    fig1 = go.Figure([\n","        go.Scatter(\n","            name='RMSE moyenne',\n","            x=n_estimatorsRF,\n","            y=GridRF.ScoresMean.where(\n","                GridRF.randomforestregressor__max_features == i).dropna(),\n","            mode='lines',\n","            marker=dict(color='red', size=2),\n","            showlegend=True),\n","        go.Scatter(\n","            name='SDup RMSE',\n","            x=n_estimatorsRF,\n","            y=GridRF.ScoresMean.where(\n","                GridRF.randomforestregressor__max_features == i).dropna() +\n","            GridRF.ScoresSD.where(\n","                GridRF.randomforestregressor__max_features == i).dropna(),\n","            mode='lines',\n","            marker=dict(color=\"#444\"),\n","            line=dict(width=1),\n","            showlegend=False),\n","        go.Scatter(\n","            name='SDdown RMSE',\n","            x=n_estimatorsRF,\n","            y=GridEN.ScoresMean.where(\n","                GridRF.randomforestregressor__max_features == i).dropna() -\n","            GridRF.ScoresSD.where(\n","                GridRF.randomforestregressor__max_features == i).dropna(),\n","            mode='lines',\n","            marker=dict(color=\"#444\"),\n","            line=dict(width=1),\n","            fillcolor='rgba(68, 68, 68, .3)',\n","            fill='tonexty',\n","            showlegend=False)\n","    ])\n","\n","    fig2 = px.line(\n","        GridRF.where(GridRF.randomforestregressor__max_features == i).dropna(),\n","        x=n_estimatorsRF,\n","        y=[\n","            'ScoresSplit0', 'ScoresSplit1', 'ScoresSplit2', 'ScoresSplit3',\n","            'ScoresSplit4'\n","        ])\n","\n","    fig3 = go.Figure(data=fig1.data + fig2.data)\n","    fig3.update_xaxes(type='log', title='n_estimators')\n","    fig3.update_yaxes(title='RMSE')\n","    fig3.update_layout(\n","        title=\n","        \"RMSE du modèle RF pour le paramètre max_features={} en fonction de l'hyperparamètre alpha\"\n","        .format(i))\n","    fig3.show()\n","    if write_data is True:\n","        fig3.write_image('./Figures/graphRMSERF{}.pdf'.format(i))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"interpreter":{"hash":"117c35e9bc21f93c21d2b781ffd59753bc10bfa2757aefbea5ab108f880d10a5"},"kernelspec":{"display_name":"Python 3.9.9 64-bit ('.env': venv)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
